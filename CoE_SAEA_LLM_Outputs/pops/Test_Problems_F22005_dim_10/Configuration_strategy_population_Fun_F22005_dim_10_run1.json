[
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = np.array(list(score_set.keys()))\n    n_actions = len(action_indices)\n    \n    # Calculate the total scores and counts of selections\n    total_scores = np.array([np.sum(scores) for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Avoid division by zero by using np.where\n    average_scores = np.where(selection_counts > 0, total_scores / selection_counts, 0.0)\n\n    # Calculate Upper Confidence Bound (UCB) values\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n    ucb_values = average_scores + exploration_bonus\n\n    # Dynamic epsilon based on time slot\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n\n    # Selection process, either explore or exploit\n    if np.random.rand() < epsilon:  # Explore\n        action_index = np.random.choice(action_indices)\n    else:  # Exploit based on UCB values\n        action_index = action_indices[np.argmax(ucb_values)]\n\n    return action_index",
          "objective": -449.99999999988165,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = np.array(list(score_set.keys()))\n    n_actions = len(action_indices)\n\n    # Initialize arrays for total scores and selection counts\n    total_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    \n    # Calculate total scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        total_scores[idx] = np.sum(scores) if selection_counts[idx] > 0 else 0.0\n\n    # Calculate average scores\n    average_scores = np.where(selection_counts > 0, total_scores / selection_counts, 0.0)\n\n    # Upper Confidence Bound calculations\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n    ucb_values = average_scores + exploration_bonus\n\n    # Adaptive epsilon-greedy exploration strategy\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n\n    if np.random.rand() < epsilon:  # Explore\n        action_index = np.random.choice(action_indices)\n    else:  # Exploit\n        action_index = action_indices[np.argmax(ucb_values)]\n    \n    return action_index",
          "objective": -449.9999999998668,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Initialize arrays for total scores, selection counts, and average scores\n    total_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    \n    # Calculate total scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        total_scores[idx] = np.sum(scores)\n    \n    average_scores = np.where(selection_counts > 0, total_scores / selection_counts, 0.0)\n\n    # Dynamic exploration term based on UCB\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n    \n    # Calculate UCB values\n    ucb_values = average_scores + exploration_bonus\n\n    # Adaptive epsilon for epsilon-greedy exploration\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n\n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:  # Explore\n        action_index = np.random.choice(action_indices)\n    else:  # Exploit\n        action_index = action_indices[np.argmax(ucb_values)]\n\n    return action_index",
          "objective": -449.9999999998444,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = np.array(list(score_set.keys()))\n    n_actions = len(action_indices)\n\n    # Calculate total scores and selection counts\n    total_scores = np.array([np.sum(scores) for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n\n    # Handle division for average scores safely\n    average_scores = np.where(selection_counts > 0, total_scores / selection_counts, 0.0)\n\n    # Implementing a more stable version of UCB\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n    ucb_values = average_scores + exploration_bonus\n\n    # Dynamic epsilon adjustment with a focus on remaining time slots\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n\n    # Epsilon-greedy selection process\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = action_indices[np.argmax(ucb_values)]\n\n    return action_index",
          "objective": -449.99999999980747,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = np.array(list(score_set.keys()))\n    n_actions = len(action_indices)\n\n    total_scores = np.array([np.sum(scores) for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n\n    average_scores = np.where(selection_counts > 0, total_scores / selection_counts, 0.0)\n\n    # Calculate UCB values\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n    ucb_values = average_scores + exploration_bonus\n\n    # Dynamic epsilon based on time slot\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n\n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = action_indices[np.argmax(ucb_values)]\n    \n    return action_index",
          "objective": -449.9999999997988,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = np.array(list(score_set.keys()))\n    n_actions = len(action_indices)\n    \n    total_scores = np.array([np.sum(scores) for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n\n    average_scores = np.where(selection_counts > 0, total_scores / selection_counts, 0.0)\n\n    # Apply Bayesian-inspired adjustment to the exploration term\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n    ucb_values = average_scores + exploration_bonus\n\n    # Dynamic epsilon adjustment based on time remaining\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n\n    # Selection process: epsilon-greedy choice to balance exploration and exploitation\n    if np.random.rand() < epsilon:  \n        action_index = np.random.choice(action_indices)\n    else:  \n        action_index = action_indices[np.argmax(ucb_values)]\n    \n    return action_index",
          "objective": -449.9999999997928,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Initialize total scores and selection counts\n    total_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    \n    # Calculate total scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        total_scores[idx] = np.sum(scores)\n    \n    # Calculate average scores, avoiding division by zero\n    average_scores = np.where(selection_counts > 0, total_scores / selection_counts, 0.0)\n\n    # Calculate UCB values\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n    ucb_values = average_scores + exploration_bonus\n\n    # Adaptive epsilon for epsilon-greedy exploration\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n\n    # Action selection\n    if np.random.rand() < epsilon:  # Explore\n        action_index = np.random.choice(action_indices)\n    else:  # Exploit\n        action_index = action_indices[np.argmax(ucb_values)]\n\n    return action_index",
          "objective": -449.99999999978206,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = np.array(list(score_set.keys()))\n    n_actions = len(action_indices)\n\n    # Initialize arrays for total scores and selection counts\n    total_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate total scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        total_scores[idx] = np.sum(scores)\n\n    average_scores = np.where(selection_counts > 0, total_scores / selection_counts, 0.0)\n\n    # Beta distribution parameters for Thompson Sampling\n    alpha = total_scores + 1  # successes\n    beta = selection_counts - total_scores + 1  # failures\n\n    # Thompson Sampling: sample from the beta distribution for each action\n    sampled_scores = np.random.beta(alpha, beta)\n\n    # Combine exploration and exploitation\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n    if np.random.rand() < epsilon:  # Explore\n        action_index = np.random.choice(action_indices)\n    else:  # Exploit\n        action_index = action_indices[np.argmax(sampled_scores)]\n\n    return action_index",
          "objective": -449.99999999978104,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    \n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Compute average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Epsilon value that gradually decreases over time\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n    \n    if np.random.rand() < epsilon or total_selection_count == 0:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        # Calculate UCB scores\n        confidence_bounds = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n        selection_scores = average_scores + confidence_bounds\n        \n        # Incorporate a safety net for actions not yet selected\n        safe_selection_scores = np.where(selection_counts == 0, float('inf'), selection_scores)\n        action_index = action_indices[np.argmax(safe_selection_scores)]  # Select action with highest UCB score\n    \n    return action_index",
          "objective": -449.9999999995778,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Dynamic epsilon for exploration\n    epsilon = max(0.01, 0.5 * (1 - (current_time_slot / total_time_slots)))\n\n    # Calculate UCB values\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n    ucb_values = average_scores + exploration_bonus * 0.5\n\n    # Adaptive exploration versus exploitation\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -449.99999999952587,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = np.array(list(score_set.keys()))\n    n_actions = len(action_indices)\n    \n    # Initialize arrays for total scores and selection counts\n    total_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    \n    # Calculate total scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        total_scores[idx] = np.sum(scores)\n    \n    average_scores = np.where(selection_counts > 0, total_scores / selection_counts, 0.0)\n    \n    # Upper Confidence Bound calculations\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n    ucb_values = average_scores + exploration_bonus\n\n    # Adaptive epsilon-greedy exploration strategy\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n    \n    if np.random.rand() < epsilon:  # Explore\n        action_index = np.random.choice(action_indices)\n    else:  # Exploit\n        action_index = action_indices[np.argmax(ucb_values)]\n    \n    return action_index",
          "objective": -449.999999999389,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    \n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Compute average scores and selection counts, handling cases with no scores\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Epsilon value decreases over time to favor exploitation\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))  # Minimum epsilon is 0.1\n\n    if np.random.rand() < epsilon or total_selection_count == 0:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        # Exploit using a modified UCB with a safety for division\n        confidence_bounds = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n        selection_scores = average_scores + confidence_bounds\n        action_index = action_indices[np.argmax(selection_scores)]  # Select action with highest UCB score\n\n    return action_index",
          "objective": -449.99999999936193,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    \n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Compute average scores and selection counts, handling cases with no scores\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Decaying exploration parameter\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots)) \n\n    if np.random.rand() < epsilon or total_selection_count == 0:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        # Enhanced UCB approach\n        confidence_bounds = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n        selection_scores = average_scores + confidence_bounds\n        action_index = action_indices[np.argmax(selection_scores)]  # Select action with highest score\n\n    return action_index",
          "objective": -449.999999999346,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Dynamic exploration factor based on time and total selection count\n    epsilon = 0.1 * (1 - (current_time_slot / total_time_slots)) ** 2 + 0.01\n\n    # Calculate uncertainty using UCB\n    ucb_values = average_scores + np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n    \n    # Epsilon-Greedy decision making\n    if np.random.rand() < epsilon:\n        # Select a random action to explore\n        action_index = np.random.choice(action_indices)\n    else:\n        # Select the action with the highest UCB value to exploit\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -449.99999999927144,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Dynamic epsilon for exploration\n    epsilon = max(0.01, 0.5 * (1 - (current_time_slot / total_time_slots)))\n    \n    # Calculate UCB values\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n    ucb_values = average_scores + exploration_bonus * 0.5\n\n    # Epsilon-Greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -449.99999999921465,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Dynamic exploration factor based on time and selection count\n    exploration_factor = np.log(total_selection_count + 1) / (selection_counts + 1e-5)\n    exploration_bonus = np.sqrt(exploration_factor)\n\n    # Calculate the UCB values\n    ucb_values = average_scores + exploration_bonus\n\n    # Epsilon-Greedy strategy with adaptive epsilon\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n    \n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -449.999999999209,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Adaptive epsilon based on current time slot\n    epsilon = max(0.1, 0.1 * (1 - (current_time_slot / total_time_slots)))\n\n    # UCB Bonus calculation\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n    ucb_values = average_scores + exploration_bonus\n\n    # Decision making using epsilon-greedy approach\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -449.99999999916645,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Initialize arrays for average scores and counts\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n    \n    # Safeguard against division by zero for exploration bonus\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n\n    # Calculate UCB values\n    ucb_values = average_scores + exploration_bonus\n\n    # Epsilon decreasing strategy based on time slot, more aggressive early on\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n\n    # Select action based on epsilon-greedy strategy or UCB\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = action_indices[np.argmax(ucb_values)]\n\n    return action_index",
          "objective": -449.9999999991293,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Dynamic epsilon for exploration\n    epsilon = max(0.01, 0.5 * (1 - (current_time_slot / total_time_slots)))\n\n    # Calculate UCB values\n    with np.errstate(divide='ignore', invalid='ignore'):\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n    ucb_values = average_scores + exploration_bonus * 0.5\n\n    # Adaptive exploration versus exploitation\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -449.9999999990747,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    \n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Compute average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Epsilon decay for exploration vs exploitation\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n    \n    if np.random.rand() < epsilon or total_selection_count == 0:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        # Calculate UCB with a small constant to avoid division by zero\n        confidence_bounds = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n        selection_scores = average_scores + confidence_bounds\n        \n        # Predictive adjustments based on previous selections\n        # Normalizing previous scores to adapt to trends\n        trend_adjustment = np.clip(np.std(scores) / (np.mean(scores) + 1e-5), 0, 1)\n        selection_scores *= (1 - trend_adjustment)\n\n        action_index = action_indices[np.argmax(selection_scores)]  # Select action with highest score\n\n    return action_index",
          "objective": -449.9999999990466,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Dynamic epsilon\n    epsilon = max(0.01, 0.1 * (1 - (current_time_slot / total_time_slots)))\n\n    # Calculate UCB for exploitation\n    ucb_values = average_scores + np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n\n    # Combine UCB with exploration using epsilon-greedy\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = action_indices[np.argmax(ucb_values)]\n\n    return action_index",
          "objective": -449.9999999990117,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Initialize arrays for sum of scores, selection counts, and average scores\n    total_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    \n    # Calculate total scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        total_scores[idx] = np.sum(scores)\n    \n    average_scores = np.where(selection_counts > 0, total_scores / selection_counts, 0.0)\n\n    # Dynamic exploration term based on UCB\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n    \n    # Calculate UCB values\n    ucb_values = average_scores + exploration_bonus\n\n    # Adaptive epsilon\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = action_indices[np.argmax(ucb_values)]\n\n    return action_index",
          "objective": -449.99999999900683,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.array([len(score_set[action]) for action in action_indices])\n\n    # Calculate average scores safely handling division by zero\n    for idx, action in enumerate(action_indices):\n        if selection_counts[idx] > 0:\n            average_scores[idx] = np.mean(score_set[action])\n\n    # Adaptive epsilon based on current time slot with a more dynamic decay\n    epsilon = max(0.05, 0.1 * (1 - (current_time_slot / (total_time_slots + 1))))\n\n    # UCB Bonus calculation, avoiding division by 0 with a small constant\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n    ucb_values = average_scores + exploration_bonus\n\n    # Thompson Sampling - assuming scores follow a Bernoulli distribution\n    success_counts = np.array([len([score for score in score_set[action] if score > 0]) for action in action_indices])\n    failure_counts = selection_counts - success_counts\n    \n    # Draw samples from Beta distribution for balancing exploration and exploitation\n    theta_samples = np.random.beta(success_counts + 1, failure_counts + 1)\n\n    # Combine UCB and Thompson Sampling scores to select the best action\n    combined_scores = ucb_values + theta_samples\n\n    # Decision making based on epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = action_indices[np.argmax(combined_scores)]\n\n    return action_index",
          "objective": -449.9999999988349,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Initialize arrays for average scores and counts\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Dynamic exploration factor: higher for actions less frequently selected\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n\n    # Calculate UCB values\n    ucb_values = average_scores + exploration_bonus\n\n    # Adaptive epsilon based on time slot\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n\n    # Select action based on epsilon-greedy strategy or UCB\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = action_indices[np.argmax(ucb_values)]\n\n    return action_index",
          "objective": -449.99999999878656,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Calculate exploration bonus\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n    \n    # UCB values\n    ucb_values = average_scores + exploration_bonus\n\n    # Adaptive epsilon based on current time slot\n    epsilon = max(0.05, 0.2 * (1 - (current_time_slot / total_time_slots)**2))\n\n    # Epsilon-greedy decision making\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -449.9999999983946,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Adaptive epsilon based on current time slot\n    epsilon = max(0.1, 0.5 * (1 - (current_time_slot / total_time_slots)))\n\n    # UCB Bonus calculation\n    exploration_bonus = np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n    ucb_values = average_scores + exploration_bonus\n\n    # Decision making using epsilon-greedy approach\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = action_indices[np.argmax(ucb_values)]\n\n    return action_index",
          "objective": -449.99999999836245,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    \n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Compute average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Epsilon value decreases over time to favor exploitation\n    epsilon = 1 / (1 + current_time_slot)  # Epsilon decay based on time slot\n\n    if np.random.rand() < epsilon or total_selection_count == 0:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        # Exploit using a modified UCB\n        confidence_bounds = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n        selection_scores = average_scores + confidence_bounds\n        action_index = action_indices[np.argmax(selection_scores)]  # Exploit\n\n    return action_index",
          "objective": -449.9999999983442,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    \n    # Initialize arrays for scores and selection counts\n    total_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Compute total scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        total_scores[idx] = sum(scores)\n\n    # Compute average scores, avoiding division by zero\n    average_scores = total_scores / (selection_counts + 1e-10)\n    \n    # Epsilon-greedy exploration parameter\n    epsilon = 0.1\n    if np.random.rand() < epsilon:\n        return np.random.choice(action_indices)\n\n    # Compute UCB values\n    exploration_terms = np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1e-10))\n    ucb_values = average_scores + exploration_terms\n    \n    # Select action with the highest UCB value\n    action_index = np.argmax(ucb_values)\n    \n    return action_index",
          "objective": -449.99999999825445,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Handle case when no selections have been made\n    if total_selection_count == 0:\n        return np.random.choice(action_indices)\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Dynamic epsilon based on time to encourage exploration\n    exploration_factor = 0.1 * (1 - (current_time_slot / total_time_slots))\n    epsilon = max(exploration_factor, 0.01)\n\n    # UCB with a dynamic bonus for exploration\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n    ucb_values = average_scores + exploration_bonus\n\n    # Epsilon-Greedy strategy for exploration/exploitation\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = np.argmax(ucb_values)\n    \n    return action_index",
          "objective": -449.9999999982523,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    if total_selection_count == 0:\n        return np.random.choice(action_indices)\n\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Epsilon decay based on a sigmoid function for smoother exploration\n    epsilon = 0.1 * (1 - np.tanh((current_time_slot / total_time_slots) * 3 - 3))\n\n    # Compute UCB with a smoothing factor for exploration\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n    ucb_values = average_scores + exploration_bonus\n\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -449.99999999812155,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Dynamic epsilon based on time slot\n    epsilon = max(0.01, 0.1 * (1 - current_time_slot / total_time_slots))\n\n    # Calculate UCB values\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n    ucb_values = average_scores + exploration_bonus\n\n    # Action selection using epsilon-greedy with UCB\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -449.9999999977835,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Compute exploration factor\n    exploration_factor = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n    \n    # Adaptive epsilon based on current time slot\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n\n    # UCB estimates\n    ucb_values = average_scores + exploration_factor\n\n    # Decision making using epsilon-greedy with UCB\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = action_indices[np.argmax(ucb_values)]\n\n    return action_index",
          "objective": -449.9999999972132,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n\n    # Initialize arrays for scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Dynamic epsilon based on the current time slot\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01)\n\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(num_actions)  # Explore\n    else:\n        # Optimistic Initial Value UCB\n        exploration_term = np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1e-5))  # Avoid division by zero\n        ucb_values = average_scores + exploration_term\n        action_index = np.argmax(ucb_values)  # Exploit\n\n    return action_index",
          "objective": -449.99999999714055,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    \n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Compute average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Epsilon value decreases over time to favor exploitation\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))  # Minimum epsilon is 0.1\n\n    if np.random.rand() < epsilon or total_selection_count == 0:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        # Exploit using Upper Confidence Bound (UCB)\n        confidence_bounds = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n        selection_scores = average_scores + confidence_bounds\n        action_index = action_indices[np.argmax(selection_scores)]  # Select action with highest UCB score\n\n    return action_index",
          "objective": -449.99999999702044,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts safely\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Dynamic exploration factor based on time and total selection count\n    epsilon = 0.1 * (1 - (current_time_slot / total_time_slots)) ** 2 + 0.01\n\n    # Calculate UCB\n    uncertainty = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n    ucb_values = average_scores + uncertainty\n\n    # Epsilon-Greedy decision making\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -449.9999999968893,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Softmax temperature\n    temperature = 1 / (1 + np.log(1 + total_selection_count))\n    \n    # Calculate adjusted scores for softmax\n    adjusted_scores = average_scores / np.max(average_scores) if np.max(average_scores) > 0 else np.zeros_like(average_scores)\n\n    # Softmax probabilities\n    exp_scores = np.exp(adjusted_scores / temperature)\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Epsilon-greedy strategy with a decreasing epsilon over time\n    epsilon = max(0.05, 0.1 * (1 - (current_time_slot / total_time_slots)))\n    \n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = np.random.choice(action_indices, p=probabilities)\n\n    return action_index",
          "objective": -449.9999999964604,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n\n    # Initialize arrays for scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Dynamic epsilon based on time slot\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01)\n\n    # Epsilon-greedy exploration/exploitation\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(num_actions)  # Explore\n    else:\n        # Upper Confidence Bound (UCB) for exploitation\n        exploration_term = np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n        ucb_values = average_scores + exploration_term\n        action_index = np.argmax(ucb_values)  # Exploit\n\n    return action_index",
          "objective": -449.99999999608,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Compute average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Dynamic epsilon based on elapsed time\n    exploration_factor = 0.1 * (1 - current_time_slot / total_time_slots)\n    epsilon = min(0.1, exploration_factor * (0.5 + 0.5 * np.tanh(total_selection_count / (total_time_slots / 2))))\n\n    # Thompson Sampling for exploration\n    if np.random.rand() < epsilon or total_selection_count == 0:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        # Calculate uncertainty and select action based on UCB with average scores\n        confidence_bounds = np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n        selection_scores = average_scores + confidence_bounds\n        action_index = action_indices[np.argmax(selection_scores)]  # Exploit\n\n    return action_index",
          "objective": -449.9999999948424,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Handle case of no previous selections\n    if total_selection_count == 0:\n        return np.random.choice(action_indices)\n\n    # Initialization\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if len(scores) > 0 else 0.0\n\n    # Dynamic exploration factor based on time\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n\n    # Calculate UCB values\n    ucb_values = average_scores + np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n\n    # Epsilon-Greedy decision making\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -449.9999999944328,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Avoid division by zero\n    if total_selection_count == 0:\n        return np.random.choice(action_indices)\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Dynamic exploration factor based on time\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n\n    # Calculate uncertainty using UCB\n    ucb_values = average_scores + np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n    \n    # Epsilon-Greedy decision making\n    if np.random.rand() < epsilon:\n        # Select a random action to explore\n        action_index = np.random.choice(action_indices)\n    else:\n        # Select the action with the highest UCB value to exploit\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -449.9999999934217,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize arrays for average scores and counts\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    \n    # Calculate average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Adjusted counts for actions not yet selected to avoid division by zero\n    adjusted_counts = selection_counts + 1e-5\n    \n    # Compute Beta distribution parameters for Thompson Sampling\n    alpha = average_scores * adjusted_counts + 1  # including pseudo-counts\n    beta = (1 - average_scores) * adjusted_counts + 1  # including pseudo-counts\n\n    # Sample from Beta distributions\n    sampled_values = np.random.beta(alpha, beta)\n\n    # Adaptive epsilon based on time slot with a minimum threshold\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n\n    # Select action based on epsilon-greedy approach or sampled Thompson values\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = action_indices[np.argmax(sampled_values)]\n    \n    return action_index",
          "objective": -449.9999999926085,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n\n    # Initialize arrays for scores, selection counts, and probabilities\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Epsilon-greedy exploration parameter\n    epsilon = 0.1\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(num_actions)  # Explore\n    else:\n        # Adjust exploration parameter using UCB\n        exploration_term = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))  # Avoid division by zero\n        ucb_values = average_scores + exploration_term\n        action_index = np.argmax(ucb_values)  # Exploit\n\n    return action_index",
          "objective": -449.9999999925696,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Handle the case when no selections have been made\n    if total_selection_count == 0:\n        return np.random.choice(action_indices)\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Compute dynamic epsilon for exploration based on time\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01)\n\n    # Softmax probabilities for exploitation-exploration balance\n    preferences = np.exp(average_scores / np.max(average_scores))  # Avoid division by zero\n    softmax_distribution = preferences / np.sum(preferences)\n\n    # Combine exploration with softmax probabilities\n    random_exploration = np.random.rand(n_actions) < epsilon\n    adjusted_probabilities = softmax_distribution + random_exploration\n\n    # Normalize the adjusted probabilities\n    adjusted_probabilities /= np.sum(adjusted_probabilities)\n\n    # Select action based on adjusted probabilities\n    action_index = np.random.choice(action_indices, p=adjusted_probabilities)\n\n    return action_index",
          "objective": -449.9999999924684,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Thompson Sampling-like approach for better exploration-exploitation balance\n    beta_params = np.array([np.random.beta(np.sum(scores) + 1, len(scores) - np.sum(scores) + 1) if scores else 0.5\n                            for scores in score_set.values()])\n    \n    # Dynamic exploration factor based on time (linearly decreasing)\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n    \n    # Epsilon-Greedy decision making\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = np.argmax(beta_params)\n\n    return action_index",
          "objective": -449.99999999170234,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Avoiding division by zero\n    if total_selection_count == 0:\n        return np.random.choice(action_indices)\n\n    # Calculate average scores and selection counts\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n        selection_counts[idx] = len(scores)\n\n    # Upper Confidence Bound\n    confidence_bounds = np.sqrt(np.log(total_selection_count) / (selection_counts + 1e-5))\n    ucb_values = average_scores + confidence_bounds\n\n    # Adaptive exploration rate\n    epsilon = 0.1 * (1 - (current_time_slot / total_time_slots))\n    epsilon = max(epsilon, 0.01)  # Ensure minimal exploration\n\n    # Epsilon-Greedy decision making\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -449.999999991397,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    \n    # Initialize arrays for scores and selection counts\n    total_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Compute total scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        total_scores[idx] = sum(scores)\n\n    # Compute average scores, handling division by zero\n    average_scores = np.divide(total_scores, (selection_counts + 1e-10))\n\n    # Softmax temperature based on time slot to encourage exploration\n    temp = max(0.1, 1 - (current_time_slot / total_time_slots))\n\n    # Compute Softmax probabilities\n    exp_scores = np.exp(average_scores / temp)\n    probabilities = exp_scores / np.sum(exp_scores)\n    \n    # Select action based on probabilities\n    action_index = np.random.choice(action_indices, p=probabilities)\n\n    return action_index",
          "objective": -449.99999999062527,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Handle the case when no selections have been made\n    if total_selection_count == 0:\n        return np.random.choice(action_indices)\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Dynamic exploration factor based on time (simulating a decaying epsilon)\n    epsilon = 0.1 * (1 - (current_time_slot / total_time_slots))\n    epsilon = max(epsilon, 0.01)\n\n    # Calculate UCB values with improved exploration factor\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n    ucb_values = average_scores + exploration_bonus\n\n    # Probability distribution for selecting action using softmax\n    softmax_probabilities = np.exp(ucb_values) / np.sum(np.exp(ucb_values))\n\n    # Epsilon-Greedy strategy with a softmax approach\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = np.random.choice(action_indices, p=softmax_probabilities)\n\n    return action_index",
          "objective": -449.9999999898483,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Handle case when no selections have been made\n    if total_selection_count == 0:\n        return np.random.choice(action_indices)\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Dynamic epsilon based on time to encourage exploration\n    exploration_factor = 0.1 * (1 - (current_time_slot / total_time_slots))\n    epsilon = max(exploration_factor, 0.01)\n\n    # Calculate exploration term for UCB values\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n    ucb_values = average_scores + exploration_bonus\n\n    # Epsilon-Greedy strategy with softmax to improve selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        # Softmax approach for exploiting UCB values\n        exp_values = np.exp(ucb_values - np.max(ucb_values))  # for numerical stability\n        probabilities = exp_values / np.sum(exp_values)\n        action_index = np.random.choice(action_indices, p=probabilities)\n\n    return action_index",
          "objective": -449.9999999896141,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Adaptive exploration factor\n    epsilon = max(0.01, 0.1 * (1 - (current_time_slot / total_time_slots)))\n\n    # UCB calculation for exploration\n    ucb_values = average_scores + np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n\n    # Decision making with a weighted choice between exploration and exploitation\n    probabilities = np.ones(n_actions) * epsilon / n_actions\n    probabilities += (1 - epsilon) * (ucb_values / np.sum(ucb_values))\n\n    action_index = np.random.choice(action_indices, p=probabilities)\n\n    return action_index",
          "objective": -449.999999988972,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n\n    # Initialize arrays for scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Compute average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Dynamic epsilon based on elapsed time\n    exploration_factor = 0.1 * (1 - current_time_slot / total_time_slots)\n    epsilon = exploration_factor * (0.5 + 0.5 * np.tanh(total_selection_count / (total_time_slots / 2)))\n\n    # Epsilon-Greedy exploration\n    if np.random.rand() < epsilon or total_selection_count == 0:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        # UCB for exploitation\n        confidence_bounds = np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n        selection_scores = average_scores + confidence_bounds\n        action_index = action_indices[np.argmax(selection_scores)]  # Exploit\n\n    return action_index",
          "objective": -449.99999998723564,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n\n    # Initialize arrays for scores and selection counts\n    total_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Compute total scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        total_scores[idx] = sum(scores)\n\n    # Compute average scores, handling division by zero\n    average_scores = np.divide(total_scores, (selection_counts + 1e-10))\n\n    # Epsilon-greedy exploration parameter based on time\n    epsilon = max(0.1, 0.9 * (1 - (current_time_slot / total_time_slots)))\n\n    # Select action based on epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        # Explore: select a random action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploit: select the action with the highest average score\n        action_index = action_indices[np.argmax(average_scores)]\n\n    return action_index",
          "objective": -449.99999998712775,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Compute UCB values\n    ucb_values = np.zeros(n_actions)\n    for idx in range(n_actions):\n        if selection_counts[idx] > 0:\n            ucb_values[idx] = average_scores[idx] + np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts[idx])\n        else:\n            ucb_values[idx] = float('inf')  # Prioritize unvisited actions\n\n    # Softmax-based selection for balanced exploration and exploitation\n    temperature = 0.5  # Exploration factor\n    softmax_probs = np.exp(ucb_values / temperature) / np.sum(np.exp(ucb_values / temperature))\n    \n    # Choose action based on Softmax probabilities\n    action_index = np.random.choice(action_indices, p=softmax_probs)\n\n    return action_index",
          "objective": -449.99999998344396,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    \n    # Initialize arrays for scores, selection counts, and averages\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    # Calculate average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n    \n    # Adjust exploration parameter using UCB\n    exploration_term = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))  # Avoid division by zero\n    ucb_values = average_scores + exploration_term\n\n    # Select action based on UCB values\n    action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -449.99999998264377,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Dynamic epsilon based on time slot\n    epsilon = max(0.01, 0.1 * (1 - current_time_slot / total_time_slots))\n\n    # Use softmax for a probabilistic action selection\n    scaled_scores = average_scores + np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n    exp_scores = np.exp(scaled_scores - np.max(scaled_scores))  # Stabilize for numerical stability\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Action selection using epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = np.random.choice(action_indices, p=probabilities)\n\n    return action_index",
          "objective": -449.99999998215606,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize variables\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Compute average scores and counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n        selection_counts[idx] = len(scores)\n\n    # Softmax temperature parameter\n    temperature = max(0.1, 1.0 - (current_time_slot / total_time_slots))\n\n    # Calculate softmax probabilities\n    exp_scores = np.exp(average_scores / temperature)\n    softmax_probs = exp_scores / np.sum(exp_scores)\n\n    # Epsilon-Greedy exploration\n    epsilon = 0.1 * (1 - (current_time_slot / total_time_slots))\n    epsilon = max(epsilon, 0.01)\n\n    # Select action using epsilon-greedy method\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = np.random.choice(action_indices, p=softmax_probs)\n\n    return action_index",
          "objective": -449.9999999808815,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    \n    # Initialize arrays for scores and selection counts\n    total_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Compute total scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        total_scores[idx] = sum(scores)\n\n    # Compute average scores, handling division by zero\n    average_scores = np.divide(total_scores, (selection_counts + 1e-10))\n\n    # Epsilon-greedy parameter\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n    \n    # Decide to explore or exploit\n    if np.random.rand() < epsilon:\n        # Exploration: Select a random action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploitation: Select action based on average scores\n        best_action_indices = np.flatnonzero(average_scores == np.max(average_scores))\n        action_index = np.random.choice(best_action_indices)\n\n    return action_index",
          "objective": -449.99999997974135,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Handle the case when no selections have been made\n    if total_selection_count == 0:\n        return np.random.choice(action_indices)\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Dynamic exploration factor based on time\n    epsilon = 0.1 * (1 - (current_time_slot / total_time_slots))\n\n    # Adjust epsilon to ensure it doesn\u2019t go below a minimum threshold\n    epsilon = max(epsilon, 0.01)\n\n    # Calculate UCB values using a factor to ensure exploration\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n    ucb_values = average_scores + exploration_bonus\n\n    # Epsilon-Greedy strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -449.99999997840433,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    \n    # Initialize arrays for scores and selection counts\n    total_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Compute total scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        total_scores[idx] = sum(scores)\n\n    # Compute average scores, handling division by zero\n    average_scores = np.divide(total_scores, (selection_counts + 1e-10))\n\n    # Epsilon-greedy exploration parameter\n    epsilon = 0.1 * (1 - (current_time_slot / total_time_slots))\n    \n    if np.random.rand() < epsilon:\n        # Explore\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploit: selecting based on scores\n        best_action_index = np.argmax(average_scores)\n        action_index = action_indices[best_action_index]\n\n    return action_index",
          "objective": -449.9999999783095,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n\n    # Initialize arrays for scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Compute average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Dynamic exploration factor decreasing over time\n    epsilon = 1.0 / (1 + total_selection_count / (total_time_slots + 1))\n\n    # Epsilon-Greedy exploration\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        # UCB for exploitation\n        exploration_bonus = np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n        selection_scores = average_scores + exploration_bonus\n        action_index = action_indices[np.argmax(selection_scores)]  # Exploit\n\n    return action_index",
          "objective": -449.9999999778767,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    \n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    if total_selection_count == 0:\n        return np.random.choice(action_indices)  # Explore if no selections have been made\n\n    # Epsilon decreases over time to favor exploitation\n    epsilon = max(0.1, (1 - (current_time_slot / total_time_slots)))\n    \n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        # UCB based on historical performance\n        confidence_bounds = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n        selection_scores = average_scores + confidence_bounds\n        action_index = action_indices[np.argmax(selection_scores)]  # Exploit\n\n    return action_index",
          "objective": -449.9999999777058,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n\n    # Initialize scores and selection counts\n    total_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Gather scores and counts for each action\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        total_scores[idx] = sum(scores)\n\n    # Handle potential division by zero by adding a small constant\n    average_scores = total_scores / (selection_counts + 1e-10)\n    \n    # Epsilon for exploration\n    epsilon = 0.1 * (1 - (current_time_slot / total_time_slots))\n\n    # Decision-making with epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        # Compute selection probabilities based on average scores\n        probabilities = average_scores / np.sum(average_scores)  # Normalize scores\n        action_index = np.random.choice(action_indices, p=probabilities)\n\n    return action_index",
          "objective": -449.9999999773768,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Compute average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Upper Confidence Bound (UCB) strategy\n    exploration_bonus = np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1e-5))  # Small constant to avoid division by zero\n    ucb_scores = average_scores + exploration_bonus\n\n    # Normalize cumulative score for better exploration-exploitation balance\n    max_score = np.max(ucb_scores) if np.any(ucb_scores) else 0\n    probabilities = np.exp(ucb_scores - max_score)  # Use softmax over UCB scores\n    probabilities /= np.sum(probabilities)\n\n    action_index = np.random.choice(action_indices, p=probabilities)\n\n    return action_index",
          "objective": -449.99999997735927,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Handle the case when no selections have been made\n    if total_selection_count == 0:\n        return np.random.choice(action_indices)\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and ensure selection counts are non-zero\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Epsilon decay strategy\n    epsilon = 0.1 * (1 - (current_time_slot / total_time_slots))\n    epsilon = max(epsilon, 0.01)\n\n    # Calculate UCB values\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n    ucb_values = average_scores + exploration_bonus\n\n    # Softmax Temperature for balancing exploration and exploitation\n    temperature = max(0.1, 1 - (current_time_slot / total_time_slots))\n    softmax_probs = np.exp(ucb_values / temperature) / np.sum(np.exp(ucb_values / temperature))\n\n    # Epsilon-Greedy strategy with Softmax exploration\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = np.random.choice(action_indices, p=softmax_probs)\n\n    return action_index",
          "objective": -449.9999999757219,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n\n    # Initialize arrays for scores and counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Compute average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Thompson Sampling for action selection\n    beta_params = [(scores.count(1) + 1, scores.count(0) + 1) for scores in score_set.values()]\n    \n    samples = np.array([np.random.beta(a, b) for a, b in beta_params])\n    \n    # Select action based on highest sample from beta distribution\n    action_index = action_indices[np.argmax(samples)]\n\n    return action_index",
          "objective": -449.99999997557296,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Dynamic parameters\n    exploration_rate = 0.1  # Exploration probability\n    temperature = 1.0 + (4.0 * (1 - float(current_time_slot) / total_time_slots))  # Softmax temperature\n\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n        selection_counts[idx] = len(scores)\n\n    # Handle case of no selections\n    if total_selection_count == 0:\n        return np.random.choice(action_indices)\n\n    # Lower confidence bound\n    confidence_bounds = np.sqrt(2 * np.log(total_selection_count) / (selection_counts + 1e-5))\n    ucb_values = average_scores + confidence_bounds\n\n    # Epsilon-Greedy selection\n    if np.random.rand() < exploration_rate:\n        # Softmax selection with UCB values\n        exp_ucb_values = np.exp(ucb_values / temperature)\n        probabilities = exp_ucb_values / np.sum(exp_ucb_values)\n        action_index = np.random.choice(action_indices, p=probabilities)\n    else:\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -449.99999997502164,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    \n    # Initialize arrays for scores and selection counts\n    total_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Compute total scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        total_scores[idx] = sum(scores)\n\n    # Compute means and UCB values\n    average_scores = total_scores / (selection_counts + 1e-10)  # Avoid division by zero\n    exploration_terms = np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1e-10))\n    ucb_values = average_scores + exploration_terms\n    \n    # Select action with the highest UCB value\n    action_index = np.argmax(ucb_values)\n    \n    return action_index",
          "objective": -449.99999997449726,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Dynamic parameters\n    epsilon = 0.1  # Exploration rate\n    temperature = 1.0 + (4.0 * (1 - float(current_time_slot) / total_time_slots))  # Softmax temperature\n\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Calculate average scores and selection counts\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n        selection_counts[idx] = len(scores)\n\n    # Handle case of no selections\n    if total_selection_count == 0:\n        return np.random.choice(action_indices)\n\n    # Upper Confidence Bound\n    confidence_bounds = np.sqrt(np.log(total_selection_count) / (selection_counts + 1e-5))\n    ucb_values = average_scores + confidence_bounds\n\n    # Epsilon-Greedy decision making\n    if np.random.rand() < epsilon:\n        # Softmax selection\n        exp_ucb_values = np.exp(ucb_values / temperature)\n        probabilities = exp_ucb_values / np.sum(exp_ucb_values)\n        action_index = np.random.choice(action_indices, p=probabilities)\n    else:\n        # Greedy selection\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -449.99999997276376,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Compute average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Dynamic exploration factor\n    exploration_factor = 0.1 * (1 - current_time_slot / total_time_slots)\n\n    # Epsilon-Greedy exploration\n    epsilon = exploration_factor\n    if np.random.rand() < epsilon or total_selection_count == 0:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        # UCB with added average score for exploitation\n        confidence_bounds = np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n        selection_scores = average_scores + confidence_bounds\n        action_index = action_indices[np.argmax(selection_scores)]  # Exploit\n\n    return action_index",
          "objective": -449.99999997144437,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Dynamic parameters\n    epsilon = 0.1 * (1 - float(current_time_slot) / total_time_slots)  # Decreasing exploration rate\n    temperature = 1.0 + (4.0 * (1 - float(current_time_slot) / total_time_slots))  # Softmax temperature\n\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Calculate average scores and selection counts\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n        selection_counts[idx] = len(scores)\n\n    # Handle case of no selections\n    if total_selection_count == 0:\n        return np.random.choice(action_indices)\n\n    # Upper Confidence Bound\n    confidence_bounds = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n    ucb_values = average_scores + confidence_bounds\n\n    # Action selection using Epsilon-Greedy strategy\n    if np.random.rand() < epsilon:\n        # Softmax selection for exploration\n        exp_ucb_values = np.exp(ucb_values / temperature)\n        probabilities = exp_ucb_values / np.sum(exp_ucb_values)\n        action_index = np.random.choice(action_indices, p=probabilities)\n    else:\n        # Greedy selection for exploitation\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -449.9999999704521,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Define exploration rate and decay for the softmax temperature\n    epsilon = 0.1  # Exploration rate\n    temperature = 1.0 + (4.0 * (1 - float(current_time_slot) / total_time_slots))  # Dynamic temperature\n    \n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    \n    # Calculate average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n        selection_counts[idx] = len(scores)\n    \n    # Handle case of no previous selections\n    if total_selection_count == 0:\n        return np.random.choice(action_indices)\n    \n    # Upper Confidence Bound computation\n    confidence_bounds = np.sqrt(np.log(total_selection_count) / (selection_counts + 1e-5))\n    ucb_values = average_scores + confidence_bounds\n\n    # Epsilon-Greedy decision making\n    if np.random.rand() < epsilon:\n        exp_ucb_values = np.exp(ucb_values / temperature)\n        probabilities = exp_ucb_values / np.sum(exp_ucb_values)\n        action_index = np.random.choice(action_indices, p=probabilities)\n    else:\n        action_index = np.argmax(ucb_values)\n    \n    return action_index",
          "objective": -449.9999999693859,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n\n    # Initialize arrays for scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Compute average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Dynamic exploration factor based on time and selection counts\n    epsilon = 1.0 / (1 + total_selection_count / (total_time_slots + 1))\n    \n    # Calculate exploration bonus using UCB\n    exploration_bonus = np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n    \n    # Calculate selection scores integrating exploration and exploitation\n    selection_scores = average_scores + exploration_bonus\n    \n    # Epsilon-greedy decision making\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        action_index = action_indices[np.argmax(selection_scores)]  # Exploit\n\n    return action_index",
          "objective": -449.9999999684705,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n\n    # Initialize arrays for scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Compute average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Dynamic exploration parameter\n    exploration_factor = 0.1 * (1 - current_time_slot / total_time_slots)\n    epsilon = exploration_factor * (0.5 + 0.5 * np.tanh(total_selection_count / (total_time_slots / 2)))\n\n    if np.random.rand() < epsilon or total_selection_count == 0:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        # Softmax for more controlled exploitation\n        scaled_scores = average_scores - np.max(average_scores)  # Shift for stability\n        exp_scores = np.exp(scaled_scores)\n        probabilities = exp_scores / np.sum(exp_scores)\n        action_index = np.random.choice(action_indices, p=probabilities)  # Exploit based on probabilities\n\n    return action_index",
          "objective": -449.99999996797084,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n\n    # Initialize arrays for scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Compute average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Calculate exploration factor\n    tau = current_time_slot / total_time_slots\n    epsilon = max(0.1, 1.0 - tau)  # Decaying exploration factor\n\n    # UCB - Upper Confidence Bound strategy\n    total_selected = sum(selection_counts)\n    ucb_values = np.zeros(num_actions)\n\n    for idx in range(num_actions):\n        if selection_counts[idx] == 0:\n            ucb_values[idx] = float('inf')  # Ensure unselected actions are prioritized\n        else:\n            confidence_interval = np.sqrt((2 * np.log(total_selected)) / selection_counts[idx])\n            ucb_values[idx] = average_scores[idx] + confidence_interval\n\n    # Epsilon-Greedy construction over UCB values\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        action_index = action_indices[np.argmax(ucb_values)]  # Exploit UCB best action\n\n    return action_index",
          "objective": -449.99999996598274,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n\n    # Initialize arrays to store average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Initialization to encourage exploration\n    if total_selection_count == 0:\n        return np.random.choice(action_indices)\n\n    # Decrease exploration over time using a dynamic epsilon\n    epsilon = max(0.1, (1 - (current_time_slot / total_time_slots)))\n\n    # Explore vs Exploit decision\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        # Upper Confidence Bound calculation for exploitation\n        confidence_bounds = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n        selection_scores = average_scores + confidence_bounds\n        action_index = action_indices[np.argmax(selection_scores)]\n\n    return action_index",
          "objective": -449.9999999565006,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Adaptive exploration factor\n    exploration_factor = 0.1 * (1 - current_time_slot / total_time_slots)\n\n    # Epsilon-Greedy Strategy\n    epsilon = exploration_factor if total_selection_count > 0 else 1.0\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        # UCB Strategy\n        confidence_bounds = np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n        selection_scores = average_scores + confidence_bounds\n        action_index = action_indices[np.argmax(selection_scores)]  # Exploit\n\n    return action_index",
          "objective": -449.99999995074467,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Calculate average scores and selection counts\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    \n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n    \n    # Avoid division by zero in UCB calculation\n    ucb_values = np.zeros(n_actions)\n    for idx in range(n_actions):\n        if selection_counts[idx] > 0:\n            ucb_values[idx] = average_scores[idx] + np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[idx])\n        else:\n            ucb_values[idx] = float('inf')  # Encourage exploration of unselected actions\n    \n    # Dynamic epsilon for exploration/exploitation balance\n    epsilon = 0.1 * (1 - float(current_time_slot) / total_time_slots)\n    \n    # Epsilon-Greedy and UCB strategy combination\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = action_indices[np.argmax(ucb_values)]\n    \n    return action_index",
          "objective": -449.9999999470324,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n\n    # Initialize arrays for average scores and counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Compute average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Dynamic exploration factor\n    exploration_factor = 0.1 * (1 - current_time_slot / total_time_slots)\n\n    # Epsilon-Greedy Exploration\n    if np.random.rand() < exploration_factor or total_selection_count == 0:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        # UCB for Exploitation\n        confidence_bounds = np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n        selection_scores = average_scores + confidence_bounds\n        action_index = action_indices[np.argmax(selection_scores)]  # Exploit\n\n    return action_index",
          "objective": -449.9999999363281,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    \n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Dynamic exploration factor based on time slot and epsilon-greedy approach\n    epsilon = 0.1 * np.sqrt(current_time_slot / total_time_slots)\n    \n    if np.random.rand() < epsilon or total_selection_count == 0:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        if np.any(selection_counts == 0):\n            action_index = action_indices[np.argmin(selection_counts)]  # Select untried action\n        else:\n            # Using UCB for exploitation\n            confidence_bounds = np.sqrt(2 * np.log(total_selection_count) / selection_counts)\n            selection_scores = average_scores + confidence_bounds\n            action_index = action_indices[np.argmax(selection_scores)]  # Exploit\n    \n    return action_index",
          "objective": -449.999999929152,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Handle case when no actions have been selected\n    if total_selection_count == 0:\n        return np.random.choice(action_indices)\n\n    # Calculate average scores and selection counts\n    average_scores = np.array([np.mean(score_set[action]) if score_set[action] else 0.0 for action in action_indices])\n    selection_counts = np.array([len(score_set[action]) for action in action_indices])\n\n    # Avoid division by zero in UCB\n    selection_counts[selection_counts == 0] = 1e-5 \n\n    # Upper Confidence Bound calculation\n    ucb_values = average_scores + np.sqrt(2 * np.log(total_selection_count) / selection_counts)\n\n    # Adaptive exploration rate\n    epsilon = 0.1 * (1 - (current_time_slot / total_time_slots))\n    epsilon = max(epsilon, 0.01)\n\n    # Epsilon-Greedy decision making\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = np.argmax(ucb_values)\n    \n    return action_index",
          "objective": -449.99999992825616,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n\n    # Initialize arrays for total scores and selection counts\n    total_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Compute total scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        total_scores[idx] = sum(scores)\n\n    # Compute average scores, avoiding division by zero\n    average_scores = np.divide(total_scores, (selection_counts + 1e-10))\n\n    # Epsilon-greedy strategy for exploration\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n    random_choice = np.random.rand()\n\n    if random_choice < epsilon:\n        # Exploration: Choose a random action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploitation: Use UCB to select action\n        ucb_scores = average_scores + np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-10))\n        action_index = action_indices[np.argmax(ucb_scores)]\n\n    return action_index",
          "objective": -449.999999927756,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    \n    # Initialize arrays\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Dynamic epsilon for exploration\n    epsilon = max(0.1, 0.5 * (1 - current_time_slot / total_time_slots))\n    \n    # Random exploration\n    if np.random.rand() < epsilon or total_selection_count == 0:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        # UCB calculation for exploitation\n        unselected_actions = np.where(selection_counts == 0)[0]\n        if unselected_actions.size > 0:\n            action_index = action_indices[unselected_actions[0]]  # Select untried action\n        else:\n            confidence_bounds = np.sqrt(2 * np.log(total_selection_count) / selection_counts)\n            selection_scores = average_scores + confidence_bounds\n            action_index = action_indices[np.argmax(selection_scores)]  # Exploit\n\n    return action_index",
          "objective": -449.9999999263162,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n\n    # Initialize arrays for scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Compute average scores and selection counts, protecting against division by zero\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Dynamic epsilon for exploration\n    epsilon = 1.0 / (1 + total_selection_count / (total_time_slots + 1))\n\n    # Compute Softmax scores for exploitation\n    softmax_denominator = np.sum(np.exp(average_scores - np.max(average_scores)))\n    softmax_scores = np.exp(average_scores - np.max(average_scores)) / softmax_denominator\n\n    # Select action based on epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        action_index = np.random.choice(action_indices, p=softmax_scores)  # Exploit using Softmax\n\n    return action_index",
          "objective": -449.99999992053785,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Compute average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Dynamic exploration factor\n    exploration_factor = 0.1 * (1 - current_time_slot / total_time_slots)\n\n    # Epsilon-Greedy Exploration\n    epsilon = exploration_factor\n    if np.random.rand() < epsilon or total_selection_count == 0:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        # Adaptive UCB for Exploitation\n        confidence_bounds = np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n        selection_scores = average_scores + confidence_bounds\n        action_index = action_indices[np.argmax(selection_scores)]  # Exploit\n\n    return action_index",
          "objective": -449.9999999200456,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters for exploration and exploitation\n    epsilon = 1.0 / (current_time_slot + 1)  # Decaying exploration rate\n    exploration_temperature = 1.0 + (total_time_slots - current_time_slot) / total_time_slots  # Temperature decays over time\n    \n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and counts\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if selection_counts[idx] > 0 else 0.0\n\n    if total_selection_count == 0:\n        return np.random.choice(action_indices)\n\n    # Calculate UCB values\n    confidence_bounds = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n    ucb_values = average_scores + confidence_bounds\n\n    # Softmax probabilities for UCB values\n    exp_ucb_values = np.exp(ucb_values / exploration_temperature)\n    probabilities = exp_ucb_values / np.sum(exp_ucb_values)\n\n    # Epsilon-greedy decision\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices, p=probabilities)\n    else:\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -449.99999991795886,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n\n    # Initialize arrays for scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Compute average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Adjust exploration factor based on time slots and total selections\n    tau = current_time_slot / total_time_slots\n    epsilon = max(0.1, 1.0 - tau)  # Decaying exploration\n\n    # Epsilon-Greedy exploration\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        # Softmax for exploitation\n        temperature = max(0.1, 1 - tau)  # Temperature decay\n        selection_scores = np.exp(average_scores / temperature)\n        probabilities = selection_scores / np.sum(selection_scores)\n        action_index = np.random.choice(action_indices, p=probabilities)  # Exploit\n\n    return action_index",
          "objective": -449.99999989491886,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n\n    # Initialize arrays for scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Calculate UCB values\n    ucb_values = np.zeros(num_actions)\n    for idx in range(num_actions):\n        if selection_counts[idx] > 0:\n            confidence_bound = np.sqrt(2 * np.log(total_selection_count) / selection_counts[idx])\n            ucb_values[idx] = average_scores[idx] + confidence_bound\n        else:\n            ucb_values[idx] = float('inf')  # Prioritize actions not yet tried\n\n    # Epsilon for exploration based on time slots\n    epsilon = max(0.1, 1.0 - current_time_slot / total_time_slots)\n    \n    # Epsilon-Greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        action_index = np.argmax(ucb_values)  # Exploit\n\n    return action_index",
          "objective": -449.9999998946965,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters for exploration and exploitation\n    epsilon = 0.1\n    temperature = 1.0 + (4.0 * (1 - float(current_time_slot) / total_time_slots))\n    \n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Initialize averages and counts\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    \n    # Compute average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n    \n    # Handle case of no previous selections\n    if total_selection_count == 0:\n        return np.random.choice(action_indices)\n\n    # Upper Confidence Bound values calculation\n    confidence_bounds = np.sqrt(np.log(total_selection_count) / (selection_counts + 1e-5))\n    ucb_values = average_scores + confidence_bounds\n\n    # Epsilon-Greedy Decision Making\n    if np.random.rand() < epsilon:\n        exp_ucb_values = np.exp(ucb_values / temperature)\n        probabilities = exp_ucb_values / np.sum(exp_ucb_values)\n        action_index = np.random.choice(action_indices, p=probabilities)\n    else:\n        action_index = np.argmax(ucb_values)\n    \n    return action_index",
          "objective": -449.99999989053583,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    \n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Compute average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Dynamic exploration factor based on time slot\n    exploration_factor = 0.1 * (1 - current_time_slot / total_time_slots)\n    \n    if np.random.rand() < exploration_factor or total_selection_count == 0:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        # Enhance selection with a modified UCB approach\n        if np.any(selection_counts == 0):\n            action_index = np.argmin(selection_counts)  # Select untried action\n        else:\n            confidence_bounds = np.sqrt(np.log(total_selection_count) / selection_counts)\n            selection_scores = average_scores + confidence_bounds\n            action_index = action_indices[np.argmax(selection_scores)]  # Exploit\n\n    return action_index",
          "objective": -449.9999998851587,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    for idx, action in enumerate(action_indices):\n        if score_set[action]:\n            scores = np.array(score_set[action])\n            selection_counts[idx] = len(scores)\n            average_scores[idx] = np.mean(scores)\n\n    # Define exploration parameter\n    exploration_rate = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n\n    # Calculate UCB values\n    ucb_values = np.zeros(num_actions)\n    for idx in range(num_actions):\n        if selection_counts[idx] > 0:\n            ucb_values[idx] = average_scores[idx] + np.sqrt(2 * np.log(total_selection_count) / selection_counts[idx])\n        else:\n            ucb_values[idx] = float('inf')  # Prioritize unselected actions\n\n    # Decision making: Use exploration or exploitation based on exploration rate\n    if np.random.rand() < exploration_rate or total_selection_count == 0:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = action_indices[np.argmax(ucb_values)]\n\n    return action_index",
          "objective": -449.999999880407,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    \n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Using a softmax strategy to balance exploration and exploitation\n    scaled_scores = average_scores + np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1e-10))\n    exp_scores = np.exp(scaled_scores - np.max(scaled_scores))  # For numerical stability\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Select action based on computed probabilities\n    action_index = np.random.choice(action_indices, p=probabilities)\n\n    return action_index",
          "objective": -449.9999998787262,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Compute average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Dynamic exploration factor\n    exploration_factor = 0.1 * (1 - current_time_slot / total_time_slots)\n\n    # Epsilon-Greedy exploration\n    epsilon = exploration_factor\n    if np.random.rand() < epsilon or total_selection_count == 0:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        # Improved exploitation using Thompson Sampling approach\n        beta_params = [(np.sum(scores) + 1, len(scores) - np.sum(scores) + 1) for scores in score_set.values()]\n        samples = [np.random.beta(a, b) for a, b in beta_params]\n        action_index = action_indices[np.argmax(samples)]  # Exploit\n\n    return action_index",
          "objective": -449.99999987329454,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters for exploration and exploitation\n    epsilon = 0.1 * (1 - float(current_time_slot) / total_time_slots)\n    temperature = 1.0 + (4.0 * (1 - float(current_time_slot) / total_time_slots))\n    \n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Average scores and selection counts\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    \n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n    \n    # Handle case of no previous selections\n    if total_selection_count == 0:\n        return np.random.choice(action_indices)\n\n    # Upper Confidence Bound values calculation\n    ucb_values = average_scores + np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n\n    # Epsilon-Greedy Decision Making\n    if np.random.rand() < epsilon:\n        exp_ucb_values = np.exp(ucb_values / temperature)\n        probabilities = exp_ucb_values / np.sum(exp_ucb_values)\n        action_index = np.random.choice(action_indices, p=probabilities)\n    else:\n        action_index = np.argmax(ucb_values)\n    \n    return action_index",
          "objective": -449.99999984908203,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    \n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    for idx, action in enumerate(action_indices):\n        if score_set[action]:\n            scores = score_set[action]\n            selection_counts[idx] = len(scores)\n            average_scores[idx] = np.mean(scores)\n    \n    exploration_rate = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n    \n    if np.random.rand() < exploration_rate or total_selection_count == 0:\n        action_index = np.random.choice(action_indices)\n    else:\n        softmax_scores = np.exp(average_scores) / np.sum(np.exp(average_scores))\n        action_index = np.random.choice(action_indices, p=softmax_scores)\n\n    return action_index",
          "objective": -449.9999998454566,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.05  # Exploration rate\n    temperature = 1.0 + (1.0 - (current_time_slot / total_time_slots)) * 4  # Dynamic temperature\n    \n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    \n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n        selection_counts[idx] = len(scores)\n    \n    # If no selections have been made, choose randomly\n    if total_selection_count == 0:\n        return np.random.choice(action_indices)\n    \n    confidence_bounds = np.sqrt(np.log(total_selection_count) / (selection_counts + 1e-5))\n    \n    ucb_values = average_scores + confidence_bounds\n    \n    if np.random.rand() < epsilon:\n        exp_ucb_values = np.exp(ucb_values / temperature)\n        probabilities = exp_ucb_values / np.sum(exp_ucb_values)\n        action_index = np.random.choice(action_indices, p=probabilities)\n    else:\n        action_index = np.argmax(ucb_values)\n    \n    return action_index",
          "objective": -449.9999998279558,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Define exploration rate and dynamic allocation factor\n    alpha = 0.1  # Exploration constant\n    exploration_factor = 1.0 + (4.0 * (1 - float(current_time_slot) / total_time_slots))\n    \n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    \n    # Calculate average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n        selection_counts[idx] = len(scores)\n        \n    # Handle the case of no previous selections\n    if total_selection_count == 0:\n        return np.random.choice(action_indices)\n    \n    # Upper Confidence Bound computation\n    confidence_bounds = np.sqrt(np.log(total_selection_count) / (selection_counts + 1e-5))\n    ucb_values = average_scores + alpha * confidence_bounds\n\n    # Softmax selection with dynamic exploration factor\n    exp_ucb_values = np.exp(ucb_values / exploration_factor)\n    probabilities = exp_ucb_values / np.sum(exp_ucb_values)\n    \n    action_index = np.random.choice(action_indices, p=probabilities)\n    \n    return action_index",
          "objective": -449.99999980952646,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters for exploration and exploitation\n    epsilon_start = 0.3\n    epsilon_decay = 0.02\n    epsilon_min = 0.05\n    epsilon = max(epsilon_min, epsilon_start - epsilon_decay * current_time_slot)\n    \n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Initialize averages and counts\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    \n    # Compute average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n    \n    # Handle case of no previous selections\n    if total_selection_count == 0:\n        return np.random.choice(action_indices)\n\n    # Upper Confidence Bound values calculation\n    ucb_values = average_scores + np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n\n    # Epsilon-Greedy Decision Making\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = np.argmax(ucb_values)\n    \n    return action_index",
          "objective": -449.9999997962597,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    \n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    # Compute average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Define adaptive epsilon for exploration/exploitation\n    exploration_probability = 1 / (1 + np.sqrt(current_time_slot + 1))  # Decreases over time\n\n    if np.random.rand() < exploration_probability or total_selection_count <= 0:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        # Select action based on a modified UCB using average scores\n        upper_confidence_bounds = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n        selection_scores = average_scores + upper_confidence_bounds\n        action_index = action_indices[np.argmax(selection_scores)]  # Exploit\n\n    return action_index",
          "objective": -449.9999997755103,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n\n    # Calculate average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Calculate exploration rate\n    beta = 1.0 / (current_time_slot + 1)\n    epsilon = max(0.1, 1.0 - beta)  # Decaying exploration factor\n\n    # Calculate UCB values taking into account selection counts\n    total_selected = total_selection_count\n    ucb_values = np.zeros(num_actions)\n\n    for idx in range(num_actions):\n        if selection_counts[idx] == 0:\n            ucb_values[idx] = float('inf')\n        else:\n            confidence_interval = np.sqrt((2 * np.log(total_selected)) / selection_counts[idx])\n            ucb_values[idx] = average_scores[idx] + confidence_interval\n\n    # Epsilon-greedy strategy with UCB\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        action_index = action_indices[np.argmax(ucb_values)]  # Exploit the best UCB action\n\n    return action_index",
          "objective": -449.9999997734762,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1 * (1 - current_time_slot / total_time_slots)  # Decrease epsilon over time\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    \n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n        selection_counts[idx] = len(scores)\n    \n    # If no selections have been made, choose randomly\n    if total_selection_count == 0:\n        return np.random.choice(action_indices)\n    \n    # Calculate Upper Confidence Bound (UCB)\n    confidence_bounds = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n    ucb_values = average_scores + confidence_bounds\n    \n    if np.random.rand() < epsilon:\n        exp_ucb_values = np.exp(ucb_values)\n        probabilities = exp_ucb_values / np.sum(exp_ucb_values)\n        action_index = np.random.choice(action_indices, p=probabilities)\n    else:\n        action_index = np.argmax(ucb_values)\n    \n    return action_index",
          "objective": -449.99999976033723,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    \n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Compute average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Epsilon-greedy exploration factor, decreasing over time\n    epsilon = 0.1 * (1 - current_time_slot / total_time_slots)\n    \n    if np.random.rand() < epsilon or total_selection_count == 0:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        # Apply UCB for exploitation\n        confidence_bounds = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n        selection_scores = average_scores + confidence_bounds\n        action_index = action_indices[np.argmax(selection_scores)]  # Exploit\n    \n    return action_index",
          "objective": -449.999999747648,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration rate\n    alpha = 0.5  # Temperature scaling factor for exploration\n    beta = 1.0 / (1.0 + current_time_slot / total_time_slots)  # Dynamic exploitation bias\n    \n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    \n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n        selection_counts[idx] = len(scores)\n    \n    if total_selection_count == 0:\n        return np.random.choice(action_indices)\n    \n    confidence_bounds = np.sqrt(np.log(total_selection_count) / (selection_counts + 1e-5))\n\n    ucb_values = average_scores + confidence_bounds\n    \n    if np.random.rand() < epsilon:\n        softmax_scores = np.exp(ucb_values / alpha)\n        probabilities = softmax_scores / np.sum(softmax_scores)\n        action_index = np.random.choice(action_indices, p=probabilities)\n    else:\n        adjusted_ucb_values = ucb_values * beta\n        action_index = np.argmax(adjusted_ucb_values)\n    \n    return action_index",
          "objective": -449.9999997391526,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    # Calculate average scores and selection counts for all actions\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Dynamic exploration parameter (epsilon)\n    epsilon = 0.1 * (1 - current_time_slot / total_time_slots)\n\n    # Softmax method for exploration vs exploitation\n    scaled_scores = average_scores * (1 - epsilon) + \\\n                    epsilon * np.random.rand(num_actions)  # Encourage exploration\n    action_probabilities = np.exp(scaled_scores) / np.sum(np.exp(scaled_scores))\n\n    # Select action based on computed probabilities\n    action_index = np.random.choice(action_indices, p=action_probabilities)\n\n    return action_index",
          "objective": -449.99999973415686,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    \n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Hybrid exploration-exploitation strategy\n    epsilon = 1.0 / (current_time_slot + 1)  # Decrease exploration over time\n    temperature = 1.0 / np.sqrt(current_time_slot + 1)  # Softmax temperature\n    \n    if np.random.rand() < epsilon or total_selection_count == 0:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        exp_scores = np.exp(average_scores / temperature)\n        probabilities = exp_scores / np.sum(exp_scores)  # Softmax probabilities\n        \n        action_index = np.random.choice(action_indices, p=probabilities)  # Exploit based on Softmax\n    \n    return action_index",
          "objective": -449.99999972750686,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    \n    # Calculate average scores and selection counts\n    average_scores = np.array([np.mean(score_set[action]) if score_set[action] else 0.0 for action in action_indices])\n    selection_counts = np.array([len(score_set[action]) for action in action_indices])\n    \n    # Dynamic exploration factor using decaying epsilon\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01)\n\n    if np.random.rand() < epsilon or total_selection_count == 0:\n        # Explore: Select a random action\n        return np.random.choice(action_indices)\n\n    # Exploit: Softmax-based action selection\n    temperature = 1.0  # Control the randomness\n    adjusted_scores = average_scores / temperature\n    exp_scores = np.exp(adjusted_scores - np.max(adjusted_scores))  # for numerical stability\n    selection_probabilities = exp_scores / np.sum(exp_scores)\n\n    # Select action based on calculated probabilities\n    action_index = np.random.choice(action_indices, p=selection_probabilities)\n    \n    return action_index",
          "objective": -449.9999996971925,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and counts\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if selection_counts[idx] > 0 else 0.0\n\n    # Calculate UCB values\n    ucb_values = average_scores + np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n\n    # Adaptive epsilon based on time\n    epsilon = max(0.1, 1.0 - (current_time_slot / total_time_slots))\n\n    # Epsilon-greedy decision\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -449.99999969379746,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Adjust exploration and exploitation parameters\n    epsilon = max(0.1, 1.0 / (current_time_slot + 1))  # Minimum epsilon to ensure exploration\n    exploration_temperature = 1.0 + (total_time_slots - current_time_slot) / total_time_slots\n\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and counts\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if selection_counts[idx] > 0 else 0.0\n\n    if total_selection_count == 0:\n        return np.random.choice(action_indices)\n\n    # Calculate UCB values with a small constant to avoid division by zero\n    confidence_bounds = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n    ucb_values = average_scores + confidence_bounds\n\n    # Softmax distribution on UCB values\n    exp_ucb_values = np.exp(ucb_values / exploration_temperature)\n    probabilities = exp_ucb_values / np.sum(exp_ucb_values)\n\n    # Epsilon-greedy decision\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices, p=probabilities)\n    else:\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -449.9999996904281,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon_start = 0.1  # Initial exploration rate\n    epsilon_decay = 0.05  # Decay rate for exploration\n    min_epsilon = 0.01  # Minimum exploration rate\n    \n    # Calculating epsilon based on the current time slot\n    epsilon = max(min_epsilon, epsilon_start * np.exp(-epsilon_decay * current_time_slot))\n    \n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Initialize arrays to hold average scores and selection counts\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    \n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n        selection_counts[idx] = len(scores)\n\n    if total_selection_count == 0:\n        return np.random.choice(action_indices)\n    \n    # Calculating UCB values\n    confidence_bounds = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n    ucb_values = average_scores + confidence_bounds\n    \n    if np.random.rand() < epsilon:\n        # Softmax exploration strategy\n        exp_ucb_values = np.exp(ucb_values - np.max(ucb_values))  # Stability improvement\n        probabilities = exp_ucb_values / np.sum(exp_ucb_values)\n        action_index = np.random.choice(action_indices, p=probabilities)\n    else:\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -449.99999968929797,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and counts\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if selection_counts[idx] > 0 else 0.0\n\n    # If no actions have been selected yet\n    if total_selection_count == 0:\n        return np.random.choice(action_indices)\n\n    # Calculate UCB values\n    ucb_values = average_scores + np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n\n    # Adapt exploration: decaying epsilon based on time\n    epsilon = max(0.1, 1.0 / (current_time_slot + 1))  # Minimum epsilon value to ensure exploration\n\n    # Epsilon-greedy decision\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -449.9999996646286,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n\n    # Initialize arrays for scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Compute average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Calculate exploration factor\n    tau = current_time_slot / total_time_slots\n    epsilon = max(0.1, 1.0 - tau)  # Decaying exploration factor\n\n    # Softmax exploration balance\n    scaled_scores = average_scores - np.min(average_scores) + 1e-10  # To avoid negative scores\n    probabilities = np.exp(scaled_scores) / np.sum(np.exp(scaled_scores))\n\n    # Epsilon-Greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        action_index = np.random.choice(action_indices, p=probabilities)  # Exploit based on Softmax\n\n    return action_index",
          "objective": -449.99999966063456,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters for exploration and exploitation\n    epsilon = 0.1\n    exploration_factor = 1.0 + (4.0 * (1 - float(current_time_slot) / total_time_slots))\n    \n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize averages and counts\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Compute average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Handle case of no previous selections\n    if total_selection_count == 0:\n        return np.random.choice(action_indices)\n\n    # Upper Confidence Bound values calculation\n    confidence_bounds = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n    ucb_values = average_scores + confidence_bounds\n\n    # Epsilon-Greedy Decision Making\n    if np.random.rand() < epsilon:\n        exploration_values = np.exp(ucb_values / exploration_factor)\n        probabilities = exploration_values / np.sum(exploration_values)\n        action_index = np.random.choice(action_indices, p=probabilities)\n    else:\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -449.9999996345473,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    \n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Compute average scores and their selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Epsilon dynamic exploration factor\n    epsilon = 0.1 * (1 - current_time_slot / total_time_slots)\n\n    if np.random.rand() < epsilon or total_selection_count == 0:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        # Use a modified UCB approach\n        ucb_values = np.zeros(num_actions)\n        for idx in range(num_actions):\n            if selection_counts[idx] == 0:\n                ucb_values[idx] = np.inf  # Assign infinity for untried actions\n            else:\n                ucb_values[idx] = average_scores[idx] + np.sqrt((2 * np.log(total_selection_count)) / selection_counts[idx])\n\n        action_index = action_indices[np.argmax(ucb_values)]  # Exploit\n    \n    return action_index",
          "objective": -449.99999962907134,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Dynamic exploration factor that decreases over time\n    epsilon = 0.1 * (1.0 - float(current_time_slot) / total_time_slots)\n\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n        selection_counts[idx] = len(scores) if scores else 0\n\n    # Handle case of no selections to ensure exploration\n    if total_selection_count == 0:\n        return np.random.choice(action_indices)\n\n    # UCB computation for exploitation\n    confidence_bounds = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n    ucb_values = average_scores + confidence_bounds\n\n    # Epsilon-greedy or random exploration\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -449.99999956005047,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    \n    # Calculate total scores and selection counts\n    total_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        total_scores[idx] = np.sum(scores)\n\n    # Calculate average scores while avoiding division by zero\n    average_scores = np.where(selection_counts > 0, total_scores / selection_counts, 0.0)\n\n    # Epsilon-greedy exploration strategy\n    base_epsilon = 0.2\n    exploration_factor = base_epsilon * (1 - current_time_slot / total_time_slots)\n\n    if np.random.rand() < exploration_factor or total_selection_count == 0:\n        # Explore: Select a random action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploit: Use a modified UCB strategy\n        confidence_bounds = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n        selection_scores = average_scores + confidence_bounds\n        \n        # Select the action with the highest selection score\n        action_index = action_indices[np.argmax(selection_scores)]\n\n    return action_index",
          "objective": -449.99999947467575,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n\n    # Initialize arrays for averages and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Compute average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Dynamic exploration factor based on time slot (epsilon)\n    exploration_probability = 0.1 * (1 - current_time_slot / total_time_slots)\n\n    # Select action using epsilon-greedy strategy\n    if np.random.rand() < exploration_probability or total_selection_count == 0:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        with np.errstate(divide='ignore', invalid='ignore'):\n            # UCB scores calculation\n            confidence_bounds = np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n            ucb_scores = average_scores + confidence_bounds\n\n            # Handling for actions never selected\n            ucb_scores[selection_counts == 0] = float('inf')\n            action_index = action_indices[np.argmax(ucb_scores)]  # Exploit\n\n    return action_index",
          "objective": -449.99999947328666,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration rate\n    decay_rate = 0.1\n    exploration_temperature = np.exp(-decay_rate * current_time_slot)  # Decay temperature\n\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n        selection_counts[idx] = len(scores)\n\n    if total_selection_count == 0:\n        return np.random.choice(action_indices)\n\n    # Calculating UCB values\n    confidence_bounds = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n    ucb_values = average_scores + confidence_bounds\n    \n    if np.random.rand() < epsilon:\n        exp_ucb_values = np.exp(ucb_values / exploration_temperature)\n        probabilities = exp_ucb_values / np.sum(exp_ucb_values)\n        action_index = np.random.choice(action_indices, p=probabilities)\n    else:\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -449.9999994577501,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Define exploration rate and decay for the softmax temperature\n    epsilon = 0.1  # Exploration rate\n    temperature = 1.0 + (3.0 * (1 - float(current_time_slot) / total_time_slots))  # Dynamic temperature based on time\n    \n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Initialize averages and counts\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    \n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n        selection_counts[idx] = len(scores)\n\n    # Handle the case of no previous selections\n    if total_selection_count == 0:\n        return np.random.choice(action_indices)\n\n    # Calculate Upper Confidence Bound values\n    ucb_values = average_scores + np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n\n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        exp_ucb_values = np.exp(ucb_values / temperature)\n        probabilities = exp_ucb_values / np.sum(exp_ucb_values)\n        action_index = np.random.choice(action_indices, p=probabilities)\n    else:\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -449.99999944533084,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1 * (1 - (current_time_slot / total_time_slots))  # Decreasing epsilon over time\n    temperature = 1.0 + (1.0 - (current_time_slot / total_time_slots)) * 3  # Dynamic temperature\n    \n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    average_scores = np.array([np.mean(score_set[action]) if score_set[action] else 0.0 for action in action_indices])\n    selection_counts = np.array([len(score_set[action]) for action in action_indices])\n    \n    # If no selections have been made, choose randomly\n    if total_selection_count == 0:\n        return np.random.choice(action_indices)\n    \n    confidence_bounds = np.sqrt(np.log(total_selection_count) / (selection_counts + 1e-5))\n    \n    ucb_values = average_scores + confidence_bounds\n    \n    if np.random.rand() < epsilon:\n        exp_ucb_values = np.exp(ucb_values / temperature)\n        probabilities = exp_ucb_values / np.sum(exp_ucb_values)\n        action_index = np.random.choice(action_indices, p=probabilities)\n    else:\n        action_index = np.argmax(ucb_values)\n    \n    return action_index",
          "objective": -449.999999340764,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Compute average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Define epsilon for exploration, decaying with time\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01)\n\n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon or total_selection_count == 0:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        # Using normalized UCB approach\n        confidence_bounds = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n        selection_scores = average_scores + confidence_bounds\n        \n        action_index = action_indices[np.argmax(selection_scores)]  # Exploit\n\n    return action_index",
          "objective": -449.99999928036976,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    \n    # Calculate average scores and selection counts\n    average_scores = np.zeros(len(action_indices))\n    selection_counts = np.zeros(len(action_indices))\n\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Dynamic exploration factor\n    base_epsilon = 0.1\n    exploration_factor = base_epsilon * (1 - current_time_slot / total_time_slots)\n\n    if np.random.rand() < exploration_factor or total_selection_count == 0:\n        # Explore: Select a random action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploit: Use a modified UCB strategy\n        confidence_bounds = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n        selection_scores = average_scores + confidence_bounds\n\n        # Select the action with the highest selection score\n        action_index = action_indices[np.argmax(selection_scores)]\n\n    return action_index",
          "objective": -449.99999922935496,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    for idx, action in enumerate(action_indices):\n        if score_set[action]:\n            scores = np.array(score_set[action])\n            selection_counts[idx] = len(scores)\n            average_scores[idx] = np.mean(scores)\n\n    # Define exploration parameter using a decaying epsilon strategy\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n\n    # Calculate UCB values and softmax probabilities for exploitation\n    ucb_values = np.zeros(num_actions)\n    for idx in range(num_actions):\n        if selection_counts[idx] > 0:\n            ucb_values[idx] = average_scores[idx] + np.sqrt(2 * np.log(total_selection_count) / selection_counts[idx])\n        else:\n            ucb_values[idx] = float('inf')  # Prioritize unselected actions\n\n    # Softmax for action selection based on UCB\n    exp_ucb = np.exp(ucb_values - np.max(ucb_values))  # for numerical stability\n    softmax_probs = exp_ucb / np.sum(exp_ucb)\n\n    # Decision making: Explore with epsilon probability or exploit based on softmax probabilities\n    if np.random.rand() < epsilon or total_selection_count == 0:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = np.random.choice(action_indices, p=softmax_probs)\n\n    return action_index",
          "objective": -449.9999991884904,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Dynamic parameters\n    base_epsilon = 0.1  # Base exploration rate\n    action_indices = list(score_set.keys())\n    \n    # Initialize average scores and selection counts\n    average_scores = np.zeros(len(action_indices))\n    selection_counts = np.zeros(len(action_indices))\n\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Dynamic epsilon based on time slot\n    exploration_factor = base_epsilon * np.exp(-current_time_slot / total_time_slots)\n    \n    if np.random.rand() < exploration_factor or total_selection_count == 0:\n        # Explore: Select a random action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Calculate UCB and select the action based on maximizing the total score estimation\n        confidence_bounds = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n        selection_scores = average_scores + confidence_bounds\n        \n        # Select action based on exploration-exploitation strategy\n        action_index = action_indices[np.argmax(selection_scores)]\n\n    return action_index",
          "objective": -449.99999910535587,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Constants for the strategy\n    epsilon = 0.1  # Exploration rate\n    action_indices = list(score_set.keys())\n    \n    # Calculate average scores and counts for each action\n    average_scores = np.zeros(len(action_indices))\n    selection_counts = np.zeros(len(action_indices))\n    \n    for action in action_indices:\n        scores = score_set[action]\n        average_scores[action] = np.mean(scores) if scores else 0.0\n        selection_counts[action] = len(scores)\n    \n    # Use Upper Confidence Bound (UCB) strategy\n    if total_selection_count > 0:\n        # Calculate UCB values\n        confidence_bounds = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n        ucb_values = average_scores + confidence_bounds\n    else:\n        # If no selections yet, explore\n        return np.random.choice(action_indices)\n    \n    # Epsilon-greedy selection between UCB and exploration\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = np.argmax(ucb_values)\n    \n    return action_index",
          "objective": -449.99999901161937,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters for exploration versus exploitation\n    epsilon = 0.05  # Exploration rate\n    temperature = 0.3  # Temperature scaling factor for softmax\n\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n        selection_counts[idx] = len(scores)\n\n    # Handle the case where no actions have been selected yet\n    if total_selection_count == 0:\n        return np.random.choice(action_indices)\n\n    # Upper Confidence Bound (UCB) calculation\n    confidence_bounds = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n    ucb_values = average_scores + confidence_bounds\n\n    # Softmax probabilities for exploration\n    softmax_scores = np.exp(ucb_values / temperature)\n    probabilities = softmax_scores / np.sum(softmax_scores)\n\n    # Dynamic exploration strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices, p=probabilities)\n    else:\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -449.99999880490157,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    \n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Compute average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Dynamic exploration factor based on time slot\n    exploration_rate = 0.1 * (1 - current_time_slot / total_time_slots)\n    \n    if np.random.rand() < exploration_rate or total_selection_count == 0:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        # Use UCB for selection\n        confidence_bounds = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n        selection_scores = average_scores + confidence_bounds\n        action_index = action_indices[np.argmax(selection_scores)]  # Exploit\n\n    return action_index",
          "objective": -449.99999869461766,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    \n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Compute average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Dynamic exploration factor based on time slot\n    exploration_probability = 0.1 * (1 - current_time_slot / total_time_slots)\n\n    if np.random.rand() < exploration_probability or total_selection_count == 0:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        with np.errstate(divide='ignore', invalid='ignore'):\n            confidence_bounds = np.sqrt(np.log(total_selection_count) / selection_counts)\n            selection_scores = average_scores + confidence_bounds\n\n            # Handling for actions never selected\n            selection_scores[selection_counts == 0] = float('inf')\n            action_index = action_indices[np.argmax(selection_scores)]  # Exploit\n\n    return action_index",
          "objective": -449.99999853040856,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and counts\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Decay factor for exploration\n    decay_factor = 0.1\n    exploration_rate = decay_factor * (1 - current_time_slot / total_time_slots)\n\n    if np.random.rand() < exploration_rate or total_selection_count == 0:\n        # Exploration: Select an action randomly\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploitation: UCB approach with a modified confidence term\n        confidence_bounds = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n        selection_scores = average_scores + confidence_bounds\n        \n        # Select the action with the highest score\n        action_index = action_indices[np.argmax(selection_scores)]\n\n    return action_index",
          "objective": -449.99999852399895,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = max(0.01, 0.1 * (1 - current_time_slot / total_time_slots))  # Decayed exploration rate\n    temperature = 1.0 + (1.0 - (current_time_slot / total_time_slots)) * 4  # Dynamic temperature\n    \n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    \n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n        selection_counts[idx] = len(scores)\n    \n    confidence_bounds = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n    ucb_values = average_scores + confidence_bounds\n    \n    if total_selection_count == 0 or np.random.rand() < epsilon:\n        exp_ucb_values = np.exp(ucb_values / temperature)\n        probabilities = exp_ucb_values / np.sum(exp_ucb_values)\n        action_index = np.random.choice(action_indices, p=probabilities)\n    else:\n        action_index = np.argmax(ucb_values)\n    \n    return action_index",
          "objective": -449.99999848197314,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    \n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Compute average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Dynamic epsilon based on remaining time slots\n    epsilon = 0.1 * (1 - current_time_slot / total_time_slots)\n\n    if np.random.rand() < epsilon or total_selection_count == 0:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        # Modified UCB approach for exploitation\n        confidence_bounds = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n        selection_scores = average_scores + confidence_bounds\n        action_index = action_indices[np.argmax(selection_scores)]  # Exploit\n\n    return action_index",
          "objective": -449.999998364444,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    \n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Compute average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Adaptive exploration factor based on time slot\n    delta = 0.5  # Exploration factor multiplier\n    exploration_probability = delta * (1 - current_time_slot / total_time_slots)\n\n    if np.random.rand() < exploration_probability or total_selection_count == 0:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        with np.errstate(divide='ignore', invalid='ignore'):\n            # Upper Confidence Bound approach\n            confidence_bounds = np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n            selection_scores = average_scores + confidence_bounds\n\n            # Handling for actions never selected\n            selection_scores[selection_counts == 0] = float('inf')\n            action_index = action_indices[np.argmax(selection_scores)]  # Exploit\n\n    return action_index",
          "objective": -449.9999981611899,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    \n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Compute average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Dynamic epsilon for exploration\n    epsilon = 1.0 / (1 + current_time_slot / (total_time_slots / 10))\n    \n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        # Softmax selection based on average scores\n        adjusted_scores = average_scores - np.max(average_scores)  # for stability\n        exp_scores = np.exp(adjusted_scores)\n        probabilities = exp_scores / np.sum(exp_scores)\n        action_index = np.random.choice(action_indices, p=probabilities)  # Exploit\n\n    return action_index",
          "objective": -449.9999981391269,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    \n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Use an adaptive epsilon based on the progress through time slots\n    epsilon = max(0.01, (1 - current_time_slot / total_time_slots))\n    \n    if np.random.rand() < epsilon or total_selection_count == 0:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        # Calculate selection scores with UCB\n        confidence_bounds = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-10))\n        selection_scores = average_scores + confidence_bounds\n        action_index = action_indices[np.argmax(selection_scores)]  # Exploit\n\n    return action_index",
          "objective": -449.99999806171365,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Define exploration rate\n    epsilon = 0.1  # Exploration rate\n    \n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Initialize averages and counts\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    \n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n        selection_counts[idx] = len(scores)\n\n    # Handle the case of no previous selections\n    if total_selection_count == 0:\n        return np.random.choice(action_indices)\n\n    # Calculate Upper Confidence Bound values\n    ucb_values = average_scores + np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n\n    # Epsilon-greedy selection or UCB exploitation\n    if np.random.rand() < epsilon:\n        # Softmax probabilities for exploration\n        exp_ucb_values = np.exp(ucb_values)\n        probabilities = exp_ucb_values / np.sum(exp_ucb_values)\n        action_index = np.random.choice(action_indices, p=probabilities)\n    else:\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -449.99999783983486,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Compute average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Dynamic exploration rate\n    exploration_rate = 0.1 * (1 - (current_time_slot / total_time_slots))\n\n    # Epsilon-greedy strategy with dynamic exploration\n    if np.random.rand() < exploration_rate or total_selection_count == 0:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        # Use Weighted UCB\n        weight_factor = 1.5  # Adjusts emphasis on exploration vs. exploitation\n        confidence_bounds = np.sqrt(weight_factor * np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n        selection_scores = average_scores + confidence_bounds\n        action_index = action_indices[np.argmax(selection_scores)]  # Exploit\n\n    return action_index",
          "objective": -449.9999977969093,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters for exploration and exploitation\n    epsilon = 0.1\n    temperature = 1.0 + (4.0 * (1 - float(current_time_slot) / total_time_slots))\n\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize averages and counts\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Compute average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    if total_selection_count == 0:\n        return np.random.choice(action_indices)\n\n    # Upper Confidence Bound (UCB) computation\n    ucb_values = average_scores + np.sqrt(np.log(total_selection_count) / (selection_counts + 1e-5))\n\n    # Softmax-based exploration\n    exp_ucb_values = np.exp(ucb_values / temperature)\n    probabilities = exp_ucb_values / np.sum(exp_ucb_values)\n\n    # Epsilon-Greedy decision\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices, p=probabilities)\n    else:\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -449.99999778484624,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters for exploration\n    epsilon = 0.1  # Exploration rate\n    temperature = 1.0 + (4.0 * (1 - float(current_time_slot) / total_time_slots))  # Dynamic temperature\n    \n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    average_scores = np.array([np.mean(score_set[action]) if score_set[action] else 0.0 for action in action_indices])\n    selection_counts = np.array([len(score_set[action]) for action in action_indices])\n\n    # Avoid division by zero in UCB calculation and initialize confidence bounds\n    with np.errstate(divide='ignore', invalid='ignore'):\n        confidence_bounds = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n    ucb_values = average_scores + confidence_bounds\n\n    # Epsilon-Greedy decision making with decayed exploration\n    if np.random.rand() < epsilon:\n        exp_ucb_values = np.exp(ucb_values / temperature)\n        probabilities = exp_ucb_values / np.sum(exp_ucb_values)\n        action_index = np.random.choice(action_indices, p=probabilities)\n    else:\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -449.99999702599837,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration factor\n    action_values = []\n    \n    for action in range(8):\n        scores = score_set[action]\n        if len(scores) > 0:\n            average_score = np.mean(scores)\n        else:\n            average_score = 0  # Handle case where no scores exist\n        action_values.append(average_score)\n    \n    # Convert action_values to a numpy array for convenience\n    action_values = np.array(action_values)\n\n    # Calculate exploration factor to favor less chosen actions\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (np.array([len(score_set[a]) for a in range(8)]) + 1))\n    \n    # Final scores combining exploitation and exploration\n    final_values = action_values + epsilon * exploration_bonus\n\n    # Select the action with the max final_value\n    action_index = np.argmax(final_values)\n    \n    return action_index",
          "objective": -449.99999583460135,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    \n    # Initialize arrays for total scores and selection counts\n    total_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Compute total scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        total_scores[idx] = sum(scores)  # Use sum for UCB calculation\n\n    # Avoid division by zero and compute average scores\n    average_scores = np.where(selection_counts > 0, total_scores / selection_counts, 0.0)\n\n    # Upper Confidence Bound (UCB) approach\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))  # Added small value to avoid division by zero\n    ucb_values = average_scores + exploration_bonus\n\n    # Select action with highest UCB value\n    action_index = action_indices[np.argmax(ucb_values)]\n\n    return action_index",
          "objective": -449.99999576962017,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Define exploration rate and dynamic temperature\n    epsilon = 0.1 * (1 - current_time_slot / total_time_slots)  # More exploration in early time slots\n    temperature = 1.0 + (1.0 - (current_time_slot / total_time_slots)) * 5  # Dynamic temperature scaling\n\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    \n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n        selection_counts[idx] = len(scores)\n    \n    # If no selections have been made, choose one uniformly at random\n    if total_selection_count == 0:\n        return np.random.choice(action_indices)\n\n    # Calculate UCB values\n    confidence_bounds = np.sqrt(np.log(total_selection_count) / (selection_counts + 1e-5))\n    ucb_values = average_scores + confidence_bounds\n\n    # Softmax selection for exploration\n    exp_ucb_values = np.exp(ucb_values / temperature)\n    probabilities = exp_ucb_values / np.sum(exp_ucb_values)\n    \n    action_index = np.random.choice(action_indices, p=probabilities)\n    \n    return action_index",
          "objective": -449.99999562502205,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Compute average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Dynamic exploration factor using a combination of epsilon and time decay\n    exploration_rate = 0.1 * (1 - (current_time_slot / total_time_slots)) + 0.05\n    \n    # Epsilon-greedy with decaying exploration\n    if np.random.rand() < exploration_rate or total_selection_count == 0:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        # Use UCB for exploitation calculations\n        confidence_bounds = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n        selection_scores = average_scores + confidence_bounds\n        action_index = action_indices[np.argmax(selection_scores)]  # Exploit\n\n    return action_index",
          "objective": -449.99999533878406,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    \n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Compute average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Dynamic exploration factor\n    exploration_rate = 0.1 * (1 - (current_time_slot / total_time_slots))\n    \n    # Epsilon-greedy strategy with dynamic exploration\n    if np.random.rand() < exploration_rate or total_selection_count == 0:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        # Modified UCB approach\n        if np.any(selection_counts == 0):\n            action_index = np.argmin(selection_counts)  # Select untried action\n        else:\n            confidence_bounds = np.sqrt(np.log(total_selection_count) / selection_counts)\n            selection_scores = average_scores + confidence_bounds\n            action_index = action_indices[np.argmax(selection_scores)]  # Exploit\n\n    return action_index",
          "objective": -449.99999445185296,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    \n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Compute average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Dynamic exploration factor\n    exploration_rate = 0.1 * (1 - (current_time_slot / total_time_slots))\n    \n    # Exploration strategy: Epsilon-greedy with softmax for exploitation\n    if np.random.rand() < exploration_rate or total_selection_count == 0:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        # Use Softmax for exploration-exploitation balance\n        max_score = np.max(average_scores)\n        prob_scores = np.exp(average_scores - max_score)  # Stabilize exponentials\n        selection_probs = prob_scores / np.sum(prob_scores)\n        action_index = np.random.choice(action_indices, p=selection_probs)  # Exploit\n    \n    return action_index",
          "objective": -449.9999943275097,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = np.array(list(score_set.keys()))\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0.0 for action in action_indices])\n    selection_counts = np.array([len(score_set[action]) for action in action_indices])\n    \n    # Adjust exploration factor based on time slot and total time slots\n    exploration_factor = 0.1 * (1 - current_time_slot / total_time_slots)\n    \n    if np.random.rand() < exploration_factor or total_selection_count == 0:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        if np.any(selection_counts == 0):\n            action_index = action_indices[np.argmin(selection_counts)]  # Select untried action\n        else:\n            # Calculate the Upper Confidence Bound scores\n            ucb_scores = scores + np.sqrt((2 * np.log(total_selection_count)) / selection_counts)\n            action_index = action_indices[np.argmax(ucb_scores)]  # Exploit at maximum UCB score\n            \n    return action_index",
          "objective": -449.99999345121904,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Define exploration rate and temperature for Softmax\n    epsilon = 0.1  # Exploration rate\n    temperature = 1.0 + (1.0 - (current_time_slot / total_time_slots)) * 5  # Dynamic temperature decreasing over time\n    action_indices = list(score_set.keys())\n    \n    # Calculate average scores and counts for each action\n    average_scores = np.zeros(len(action_indices))\n    selection_counts = np.zeros(len(action_indices))\n    \n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n        selection_counts[idx] = len(scores)\n\n    # If no selections yet, explore\n    if total_selection_count == 0:\n        return np.random.choice(action_indices)\n    \n    # Calculate exploration bonus\n    confidence_bounds = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n\n    # Calculate value for each action\n    ucb_values = average_scores + confidence_bounds\n\n    # Epsilon-greedy strategy with softmax for selection\n    if np.random.rand() < epsilon:\n        # Softmax probabilities for actions based on UCB values\n        exp_ucb_values = np.exp(ucb_values / temperature)\n        probabilities = exp_ucb_values / np.sum(exp_ucb_values)\n        action_index = np.random.choice(action_indices, p=probabilities)\n    else:\n        action_index = np.argmax(ucb_values)\n    \n    return action_index",
          "objective": -449.99999331907816,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    \n    # Initialize arrays for scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Compute average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n        \n    # Epsilon parameter decreasing over time\n    epsilon = 0.5 * (1 - current_time_slot / total_time_slots)\n\n    # Exploration or Exploitation decision\n    if np.random.rand() < epsilon or total_selection_count == 0:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        # Enhanced selection with UCB\n        confidence_bounds = np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n        selection_scores = average_scores + confidence_bounds\n        action_index = action_indices[np.argmax(selection_scores)]  # Exploit\n\n    return action_index",
          "objective": -449.99999238973936,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Define exploration rate and dynamic temperature for Softmax exploration\n    epsilon = 0.1  # Exploration rate\n    temperature = 1.0 + ((1.0 - current_time_slot / total_time_slots) * 3.0)\n    \n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Initialize arrays to store average scores and selection counts\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    \n    # Calculate average scores and selection counts for each action\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n    \n    # Handle case when no selections have been made\n    if total_selection_count == 0:\n        return np.random.choice(action_indices)\n    \n    # Calculate Upper Confidence Bound (UCB)\n    confidence_bounds = np.sqrt(2 * np.log(total_selection_count) / (selection_counts + 1e-5))\n    \n    # Compute UCB values\n    ucb_values = average_scores + confidence_bounds\n    \n    # Epsilon-greedy exploration vs exploitation\n    if np.random.rand() < epsilon:\n        exp_ucb_values = np.exp(ucb_values / temperature)\n        probabilities = exp_ucb_values / np.sum(exp_ucb_values)\n        action_index = np.random.choice(action_indices, p=probabilities)\n    else:\n        action_index = np.argmax(ucb_values)\n    \n    return action_index",
          "objective": -449.99999059810744,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    \n    # Initialize arrays for averages and selection counts\n    average_scores = np.zeros(len(action_indices))\n    selection_counts = np.zeros(len(action_indices))\n\n    # Compute average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Compute epsilon based on current time slot\n    max_epsilon = 0.1\n    epsilon = max_epsilon * (1 - current_time_slot / total_time_slots)\n\n    # Epsilon-greedy exploration strategy\n    if np.random.rand() < epsilon or total_selection_count < len(action_indices):\n        return np.random.choice(action_indices)\n\n    # Calculate UCB scores\n    confidence_bounds = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n    selection_scores = average_scores + confidence_bounds\n\n    # Select the action with the highest selection score\n    action_index = action_indices[np.argmax(selection_scores)]\n    \n    return action_index",
          "objective": -449.99998944349403,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Compute average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Calculate the exploration rate\n    exploration_rate = max(0.1, 1.0 - (current_time_slot / total_time_slots))\n\n    # Upper Confidence Bound (UCB) strategy implementation\n    ucb_values = np.zeros(num_actions)\n    for idx in range(num_actions):\n        if selection_counts[idx] > 0:\n            ucb_values[idx] = average_scores[idx] + np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[idx])\n        else:\n            ucb_values[idx] = float('inf')  # For unselected actions\n\n    # Choose action based on UCB\n    action_index = np.argmax(ucb_values)\n\n    # Further explore with probability to keep balance\n    if np.random.rand() < exploration_rate:\n        action_index = np.random.choice(action_indices)\n\n    return action_index",
          "objective": -449.9999889005362,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Compute average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Adaptive epsilon for exploration\n    epsilon = 0.1 * (1 - current_time_slot / total_time_slots)\n\n    if np.random.rand() < epsilon or total_selection_count == 0:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        if np.any(selection_counts == 0):\n            action_index = action_indices[np.argmin(selection_counts)]  # Select untried action\n        else:\n            confidence_bounds = np.sqrt((2 * np.log(total_selection_count)) / selection_counts)\n            selection_scores = average_scores + confidence_bounds\n            action_index = action_indices[np.argmax(selection_scores)]  # Exploit\n\n    return action_index",
          "objective": -449.99998825367993,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    \n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Exploration factor - dynamic based on time\n    epsilon = 0.1 * (1 - current_time_slot / total_time_slots)\n\n    if np.random.rand() < epsilon or total_selection_count == 0:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        # UCB-based exploitation\n        confidence_bounds = np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n        selection_scores = average_scores + confidence_bounds\n        action_index = action_indices[np.argmax(selection_scores)]  # Exploit\n\n    return action_index",
          "objective": -449.9999876888605,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize parameters\n    exploration_ratio = 0.2  # Exploration parameter\n    action_count = len(score_set)\n    \n    # Calculate average scores for each action\n    action_values = np.array([np.mean(scores) if len(scores) > 0 else 0 for scores in score_set.values()])\n    \n    # Calculate count of selections for each action\n    action_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Calculate exploration bonuses using UCB\n    ucb_values = np.zeros(action_count)\n    for i in range(action_count):\n        if action_counts[i] > 0:\n            ucb_values[i] = (action_values[i] + \n                             np.sqrt((2 * np.log(total_selection_count + 1)) / action_counts[i]))\n        else:\n            ucb_values[i] = float('inf')  # Encourage exploration of unselected actions\n    \n    # Temporal decay factor\n    time_factor = (total_time_slots - current_time_slot) / total_time_slots\n    \n    # Adjust values with time factor\n    final_values = action_values + exploration_ratio * ucb_values * time_factor\n    \n    # Select the action with the maximum adjusted value\n    action_index = np.argmax(final_values)\n    \n    return action_index",
          "objective": -449.99998763653537,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    \n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Compute average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Epsilon-greedy exploration factor\n    exploration_factor = 0.1 * (1 - current_time_slot / (total_time_slots + 1))\n\n    if np.random.rand() < exploration_factor or total_selection_count == 0:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        # Calculate selection scores with UCB\n        total_counts = total_selection_count + 1  # To avoid division by zero\n        confidence_bounds = np.sqrt((2 * np.log(total_counts)) / (selection_counts + 1e-5))\n        selection_scores = average_scores + confidence_bounds\n        action_index = action_indices[np.argmax(selection_scores)]  # Exploit\n\n    return action_index",
          "objective": -449.99998677520915,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    \n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Compute average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Dynamic exploration factor\n    exploration_rate = 0.1 * (1 - current_time_slot / total_time_slots)\n\n    # Determine action to select\n    if np.random.rand() < exploration_rate or total_selection_count == 0:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        # Calculate selection scores using a modified UCB approach\n        with np.errstate(divide='ignore', invalid='ignore'):\n            confidence_bounds = np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n        selection_scores = average_scores + confidence_bounds\n\n        action_index = action_indices[np.argmax(selection_scores)]  # Exploit\n\n    return action_index",
          "objective": -449.99998628488834,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Compute average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Calculating the exploration rate based on time slots\n    exploration_rate = 0.2 * (1 - (current_time_slot / total_time_slots)) if total_time_slots > 0 else 0.2\n\n    # Softmax function for action selection probabilities\n    def softmax(values, temperature=1.0):\n        exp_values = np.exp(values / temperature)\n        return exp_values / np.sum(exp_values)\n\n    # Selection logic\n    if np.random.rand() < exploration_rate or total_selection_count == 0:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        # Calculate adjusted scores for exploitation\n        adjusted_scores = average_scores + np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n        action_probs = softmax(adjusted_scores)  # Compute probabilities using softmax\n        action_index = np.random.choice(action_indices, p=action_probs)  # Exploit based on probabilities\n\n    return action_index",
          "objective": -449.9999846685724,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Configure exploration parameters\n    epsilon = 0.1  # Exploration parameter\n    exploration_weight = np.sqrt((np.log(total_selection_count + 1) / (np.array([len(scores) for scores in score_set.values()]) + 1)))\n    \n    # Calculate average scores for each action\n    action_values = np.array([np.mean(scores) if len(scores) > 0 else 0 for scores in score_set.values()])\n    \n    # Calculate the time-based exploration factor\n    time_factor = (total_time_slots - current_time_slot) / total_time_slots\n    \n    # Combine scores with exploration adjustments\n    final_values = action_values + epsilon * exploration_weight * time_factor\n    \n    # Select the action with the maximum final value\n    action_index = np.argmax(final_values)\n    \n    return action_index",
          "objective": -449.99998457468377,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    \n    # Initialize arrays for cumulative scores and selection counts\n    cumulative_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    # Compute cumulative scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        cumulative_scores[idx] = np.sum(scores)\n    \n    # Calculate average scores\n    average_scores = np.divide(cumulative_scores, selection_counts, out=np.zeros_like(cumulative_scores), where=selection_counts!=0)\n\n    # Dynamic epsilon for exploration\n    epsilon = 1.0 / (current_time_slot + 1)\n\n    # Softmax probabilities for actions based on average scores\n    exp_scores = np.exp(average_scores / (1 + epsilon))\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Draw action based on weighted probabilities\n    action_index = np.random.choice(action_indices, p=probabilities)\n    \n    return action_index",
          "objective": -449.99997714777703,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    \n    # Initialize average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Define epsilon for exploration, decaying over time\n    base_epsilon = 0.1\n    exploration_rate = base_epsilon * (1 - current_time_slot / total_time_slots)\n    \n    # Choose action based on exploration-exploitation strategy\n    if np.random.rand() < exploration_rate or total_selection_count == 0:\n        action_index = np.random.choice(action_indices)\n    else:\n        confidence_bounds = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n        selection_scores = average_scores + confidence_bounds\n        \n        # Softmax selection for a smoother balance between exploration and exploitation\n        exp_scores = np.exp(selection_scores - np.max(selection_scores))  # For numerical stability\n        probabilities = exp_scores / np.sum(exp_scores)\n        action_index = np.random.choice(action_indices, p=probabilities)\n    \n    return action_index",
          "objective": -449.99997575574673,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    \n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Compute average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Define the exploration probability with a decay factor\n    exploration_probability = 1.0 - (current_time_slot / total_time_slots)\n    epsilon = max(0.1, exploration_probability)  # Ensure a minimum exploration rate\n\n    # Epsilon-greedy approach\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        # Exploit: Select action with highest average score\n        action_index = action_indices[np.argmax(average_scores)]\n\n    return action_index",
          "objective": -449.99997374026543,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Define exploration parameters\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)  # Decrease exploration with time\n    action_indices = list(score_set.keys())\n\n    # Calculate average scores and selection counts for UCB\n    average_scores = {}\n    counts = {}\n    \n    for action in action_indices:\n        scores = score_set[action]\n        counts[action] = len(scores)\n        average_scores[action] = np.mean(scores) if scores else 0.0\n\n    # UCB calculation\n    ucb_values = {}\n    for action in action_indices:\n        if counts[action] == 0:\n            ucb_values[action] = float('inf')  # Explore unselected actions\n        else:\n            ucb_values[action] = average_scores[action] + np.sqrt(2 * np.log(total_selection_count) / counts[action])\n\n    # Epsilon-greedy strategy with UCB consideration\n    if np.random.rand() < epsilon or total_selection_count == 0:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = max(ucb_values, key=ucb_values.get)\n\n    return action_index",
          "objective": -449.99996586701786,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = len(score_set)\n    action_scores = np.zeros(action_count)\n    \n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        avg_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n        \n        # Epsilon-greedy strategy: exploration parameter\n        epsilon = 1 / (current_time_slot + 1)\n        exploration_factor = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else 1\n\n        # Combining exploration and exploitation\n        action_scores[action_index] = avg_score + (epsilon * exploration_factor)\n    \n    # Select action with the highest identified score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.9999587691761,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    \n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    # Compute average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Dynamic exploration factor using a decaying epsilon\n    exploration_probability = max(0.1, 1 - (current_time_slot / total_time_slots))\n    \n    if np.random.rand() < exploration_probability or total_selection_count == 0:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        # Use Upper Confidence Bound (UCB) for selection\n        with np.errstate(divide='ignore', invalid='ignore'):\n            confidence_bounds = np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n            selection_scores = average_scores + confidence_bounds\n            \n            # Handling for actions never selected\n            selection_scores[selection_counts == 0] = float('inf')\n            action_index = action_indices[np.argmax(selection_scores)]  # Exploit\n    \n    return action_index",
          "objective": -449.9999558862508,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    \n    # Calculate total scores and selection counts\n    total_scores = np.zeros(len(action_indices))\n    selection_counts = np.zeros(len(action_indices))\n\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        total_scores[idx] = sum(scores)\n\n    # Calculate average scores\n    average_scores = np.divide(total_scores, selection_counts, out=np.zeros_like(total_scores), where=selection_counts != 0)\n\n    # Dynamic exploration rate\n    base_epsilon = 0.1\n    exploration_rate = base_epsilon * (1 - current_time_slot / total_time_slots)\n    \n    # Determine selection mechanism with exploration-exploitation trade-off\n    if np.random.rand() < exploration_rate or total_selection_count == 0:\n        # Explore: Select a random action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Enhanced exploitation: Use a Softmax distribution over the scores\n        exp_scores = np.exp(average_scores - np.max(average_scores))  # Avoid overflow\n        probabilities = exp_scores / np.sum(exp_scores)\n        \n        # Sample an action based on the calculated probabilities\n        action_index = np.random.choice(action_indices, p=probabilities)\n\n    return action_index",
          "objective": -449.9999548027301,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    \n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Compute average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Dynamic exploration factor\n    exploration_factor = 1 / (1 + np.exp(-5 * (0.5 - (current_time_slot / total_time_slots))))\n    \n    # Epsilon value for exploration (decreases over time)\n    epsilon = 0.1 * exploration_factor\n    \n    if np.random.rand() < epsilon or total_selection_count == 0:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        # Calculate rewards with UCB\n        confidence_bounds = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n        selection_scores = average_scores + confidence_bounds\n        action_index = action_indices[np.argmax(selection_scores)]  # Exploit\n\n    return action_index",
          "objective": -449.9999477422175,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Hyperparameter for exploration versus exploitation\n    exploration_factor = np.sqrt(np.log(total_selection_count + 1) / (np.array([len(scores) for scores in score_set.values()]) + 1))\n\n    # Calculate average scores for each action\n    average_scores = np.array([np.mean(scores) if scores else 0 for scores in score_set.values()])\n\n    # Dynamic adjustment based on current time slot\n    decay_factor = 1 - (current_time_slot / total_time_slots)\n    \n    # Final values combining average scores and exploration bonus\n    final_values = average_scores * decay_factor + exploration_factor\n\n    # Select the action with the max final_value\n    action_index = np.argmax(final_values)\n    \n    return action_index",
          "objective": -449.9999470395035,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1 * (1 - current_time_slot / total_time_slots)  # Decreasing exploration rate\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # If there are no selections, explore all actions evenly\n    if total_selection_count == 0:\n        return np.random.choice(action_indices)\n\n    # Calculate the UCB values\n    ucb_values = average_scores + np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n\n    # Select action based on epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -449.9999300493543,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    \n    # Calculate average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Calculate epsilon based on the current time slot\n    exploration_fraction = (1 - (current_time_slot / total_time_slots)) * 0.1\n    \n    if np.random.rand() < exploration_fraction or total_selection_count == 0:\n        # Explore: Select a random action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploit: Use a weighted approach combining average score and selection count\n        ucb_scores = average_scores + np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n        selection_probabilities = ucb_scores / np.sum(ucb_scores)\n        \n        # Choose an action based on the computed probabilities\n        action_index = np.random.choice(action_indices, p=selection_probabilities)\n\n    return action_index",
          "objective": -449.9999300354203,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    \n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Dynamic exploration factor using the decay of epsilon\n    base_epsilon = 0.1\n    exploration_factor = base_epsilon * (1 - current_time_slot / total_time_slots)\n\n    if np.random.rand() < exploration_factor or total_selection_count == 0:\n        # Explore: Select a random action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploit: Use Upper Confidence Bound (UCB) with a modifier for exploration\n        confidence_bounds = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n        selection_scores = average_scores + confidence_bounds\n\n        # Select the action with the highest selection score\n        action_index = action_indices[np.argmax(selection_scores)]\n\n    return action_index",
          "objective": -449.99992010605854,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Constants for the strategy\n    epsilon = 0.1  # Exploration rate\n    action_indices = list(score_set.keys())\n    \n    # Initialize average scores and counts\n    average_scores = np.zeros(len(action_indices))\n    selection_counts = np.zeros(len(action_indices))\n    \n    for action in action_indices:\n        scores = score_set[action]\n        selection_counts[action] = len(scores)\n        if selection_counts[action] > 0:\n            average_scores[action] = np.mean(scores)\n\n    # Use Softmax for action selection\n    scaled_scores = average_scores / (np.max(average_scores) + 1e-5)  # Scale to avoid overflow\n    exp_scores = np.exp(scaled_scores)\n    softmax_probs = exp_scores / np.sum(exp_scores)\n    \n    # Epsilon-greedy logic\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = np.random.choice(action_indices, p=softmax_probs)\n    \n    return action_index",
          "objective": -449.999873974391,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon = 0.1  # Exploration factor\n    action_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        avg_score = np.mean(scores) if selection_counts[action_index] > 0 else 0\n        action_scores[action_index] = avg_score\n\n    # Epsilon-greedy exploration\n    if np.random.rand() < epsilon:\n        return np.random.randint(num_actions)\n    \n    # Calculate adjusted scores with a temporal component\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-10))\n    total_scores = action_scores + exploration_bonus * (1.0 / (current_time_slot + 1))\n    \n    action_index = np.argmax(total_scores)\n    return action_index",
          "objective": -449.99985161998853,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Dynamic parameters\n    epsilon = 0.1  # Exploration rate\n    action_indices = list(score_set.keys())\n    \n    # Calculate average scores and selection counts\n    average_scores = {}\n    selection_counts = {}\n    \n    for action in action_indices:\n        scores = score_set[action]\n        if scores:\n            average_scores[action] = np.mean(scores)\n            selection_counts[action] = len(scores)\n        else:\n            average_scores[action] = 0.0\n            selection_counts[action] = 0\n    \n    # Dynamic epsilon-greedy strategy with more exploration early in the time slots\n    dynamic_epsilon = epsilon * (1 - current_time_slot / total_time_slots)\n    \n    if np.random.rand() < dynamic_epsilon or total_selection_count == 0:\n        # Explore: Select a random action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploit: Calculate the selection score considering exploration\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (np.array(list(selection_counts.values())) + 1e-5))\n        selection_scores = np.array(list(average_scores.values())) + exploration_bonus\n        \n        # Select the action with the highest selection score\n        action_index = action_indices[np.argmax(selection_scores)]\n    \n    return action_index",
          "objective": -449.9998477274685,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Dynamic parameters\n    base_epsilon = 0.1  # Base exploration rate\n    action_indices = list(score_set.keys())\n\n    # Calculate average scores and selection counts\n    average_scores = np.zeros(len(action_indices))\n    selection_counts = np.zeros(len(action_indices))\n\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Dynamic epsilon based on time slot\n    exploration_factor = base_epsilon * (1 - current_time_slot / total_time_slots)\n    \n    if np.random.rand() < exploration_factor or total_selection_count == 0:\n        # Explore: Select a random action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploit: Use Upper Confidence Bound (UCB)\n        confidence_bounds = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n        selection_scores = average_scores + confidence_bounds\n        \n        # Select the action with the highest selection score\n        action_index = action_indices[np.argmax(selection_scores)]\n\n    return action_index",
          "objective": -449.9997959864879,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    \n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Compute average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        selection_counts[idx] = len(scores)\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n\n    # Dynamic exploration factor based on time slot and epsilon-greedy approach\n    exploration_factor = 0.1 * (1 - current_time_slot / total_time_slots)\n    \n    if np.random.rand() < exploration_factor or total_selection_count == 0:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        # Using a modified UCB approach for exploitation\n        confidence_bounds = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-6))\n        selection_scores = average_scores + confidence_bounds\n        action_index = action_indices[np.argmax(selection_scores)]  # Exploit\n\n    return action_index",
          "objective": -449.9997957530901,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration rate\n    action_indices = list(score_set.keys())\n    \n    # Calculate average scores for each action\n    average_scores = {}\n    for action in action_indices:\n        scores = score_set[action]\n        if scores:\n            average_scores[action] = np.mean(scores)\n        else:\n            average_scores[action] = 0.0\n    \n    # Epsilon-greedy strategy\n    if np.random.rand() < epsilon or total_selection_count == 0:\n        # Explore: Select a random action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploit: Select the action with the highest average score\n        action_index = max(average_scores, key=average_scores.get)\n    \n    return action_index",
          "objective": -449.9997396165615,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Dynamic parameters\n    epsilon = 0.1\n    action_indices = list(score_set.keys())\n    \n    # Calculate average scores and selection counts\n    average_scores = {}\n    selection_counts = {}\n    \n    for action in action_indices:\n        scores = score_set[action]\n        if scores:\n            average_scores[action] = np.mean(scores)\n            selection_counts[action] = len(scores)\n        else:\n            average_scores[action] = 0.0\n            selection_counts[action] = 0\n    \n    # Dynamic epsilon-greedy strategy\n    dynamic_epsilon = epsilon * (1 - current_time_slot / total_time_slots)\n    \n    if np.random.rand() < dynamic_epsilon or total_selection_count == 0:\n        # Explore: Select a random action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Calculate Upper Confidence Bound (UCB) scores\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (np.array(list(selection_counts.values())) + 1e-5))\n        # UCB scores combining average scores and exploration bonus\n        ucb_scores = np.array(list(average_scores.values())) + exploration_bonus\n        \n        # Select the action with the highest UCB score\n        action_index = action_indices[np.argmax(ucb_scores)]\n    \n    return action_index",
          "objective": -449.99971836444774,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration rate\n    temperature = 0.5  # Temperature scaling factor for softmax exploration\n\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        average_scores[idx] = np.mean(scores) if scores else 0.0\n        selection_counts[idx] = len(scores)\n\n    if total_selection_count == 0:\n        return np.random.choice(action_indices)\n\n    # Calculate UCB values\n    confidence_bounds = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n    ucb_values = average_scores + confidence_bounds\n\n    # Softmax probabilities for exploration\n    softmax_scores = np.exp(ucb_values / temperature)\n    probabilities = softmax_scores / np.sum(softmax_scores)\n\n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices, p=probabilities)\n    else:\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -449.99971766170086,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # If no selections have been made, select randomly to encourage exploration\n    if total_selection_count == 0:\n        return np.random.randint(0, 8)\n    \n    action_scores = []\n    exploration_coefficient = 1.0 / (current_time_slot + 1)\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        avg_score = np.mean(scores) if scores else 0\n        \n        selection_count = len(scores)\n        # Use the total selection count to calculate a baseline for exploration\n        if selection_count > 0:\n            # Upper Confidence Bound model implementation\n            exploration_bonus = exploration_coefficient * np.sqrt(np.log(total_selection_count) / selection_count)\n        else:\n            exploration_bonus = exploration_coefficient  # Encourage exploration for unselected actions\n\n        action_scores.append(avg_score + exploration_bonus)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    \n    # Initialize average scores and selection counts\n    average_scores = np.zeros(len(action_indices))\n    selection_counts = np.zeros(len(action_indices))\n\n    for idx, action in enumerate(action_indices):\n        if score_set[action]:\n            average_scores[idx] = np.mean(score_set[action])\n            selection_counts[idx] = len(score_set[action])\n    \n    # Softmax function for action probabilities\n    def softmax(x):\n        exp_x = np.exp(x - np.max(x))  # stability improvement\n        return exp_x / np.sum(exp_x)\n\n    # Dynamic epsilon\n    epsilon = 0.1 * (1 - current_time_slot / total_time_slots)\n\n    if np.random.rand() < epsilon or total_selection_count == 0:\n        action_index = np.random.choice(action_indices)\n    else:\n        # Calculate Softmax probabilities for actions based on average scores\n        probabilities = softmax(average_scores)\n        action_index = np.random.choice(action_indices, p=probabilities)\n\n    return action_index",
          "objective": -449.99951843233583,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Adaptive exploration rate that decreases over time\n    epsilon = max(0.1 * (total_time_slots - current_time_slot) / total_time_slots, 0.01)\n    action_indices = list(score_set.keys())\n    \n    # Calculate average scores and counts for each action\n    average_scores = np.zeros(len(action_indices))\n    selection_counts = np.zeros(len(action_indices))\n    \n    for action in action_indices:\n        scores = score_set[action]\n        selection_counts[action] = len(scores)\n        average_scores[action] = np.mean(scores) if scores else 0.0\n    \n    # Use Softmax to calculate action probabilities\n    exp_scores = np.exp(average_scores - np.max(average_scores))  # For numerical stability\n    action_probabilities = exp_scores / np.sum(exp_scores)\n\n    # Epsilon-greedy-like exploration\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = np.random.choice(action_indices, p=action_probabilities)\n\n    return action_index",
          "objective": -449.99941154996566,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Dynamic parameters\n    epsilon = 0.1\n    action_indices = list(score_set.keys())\n    \n    # Calculate average scores and selection counts\n    average_scores = np.zeros(len(action_indices))\n    selection_counts = np.zeros(len(action_indices))\n    \n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        if scores:\n            average_scores[idx] = np.mean(scores)\n            selection_counts[idx] = len(scores)\n    \n    # Dynamic epsilon-greedy strategy\n    dynamic_epsilon = epsilon * (1 - current_time_slot / total_time_slots)\n\n    if np.random.rand() < dynamic_epsilon or total_selection_count == 0:\n        # Explore: Select a random action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Calculate Upper Confidence Bound (UCB) scores\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n        ucb_scores = average_scores + exploration_bonus\n        \n        # Handle actions that have never been selected\n        ucb_scores[selection_counts == 0] = float('inf')\n        \n        # Select the action with the highest UCB score\n        action_index = action_indices[np.argmax(ucb_scores)]\n    \n    return action_index",
          "objective": -449.99939288908143,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8  # Number of actions\n    action_values = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n\n    # Calculate average scores and counts for actions\n    for action in range(action_count):\n        scores = score_set.get(action, [])\n        selection_counts[action] = len(scores)\n        action_values[action] = np.mean(scores) if selection_counts[action] > 0 else 0\n\n    # Epsilon-greedy strategy parameter\n    epsilon = max(0.1, 1.0 / (current_time_slot + 1))  # Lower bound for exploration\n\n    # Upper Confidence Bound (UCB) calculation\n    exploration_bonus = np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n    \n    # Combining exploration and exploitation\n    combined_values = action_values + exploration_bonus\n\n    # Select an action based on epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(action_count)\n    else:\n        action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": -449.9991362182059,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialization\n    action_count = len(score_set)\n    action_values = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n\n    # Calculate average scores and selection counts\n    for action in range(action_count):\n        scores = score_set[action]\n        action_counts = len(scores)\n        selection_counts[action] = action_counts\n        if action_counts > 0:\n            action_values[action] = np.mean(scores)\n    \n    # Epsilon-greedy strategy parameter\n    epsilon = 1 / (current_time_slot + 1)\n    \n    # Exploration component (UCB)\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n    \n    # Combining exploration and exploitation\n    final_values = (1 - epsilon) * action_values + epsilon * exploration_bonus\n    \n    # Random choice for exploration\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(action_count)\n    else:\n        action_index = np.argmax(final_values)\n    \n    return action_index",
          "objective": -449.9990145861755,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 1.0 / (current_time_slot + 1)  # Decaying exploration factor\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        avg_score = np.mean(scores) if scores else 0  # Handle empty score cases\n        selection_count = len(scores)\n        exploration_bonus = epsilon * (1 - (selection_count / (total_selection_count + 1))) if total_selection_count > 0 else 1.0\n        action_scores.append(avg_score + exploration_bonus)\n    \n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99592823154046,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8  # Number of actions (0 to 7)\n    action_values = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n\n    # Calculate average scores and counts for actions\n    for action in range(action_count):\n        scores = score_set.get(action, [])\n        action_counts = len(scores)\n        selection_counts[action] = action_counts\n        action_values[action] = np.mean(scores) if action_counts > 0 else 0\n\n    # Epsilon-greedy strategy parameter\n    epsilon = 1.0 / (current_time_slot + 1)\n\n    # Exploration component (UCB)\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n\n    # Combining exploration (UCB) and exploitation (average scores)\n    combined_values = (1 - epsilon) * action_values + epsilon * exploration_bonus\n\n    # Select an action\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(action_count)\n    else:\n        action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": -449.9931313195814,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = len(score_set)\n    exploration_factor = np.log(total_selection_count + 1) / (current_time_slot + 1) if total_selection_count > 0 else 1.0\n    action_scores = np.zeros(action_count)\n    \n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        avg_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n        \n        exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else exploration_factor\n        action_scores[action_index] = avg_score + exploration_bonus\n    \n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.9929285976704,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    if total_selection_count == 0:\n        return np.random.randint(0, 8)  # Random selection at the start\n\n    action_scores = []\n    exploration_factor = 1.0 / (current_time_slot + 1)\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        avg_score = np.mean(scores) if scores else 0\n        \n        # Calculating exploration bonus based on selection count\n        selection_count = len(scores)\n        if selection_count > 0:\n            exploration_bonus = exploration_factor * (1 - (selection_count / total_selection_count))\n        else:\n            exploration_bonus = exploration_factor  # Encourage exploration for unselected actions\n\n        action_scores.append(avg_score + exploration_bonus)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.9791010893257,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = len(score_set)\n    action_values = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n    \n    for action in range(action_count):\n        scores = score_set[action]\n        action_counts = len(scores)\n        selection_counts[action] = action_counts\n        if action_counts > 0:\n            action_values[action] = np.mean(scores)\n    \n    epsilon = 1 / (current_time_slot + 1)\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n    \n    final_values = (1 - epsilon) * action_values + epsilon * exploration_bonus\n    \n    if current_time_slot < total_time_slots / 10:  # Early exploration phase\n        action_index = np.random.randint(action_count)\n    else:\n        action_index = np.argmax(final_values)\n    \n    return action_index",
          "objective": -449.97338830681724,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialization\n    action_count = len(score_set)\n    action_values = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n\n    # Calculate average scores and selection counts\n    for action in range(action_count):\n        scores = score_set[action]\n        selection_counts[action] = len(scores)\n        if selection_counts[action] > 0:\n            action_values[action] = np.mean(scores)\n\n    # Epsilon parameter for exploration\n    epsilon = 1 / (current_time_slot + 1)  # Decaying exploration\n    exploration_bonus = np.random.binomial(1, epsilon, action_count)\n\n    # Final selection score combining exploitation and exploration\n    final_values = action_values + exploration_bonus\n\n    # Select action with highest score\n    action_index = np.argmax(final_values)\n\n    return action_index",
          "objective": -449.9335105264816,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = len(score_set)\n    action_values = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n\n    # Calculate average scores and selection counts\n    for action in range(action_count):\n        scores = score_set[action]\n        action_counts = len(scores)\n        selection_counts[action] = action_counts\n        if action_counts > 0:\n            action_values[action] = np.mean(scores)\n\n    # Dynamic epsilon for exploration\n    epsilon = max(0.1, min(1, 1 - (current_time_slot / total_time_slots)))\n\n    # Exploration bonus using UCB\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n\n    # Final value computation\n    final_values = action_values + exploration_bonus\n\n    # Choose action based on epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(action_count)\n    else:\n        action_index = np.argmax(final_values)\n    \n    return action_index",
          "objective": -449.93290707418794,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = len(score_set)\n    action_values = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n\n    # Calculate average scores and selection counts\n    for action in range(action_count):\n        scores = score_set[action]\n        action_counts = len(scores)\n        selection_counts[action] = action_counts\n        if action_counts > 0:\n            action_values[action] = np.mean(scores)\n\n    # Epsilon parameter for exploration\n    epsilon = 1 / (current_time_slot + 1)\n\n    # Incorporate UCB for exploration\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n\n    # Combine exploration and exploitation using Softmax\n    combined_values = action_values + exploration_bonus\n    exp_values = np.exp(combined_values - np.max(combined_values))\n    probabilities = exp_values / np.sum(exp_values)\n\n    # Perform epsilon-greedy strategy for exploration\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_count, p=[1/action_count] * action_count)\n    else:\n        action_index = np.random.choice(action_count, p=probabilities)\n\n    return action_index",
          "objective": -449.89434041159063,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    if total_selection_count == 0:\n        return np.random.randint(0, 8)  # Random selection at the start\n\n    action_scores = []\n    exploration_factor = 1.0 / (current_time_slot + 1)  # Decay exploration over time\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        avg_score = np.mean(scores) if scores else 0.0\n        \n        selection_count = len(scores)\n        if selection_count > 0:\n            # Upper Confidence Bound (UCB)\n            ucb_score = avg_score + np.sqrt((2 * np.log(total_selection_count)) / selection_count)\n        else:\n            ucb_score = avg_score + exploration_factor  # Prioritize unexplored actions\n\n        action_scores.append(ucb_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.7607463396896,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = len(score_set)\n    action_values = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n\n    # Calculate average scores and selection counts\n    for action in range(action_count):\n        scores = score_set[action]\n        action_counts = len(scores)\n        selection_counts[action] = action_counts\n        if action_counts > 0:\n            action_values[action] = np.mean(scores)\n\n    # Epsilon decay strategy\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n\n    # Calculating UCB with safety against division by zero\n    ucb = action_values + np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n\n    # Combine exploration (UCB) and exploitation (average scores)\n    adjusted_values = (1 - epsilon) * action_values + epsilon * ucb\n\n    # Softmax for selection\n    exp_values = np.exp(adjusted_values - np.max(adjusted_values))\n    probabilities = exp_values / np.sum(exp_values)\n\n    # Epsilon-greedy strategy for exploration\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_count)\n    else:\n        action_index = np.random.choice(action_count, p=probabilities)\n\n    return action_index",
          "objective": -449.5603476241091,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = len(score_set)\n    action_values = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n\n    # Calculate average scores and selection counts\n    for action in range(action_count):\n        scores = score_set[action]\n        action_counts = len(scores)\n        selection_counts[action] = action_counts\n        if action_counts > 0:\n            action_values[action] = np.mean(scores)\n\n    # Dynamic epsilon for exploration\n    epsilon = max(0.1, min(1, 1 - (current_time_slot / total_time_slots)))\n\n    # UCB exploration bonus\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n\n    # Calculate final values for actions\n    final_values = action_values + exploration_bonus\n\n    # Epsilon-greedy action selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_count)\n    else:\n        action_index = np.argmax(final_values)\n    \n    return action_index",
          "objective": -447.99639354093955,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Set exploration parameters\n    exploration_factor = 1.0  # Constant to adjust exploration abundance\n    action_indices = list(score_set.keys())\n    \n    # Calculate average scores and selection counts for each action\n    average_scores = {}\n    selection_counts = {}\n    for action in action_indices:\n        scores = score_set[action]\n        selection_counts[action] = len(scores)\n        if scores:\n            average_scores[action] = np.mean(scores)\n        else:\n            average_scores[action] = 0.0\n\n    # Upper Confidence Bound (UCB) calculation\n    ucb_values = {}\n    for action in action_indices:\n        if selection_counts[action] == 0:\n            ucb_values[action] = float('inf')  # Prioritize unselected actions\n        else:\n            exploration_term = exploration_factor * np.sqrt(np.log(total_selection_count) / selection_counts[action])\n            ucb_values[action] = average_scores[action] + exploration_term\n    \n    # Select the action with the highest UCB value\n    action_index = max(ucb_values, key=ucb_values.get)\n    \n    return action_index",
          "objective": -447.97261237835625,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = len(score_set)  # Assuming there are 8 actions (0 to 7)\n    average_scores = np.zeros(n_actions)\n    exploration_bonus = np.zeros(n_actions)\n\n    for action_index in range(n_actions):\n        scores = score_set[action_index]\n        if len(scores) > 0:\n            average_scores[action_index] = np.mean(scores)\n            exploration_bonus[action_index] = np.sqrt(np.log(total_selection_count + 1) / (len(scores) + 1))\n        else:\n            # If an action has never been selected, set its avg score to a low value and give it exploration bonus\n            average_scores[action_index] = -1  # Arbitrarily low score\n            exploration_bonus[action_index] = np.sqrt(np.log(total_selection_count + 1))  # Bonus for trying new actions\n\n    # Calculate selection value\n    selection_values = average_scores + exploration_bonus\n\n    # Introduce a decay factor based on current time slot\n    decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n    selection_values *= decay_factor\n\n    # Select the action with the highest calculated selection value\n    action_index = np.argmax(selection_values)\n\n    return action_index",
          "objective": -447.34882693617783,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize lists for action scores and counts\n    action_scores = np.zeros(8)\n    action_counts = np.zeros(8, dtype=int)\n    \n    # Calculate average scores and counts\n    for action_index, scores in score_set.items():\n        action_scores[action_index] = np.mean(scores) if scores else 0.0\n        action_counts[action_index] = len(scores)\n\n    # Parameter for epsilon-greedy strategy\n    epsilon = 0.1\n\n    # Avoid division by zero and calculate UCB values\n    ucb_values = np.zeros(8)\n    exploration_factor = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1))\n\n    # Calculate UCB values\n    for i in range(8):\n        if action_counts[i] > 0:\n            ucb_values[i] = action_scores[i] + exploration_factor[i]\n        else:  # For unselected actions, set a high exploration value\n            ucb_values[i] = 1e6  # Arbitrarily high value for exploration\n\n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(np.arange(8))  # Random selection\n    else:\n        action_index = np.argmax(ucb_values)  # Select action with highest UCB value\n    \n    return action_index",
          "objective": -445.67168755871535,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = len(score_set)\n    action_values = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n\n    for action in range(action_count):\n        scores = score_set[action]\n        selection_counts[action] = len(scores)\n        if selection_counts[action] > 0:\n            action_values[action] = np.mean(scores)\n\n    # Handle exploration using Upper Confidence Bound (UCB)\n    exploration_bonus = np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1))\n    \n    # Final values combining exploitation and exploration\n    final_values = action_values + exploration_bonus\n    \n    # Select the action with the max final_value\n    action_index = np.argmax(final_values)\n    \n    return action_index",
          "objective": -444.01931269244125,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions, dtype=int)\n    \n    # Calculate average scores and counts\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        action_scores[action_index] = np.mean(scores) if scores else 0.0\n\n    # Softmax-based exploration found to be more effective for balancing exploration and exploitation\n    temperature = 1.0\n    adjusted_scores = action_scores + np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1))\n    exp_scores = np.exp(adjusted_scores / temperature)\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Use probabilities to select an action\n    action_index = np.random.choice(np.arange(num_actions), p=probabilities)\n    \n    return action_index",
          "objective": -443.3532360008473,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialization\n    action_count = len(score_set)\n    action_values = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n\n    # Calculate average scores and selection counts\n    for action in range(action_count):\n        scores = score_set[action]\n        action_counts = len(scores)\n        selection_counts[action] = action_counts\n        if action_counts > 0:\n            action_values[action] = np.mean(scores)\n    \n    # Exploration parameter (Upper Confidence Bound)\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n    \n    # Final selection score combining exploitation and exploration\n    final_values = action_values + exploration_bonus\n\n    # Select action with highest score\n    action_index = np.argmax(final_values)\n    \n    return action_index",
          "objective": -441.3195442856099,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n\n    # Calculate average scores, avoiding division by zero\n    average_scores = np.array([np.mean(score_set[action]) if score_set[action] else 0.0 for action in action_indices])\n    selection_counts = np.array([len(score_set[action]) for action in action_indices])\n    \n    # Handle cases with zero selections\n    if total_selection_count == 0:\n        return np.random.choice(action_indices)\n    \n    # Epsilon-greedy with decay strategy\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01)  # Decay epsilon over time\n    if np.random.rand() < epsilon:\n        return np.random.choice(action_indices)\n\n    # Calculate UCB values\n    exploration_bonus = np.sqrt(np.log(total_selection_count) / (selection_counts + 1))\n    ucb_values = average_scores + exploration_bonus\n    \n    # Select action with the highest UCB\n    action_index = action_indices[np.argmax(ucb_values)]\n    \n    return action_index",
          "objective": -435.8447661600394,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize lists to hold action scores and selection counts\n    action_scores = np.zeros(8)\n    action_counts = np.zeros(8, dtype=int)\n    \n    # Calculate average scores and selection counts for each action\n    for action_index, scores in score_set.items():\n        action_scores[action_index] = np.mean(scores) if scores else 0.0\n        action_counts[action_index] = len(scores)\n\n    # Parameter for epsilon-greedy exploration\n    epsilon = 0.1\n    exploration_weight = (np.log(total_selection_count + 1) / (action_counts + 1)) ** 0.5\n    ucb_values = action_scores + exploration_weight\n\n    # Adjust for exploration based on epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(np.where(action_counts == 0)[0]) if np.any(action_counts == 0) else np.random.randint(8)\n    else:\n        action_index = np.argmax(ucb_values)\n        \n    return action_index",
          "objective": -433.7568949741375,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions, dtype=int)\n    \n    # Calculate average scores and selection counts for each action\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        action_scores[action_index] = np.mean(scores) if scores else 0.0\n\n    # Alpha for Bayesian adjustment (can be tuned)\n    alpha = 1.0\n    beta = 1.0\n    \n    # Perform a simple Thompson Sampling approach\n    sampled_theta = np.random.beta(action_scores + alpha, (action_counts - action_scores) + beta)\n    \n    # Use UCB for exploration bonus scaled by time remaining\n    time_factor = (total_time_slots - current_time_slot) / total_time_slots\n    exploration_bonus = (np.log(total_selection_count + 1) / (action_counts + 1)) ** 0.5\n    adjusted_values = sampled_theta + time_factor * exploration_bonus\n    \n    action_index = np.argmax(adjusted_values)\n    \n    return action_index",
          "objective": -431.8489038268177,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    if total_selection_count == 0:\n        return np.random.choice(list(score_set.keys()))\n    \n    action_indices = list(score_set.keys())\n    \n    # Calculate average scores and selection counts\n    average_scores = {}\n    action_count = {}\n    for action in action_indices:\n        scores = score_set[action]\n        action_count[action] = len(scores)\n        average_scores[action] = np.mean(scores) if scores else 0.0\n    \n    # Epsilon-greedy or UCB with exploration bonus\n    epsilon = 0.1\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (np.array(list(action_count.values())) + 1))\n    \n    ucb_values = np.array(list(average_scores.values())) + exploration_bonus\n    \n    if np.random.rand() < epsilon:\n        # Explore: Select a random action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploit: Select the action with the highest UCB\n        action_index = action_indices[np.argmax(ucb_values)]\n    \n    return action_index",
          "objective": -426.87681279710847,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8  # Number of actions (0 to 7)\n    action_values = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n\n    # Retrieve scores and counts for each action\n    for action in range(action_count):\n        scores = score_set.get(action, [])\n        selection_counts[action] = len(scores)\n        action_values[action] = np.mean(scores) if selection_counts[action] > 0 else 0\n\n    # Calculate Epsilon for exploration\n    epsilon = max(0.1, 1.0 - (current_time_slot / total_time_slots))  # Decaying exploration factor\n\n    # Exploration bonus using UCB\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n\n    # Combined values for actions: UCB and average scores\n    combined_values = (1 - epsilon) * action_values + epsilon * exploration_bonus\n\n    # Select action based on probabilities\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_count)\n    else:\n        action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": -426.1674817437678,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    \n    # Initialize average scores and counts\n    average_scores = np.zeros(len(action_indices))\n    action_count = np.zeros(len(action_indices))\n    \n    for i, action in enumerate(action_indices):\n        scores = score_set[action]\n        action_count[i] = len(scores)\n        average_scores[i] = np.mean(scores) if scores else 0.0\n\n    # Dynamic epsilon based on current_time_slot\n    epsilon = max(0.1 * (total_time_slots - current_time_slot) / total_time_slots, 0.01)\n    \n    # Upper Confidence Bound with exploration bonus\n    exploration_bonus = np.sqrt(2 * np.log(total_selection_count + 1) / (action_count + 1))\n    ucb_values = average_scores + exploration_bonus\n\n    if np.random.rand() < epsilon:\n        # Explore: Select a random action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploit: Select the action with the highest UCB\n        action_index = action_indices[np.argmax(ucb_values)]\n    \n    return action_index",
          "objective": -294.08305370463574,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters\n    exploration_factor = 0.1  # Base exploration rate\n    action_indices = list(score_set.keys())\n    \n    # Calculate average scores and selection counts\n    average_scores = {}\n    selection_counts = {}\n    \n    for action in action_indices:\n        scores = score_set[action]\n        selection_counts[action] = len(scores)\n        average_scores[action] = np.mean(scores) if scores else 0.0\n    \n    # Dynamic epsilon based on the current time slot and total time slots\n    epsilon = exploration_factor * (1 - (current_time_slot / total_time_slots))\n    \n    # Epsilon-greedy strategy\n    if np.random.rand() < epsilon or total_selection_count == 0:\n        # Explore: Select a random action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploit: Select the action with the highest upper confidence bound (UCB)\n        ucb_scores = {}\n        for action in action_indices:\n            if selection_counts[action] == 0:\n                ucb_scores[action] = float('inf')  # Encourage selection of actions not yet tried\n            else:\n                ucb_scores[action] = average_scores[action] + np.sqrt((2 * np.log(total_selection_count)) / selection_counts[action])\n        \n        action_index = max(ucb_scores, key=ucb_scores.get)\n    \n    return action_index",
          "objective": -62.095378979282884,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    average_scores = np.zeros(n_actions)\n    exploration_bonus = np.zeros(n_actions)\n\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n\n        # Calculate average score\n        if score_count > 0:\n            average_scores[action_index] = np.mean(scores)\n        else:\n            average_scores[action_index] = 0  # Default to 0 for unselected actions\n\n        # Calculate exploration bonus\n        if score_count > 0:\n            exploration_bonus[action_index] = np.sqrt(np.log(total_selection_count) / score_count)\n        else:\n            exploration_bonus[action_index] = np.sqrt(np.log(total_selection_count + 1))  # Encourage new actions\n\n    # Apply a decay factor based on current time slot\n    decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n    selection_values = average_scores + exploration_bonus * decay_factor\n\n    # Select the action with the maximum selection value\n    action_index = np.argmax(selection_values)\n\n    return action_index",
          "objective": -51.64723935611681,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = np.sqrt(np.log(total_time_slots) / (1 + np.array([len(score_set[i]) for i in range(8)])))\n    \n    for i in range(8):\n        if len(score_set[i]) == 0:\n            avg_score = 0  # Action has never been selected, treat as score 0\n        else:\n            avg_score = np.mean(score_set[i])\n        \n        action_scores.append(avg_score + exploration_factor[i] if total_selection_count > 0 else 1)  # Add exploration bonus\n\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": 6643.255527255505,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize lists to hold action scores and selection counts\n    action_scores = []\n    action_counts = []\n    \n    # Calculate average scores and selection counts for each action\n    for action_index in range(8):\n        if action_index in score_set:\n            scores = score_set[action_index]\n            avg_score = np.mean(scores) if scores else 0.0\n            action_scores.append(avg_score)\n            action_counts.append(len(scores))\n        else:\n            action_scores.append(0.0)\n            action_counts.append(0)\n\n    # Exploration term (can be adjusted with a parameter)\n    exploration_factor = 1.0\n    \n    # Calculate UCB values\n    ucb_values = []\n    for i in range(8):\n        if action_counts[i] > 0:\n            ucb = action_scores[i] + exploration_factor * np.sqrt(np.log(total_selection_count) / action_counts[i])\n        else:\n            ucb = float('inf')  # Ensure unselected actions are prioritized\n        ucb_values.append(ucb)\n    \n    # Select action with maximum UCB\n    action_index = np.argmax(ucb_values)\n    \n    return action_index",
          "objective": 8711.158584985378,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions, dtype=int)\n    \n    # Calculate average scores and counts for each action\n    for action_index, scores in score_set.items():\n        if scores:\n            action_scores[action_index] = np.mean(scores)\n        action_counts[action_index] = len(scores)\n\n    # Define epsilon for exploration probability\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n    \n    # Calculate UCB values and incorporate action counts\n    exploration_weight = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n    ucb_values = action_scores + exploration_weight\n    \n    # Select action based on epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        # Randomly select from actions with zero selection counts to encourage exploration\n        if np.any(action_counts == 0):\n            action_index = np.random.choice(np.where(action_counts == 0)[0])\n        else:\n            action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(ucb_values)\n        \n    return action_index",
          "objective": 9536.19542615137,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    exploration_bonus = np.zeros(n_actions)\n\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n        else:\n            average_scores[action_index] = 0  # Using 0 for unselected actions\n    \n    # Calculate Exploration Bonus using UCB\n    for action_index in range(n_actions):\n        if selection_counts[action_index] > 0:\n            exploration_bonus[action_index] = np.sqrt(np.log(total_selection_count) / selection_counts[action_index])\n        else:\n            # New action exploration\n            exploration_bonus[action_index] = np.sqrt(np.log(total_selection_count + 1))\n\n    # Calculate selection value considering decay based on the current time slot\n    decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n    selection_values = average_scores + exploration_bonus\n    \n    # Weight selection values by the decay factor\n    selection_values *= decay_factor\n    \n    # Select the action with the highest value\n    action_index = np.argmax(selection_values)\n\n    return action_index",
          "objective": 28796.317676412735,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # exploration rate\n    n_actions = 8\n    action_index = 0\n\n    # Calculate the average scores for each action\n    average_scores = []\n    for action in range(n_actions):\n        scores = score_set.get(action, [])\n        if len(scores) > 0:\n            average_score = np.mean(scores)\n        else:\n            average_score = 0  # Action has not been selected yet\n        average_scores.append(average_score)\n    \n    # Add UCB exploration\n    ucb_values = []\n    for action in range(n_actions):\n        if total_selection_count == 0:  # Prioritize exploration if no selections\n            ucb_value = float('inf')  # Ensure exploration for unselected actions\n        else:\n            n_action_selected = len(score_set.get(action, []))\n            average_score = average_scores[action]\n            exploration_term = np.sqrt(np.log(total_selection_count) / (n_action_selected + 1e-5))\n            ucb_value = average_score + exploration_term\n        ucb_values.append(ucb_value)\n    \n    # Epsilon-greedy approach\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, n_actions)  # Explore\n    else:\n        action_index = np.argmax(ucb_values)  # Exploit\n\n    return action_index",
          "objective": 42315.788611315096,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize parameters\n    n_actions = 8\n    scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(n_actions):\n        action_scores = score_set.get(action_index, [])\n        scores[action_index] = np.mean(action_scores) if action_scores else 0.0\n        selection_counts[action_index] = len(action_scores)\n\n    # Upper Confidence Bound (UCB) calculation\n    ucb_values = np.zeros(n_actions)\n    for action_index in range(n_actions):\n        if selection_counts[action_index] == 0:\n            ucb_values[action_index] = float('inf')  # Encourage exploration for unselected actions\n        else:\n            confidence_bound = np.sqrt(np.log(total_selection_count) / selection_counts[action_index])\n            ucb_values[action_index] = scores[action_index] + confidence_bound\n\n    # Select action with the highest UCB value\n    action_index = np.argmax(ucb_values)\n    \n    return action_index",
          "objective": 46026.83642351608,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Calculate average scores and selection counts\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for action in action_indices:\n        scores = score_set[action]\n        selection_counts[action] = len(scores)\n        if selection_counts[action] > 0:\n            average_scores[action] = np.mean(scores)\n\n    # UCB-based exploration strategy\n    exploration_factor = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n    ucb_scores = average_scores + exploration_factor\n\n    # Select action with maximum UCB score\n    action_index = np.argmax(ucb_scores)\n\n    return action_index",
          "objective": 51775.80316009483,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = np.log((current_time_slot + 1) / (total_time_slots + 1))  # Log-scale exploration factor adjustment\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        avg_score = np.mean(scores) if scores else 0\n        \n        selection_count = len(scores)\n        uncertainty = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        action_value = avg_score + uncertainty * exploration_factor\n        action_scores.append(action_value)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 100019.38681116287,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    \n    # Calculate average scores and selection counts\n    for action_index in range(n_actions):\n        action_scores = score_set.get(action_index, [])\n        selection_count = len(action_scores)\n        selection_counts[action_index] = selection_count\n        scores[action_index] = np.mean(action_scores) if selection_count > 0 else 0.0\n\n    # Implement a dynamic exploration-exploitation trade-off\n    exploration_factor = 1.5  # Adjusts exploration sensitivity\n    ucb_values = np.zeros(n_actions)\n\n    for action_index in range(n_actions):\n        if selection_counts[action_index] == 0:\n            ucb_values[action_index] = float('inf')  # Encourage exploration for unselected actions\n        else:\n            exploration_term = exploration_factor * np.sqrt(np.log(total_selection_count) / selection_counts[action_index])\n            ucb_values[action_index] = scores[action_index] + exploration_term\n\n    # Calculate an additional penalty for actions based on their selection frequency\n    penalty = selection_counts / (total_selection_count + 1e-5)  # Prevent division by zero\n    final_scores = ucb_values - penalty\n\n    # Select action with the highest adjusted score\n    action_index = np.argmax(final_scores)\n    \n    return action_index",
          "objective": 102031.78758295985,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = np.sqrt(np.log(total_time_slots + 1) / (1 + np.array([len(score_set[i]) for i in range(8)])))\n\n    for i in range(8):\n        if len(score_set[i]) == 0:\n            avg_score = 0  # Action has never been selected, score defaults to 0\n        else:\n            avg_score = np.mean(score_set[i])\n        \n        action_scores.append(avg_score + exploration_factor[i])\n\n    # Epsilon-greedy strategy\n    epsilon = max(1 - (current_time_slot / total_time_slots), 0.1)  # Gradually decrease exploration\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, 8)  # Select a random action for exploration\n    else:\n        action_index = np.argmax(action_scores)  # Select the best action based on scores\n\n    return action_index",
          "objective": 122991.18194633129,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate the average scores for each action\n    average_scores = []\n    for action_index in range(8):\n        if score_set[action_index]:\n            average_score = np.mean(score_set[action_index])\n        else:\n            average_score = 0  # No scores yet\n        average_scores.append(average_score)\n    \n    # Exploration factor: epsilon (small amount of randomness)\n    epsilon = 0.1 * (1 - current_time_slot / total_time_slots)\n    \n    # Select action with epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        # Explore: Pick a random action\n        action_index = np.random.randint(0, 8)\n    else:\n        # Exploit: Pick the action with the highest average score\n        action_index = np.argmax(average_scores)\n\n    return action_index",
          "objective": 124548.68067438157,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts for each action\n    for action_index in range(num_actions):\n        if action_index in score_set:\n            scores = score_set[action_index]\n            action_counts[action_index] = len(scores)\n            action_scores[action_index] = np.mean(scores) if scores else 0.0\n\n    # Epsilon-greedy strategy parameters\n    epsilon = 0.1\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)  # Explore\n    else:\n        # Calculate Upper Confidence Bound (UCB) values\n        exploration_factor = np.sqrt(2 * np.log(total_selection_count + 1) / (action_counts + 1e-5))\n        ucb_values = action_scores + exploration_factor\n        action_index = np.argmax(ucb_values)  # Exploit\n\n    return action_index",
          "objective": 127266.37935323044,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions, dtype=int)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            action_scores[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Calculate upper confidence bounds\n    exploration_term = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n    ucb_values = action_scores + exploration_term\n\n    # Epsilon-greedy exploration\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01)  # Decay epsilon over time\n    if np.random.rand() < epsilon:\n        unexplored_actions = np.where(action_counts == 0)[0]\n        if unexplored_actions.size > 0:\n            action_index = np.random.choice(unexplored_actions)\n        else:\n            action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 146072.23974656468,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Dynamic epsilon based on remaining time slots\n    epsilon = max(0.1 * (total_time_slots - current_time_slot) / total_time_slots, 0.01)\n    n_actions = 8\n    action_index = 0\n\n    average_scores = np.zeros(n_actions)\n    n_action_selected = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for action in range(n_actions):\n        scores = score_set.get(action, [])\n        n_action_selected[action] = len(scores)\n        if n_action_selected[action] > 0:\n            average_scores[action] = np.mean(scores)\n\n    # UCB exploration term calculation\n    ucb_values = np.zeros(n_actions)\n    for action in range(n_actions):\n        if total_selection_count == 0:  # First selection case\n            ucb_values[action] = float('inf')\n        else:\n            exploration_term = np.sqrt(np.log(total_selection_count) / (n_action_selected[action] + 1e-5))\n            ucb_values[action] = average_scores[action] + exploration_term\n\n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, n_actions)  # Explore\n    else:\n        action_index = np.argmax(ucb_values)  # Exploit\n\n    return action_index",
          "objective": 150130.62416324872,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # exploration factor\n    exploration_threshold = epsilon * (total_time_slots - current_time_slot) / total_time_slots\n\n    averages = []\n    for key in range(8):\n        scores = score_set.get(key, [])\n        avg_score = np.mean(scores) if scores else 0.0\n        averages.append(avg_score)\n    \n    if np.random.rand() < exploration_threshold:  # Explore\n        action_index = np.random.randint(0, 8)\n    else:  # Exploit\n        action_index = np.argmax(averages)\n    \n    return action_index",
          "objective": 164308.82029201364,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize action score and count arrays\n    action_scores = np.zeros(8)\n    action_counts = np.zeros(8, dtype=int)\n    \n    # Calculate average scores and counts for each action\n    for action_index, scores in score_set.items():\n        action_scores[action_index] = np.mean(scores) if scores else 0.0\n        action_counts[action_index] = len(scores)\n\n    # Dynamically set epsilon based on the current time slot\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n\n    # Compute upper confidence bounds (UCB)\n    exploration_weights = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n    ucb_values = action_scores + exploration_weights\n\n    # Epsilon-greedy exploration\n    if np.random.rand() < epsilon:\n        # Randomly select from actions that have not been selected or any action\n        action_index = np.random.choice(np.where(action_counts == 0)[0]) if np.any(action_counts == 0) else np.random.randint(8)\n    else:\n        action_index = np.argmax(ucb_values)\n        \n    return action_index",
          "objective": 164974.85042351583,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    \n    # Calculate average scores, avoiding division by zero\n    average_scores = np.array([np.mean(score_set[action]) if score_set[action] else 0.0 for action in action_indices])\n    selection_counts = np.array([len(score_set[action]) for action in action_indices])\n    \n    # Handle cases of zero selections\n    if total_selection_count == 0:\n        return np.random.choice(action_indices)\n    \n    # Epsilon-greedy with decay strategy\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01)\n    if np.random.rand() < epsilon:\n        return np.random.choice(action_indices)\n\n    # Calculate UCB values to balance exploration and exploitation\n    exploration_bonus = np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1))\n    ucb_values = average_scores + exploration_bonus\n    \n    # Select action with the highest UCB value\n    action_index = action_indices[np.argmax(ucb_values)]\n    \n    return action_index",
          "objective": 166457.42091436236,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions, dtype=int)\n    \n    # Calculate average scores and the number of times each action was selected\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        action_scores[action_index] = np.mean(scores) if scores else 0.0\n        \n    # Define epsilon and calculate a dynamic exploration factor\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01)\n    \n    # Compute UCB values with safety against division by zero\n    exploration_weight = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1))\n    ucb_values = action_scores + exploration_weight\n    \n    # Epsilon-greedy exploration\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(np.where(action_counts == 0)[0]) if np.any(action_counts == 0) else np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(ucb_values)\n        \n    return action_index",
          "objective": 176032.54625532185,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Constants\n    epsilon = 0.1  # Exploration rate\n    confidence_factor = 1.96  # For UCB, represents confidence interval\n\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n\n    # Calculate average scores and counts of selections\n    average_scores = {action: np.mean(score_set[action]) if score_set[action] else 0.0 for action in action_indices}\n    selection_counts = {action: len(score_set[action]) for action in action_indices}\n    \n    # Epsilon-greedy strategy\n    if np.random.rand() < epsilon or total_selection_count == 0:\n        # Explore: Select a random action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploit: Use UCB to select the best action\n        ucb_scores = {}\n        for action in action_indices:\n            if selection_counts[action] == 0:\n                ucb_scores[action] = float('inf')  # Ensure unselected actions are preferred\n            else:\n                ucb_scores[action] = average_scores[action] + confidence_factor * np.sqrt(np.log(total_selection_count) / selection_counts[action])\n        \n        action_index = max(ucb_scores, key=ucb_scores.get)\n\n    return action_index",
          "objective": 189894.63897483604,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_rate = 0.1  # Parameter for exploration\n    action_indices = list(score_set.keys())\n    \n    # Calculate average scores and selection counts\n    average_scores = np.zeros(len(action_indices))\n    action_counts = np.zeros(len(action_indices))\n    \n    for i, action in enumerate(action_indices):\n        scores = score_set[action]\n        action_counts[i] = len(scores)\n        if scores:\n            average_scores[i] = np.mean(scores)\n    \n    # Apply Upper Confidence Bound (UCB) for exploration-exploitation trade-off\n    if total_selection_count > 0:\n        ucbs = average_scores + np.sqrt((2 * np.log(total_selection_count)) / (action_counts + 1e-5))\n    else:\n        ucbs = np.zeros(len(action_indices))\n    \n    # Epsilon-greedy exploration\n    if np.random.rand() < exploration_rate or total_selection_count == 0:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = action_indices[np.argmax(ucbs)]\n    \n    return action_index",
          "objective": 283293.2839867909,
          "other_inf": null
     }
]