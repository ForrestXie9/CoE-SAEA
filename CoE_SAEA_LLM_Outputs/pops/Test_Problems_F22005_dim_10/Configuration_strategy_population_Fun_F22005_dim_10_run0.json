[
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = []\n    \n    # Epsilon-greedy parameters\n    epsilon_start = 1.0\n    epsilon_end = 0.1\n    decay_duration = total_time_slots / 5  # Decay over the first 20% of time slots\n    epsilon = max(epsilon_end, epsilon_start * (1 - current_time_slot / decay_duration))\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n\n        # Calculate average score or use a small epsilon to avoid division by zero\n        average_score = np.mean(scores) if scores else 0.0\n\n        # Safety net for selection count\n        selection_count = max(score_count, 1)\n\n        # Upper Confidence Bound calculation\n        ucb = average_score + np.sqrt(np.log(total_selection_count + 1) / selection_count)\n\n        # Combination of exploration and exploitation\n        if np.random.rand() < epsilon:\n            action_scores.append(np.random.rand())  # Random score for exploration\n        else:\n            action_scores.append(ucb)\n\n    # Select the action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.9999999991487,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = []\n    \n    # Epsilon-greedy parameters\n    epsilon_start = 1.0\n    epsilon_end = 0.1\n    decay_rate = total_time_slots / 10  # Decay over the first 10% of time slots\n    exploration_probability = max(epsilon_end, epsilon_start * (1 - current_time_slot / decay_rate))\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n\n        # Calculate average score and handle zero scores\n        average_score = np.mean(scores) if score_count > 0 else 0.0\n\n        # Calculate selection frequency with safeguard\n        selection_count = score_count + 1  # Avoid division by zero\n        \n        # Calculate UCB\n        ucbs = average_score + np.sqrt(np.log(total_selection_count + 1) / selection_count)\n\n        # Apply probability of exploration\n        if np.random.rand() < exploration_probability:\n            action_scores.append(np.random.rand())  # Random score for exploratory actions\n        else:\n            action_scores.append(ucbs)\n\n    # Select the action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.999999999029,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = []\n    \n    # Dynamic epsilon parameters\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    decay_period = total_time_slots / 2\n    exploration_probability = max(epsilon_min, epsilon_max * (1 - current_time_slot / decay_period))\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n        \n        # Safe average score calculation\n        average_score = np.mean(scores) if score_count > 0 else 0.0\n        \n        # Safe selection count calculation\n        selection_count = score_count + 1\n        \n        # UCB calculation\n        ucb = average_score + np.sqrt((2 * np.log(total_selection_count + 1)) / selection_count)\n\n        # Explore with defined probability\n        if np.random.rand() < exploration_probability:\n            action_scores.append(np.random.rand())  # Random score for exploration\n        else:\n            action_scores.append(ucb)\n\n    # Select the action based on the maximum score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999885455,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = []\n\n    # Epsilon parameters\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    decay_period = total_time_slots / 2  \n    exploration_probability = max(epsilon_min, epsilon_max * (1 - current_time_slot / decay_period))\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n\n        # Safe average score calculation\n        average_score = np.mean(scores) if score_count > 0 else 0.0\n        \n        # Avoid division by zero in selection count\n        selection_count = score_count + 1\n        \n        # Upper Confidence Bound calculation\n        ucb = average_score + np.sqrt((2 * np.log(total_selection_count + 1)) / selection_count)\n\n        # Explore with some defined probability\n        if np.random.rand() < exploration_probability:\n            action_scores.append(np.random.rand())  # Random score for exploration\n        else:\n            action_scores.append(ucb)\n\n    # Select the action based on the maximum score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.9999999988148,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = []\n    \n    # Dynamic exploration rate based on time\n    exploration_rate = max(0.01, 0.1 * (1 - total_selection_count / (total_time_slots * num_actions)))\n    \n    # Calculate the selection count and average scores while preventing division by zero\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n        \n        average_score = np.mean(scores) if scores else 0.0\n        selection_count = score_count + 1  # To avoid division by zero\n        \n        # UCB calculation\n        ucb_value = average_score + np.sqrt((2 * np.log(total_selection_count + 1)) / selection_count)\n        action_scores.append(ucb_value)\n\n    # Incorporating adaptive Epsilon-Greedy approach\n    if np.random.rand() < exploration_rate:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -449.99999999874143,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = []\n    \n    # Dynamic exploration parameter that balances exploration and exploitation\n    exploration_strength = 0.1 * (1 - total_selection_count / (total_time_slots * num_actions))\n    exploration_strength = max(0.01, exploration_strength)  # Prevent exploration from diminishing too low\n    \n    # Calculate average score and UCB for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n\n        average_score = np.mean(scores) if scores else 0.0\n        selection_count = score_count + 1  # Avoid division by zero\n\n        # UCB value\n        ucb_value = average_score + np.sqrt((2 * np.log(total_selection_count + 1)) / selection_count)\n        action_scores.append(ucb_value)\n    \n    # Epsilon-Greedy selection mechanism\n    if np.random.rand() < exploration_strength:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -449.9999999985779,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = []\n    \n    # Adaptive epsilon for exploration\n    exploration_rate = max(0.01, 0.1 * (1 - total_selection_count / (total_time_slots * num_actions)))\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n        average_score = np.mean(scores) if scores else 0.0\n        \n        # Adjusted selection count to avoid division by zero\n        adjusted_count = score_count + 1\n        \n        # UCB computation\n        ucb_value = average_score + np.sqrt((2 * np.log(total_selection_count + 1)) / adjusted_count)\n        action_scores.append(ucb_value)\n\n    # Epsilon-Greedy selection mechanism\n    if np.random.rand() < exploration_rate:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -449.9999999985145,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    exploration_fraction = 0.1  # Minimum exploration probability\n    num_initial_explorations = max(1, int(num_actions * 0.1))  # Ensure at least 10% exploration\n    action_scores = []\n    \n    # Adjust exploration probability over time\n    exploration_probability = max(exploration_fraction, 1 - (current_time_slot / total_time_slots))\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n        average_score = np.mean(scores) if score_count > 0 else 0.0\n        \n        # Safeguard against zero selection count\n        selection_count = score_count + 1  # Ensure we avoid division by zero when calculating UCB\n\n        # Calculate UCB\n        ucbs = average_score + np.sqrt(np.log(total_selection_count + 1) / selection_count)\n\n        # Conditional exploration\n        if np.random.rand() < exploration_probability:\n            action_scores.append(np.random.rand())  # Random score for exploration\n        else:\n            action_scores.append(ucbs)\n\n    # Select the action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999829083,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = []\n    action_counts = np.zeros(num_actions)\n    \n    # Initialize the counts and scores\n    for action_index in range(num_actions):\n        action_counts[action_index] = len(score_set.get(action_index, []))\n        scores = score_set.get(action_index, [])\n        average_score = np.mean(scores) if scores else 0.0\n        action_scores.append(average_score)\n\n    # Dynamic exploration rate\n    exploration_rate = max(0.05, 0.1 * (1 - total_selection_count / (total_time_slots * num_actions)))\n\n    # UCB calculation\n    for action_index in range(num_actions):\n        if action_counts[action_index] > 0:\n            ucb_value = action_scores[action_index] + np.sqrt(np.log(total_selection_count + 1) / action_counts[action_index])\n        else:\n            ucb_value = float('inf')  # Prioritize unexplored actions\n\n        action_scores[action_index] = ucb_value\n\n    # Epsilon-Greedy selection\n    if np.random.rand() < exploration_rate:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -449.9999999980728,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = []\n    \n    # Epsilon parameters\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    decay_period = total_time_slots / 2  \n    exploration_probability = max(epsilon_min, epsilon_max * (1 - current_time_slot / decay_period))\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n\n        # Safe average score calculation\n        average_score = np.mean(scores) if score_count > 0 else 0.0\n        \n        # Safe selection count calculation\n        selection_count = score_count + 1\n        \n        # UCB calculation\n        ucb = average_score + np.sqrt((2 * np.log(total_selection_count + 1)) / selection_count)\n\n        # Explore with some defined probability\n        if np.random.rand() < exploration_probability:\n            action_scores.append(np.random.rand())  # Random score for exploration\n        else:\n            action_scores.append(ucb)\n\n    # Select the action based on the maximum score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999790265,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = []\n    \n    # Epsilon decay parameters\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    decay_period = total_time_slots / 2  \n    exploration_probability = max(epsilon_min, epsilon_max * (1 - current_time_slot / decay_period))\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n        \n        # Average score with safe handling for cases where score count is zero\n        average_score = np.mean(scores) if score_count > 0 else 0.0\n        selection_count = score_count + 1  # Avoid division by zero\n        \n        # Upper Confidence Bound calculation\n        ucb = average_score + np.sqrt((2 * np.log(total_selection_count + 1)) / selection_count)\n        \n        # Combine exploration strategy\n        exploration_score = np.random.rand() if np.random.rand() < exploration_probability else 0\n        total_score = ucb + exploration_score\n        \n        action_scores.append(total_score)\n\n    # Select the action based on the maximum score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.9999999978658,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = []\n    \n    # Dynamic epsilon-greedy parameters\n    epsilon_start = 1.0\n    epsilon_end = 0.1\n    exploration_decay = total_time_slots / 10\n    exploration_probability = max(epsilon_end, epsilon_start * (1 - current_time_slot / exploration_decay))\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n        \n        # Calculate average score and prevent division by zero\n        average_score = np.mean(scores) if scores else 0.0\n        selection_count = score_count + 1  # To prevent division by zero\n\n        # Calculate UCB\n        ucb = average_score + np.sqrt((2 * np.log(total_selection_count + 1)) / selection_count)\n\n        # Apply epsilon-greedy exploration\n        if np.random.rand() < exploration_probability:\n            action_scores.append(np.random.rand())  # Assign a random score for exploration\n        else:\n            action_scores.append(ucb)\n\n    # Select the action with the maximum score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999782983,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = []\n    \n    # Dynamic exploration parameter\n    epsilon = max(0.1 * (1 - total_selection_count / (total_time_slots * num_actions)), 0.01)\n    \n    # Calculate average score and UCB for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n\n        average_score = np.mean(scores) if scores else 0.0\n        selection_count = score_count + 1  # Avoid division by zero for UCB\n\n        # UCB value\n        ucb_value = average_score + np.sqrt((2 * np.log(total_selection_count + 1)) / selection_count)\n        action_scores.append(ucb_value)\n    \n    # Epsilon-Greedy selection mechanism\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(num_actions)\n    else:\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -449.9999999977191,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = []\n    \n    # Exploration rate decreasing over time and ensuring minimum value\n    if total_selection_count > 0:\n        exploration_rate = max(0.05, 0.1 * (1 - total_selection_count / (total_time_slots * num_actions)))\n    else:\n        exploration_rate = 1.0  # Full exploration when no selections have been made\n\n    # Calculate scores for each action based on UCB\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n\n        average_score = np.mean(scores) if score_count > 0 else 0.0\n        selection_count = score_count + 1  # Avoiding division by zero\n        \n        # UCB value\n        ucb_value = average_score + np.sqrt(np.log(total_selection_count + 1) / selection_count)\n        action_scores.append(ucb_value)\n\n    # Epsilon-Greedy selection\n    if np.random.rand() < exploration_rate:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -449.999999997659,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = []\n    \n    # Calculate exploration factor, inversely proportional to total selections\n    exploration_factor = max(0.1, 1.0 - total_selection_count / (total_time_slots * num_actions))\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if scores else 0.0\n        \n        # Selection count with a safeguard for zero selections\n        selection_count = score_count + 1\n        \n        # UCB value calculation\n        ucb_value = average_score + exploration_factor * np.sqrt(np.log(total_selection_count + 1) / selection_count)\n        action_scores.append(ucb_value)\n    \n    # Dynamic epsilon calculation for exploration-exploitation strategy\n    epsilon = max(0.05, 0.1 - (total_selection_count / (total_time_slots * 0.5)))\n    \n    if np.random.rand() < epsilon:  # Exploration phase\n        action_index = np.random.randint(num_actions)\n    else:  # Exploitation phase\n        action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -449.99999999764634,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = []\n\n    # Dynamic epsilon parameters\n    epsilon_initial = 1.0\n    epsilon_min = 0.1\n    decay_period = total_time_slots / 2\n    exploration_probability = max(epsilon_min, epsilon_initial * (1 - current_time_slot / decay_period))\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n\n        # Safe average score and count\n        if score_count > 0:\n            average_score = np.mean(scores)\n        else:\n            average_score = 0.0\n        \n        selection_count = score_count + 1\n        \n        # Upper Confidence Bound calculation\n        ucb = average_score + np.sqrt((2 * np.log(total_selection_count + 1)) / selection_count)\n\n        # Epsilon-Greedy exploration adjustment\n        exploration_score = np.random.uniform(0, 1) if np.random.rand() < exploration_probability else ucb\n\n        action_scores.append(exploration_score)\n\n    # Select the action based on the maximum score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999744733,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = []\n    \n    # Dynamic exploration factor\n    exploration_factor = 1 - current_time_slot / total_time_slots\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n\n        # Calculate average score with safe handling of zero counts\n        average_score = np.mean(scores) if score_count > 0 else 0.0\n        \n        # Calculate selection frequency with safeguard against division by zero\n        selection_count = score_count + 1  # Adding 1 to avoid division by zero\n        \n        # UCB calculation\n        ucbs = average_score + exploration_factor * np.sqrt(np.log(total_selection_count + 1) / selection_count)\n\n        action_scores.append(ucbs)\n    \n    # Select the action with the highest UCB score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999718847,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = []\n    \n    # Epsilon parameters\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    decay_period = total_time_slots / 2  # Decay over the first half of time slots\n    exploration_probability = max(epsilon_min, epsilon_max * (1 - current_time_slot / decay_period))\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n\n        # Calculate average score with a safe fallback for unselected actions\n        average_score = np.mean(scores) if score_count > 0 else 0.0\n        \n        # Calculate selection frequency safely\n        selection_count = score_count + 1\n\n        # Upper Confidence Bound calculation\n        ucb = average_score + np.sqrt((2 * np.log(total_selection_count + 1)) / selection_count)\n\n        # Incorporate exploration element\n        if np.random.rand() < exploration_probability:\n            action_scores.append(np.random.rand())  # Random score for exploration\n        else:\n            action_scores.append(ucb)\n\n    # Select the action based on the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.9999999970871,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = []\n\n    # Dynamic epsilon decay\n    epsilon_start = 1.0\n    epsilon_end = 0.1\n    epsilon_decay = (epsilon_start - epsilon_end) / (total_time_slots)\n    exploration_probability = max(epsilon_end, epsilon_start - epsilon_decay * current_time_slot)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n\n        # Calculate average score and handle zero scores\n        average_score = np.mean(scores) if score_count > 0 else 0.0\n        \n        # Calculate selection frequency safely\n        selection_count = score_count + 1  # Add one to avoid division by zero\n\n        # Calculate UCB\n        ucbs = average_score + np.sqrt(2 * np.log(total_selection_count + 1) / selection_count)\n\n        # Apply exploration probability\n        if np.random.rand() < exploration_probability:\n            action_scores.append(np.random.rand())  # Random score for exploration\n        else:\n            action_scores.append(ucbs)\n\n    # Select the action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999647815,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = []\n\n    # Dynamic exploration factor based on the current time slot\n    exploration_rate = max(0.05, 0.1 * (1 - current_time_slot / total_time_slots))\n\n    # Calculate scores for each action based on UCB\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n\n        average_score = np.mean(scores) if score_count > 0 else 0.0\n        selection_count = score_count + 1  # To avoid division by zero\n        \n        # UCB value\n        ucb_value = average_score + np.sqrt(np.log(total_selection_count + 1) / selection_count)\n        action_scores.append(ucb_value)\n\n    # Epsilon-Greedy selection\n    if np.random.rand() < exploration_rate:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -449.99999999615875,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = []\n    \n    # Adjust exploration factor\n    exploration_factor = max(0.1, 1.0 - total_selection_count / (total_time_slots * num_actions))\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if scores else 0.0\n        \n        # Effective selection count\n        selection_count = score_count + 1  # Adding 1 to handle cases of zero selections\n        \n        # UCB value computation\n        ucb_value = average_score + exploration_factor * np.sqrt(np.log(total_selection_count + 1) / selection_count)\n        action_scores.append(ucb_value)\n    \n    # Decaying epsilon for exploration-exploitation balance\n    epsilon = max(0.05, 0.1 - (total_selection_count / (total_time_slots * 0.5)))\n    \n    if np.random.rand() < epsilon:  # Random exploration\n        action_index = np.random.randint(num_actions)\n    else:  # Exploit the best UCB action\n        action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -449.9999999961215,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = []\n    \n    # Dynamic exploration factor based on total selections\n    exploration_factor = max(0.1, 1.0 - total_selection_count / (total_time_slots * num_actions))\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n        \n        # Safe average score handling\n        total_score = sum(scores)\n        average_score = total_score / score_count if score_count > 0 else 0.0\n        \n        # Calculate effective selection count\n        selection_count = score_count + 1  # Adding 1 to avoid division by zero\n        \n        # Calculate UCB value\n        ucb_value = average_score + exploration_factor * np.sqrt(np.log(total_selection_count + 1) / selection_count)\n        \n        action_scores.append(ucb_value)\n    \n    # Hybrid exploration-exploitation strategy\n    exploration_rate = max(0.05, 0.1 - total_selection_count / (total_time_slots * 0.5))\n    if np.random.rand() < exploration_rate:  # Random exploration\n        action_index = np.random.randint(num_actions)\n    else:  # Exploit the action with the highest UCB score\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -449.9999999961059,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = []\n    \n    # Dynamic exploration rate\n    if total_selection_count < num_actions:\n        # Prioritize exploration in the initial phase\n        exploration_rate = 1.0\n    else:\n        exploration_rate = max(0.05, 0.1 * (1 - total_selection_count / (total_time_slots * num_actions)))\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n\n        average_score = np.mean(scores) if score_count > 0 else 0.0\n        selection_count = score_count if score_count > 0 else 1  # Avoid division by zero\n        \n        # UCB value\n        ucb_value = average_score + np.sqrt(2 * np.log(total_selection_count + 1) / selection_count)\n        action_scores.append(ucb_value)\n\n    # Epsilon-Greedy selection\n    if np.random.rand() < exploration_rate:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -449.9999999956054,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = []\n    \n    # Adjust exploration factor\n    exploration_factor = max(0.1, 1.0 - total_selection_count / (total_time_slots * num_actions))\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if scores else 0.0\n        \n        # Effective selection count\n        selection_count = score_count + 1  # Adding 1 to handle zero selections\n        \n        # UCB value computation\n        ucb_value = average_score + exploration_factor * np.sqrt(np.log(total_selection_count + 1) / selection_count)\n        action_scores.append(ucb_value)\n    \n    # Decay epsilon dynamically for exploration-exploitation balance\n    epsilon = max(0.05, 0.1 - (total_selection_count / (total_time_slots * 0.5)))\n    \n    if np.random.rand() < epsilon:  # Random exploration\n        action_index = np.random.randint(num_actions)\n    else:  # Exploit the best UCB action\n        action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -449.9999999954896,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = []\n    exploration_threshold = 0.2  # Threshold for exploration vs exploitation\n\n    # Dynamic epsilon decay for exploration\n    epsilon_start = 1.0\n    epsilon_end = 0.1\n    epsilon_decay = (epsilon_start - epsilon_end) / total_time_slots\n    exploration_probability = max(epsilon_end, epsilon_start - epsilon_decay * current_time_slot)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n\n        # Average score calculation with handling for zero selections\n        average_score = np.mean(scores) if scores else 0.0\n\n        # Use selection counts to compute UCB, ensuring safe division\n        selection_count = score_count if score_count > 0 else 1  # Avoid division by zero\n        \n        # Calculate UCB\n        ucb = average_score + np.sqrt(2 * np.log(total_selection_count + 1) / selection_count)\n\n        # Decision-making: exploration vs exploitation\n        if np.random.rand() < exploration_probability:\n            action_scores.append(np.random.uniform(0, 1))  # Random exploration score\n        else:\n            action_scores.append(ucb)\n\n    # Select the action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.9999999954191,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = []\n    \n    # Epsilon parameters\n    epsilon_start = 1.0\n    epsilon_end = 0.05\n    decay_rate = total_time_slots / 10  # Decay over the first 10% of time slots\n    exploration_probability = max(epsilon_end, epsilon_start * (1 - current_time_slot / decay_rate))\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n\n        # Calculate average score and handle zero scores\n        average_score = np.mean(scores) if score_count > 0 else 0.0\n        \n        # Calculate selection frequency (safeguard against division by zero)\n        selection_count = score_count + 1\n        \n        # Calculate UCB with an added term for exploration\n        ucbs = average_score + np.sqrt((2 * np.log(total_selection_count + 1)) / selection_count)\n\n        # Apply exploration probability for noisy actions\n        if np.random.rand() < exploration_probability:\n            action_scores.append(np.random.rand())  # Random score for exploratory actions\n        else:\n            action_scores.append(ucbs)\n\n    # Select the action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.9999999950825,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = []\n    \n    # Calculate exploration factor: adaptively decreasing\n    exploration_factor = max(0.1, 1.0 - (total_selection_count / (total_time_slots * num_actions)))\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n        \n        # Calculate average score or fallback to a small value\n        average_score = np.mean(scores) if scores else 0.01\n        \n        # Selection count safeguard\n        selection_count = score_count + 1\n        \n        # UCB value calculation\n        ucb_value = average_score + exploration_factor * np.sqrt(np.log(total_selection_count + 1) / selection_count)\n        action_scores.append(ucb_value)\n\n    # Dynamic epsilon calculation for exploration-exploitation\n    epsilon = max(0.05, 0.1 * (1 - (current_time_slot / total_time_slots)))\n\n    # Epsilon-greedy strategy for action selection\n    if np.random.rand() < epsilon:  # Exploration phase\n        action_index = np.random.randint(num_actions)\n    else:  # Exploitation phase\n        action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -449.9999999950261,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        if selection_counts[action_index] > 0:\n            action_scores[action_index] = np.mean(scores)\n\n    # Exploration rate based on the current time slot\n    exploration_rate = max(0.05, 0.1 * (1 - current_time_slot / total_time_slots))\n    \n    # Compute UCB values\n    ucb_values = action_scores + np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n\n    # Hybrid strategy: Epsilon-Greedy with UCB\n    if np.random.rand() < exploration_rate:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -449.9999999949045,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = []\n    \n    # Dynamic exploration factor based on the current time slot\n    exploration_factor = max(0.1, 1 - current_time_slot / total_time_slots)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n        total_score = sum(scores)\n\n        # Safe average score handling\n        average_score = total_score / score_count if score_count > 0 else 0.0\n        \n        # Calculate effective selection count\n        selection_count = score_count + 1  # Adding 1 to avoid division by zero\n        \n        # Calculate UCB value\n        ucbs = average_score + exploration_factor * np.sqrt(np.log(total_selection_count + 1) / selection_count)\n        \n        action_scores.append(ucbs)\n    \n    # Epsilon-Greedy component\n    epsilon = 0.1\n    if np.random.rand() < epsilon:  # Random exploration\n        action_index = np.random.randint(num_actions)\n    else:  # Exploit the action with the highest UCB score\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -449.9999999936404,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = []\n    \n    # Dynamic exploration rate based on the progress of time\n    epsilon = max(0.01, 0.1 * (1 - total_selection_count / (total_time_slots * num_actions)))\n    \n    # Calculate average scores and apply UCB\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n        \n        average_score = np.mean(scores) if scores else 0.0\n        variance_score = np.var(scores) if scores else 0.0\n        selection_count = score_count + 1  # Prevent division by zero\n        \n        # UCB calculation\n        ucb_value = average_score + np.sqrt((2 * np.log(total_selection_count + 1)) / selection_count) + variance_score\n        \n        action_scores.append(ucb_value)\n\n    # Adaptive Epsilon-Greedy decision\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -449.9999999931949,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate scores and counts from score_set\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        action_scores[action_index] = np.mean(scores) if scores else 0.0\n    \n    # Define exploration rate\n    exploration_rate = max(0.05, 1.0 - (total_selection_count / (total_time_slots * num_actions)))\n    \n    # UCB calculation\n    total_count = total_selection_count + 1  # Avoid division by zero in log\n    ucb_values = action_scores + np.sqrt(np.log(total_count) / (selection_counts + 1e-5))  # add epsilon to avoid div by 0\n    \n    # Epsilon-Greedy selection\n    if np.random.rand() < exploration_rate:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -449.99999999295443,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    \n    # Dynamic exploration factor\n    exploration_factor = 1 - (current_time_slot / total_time_slots)\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n        \n        # Calculate average score and selection count\n        average_score = np.mean(scores) if score_count > 0 else 0\n        selection_count = score_count if score_count > 0 else 1  # Ensure non-zero\n        \n        # Calculate UCB score\n        ucb_score = average_score + exploration_factor * np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1e-5))\n        \n        action_scores[action_index] = ucb_score\n    \n    # Select the action with the highest UCB score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.9999999911936,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n    \n    # Calculate average scores and counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_scores[action_index] = np.mean(scores) if scores else 0.0\n    \n    # Dynamic epsilon based on the exploration phase\n    epsilon = max(0.1 * (1 - total_selection_count / (total_time_slots * num_actions)), 0.05)\n    \n    # UCB calculation\n    total_exploration_count = total_selection_count + 1\n    ucb_values = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        if action_counts[action_index] > 0:\n            ucb_values[action_index] = action_scores[action_index] + np.sqrt(np.log(total_exploration_count) / action_counts[action_index])\n        else:\n            ucb_values[action_index] = float('inf')  # Prioritize unexplored actions\n    \n    # Epsilon-Greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(ucb_values)\n    \n    return action_index",
          "objective": -449.99999999109656,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = []\n    \n    # Epsilon-decay parameters\n    epsilon_start = 1.0\n    epsilon_end = 0.1\n    exploration_fraction = 0.1  # Explore in the first 10% of time slots\n    decay_time_slots = total_time_slots * exploration_fraction\n    \n    # Epsilon calculation\n    if current_time_slot < decay_time_slots:\n        exploration_probability = epsilon_start - (epsilon_start - epsilon_end) * (current_time_slot / decay_time_slots)\n    else:\n        exploration_probability = epsilon_end\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n\n        # Average score calculation\n        average_score = np.mean(scores) if score_count > 0 else 0.0\n        \n        # Selection count with safeguard\n        selection_count = score_count + 1  # Avoid division by zero\n        \n        # UCB calculation\n        ucb_value = average_score + np.sqrt(np.log(total_selection_count + 1) / selection_count)\n\n        # Combine exploratory and exploitative score\n        if np.random.rand() < exploration_probability:\n            action_scores.append(np.random.rand())  # Random score for exploration\n        else:\n            action_scores.append(ucb_value)  # UCB score for exploitation\n\n    # Select the action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.9999999909754,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = []\n\n    # Dynamic exploration factor based on the total selections and time slots\n    exploration_factor = max(0.1, 1.0 - total_selection_count / (total_time_slots * num_actions))\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n        \n        # Safe average score handling\n        average_score = np.mean(scores) if scores else 0.0\n        \n        # Calculate effective selection count with safe division\n        selection_count = score_count + 1  # Adding 1 to avoid division by zero\n        \n        # Calculate UCB value\n        ucb_value = average_score + exploration_factor * np.sqrt(np.log(total_selection_count + 1) / selection_count)\n        \n        action_scores.append(ucb_value)\n\n    # Balanced exploration-exploitation strategy\n    exploration_rate = max(0.05, 0.1 - total_selection_count / (total_time_slots * 0.5))\n    if np.random.rand() < exploration_rate:  # Random exploration\n        action_index = np.random.randint(num_actions)\n    else:  # Exploit the action with the highest UCB score\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -449.9999999899512,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = []\n    \n    # Adaptive exploration factor decreases over time\n    exploration_factor = 1.0 - (current_time_slot / total_time_slots)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n        \n        # Safe average score handling\n        total_score = sum(scores)\n        average_score = total_score / score_count if score_count > 0 else 0.0\n        \n        # Effective selection count considers historical selection\n        effective_selection_count = score_count + 1\n        \n        # Calculate UCB value\n        ucb_value = average_score + exploration_factor * np.sqrt(np.log(total_selection_count + 1) / effective_selection_count)\n        \n        action_scores.append(ucb_value)\n    \n    # Adaptive epsilon-greedy strategy\n    epsilon = max(0.1, 1.0 - (total_selection_count / (total_time_slots * 0.5)))\n    if np.random.rand() < epsilon:  # Random exploration\n        action_index = np.random.randint(num_actions)\n    else:  # Exploit the action with the highest UCB score\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -449.999999989386,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n\n    # Dynamic exploration parameter\n    epsilon = 0.1 * (1 - (current_time_slot / total_time_slots))\n    exploration_weight = epsilon * np.sqrt(total_selection_count + 1)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n\n        # Calculate average score and ensure non-zero selection count\n        average_score = np.mean(scores) if score_count > 0 else 0\n        normalized_selection_count = score_count if score_count > 0 else 1\n\n        # Calculate UCB score\n        ucb_score = average_score + exploration_weight * np.sqrt(np.log(total_selection_count + 1) / (normalized_selection_count + 1e-5))\n        \n        action_scores[action_index] = ucb_score\n\n    # Select the action with the highest UCB score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.9999999883347,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = []\n    \n    # Dynamic exploration factor and decay epsilon based on current time slot\n    epsilon = max(0.05, 1.0 - (current_time_slot / total_time_slots))\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n        \n        # Calculate average score or use a small value if not selected\n        average_score = np.mean(scores) if scores else 0.0\n        \n        # Effective selection count\n        effective_selection_count = score_count + 1  # Handle zero selections\n        \n        # UCB value computation\n        ucb_value = average_score + np.sqrt(np.log(total_selection_count + 1) / effective_selection_count)\n        action_scores.append(ucb_value)\n    \n    # Random exploration with epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -449.99999998820863,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = np.sqrt(np.log(total_selection_count + 1) / (np.array([len(score_set.get(i, [])) for i in range(8)]) + 1))  # UCB approach for exploration\n    epsilon = 1 - (current_time_slot / total_time_slots)  # Decreasing exploration over time\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        average_score = np.mean(scores) if scores else 0 \n\n        # Adjusted score for exploration\n        adjusted_score = average_score + (epsilon * exploration_factor[action_index])\n        \n        action_scores.append(adjusted_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.9999999848944,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = []\n    \n    # Adjust exploration rate dynamically over time to balance exploration and exploitation\n    exploration_rate = max(0.01, 0.1 * (1 - total_selection_count / (total_time_slots * num_actions)))\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n\n        average_score = np.mean(scores) if scores else 0.0\n        selection_count = score_count + 1  # To avoid division by zero\n        \n        # UCB value computation\n        ucb_value = average_score + np.sqrt((2 * np.log(total_selection_count + 1)) / selection_count)\n        action_scores.append(ucb_value)\n\n    # Epsilon-Greedy selection mechanism\n    if np.random.rand() < exploration_rate:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -449.99999998384925,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = []\n    \n    # Dynamic exploration factor\n    exploration_factor = 1 - current_time_slot / total_time_slots\n    \n    # Decay factor for exploration\n    epsilon = max(0.1, 1 - current_time_slot / (total_time_slots * 2))\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n\n        # Calculate average score\n        average_score = np.mean(scores) if score_count > 0 else 0.0\n        \n        # Calculate adjusted selection count for UCB\n        selection_count = score_count + 1  # Avoid division by zero\n        \n        # Calculate UCB with exploration\n        ucbs = average_score + exploration_factor * np.sqrt(np.log(total_selection_count + 1) / selection_count)\n\n        # Incorporate epsilon for Epsilon-Greedy strategy\n        score_with_epsilon = (1 - epsilon) * ucbs + epsilon * (np.random.rand() * (1 if score_count > 0 else 0))\n\n        action_scores.append(score_with_epsilon)\n    \n    # Select the action with the highest value\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999998343435,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    \n    # Dynamic exploration factor\n    exploration_factor = 1 - (current_time_slot / total_time_slots)\n    \n    # Epsilon decay factor\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n        \n        # Calculate average score and selection count\n        average_score = np.mean(scores) if scores else 0\n        selection_count = score_count if score_count > 0 else 1  # Ensure non-zero for division\n        \n        # Calculate UCB score\n        ucb_score = average_score + exploration_factor * np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1e-5))\n        \n        # Apply epsilon-greedy approach\n        if np.random.rand() < epsilon:\n            action_scores[action_index] = np.random.rand()  # Explore randomly\n        else:\n            action_scores[action_index] = ucb_score  # Exploit based on UCB\n    \n    # Select the action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.9999999829913,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = []\n    \n    # Dynamic exploration factor\n    exploration_factor = 1 - current_time_slot / total_time_slots\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n\n        # Calculate average score and registration of zero scores\n        average_score = np.mean(scores) if score_count > 0 else 0.0\n        \n        # Selection frequency with safeguard\n        selection_count = score_count + 1  # Avoid division by zero\n        \n        # Calculate UCB\n        ucbs = average_score + exploration_factor * np.sqrt(np.log(total_selection_count + 1) / selection_count)\n\n        action_scores.append(ucbs)\n    \n    # Select the action with the highest UCB score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999997968786,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n    \n    # Calculate average scores and counts for actions\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        if scores:\n            action_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Calculate UCB for exploration\n    exploration_factor = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))  # Add small constant to avoid division by zero\n\n    # Decrease exploration over time\n    exploration_weight = 1 - (current_time_slot / total_time_slots)\n    \n    # UCB calculation\n    ucb_scores = action_values + exploration_weight * exploration_factor\n    \n    # Select action with the highest UCB score\n    action_index = np.argmax(ucb_scores)\n    \n    return action_index",
          "objective": -449.99999997883407,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    action_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        average_score = np.mean(scores) if scores else 0\n        selection_counts[action_index] = len(scores)\n\n        # UCB exploration factor\n        ucb_exploration = np.sqrt(np.log(total_selection_count + 1) / (selection_counts[action_index] + 1)) if selection_counts[action_index] > 0 else float('inf')\n        \n        # Epsilon decay based on the total selection count\n        epsilon = max(0.1, 1 - (total_selection_count / (total_time_slots * 10)))\n        \n        # Adjusted score using a blend of average score and exploration factor\n        adjusted_score = average_score + epsilon * ucb_exploration\n        action_scores[action_index] = adjusted_score\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.9999999631621,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        selection_counts[action_index] = selection_count\n\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate UCB exploration term\n        exploration_term = np.sqrt((np.log(total_selection_count + 1) / (selection_count + 1))) if selection_count > 0 else np.inf\n        action_scores[action_index] = average_score + exploration_term\n\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots) * 0.9)\n\n    if np.random.rand() < epsilon:  # Explore\n        action_index = np.random.choice(num_actions)\n    else:  # Exploit\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -449.99999995626666,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = []\n    action_variances = []\n    \n    # Dynamically adjust exploration rate based on total selections\n    if total_selection_count > 0:\n        exploration_rate = max(0.05, 0.1 * (1 - total_selection_count / (total_time_slots * num_actions)))\n    else:\n        exploration_rate = 1.0  # Full exploration when no selections have been made\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n\n        average_score = np.mean(scores) if score_count > 0 else 0.0\n        score_variance = np.var(scores) if score_count > 1 else 0.0  # Variance based on sample size\n\n        # UCB calculation\n        ucb_value = average_score + np.sqrt((np.log(total_selection_count + 1) / (score_count + 1))) + score_variance\n        action_scores.append(ucb_value)\n        action_variances.append(score_variance)\n\n    # Epsilon-Greedy selection\n    if np.random.rand() < exploration_rate:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -449.9999999482556,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = []\n    \n    # Dynamic epsilon\n    exploration_rate = max(0.1, 0.5 * (1 - total_selection_count / (total_time_slots * num_actions)))\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n\n        average_score = np.mean(scores) if score_count > 0 else 0.0\n        selection_count = score_count if score_count > 0 else 1  # Avoid division by zero\n        \n        # UCB calculation\n        ucb_value = average_score + np.sqrt(2 * np.log(total_selection_count + 1) / selection_count)\n        action_scores.append(ucb_value)\n\n    # Epsilon-Greedy selection\n    if np.random.rand() < exploration_rate:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -449.99999994812805,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = []\n    \n    # Adaptive exploration rate\n    exploration_rate = max(0.05, 1.0 - (total_selection_count / (total_time_slots * num_actions)))\n    \n    # Calculate scores for each action based on UCB and average scores\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n\n        average_score = np.mean(scores) if score_count > 0 else 0.0\n        selection_count = score_count + 1  # To avoid division by zero\n\n        # UCB value calculation\n        ucb_value = average_score + np.sqrt((2 * np.log(total_selection_count + 1)) / selection_count)\n        action_scores.append(ucb_value)\n\n    # Epsilon-Greedy selection\n    if np.random.rand() < exploration_rate:\n        action_index = np.random.randint(num_actions)  # Explore\n    else:\n        action_index = np.argmax(action_scores)  # Exploit\n    \n    return action_index",
          "objective": -449.9999999441424,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    action_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        average_score = np.mean(scores) if scores else 0\n        selection_counts[action_index] = len(scores)\n\n        # UCB exploration factor\n        ucb_exploration = np.sqrt(np.log(total_selection_count + 1) / (selection_counts[action_index] + 1)) if selection_counts[action_index] > 0 else float('inf')\n        \n        # Epsilon decay for exploration\n        epsilon = 0.1 + 0.9 * (1 - (current_time_slot / total_time_slots))\n        \n        # Adjusted score using a weighted sum of average score and exploration factor\n        adjusted_score = average_score + (epsilon * ucb_exploration)\n        action_scores[action_index] = adjusted_score\n    \n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999993114903,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        selection_counts[action_index] = selection_count\n\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate UCB exploration term\n        exploration_term = np.sqrt((np.log(total_selection_count + 1) / (selection_count + 1))) if selection_count > 0 else np.inf\n        action_scores[action_index] = average_score + exploration_term\n\n    # Dynamic exploration rate\n    epsilon = 1 - (current_time_slot / total_time_slots) ** 2\n\n    if np.random.rand() < epsilon:  # Explore\n        action_index = np.random.choice(num_actions)\n    else:  # Exploit\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -449.9999999082224,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    action_scores = np.zeros(n_actions)\n    exploration_factors = np.zeros(n_actions)\n\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        average_score = np.mean(scores) if scores else 0\n        selection_count = len(scores)\n\n        # UCB exploration factor\n        if selection_count > 0:\n            exploration_factors[action_index] = np.sqrt(np.log(total_selection_count) / selection_count)\n        \n        # Epsilon decay for exploration\n        epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n        adjusted_score = average_score + (epsilon * exploration_factors[action_index])\n        \n        action_scores[action_index] = adjusted_score\n    \n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999989714274,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        selection_counts[action_index] = selection_count\n\n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate UCB exploration term\n        exploration_term = np.sqrt((np.log(total_selection_count + 1) / (selection_count + 1))) if selection_count > 0 else np.inf\n        action_scores[action_index] = average_score + exploration_term\n\n    # Dynamic epsilon based on current time slot\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots) ** 2)\n    \n    # Select action based on exploration or exploitation\n    if np.random.rand() < epsilon:  # Explore\n        action_index = np.random.choice(num_actions)\n    else:  # Exploit\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -449.9999998943001,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    exploration_weights = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate the average score, ensuring to handle potential division by zero\n        average_score = np.mean(scores) if scores else 0\n\n        # Calculate UCB exploration term\n        exploration_term = np.sqrt((np.log(total_selection_count + 1) / (selection_count + 1))) if selection_count > 0 else np.inf\n        exploration_weights[action_index] = exploration_term\n\n        # Adjusted score is the average score plus exploration weight\n        action_scores[action_index] = average_score + exploration_weights[action_index]\n    \n    # Epsilon-greedy exploration strategy\n    epsilon = np.clip(1 - (current_time_slot / total_time_slots), 0.1, 0.9)  # Minimum epsilon to avoid zero explore\n    \n    if np.random.rand() < epsilon:  # Explore\n        action_index = np.random.choice(num_actions)\n    else:  # Exploit\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -449.9999998841983,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        selection_counts[action_index] = selection_count\n\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        exploration_term = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else np.inf\n        \n        action_scores[action_index] = average_score + exploration_term\n\n    epsilon = max(0.05, 1 - (current_time_slot / total_time_slots) ** 2)\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(num_actions)\n    else:\n        action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -449.99999986442083,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        selection_counts[action_index] = selection_count\n\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        action_scores[action_index] = average_score\n\n    # Explorer-exploiter balance\n    epsilon = max(0.05, 0.1 * (1 - (total_selection_count / (total_time_slots * 10))))  # Adaptive epsilon\n\n    if np.random.rand() < epsilon:  # Explore\n        action_index = np.random.choice(num_actions)\n    else:  # Exploit with UCB\n        exploration_terms = np.zeros(num_actions)\n        for action_index in range(num_actions):\n            selection_count = selection_counts[action_index]\n            exploration_terms[action_index] = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else np.inf\n            \n        action_scores += exploration_terms\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -449.9999998606714,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n\n    # Adaptive exploration factor (epsilon decreases over time)\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n\n        # Calculate average score and selection count\n        average_score = np.mean(scores) if score_count > 0 else 0\n        selection_count = score_count if score_count > 0 else 1  # Ensure non-zero to avoid div by zero\n\n        # Calculate UCB score\n        ucb_score = average_score + (np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1e-5))) * (1 + epsilon * (1 / (total_selection_count + 1)))\n\n        action_scores[action_index] = ucb_score\n    \n    # Select the action with the highest UCB score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999985107496,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = []\n    \n    # Exploration-exploitation parameters\n    exploration_factor = 1 - (current_time_slot / total_time_slots)\n    epsilon = max(0.1, 0.5 * (1 - current_time_slot / total_time_slots))  # Decay epsilon over time\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n\n        # Calculate average score\n        average_score = np.mean(scores) if score_count > 0 else 0.0\n        \n        # Selection frequency safeguard\n        selection_count = score_count + 1  # Avoid division by zero\n\n        # Calculate UCB\n        ucb = average_score + exploration_factor * np.sqrt(np.log(total_selection_count + 1) / selection_count)\n        action_scores.append(ucb)\n\n    # Epsilon-Greedy selection\n    if np.random.random() < epsilon:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -449.9999997810992,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = []\n    \n    # Dynamic exploration factor using epsilon-greedy approach\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        \n        # Calculate average score with safeguard\n        average_score = np.mean(scores) if scores else 0.0\n        \n        # Calculate selection count\n        selection_count = len(scores) + 1  # Avoid division by zero\n        \n        # Compute UCB\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / selection_count)\n        ucbs = average_score + exploration_bonus * (1 - epsilon)\n        \n        # Incorporate random exploration based on epsilon\n        if np.random.rand() < epsilon:\n            ucbs += np.random.rand() * epsilon\n        \n        action_scores.append(ucbs)\n    \n    # Select the action with the maximum adjusted UCB score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.9999997689868,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    \n    # Dynamic exploration factor\n    epsilon = max(0.1, 1 - (total_selection_count / (total_time_slots * 10)))  # Decreasing epsilon\n    exploration_factor = 1 - (current_time_slot / total_time_slots)\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n        \n        # Calculate average score and selection count safely\n        average_score = np.mean(scores) if score_count > 0 else 0\n        selection_count = score_count if score_count > 0 else 1  # Avoid zero for division\n        \n        # Calculate UCB score\n        ucb_score = average_score + exploration_factor * np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1e-5))\n        \n        # Apply epsilon-greedy strategy\n        if np.random.rand() < epsilon:\n            action_scores[action_index] = np.random.rand()  # Random exploration\n        else:\n            action_scores[action_index] = ucb_score\n    \n    # Select the action with the highest adjusted score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.9999997689411,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = []\n    \n    # Dynamic exploration factor\n    exploration_factor = 1 - (current_time_slot / total_time_slots)\n    \n    # Epsilon for exploration-exploitation balance\n    epsilon = max(0.1, 1 - (total_selection_count / (total_time_slots * 20)))\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n\n        # Average score computation\n        average_score = np.mean(scores) if score_count > 0 else 0.0\n        \n        # Selection frequency safeguard against division by zero\n        selection_count = score_count + 1\n        \n        # Calculate UCB\n        ucb = average_score + exploration_factor * np.sqrt(np.log(total_selection_count + 1) / selection_count)\n\n        # Epsilon-Greedy adjustment\n        if np.random.rand() < epsilon:\n            action_score = np.random.rand()  # Random exploration\n        else:\n            action_score = ucb  # Exploit based on UCB\n\n        action_scores.append(action_score)\n    \n    # Select the action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999972955567,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    \n    # Exploration factor decreases over time\n    exploration_factor = np.max([0.1, 0.5 * (1 - current_time_slot / total_time_slots)])\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n        average_score = np.mean(scores) if score_count > 0 else 0\n        \n        # Calculate the confidence interval, adjusting for zero selections\n        confidence_interval = np.sqrt(np.log(total_selection_count + 1) / (score_count + 1)) if score_count > 0 else np.inf\n        \n        # Combined score with adaptive exploration\n        adjusted_score = average_score + exploration_factor * confidence_interval\n        action_scores[action_index] = adjusted_score\n    \n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999972661595,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    action_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        average_score = np.mean(scores) if scores else 0\n        selection_count = len(scores)\n        selection_counts[action_index] = selection_count\n\n        # UCB exploration factor\n        if selection_count > 0:\n            ucb_exploration = np.sqrt(np.log(total_selection_count) / selection_count)\n        else:\n            ucb_exploration = np.inf  # Encourage exploration for unselected actions\n\n        # Epsilon decay for exploration\n        epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n        adjusted_score = average_score + (epsilon * ucb_exploration)\n        \n        action_scores[action_index] = adjusted_score\n\n    # Normalize scores to enhance robustness and prevent skewing\n    action_scores -= np.min(action_scores)  # Shift scores to be non-negative\n    action_scores /= np.sum(action_scores)  # Normalize to sum to 1\n\n    # Softmax selection based on adjusted scores\n    probabilities = np.exp(action_scores) / np.sum(np.exp(action_scores))\n    \n    action_index = np.random.choice(n_actions, p=probabilities)\n    return action_index",
          "objective": -449.99999972522266,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    action_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        average_score = np.mean(scores) if scores else 0\n        selection_counts[action_index] = len(scores)\n\n        # UCB exploration factor\n        if selection_counts[action_index] > 0:\n            ucb_exploration = np.sqrt(np.log(total_selection_count) / selection_counts[action_index])\n        else:\n            ucb_exploration = np.inf  # Encourage exploration for unselected actions\n\n        # Epsilon decay for exploration\n        epsilon = 1 - (current_time_slot / total_time_slots) if total_time_slots > 0 else 0\n        epsilon = max(0.1, epsilon)  # Lower bound for epsilon\n\n        # Mixed strategy combining exploration and exploitation\n        adjusted_score = (1 - epsilon) * average_score + epsilon * ucb_exploration\n        action_scores[action_index] = adjusted_score\n\n    # Normalize scores to enhance robustness\n    action_scores -= np.min(action_scores)  # Shift scores to be non-negative\n    total_score = np.sum(action_scores)\n    if total_score > 0:\n        action_scores /= total_score  # Normalize to sum to 1\n\n    # Softmax selection based on adjusted scores\n    probabilities = np.exp(action_scores) / np.sum(np.exp(action_scores))\n    \n    # Select action index based on the computed probabilities\n    action_index = np.random.choice(n_actions, p=probabilities)\n    return action_index",
          "objective": -449.99999968925897,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    \n    # Dynamic exploration factor\n    exploration_factor = 0.5 * (1 - (current_time_slot / total_time_slots))\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n        \n        # Handle average score calculation\n        average_score = np.mean(scores) if score_count > 0 else 0\n        \n        # Handle selection count\n        selection_count = score_count if score_count > 0 else 0.1\n        \n        # UCB calculation\n        ucb_score = average_score + exploration_factor * np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1e-5))\n        \n        action_scores[action_index] = ucb_score\n    \n    # Select the action with the highest UCB score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999968589094,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = []\n    \n    # Dynamic exploration factor\n    exploration_factor = (1 - current_time_slot / total_time_slots) * 0.5\n    \n    # Epsilon for exploration-exploitation balance\n    epsilon = max(0.1, 1 - (total_selection_count / (total_time_slots * 10)))\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n\n        # Average score and selection count\n        average_score = np.mean(scores) if score_count > 0 else 0.0\n        selection_count = score_count if score_count > 0 else 1  # Avoid division by zero\n\n        # Calculate UCB\n        ucb = average_score + exploration_factor * np.sqrt(np.log(total_selection_count + 1) / selection_count)\n\n        # Epsilon-Greedy adjustment\n        if np.random.rand() < epsilon:\n            action_score = np.random.rand()  # Randomly explore\n        else:\n            action_score = ucb  # Exploit based on UCB\n        \n        action_scores.append(action_score)\n    \n    # Select the action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999965789726,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = []\n    \n    # Dynamic exploration factor (reducing exploration over time)\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n\n        # Calculate average score safely\n        average_score = np.mean(scores) if score_count > 0 else 0.0\n        \n        # Adjusted selection frequency to avoid division by zero\n        selection_count = score_count + 1  # Ensures we avoid zero division\n        \n        # UCB calculation\n        ucb_value = average_score + exploration_weight * np.sqrt(np.log(total_selection_count + 1) / selection_count)\n\n        action_scores.append(ucb_value)\n    \n    # Select the action with the highest UCB score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999962859846,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = np.sqrt(np.log(total_selection_count + 1) / (np.array([len(score_set.get(i, [])) for i in range(8)]) + 1))  # UCB component\n\n    # Epsilon-greedy decay factor\n    epsilon = np.clip(1 - (current_time_slot / total_time_slots), 0.1, 1)  # Minimum epsilon for exploration\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        average_score = np.mean(scores) if scores else 0\n\n        # Adjust score considering exploration\n        adjusted_score = average_score + (exploration_factor[action_index] * epsilon)\n        \n        action_scores.append(adjusted_score)\n\n    # With epsilon-greedy strategy, explore randomly based on current epsilon\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, 8)  # Explore\n    else:\n        action_index = np.argmax(action_scores)  # Exploit\n\n    return action_index",
          "objective": -449.9999996096481,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_scores = np.zeros(action_count)\n    exploration_factor = np.sqrt(np.log(total_selection_count + 1) / (np.array([len(score_set.get(i, [])) for i in range(action_count)]) + 1))\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate an adjusted score for UCB\n        adjusted_score = average_score + exploration_factor[action_index]\n        \n        # Store the calculated score for this action\n        action_scores[action_index] = adjusted_score\n\n    # Softmax exploration adjusting based on decreasing exploration factor\n    temperature = max(0.1, 1 - (current_time_slot / total_time_slots))  # Ensuring a minimum temperature for exploration\n    probabilities = np.exp(action_scores / temperature) / np.sum(np.exp(action_scores / temperature))\n    \n    # Sample an action based on the computed probabilities\n    action_index = np.random.choice(range(action_count), p=probabilities)\n    \n    return action_index",
          "objective": -449.999999608735,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    epsilon = 0.1  # Epsilon for exploration\n    exploration_rate = 1.0 - (current_time_slot / total_time_slots)\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n        \n        # Calculate average score and handle edge cases\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate selection count and protection against division by zero\n        selection_count = max(score_count, 1)\n        \n        # UCB exploration bonus\n        ucb_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1))\n        \n        # Combined score with epsilon-greedy exploration\n        adjusted_score = average_score + exploration_rate * ucb_bonus\n        \n        action_scores.append(adjusted_score)\n\n    # Epsilon-greedy decision making\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(8)  # Explore randomly\n    else:\n        action_index = np.argmax(action_scores)  # Exploit the best action\n\n    return action_index",
          "objective": -449.9999995949092,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    \n    # Dynamic epsilon based on the time slot\n    epsilon = 1.0 - (current_time_slot / total_time_slots)\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n\n        # Calculate average score with error handling\n        average_score = np.mean(scores) if scores else 0\n        \n        # Adjusted selection count\n        selection_count = max(score_count, 1)  # Avoid division by zero\n        adjusted_selection_count = total_selection_count / selection_count\n\n        # Upper Confidence Bound (UCB) exploration bonus\n        ucb_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1))\n        \n        # Combined score considering UCB exploration and dynamic epsilon\n        adjusted_score = average_score + epsilon * ucb_bonus\n        action_scores.append(adjusted_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999955799956,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_rate = 1.0 - (current_time_slot / total_time_slots)\n    \n    epsilon = 1.0 / (current_time_slot + 1)  # Epsilon decreases over time\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n\n        # Calculate average score\n        average_score = np.mean(scores) if scores else 0\n        \n        # Adjusted selection count for UCB\n        selection_count = max(score_count, 1)  # At least 1 to avoid division by zero\n        adjusted_selection_count = total_selection_count / selection_count\n        \n        # Upper Confidence Bound (UCB) exploration bonus\n        ucb_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1))\n\n        # Modified score with exploration factor\n        adjusted_score = average_score + exploration_rate * ucb_bonus + epsilon * (1 - average_score)\n        action_scores.append(adjusted_score)\n\n    # Select action with the highest adjusted score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.9999995383933,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    \n    # Exploration percentage based on current time slot\n    exploration_factor = 0.5 * (1 - current_time_slot / total_time_slots)\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n        average_score = np.mean(scores) if score_count > 0 else 0\n        \n        # Upper Confidence Bound approach\n        if score_count > 0:\n            confidence_interval = np.sqrt(np.log(total_selection_count) / score_count)\n        else:\n            confidence_interval = np.inf  # Encourage exploration for actions never selected\n            \n        # Combine exploitation (average score) and exploration (confidence interval)\n        adjusted_score = average_score + exploration_factor * confidence_interval\n        action_scores[action_index] = adjusted_score\n    \n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.9999995154704,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = []\n    \n    # Dynamic exploration factor\n    exploration_factor = 1 - (current_time_slot / total_time_slots)\n    \n    # Epsilon for exploration-exploitation balance\n    epsilon = max(0.1, 1 - (total_selection_count / (total_time_slots * 15)))\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n\n        # Average score computation\n        average_score = np.mean(scores) if score_count > 0 else 0.0\n        \n        # Selection frequency safeguard against division by zero\n        selection_count = score_count + 1  # To prevent division by zero\n\n        # Calculate UCB\n        ucb = average_score + exploration_factor * np.sqrt(2 * np.log(total_selection_count + 1) / selection_count)\n\n        # Epsilon-Greedy adjustment: explore with a probability of epsilon\n        if np.random.rand() < epsilon:\n            action_score = np.random.rand()  # Randomly explore\n        else:\n            action_score = ucb  # Exploit based on UCB\n        \n        action_scores.append(action_score)\n    \n    # Select the action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999951546147,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = np.sqrt(np.log(total_selection_count + 1) / (np.array([len(score_set.get(i, [])) for i in range(8)]) + 1))  # UCB approach for exploration\n    epsilon = 1 - (current_time_slot / total_time_slots)  # Decreasing exploration over time\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        average_score = np.mean(scores) if scores else 0 \n\n        # Adjusting score with exploration based on UCB\n        adjusted_score = average_score + (epsilon * exploration_factor[action_index])\n        action_scores.append(adjusted_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.9999994830436,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_rate = 1.0 - (current_time_slot / total_time_slots)\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n        \n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate adjusted selection count\n        selection_count = max(score_count, 1)  # At least 1 to avoid division by zero\n        adjusted_selection_count = total_selection_count / selection_count\n        \n        # Upper Confidence Bound (UCB) exploration bonus\n        ucb_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1))\n        \n        # Combined score considering UCB exploration\n        adjusted_score = average_score + exploration_rate * ucb_bonus\n        action_scores.append(adjusted_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.9999993022183,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    total_scores = np.array([np.sum(score_set.get(i, [])) for i in range(num_actions)])\n    selection_counts = np.array([len(score_set.get(i, [])) for i in range(num_actions)])\n    \n    # Handle cases where count is zero to avoid division by zero\n    average_scores = np.divide(total_scores, selection_counts, out=np.zeros_like(total_scores, dtype=float), where=selection_counts != 0)\n\n    # Exploration factor using UCB\n    exploration_factor = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-3))  # small value to avoid division by zero\n\n    # Epsilon decay for exploration\n    epsilon = 1 - (current_time_slot / total_time_slots)  \n    exploration_adjustment = epsilon * exploration_factor\n    \n    # Compute adjusted scores\n    adjusted_scores = average_scores + exploration_adjustment\n\n    # Select the action with the highest adjusted score\n    action_index = np.argmax(adjusted_scores)\n    return action_index",
          "objective": -449.9999992083737,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = []\n    \n    # Dynamic exploration factor\n    exploration_factor = 1 - current_time_slot / total_time_slots\n    \n    # Epsilon for exploration-exploitation balance\n    epsilon = max(0.1, 1 - (total_selection_count / (total_time_slots * 10)))\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n\n        # Average score computation\n        average_score = np.mean(scores) if score_count > 0 else 0.0\n        \n        # Selection frequency safeguard against division by zero\n        selection_count = score_count + 1  # Avoid division by zero\n        \n        # Calculate UCB\n        ucb = average_score + exploration_factor * np.sqrt(np.log(total_selection_count + 1) / selection_count)\n\n        # Epsilon-Greedy adjustment\n        if np.random.rand() < epsilon:\n            action_score = np.random.rand()  # Randomly explore\n        else:\n            action_score = ucb  # Exploit based on UCB\n        \n        action_scores.append(action_score)\n    \n    # Select the action with the highest score (either from exploration or UCB)\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.9999991850219,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n\n    # Adaptive epsilon for exploration\n    epsilon_base = 0.1\n    epsilon_decay = 0.9\n    exploration_probability = epsilon_base * (epsilon_decay ** current_time_slot)\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n        average_score = np.mean(scores) if score_count > 0 else 0\n        \n        # UCB calculation\n        if score_count > 0:\n            confidence_interval = np.sqrt(2 * np.log(total_selection_count) / score_count)\n        else:\n            confidence_interval = np.inf  # Ensure exploration for untried actions\n            \n        # Combined score calculation\n        ucb_score = average_score + confidence_interval\n        action_scores[action_index] = ucb_score\n    \n    # Applying epsilon-greedy approach\n    if np.random.random() < exploration_probability:\n        action_index = np.random.choice(num_actions)  # Select random action for exploration\n    else:\n        action_index = np.argmax(action_scores)  # Select action with highest score\n    \n    return action_index",
          "objective": -449.99999913356544,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    \n    # Dynamic exploration factor based on current time slot\n    exploration_factor = (1 - (current_time_slot / total_time_slots)) ** 2\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n        \n        # Calculate the average score safely\n        average_score = np.mean(scores) if score_count > 0 else 0\n        \n        # Safety adjustment for selection count\n        selection_count = max(score_count, 1)  # Ensure it is at least 1\n        \n        # UCB calculation\n        ucb_score = average_score + exploration_factor * np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1e-5))\n        \n        action_scores[action_index] = ucb_score\n    \n    # Select and return the action with the highest UCB score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999894376555,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    \n    # Calculate average scores and selection counts\n    average_scores = np.array([np.mean(scores) if scores else 0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n\n    # Thompson Sampling as an alternative exploration strategy\n    # Assume scores are Beta distributed for Thompson Sampling\n    alpha = average_scores * selection_counts + 1  # Convert scores to count-like for Beta alpha\n    beta = (1 - average_scores) * selection_counts + 1  # Convert scores to count-like for Beta beta\n\n    sampled_means = np.random.beta(alpha, beta)\n\n    # Favor actions with higher sampled means\n    action_index = np.argmax(sampled_means)\n\n    # Epsilon-greedy parameter to encourage exploration\n    epsilon = np.clip(1 - (current_time_slot / total_time_slots), 0.1, 1)\n    \n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, 8)  # Explore\n\n    return action_index",
          "objective": -449.9999983833561,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    epsilon = max(0.1, 1.0 - (current_time_slot / total_time_slots))\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n        \n        average_score = np.mean(scores) if scores else 0\n        \n        # Handling selection count to avoid division by zero\n        adjusted_selection_count = max(score_count, 1)\n        \n        ucb_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / (adjusted_selection_count + 1))\n        \n        # Epsilon-Greedy exploration\n        if np.random.rand() < epsilon:\n            adjusted_score = np.random.rand()  # Random action for exploration\n        else:\n            adjusted_score = average_score + ucb_bonus\n        \n        action_scores.append(adjusted_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999834991297,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    \n    # Exploration rate\n    exploration_factor = 0.1 * (1 - current_time_slot / total_time_slots)\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if score_count > 0 else 0\n        \n        # Selection count adjustment\n        selection_count = score_count + 1\n        \n        # UCB score calculation\n        ucb_score = average_score + exploration_factor * np.sqrt(np.log(total_selection_count + 1) / selection_count)\n        \n        action_scores[action_index] = ucb_score\n    \n    # Select action with the highest UCB score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999831041623,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    \n    # Epsilon parameter for exploration\n    epsilon = max(1 - (total_selection_count / (total_time_slots * 10)), 0.1)\n    exploration_prob = np.random.rand()\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n        \n        # Calculate average score and selection count\n        average_score = np.mean(scores) if score_count > 0 else 0\n        selection_count = score_count if score_count > 0 else 1  # Ensure non-zero\n        \n        # Calculate UCB score\n        ucb_score = average_score + np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1e-5))\n        \n        # Combining exploration and exploitation\n        if exploration_prob < epsilon:\n            action_scores[action_index] = np.random.uniform(0, 1)  # Random exploration\n        else:\n            action_scores[action_index] = ucb_score\n    \n    # Select the action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999829437996,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = len(score_set)  # Should be 8 for actions 0-7\n    action_scores = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        action_scores[action_index] = np.mean(scores) if scores else 0.0\n\n    # Epsilon-greedy approach for exploration\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))  # Base exploration\n    if np.random.rand() < epsilon:  # Exploration\n        action_index = np.random.choice(action_count)\n    else:  # Exploitation\n        # Adding a small value to avoid division by zero and ensure exploration\n        bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n        adjusted_scores = action_scores + bonus\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": -449.9999981261715,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    num_actions = 8\n    \n    # Exploration parameter based on the current time slot\n    exploration_factor = 0.1 * (1 - current_time_slot / total_time_slots)\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n        \n        # Compute average score\n        average_score = np.mean(scores) if score_count > 0 else 0\n        \n        # Compute selection count (with a safeguard against zero)\n        selection_count = total_selection_count / (score_count + 1) if score_count > 0 else total_selection_count + 1\n        \n        # UCB calculation\n        ucb_score = average_score + exploration_factor * np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1e-5))\n        \n        action_scores.append(ucb_score)\n    \n    # Select the action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999759334025,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    epsilon = 0.1  # Exploration factor for Epsilon-Greedy\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n        \n        # Compute average score\n        average_score = np.mean(scores) if score_count > 0 else 0\n        \n        # Compute selection count with safety against division by zero\n        selection_count = score_count if score_count > 0 else 1\n        \n        # UCB calculation with added exploration\n        ucb_score = average_score + np.sqrt((2 * np.log(total_selection_count + 1) / (selection_count + 1e-5)))\n        \n        # Epsilon-Greedy exploration\n        if np.random.rand() < epsilon:\n            action_scores[action_index] = np.random.rand()\n        else:\n            action_scores[action_index] = ucb_score\n\n    # Select the action index with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999670487534,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = 0.1 * (1 - current_time_slot / total_time_slots)\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n        average_score = np.mean(scores) if score_count > 0 else 0  \n\n        # Explore less frequently chosen actions\n        selection_count = total_selection_count / (score_count + 1) if score_count > 0 else total_selection_count + 1\n        adjusted_exploration_bonus = exploration_factor * (1 / selection_count)\n\n        adjusted_score = average_score + adjusted_exploration_bonus\n        action_scores.append(adjusted_score)\n    \n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.9999964642508,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    exploration_factor = 0.1 * (1 - current_time_slot / total_time_slots)\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n        average_score = np.mean(scores) if score_count > 0 else 0\n        \n        # Epsilon-Greedy exploration\n        epsilon = max(0.1, exploration_factor * (1 / (score_count + 1))) if score_count > 0 else 0.1\n        exploration_bonus = np.random.rand() * epsilon\n        \n        adjusted_score = average_score + exploration_bonus\n        action_scores[action_index] = adjusted_score\n    \n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.9999944779173,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    exploration_factor = 0.1 * (1 - current_time_slot / total_time_slots)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n        average_score = np.mean(scores) if score_count > 0 else 0\n        \n        # Upper Confidence Bound (UCB) implementation\n        exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1) / (score_count + 1))) if score_count > 0 else np.inf\n        adjusted_score = average_score + exploration_factor * exploration_bonus\n        \n        action_scores[action_index] = adjusted_score\n    \n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.9999941849983,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    \n    # Avoid division by zero and initialize total scores\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n        average_score = np.mean(scores) if score_count > 0 else 0\n        \n        # Softmax for action selection probabilities\n        if total_selection_count > 0:\n            exploration_factor = 1 / (1 + score_count)  # Encourage exploration inversely proportional to selection count\n            adjusted_score = average_score + exploration_factor\n            \n            action_scores[action_index] = adjusted_score\n    \n    # Normalizing action scores for softmax probabilities\n    action_scores_exp = np.exp(action_scores - np.max(action_scores))  # Stable softmax\n    probabilities = action_scores_exp / np.sum(action_scores_exp)\n    \n    # Sample action based on calculated probabilities\n    action_index = np.random.choice(np.arange(num_actions), p=probabilities)\n    \n    return action_index",
          "objective": -449.9999898912797,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_weight = 1.0 - (current_time_slot / total_time_slots)\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n        average_score = np.mean(scores) if score_count > 0 else 0 \n\n        # Handling selection count for exploration\n        adjusted_selection_count = (total_selection_count / (score_count + 1)) if score_count > 0 else total_selection_count + 1\n        \n        # Exploration bonus calculated using UCB\n        exploration_bonus = exploration_weight * (np.sqrt(np.log(total_selection_count + 1) / (adjusted_selection_count + 1e-5)))\n\n        # Calculate the final adjusted score\n        adjusted_score = average_score + exploration_bonus\n        action_scores.append(adjusted_score)\n    \n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99998877726,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = []\n    \n    # Epsilon for exploration\n    epsilon = 1 / (current_time_slot + 1)  # Decaying exploration rate\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n\n        # Calculate average score and handle empty score situation\n        average_score = np.mean(scores) if score_count > 0 else 0.0\n        \n        # Selection frequency with safeguard against zero division\n        selection_count = score_count + 1  # Avoid division by zero\n        \n        # Calculate UCB\n        exploration_factor = np.sqrt(np.log(total_selection_count + 1) / selection_count)\n        ucb = average_score + exploration_factor\n        \n        # Append UCB to action scores\n        action_scores.append(ucb)\n\n    # Epsilon-Greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(num_actions)  # Explore randomly\n    else:\n        action_index = np.argmax(action_scores)  # Exploit best-known action\n    \n    return action_index",
          "objective": -449.999988507434,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    action_scores = np.zeros(n_actions)\n    exploration_factor = 0.1 * (1 - current_time_slot / total_time_slots)\n    \n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n        \n        # Calculate average score and handle the case of no scores\n        average_score = np.mean(scores) if score_count > 0 else 0\n       \n        # Calculate exploration term based on selection count\n        selection_count = total_selection_count / (score_count + 1) if score_count > 0 else total_selection_count + 1\n        adjusted_exploration_bonus = exploration_factor * (1 / selection_count)\n        \n        # Total score as average score plus exploration bonus\n        action_scores[action_index] = average_score + adjusted_exploration_bonus\n    \n    # Select the action with the highest adjusted score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.9999876186433,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    \n    # Exploration probability\n    epsilon = max(0.1, 0.5 * (1 - current_time_slot / total_time_slots))\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if score_count > 0 else 0\n        \n        # Calculate exploration factor\n        exploration_penalty = epsilon * np.sqrt(np.log(total_selection_count + 1) / (score_count + 1e-5))\n        \n        # Combine exploitation and exploration\n        adjusted_score = average_score + exploration_penalty\n        action_scores.append(adjusted_score)\n    \n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99998508585406,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = 0.1 * (1 - current_time_slot / total_time_slots)\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n        average_score = np.mean(scores) if score_count > 0 else 0 \n        \n        # Calculate the exploration bonus based on the selection count\n        selection_count = score_count + 1  # Adding 1 to avoid division by zero\n        exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / selection_count)\n\n        # Calculate the adjusted score\n        adjusted_score = average_score + exploration_bonus\n        action_scores.append(adjusted_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.9999846907805,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n        average_score = np.mean(scores) if score_count > 0 else 0\n        \n        # Avoid division by zero\n        action_count = score_count if score_count > 0 else 1\n        \n        # UCB formula: average_score + sqrt(2 * log(total_selection_count) / action_count)\n        ucb_value = average_score + np.sqrt(2 * np.log(total_selection_count + 1) / action_count)\n        action_scores[action_index] = ucb_value\n    \n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.999984017803,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters\n    exploration_factor = 0.1  # epsilon for exploration\n    min_selections = 1  # Minimum selections for UCB\n    total_actions = len(score_set)  # Total number of actions (0 to 7)\n\n    action_indices = list(score_set.keys())\n    average_scores = np.zeros(total_actions)\n    selection_counts = np.zeros(total_actions)\n\n    # Calculate average scores and selection counts\n    for index in action_indices:\n        scores = score_set[index]\n        if scores:\n            average_scores[index] = np.mean(scores)\n            selection_counts[index] = len(scores)\n\n    # UCB Calculation\n    ucb_values = np.zeros(total_actions)\n    for i in range(total_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt((2 * np.log(total_selection_count)) / selection_counts[i])\n            ucb_values[i] = average_scores[i] + confidence_interval\n        else:\n            ucb_values[i] = float('inf')  # Explore unselected actions\n\n    # Epsilon-greedy decision: explore or exploit\n    if np.random.rand() < exploration_factor:\n        # Exploration: choose a random action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploitation: choose the action with the highest UCB\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -449.9999833732864,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    epsilon = 0.1 * (1 - current_time_slot / total_time_slots)\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n        average_score = np.mean(scores) if score_count > 0 else 0  \n        \n        # Incorporate selection count to avoid division by zero\n        selection_count = score_count + 1  # adding 1 to avoid division by zero\n        exploration_bonus = epsilon * (1 / selection_count)\n\n        # Implement an adjusted average score combining exploration and exploitation\n        adjusted_score = average_score + exploration_bonus\n        action_scores.append(adjusted_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99997838788136,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    action_scores = np.zeros(n_actions)\n    \n    # Exploration factor decreases as time slots progress\n    exploration_factor = 0.1 * (1 - current_time_slot / total_time_slots)\n    minimum_exploration_threshold = 0.01  # lower limit for exploration\n    \n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n        \n        # Calculate average score, handling no scores gracefully\n        average_score = np.mean(scores) if score_count > 0 else 0.0\n        \n        # Adjust selection count to avoid division by zero\n        adjusted_selection_count = total_selection_count if score_count > 0 else 1\n        selection_count = score_count + 1  # +1 to avoid zero for UCB\n        \n        # UCB calculation\n        upper_confidence_bound = np.sqrt((2 * np.log(adjusted_selection_count)) / selection_count)\n\n        # Total score combines average score and upper confidence bound\n        action_scores[action_index] = average_score + max(exploration_factor * upper_confidence_bound, minimum_exploration_threshold)\n    \n    # Select the action with the highest score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -449.999975655354,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Gather selection counts for each action\n    for action_index in range(num_actions):\n        selection_counts[action_index] = len(score_set.get(action_index, []))\n\n    # Calculate average scores and UCB\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        average_score = np.mean(scores) if scores else 0\n        if selection_counts[action_index] > 0:\n            exploration_bonus = np.sqrt((np.log(total_selection_count + 1) / selection_counts[action_index]))\n        else:\n            exploration_bonus = float('inf')  # Infinite exploration if never selected\n\n        # Combined score: average + exploration\n        action_scores[action_index] = average_score + exploration_bonus\n\n    # Epsilon decay for exploration\n    epsilon = 1 - (current_time_slot / total_time_slots)\n\n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(np.arange(num_actions)[selection_counts == 0]) if np.all(selection_counts == 0) else np.random.choice(np.arange(num_actions))\n    else:\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -449.9999752344613,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    \n    # Dynamic epsilon decay factor\n    epsilon_decay = 1.0 - (current_time_slot / total_time_slots)\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n\n        # Calculate average score and handle the case with no selections\n        average_score = np.mean(scores) if scores else 0.0\n        \n        # If the action has never been selected, give it an index-based bonus\n        exploration_bonus = 0.0 if score_count > 0 else np.sqrt(total_selection_count) * epsilon_decay\n        \n        # Upper Confidence Bound (UCB) exploration bonus\n        ucb_bonus = (np.sqrt(np.log(total_selection_count + 1) / (score_count + 1))) if score_count > 0 else float('inf')\n        \n        # Combined score calculation\n        adjusted_score = average_score + exploration_bonus + ucb_bonus\n        action_scores.append(adjusted_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.9999733684195,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    exploration_rate = 0.1 * (1 - current_time_slot / total_time_slots)\n    epsilon = 0.1  # Epsilon for epsilon-greedy strategy\n\n    action_scores = []\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n        \n        # Compute average score\n        average_score = np.mean(scores) if score_count > 0 else 0\n        \n        # Compute selection count with safeguard against zero\n        selection_count = score_count if score_count > 0 else 1\n        \n        # UCB calculation\n        ucb_score = average_score + exploration_rate * np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1e-5))\n        \n        action_scores.append(ucb_score)\n\n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -449.999972870479,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    n_actions = 8\n    epsilon = 1 - (current_time_slot / total_time_slots)\n    \n    # Handle cases where action has not been selected yet (to prevent division by zero)\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        n_selections = len(scores)\n        \n        average_score = np.mean(scores) if n_selections > 0 else 0\n        exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / (n_selections + 1)) if n_selections > 0 else float('inf')\n        \n        # Combined score with exploration\n        adjusted_score = average_score + (epsilon * exploration_bonus)\n        action_scores.append(adjusted_score)\n\n    # Select the action with the highest adjusted score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99997185340254,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n        \n        # Calculate average score and selection count\n        average_score = np.mean(scores) if scores else 0\n        selection_count = score_count if score_count > 0 else 1\n        selection_counts[action_index] = selection_count\n        \n        # Epsilon-greedy exploration factor\n        epsilon = 1 / (1 + current_time_slot)\n        \n        # UCB calculation\n        ucb_score = average_score + np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1e-5))\n        action_scores[action_index] = (1 - epsilon) * ucb_score + epsilon * np.random.rand()\n    \n    # Select the action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.9999717475739,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = 0.1 * (1 - current_time_slot / total_time_slots)\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n        average_score = np.mean(scores) if score_count > 0 else 0\n        \n        # Compute selection count with a safeguard against division by zero\n        selection_count = (total_selection_count / (score_count + 1)) if score_count > 0 else total_selection_count + 1\n        \n        # Configure a more refined exploration bonus based on frequency\n        adjusted_exploration_bonus = exploration_factor * (1 / selection_count)\n        \n        # Combined score for balancing exploration and exploitation\n        adjusted_score = average_score + adjusted_exploration_bonus\n        action_scores.append(adjusted_score)\n\n    # Select the action with the highest adjusted score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.9999701698371,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # exploration factor\n    action_indices = list(score_set.keys())\n    average_scores = []\n\n    for index in action_indices:\n        scores = score_set[index]\n        if scores:\n            average_scores.append(np.mean(scores))\n        else:\n            average_scores.append(0)  # Assign zero if no scores recorded\n\n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        # Exploration: select randomly from the action indices\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploitation: select the action with the highest average score\n        max_score = max(average_scores)\n        best_actions = [i for i, score in zip(action_indices, average_scores) if score == max_score]\n        action_index = np.random.choice(best_actions)  # In case of ties, select randomly\n    \n    return action_index",
          "objective": -449.99996998215863,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = 0.1 * (1 - current_time_slot / total_time_slots)\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n        average_score = np.mean(scores) if score_count > 0 else 0  \n\n        # Modified selection count to encourage exploration\n        selection_count = score_count + 1  # adding 1 to avoid division by zero\n        adjusted_exploration_bonus = exploration_factor * (1 / selection_count)\n\n        # Implement UCB-like formula\n        adjusted_score = average_score + adjusted_exploration_bonus\n        action_scores.append(adjusted_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99996503967213,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    \n    # Calculation of exploration factor\n    exploration_factor = 0.1 * (1 - current_time_slot / total_time_slots)\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if score_count > 0 else 0\n        \n        # Calculate selection frequency\n        selection_count = total_selection_count / (score_count + 1) if score_count > 0 else total_selection_count + 1\n        \n        # Upper Confidence Bound (UCB) formula\n        ucb_score = average_score + exploration_factor * np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1e-5))\n        \n        action_scores.append(ucb_score)\n    \n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99995763303,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores for each action\n    action_indices = list(score_set.keys())\n    average_scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in action_indices])\n    \n    # Dynamic exploration coefficient based on remaining time slots\n    exploration_coefficient = (total_time_slots - current_time_slot) / total_time_slots\n    epsilon = 0.1 * exploration_coefficient\n\n    # Calculate selection frequencies\n    selection_frequencies = np.array([len(score_set[action]) for action in action_indices])\n    \n    # Compute exploration bonus based on frequency\n    exploration_bonus = np.where(selection_frequencies > 0, 1 / selection_frequencies, 1)\n    \n    # Explore or exploit\n    if np.random.rand() < epsilon:  # Exploration phase\n        # Compute probabilities for exploration\n        probabilities = exploration_bonus / exploration_bonus.sum()\n        action_index = np.random.choice(action_indices, p=probabilities)\n    else:  # Exploitation phase\n        # Combine average scores and exploration bonus\n        combined_scores = average_scores + exploration_bonus\n        action_index = action_indices[np.argmax(combined_scores)]\n    \n    return action_index",
          "objective": -449.99995636288094,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        average_score = np.mean(scores) if action_counts[action_index] > 0 else 0\n        action_scores[action_index] = average_score\n\n    # Calculate exploration bonus using UCB\n    for action_index in range(num_actions):\n        if action_counts[action_index] > 0:\n            exploration_bonus = np.sqrt((2 * np.log(total_selection_count)) / action_counts[action_index])\n            action_scores[action_index] += exploration_bonus\n\n    # Select the action with the maximum adjusted score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.9999537220994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = 0.1 * (1 - current_time_slot / total_time_slots)\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n        average_score = np.mean(scores) if score_count > 0 else 0\n        \n        # Use a small value to avoid division by zero and encourage exploration\n        selection_count = (total_selection_count / (score_count + 1)) if score_count > 0 else total_selection_count + 1\n        exploration_bonus = exploration_factor * (1 / selection_count)\n        \n        # Combine average score with exploration bonus\n        adjusted_score = average_score + exploration_bonus\n        action_scores.append(adjusted_score)\n    \n    # Select the action with the highest adjusted score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99993406145205,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = 0.1 * (1 - current_time_slot / total_time_slots)\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n        average_score = np.mean(scores) if score_count > 0 else 0 \n        \n        # Regularized selection count to prevent division by zero\n        adjusted_selection_count = total_selection_count + 1\n        if score_count > 0:\n            adjusted_selection_count = total_selection_count / score_count\n\n        # Adding a small exploration bonus for unchosen or less chosen actions\n        adjusted_exploration_bonus = exploration_factor * (1 / (adjusted_selection_count + 1e-5))  # use a small epsilon to avoid division by zero\n        \n        # Calculate the final adjusted score\n        adjusted_score = average_score + adjusted_exploration_bonus\n        action_scores.append(adjusted_score)\n    \n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99989342268907,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Dynamic exploration coefficient based on time\n    exploration_coefficient = (total_time_slots - current_time_slot) / total_time_slots\n    epsilon = 0.1 * exploration_coefficient  # Adjust epsilon based on remaining time\n    \n    action_indices = list(score_set.keys())\n    scores = []\n    \n    for action in action_indices:\n        if len(score_set[action]) == 0:  # If the action has never been selected\n            scores.append(0)  # Default score for unselected actions\n        else:\n            scores.append(np.mean(score_set[action]))  # Average score of selected actions\n\n    if np.random.rand() < epsilon:  # Exploration phase\n        # Favor less frequently selected actions for exploration\n        probabilities = [(1 / (len(score_set[action]) + 1)) if len(score_set[action]) > 0 else 1 \n                         for action in action_indices]\n        action_index = np.random.choice(action_indices, p=np.array(probabilities) / sum(probabilities))\n    else:  # Exploitation phase\n        # Get the index of the action with the highest average score\n        action_index = action_indices[np.argmax(scores)]\n    \n    return action_index",
          "objective": -449.9998820496256,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = 0.1 * (1 - current_time_slot / total_time_slots)\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n        average_score = np.mean(scores) if score_count > 0 else 0\n        \n        # Safeguard against division by zero\n        selection_count = score_count + 1  # Minimum count to avoid zero division\n        upper_confidence_bound = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_count)\n        \n        # Combined score for balancing exploration and exploitation with UCB\n        adjusted_score = average_score + exploration_factor * upper_confidence_bound\n        action_scores.append(adjusted_score)\n\n    # Select the action with the highest adjusted score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.999881896869,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    action_indices = range(8)\n    \n    # Adaptive exploration factor\n    exploration_factor = np.sqrt(np.log(total_selection_count + 1) / (total_selection_count + 1e-5))\n    \n    for action_index in action_indices:\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n        \n        # Calculate the average score\n        average_score = np.mean(scores) if score_count > 0 else 0\n        \n        # Apply a balance between exploration and exploitation\n        ucb_score = average_score + exploration_factor * np.sqrt(np.log(total_selection_count + 1) / (score_count + 1e-5))\n        \n        action_scores.append(ucb_score)\n\n    # Incorporate a small epsilon-greedy approach for further exploration\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        action_index = np.argmax(action_scores)  # Exploit\n   \n    return action_index",
          "objective": -449.99984448793384,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters for exploration and exploitation trade-off\n    epsilon_base = 0.1  # base exploration rate\n    exploration_weight = 0.5  # weight for exploration as time progresses\n    num_actions = len(score_set)\n    \n    # Calculate average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    for index in range(num_actions):\n        scores = score_set.get(index, [])\n        selection_counts[index] = len(scores)\n        if scores:\n            average_scores[index] = np.mean(scores)\n    \n    # Adaptive epsilon based on time slot\n    epsilon = epsilon_base * (1 - (current_time_slot / total_time_slots)) + exploration_weight * (current_time_slot / total_time_slots)\n\n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        # Exploration: select randomly from the action indices\n        action_index = np.random.choice(num_actions)\n    else:\n        # Exploitation: using Upper Confidence Bound (UCB)\n        ucb_values = average_scores + np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n        action_index = np.argmax(ucb_values)  # choose the action with highest UCB\n\n    return action_index",
          "objective": -449.9998143520987,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    \n    exploration_weight = 1.0  # Weight applied for exploration\n    exploration_bonus = exploration_weight * (1 - current_time_slot / total_time_slots)\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n\n        average_score = np.mean(scores) if score_count > 0 else 0 \n        \n        if total_selection_count > 0 and score_count > 0:\n            selection_ratio = total_selection_count / score_count\n        else:\n            selection_ratio = float('inf')  # Try to explore this action if never selected\n\n        exploration_term = exploration_bonus / (selection_ratio + 1e-5)  # Avoid division by zero\n        \n        adjusted_score = average_score + exploration_term\n        action_scores.append(adjusted_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.9995821819593,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    num_actions = len(score_set)\n    \n    # Epsilon-decay value for exploration\n    epsilon = 1 - (current_time_slot / total_time_slots)\n    epsilon = max(epsilon, 0.1)  # minimum epsilon to ensure exploration\n    \n    # Calculate average scores and explore factors\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        average_score = np.mean(scores) if scores else 0\n        # Count of how many times this action has been selected\n        action_count = len(scores)\n        \n        # Confidence level increase for exploration\n        explore_factor = np.sqrt(np.log(total_selection_count + 1) / (action_count + 1))\n        \n        # Combined adjusted score with exploration consideration \n        adjusted_score = average_score + (epsilon * explore_factor)\n        \n        action_scores.append(adjusted_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.9995632151378,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    \n    # Dynamic exploration factor based on time slot\n    exploration_factor = 0.1 * (1 - current_time_slot / total_time_slots)\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if score_count > 0 else 0\n        \n        # Calculate selection frequency with a minimum for stability\n        selection_count = score_count if score_count > 0 else 1\n        \n        # Upper Confidence Bound (UCB) calculation\n        ucb_score = average_score + exploration_factor * np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1e-5))\n        \n        action_scores.append(ucb_score)\n    \n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.9994331561529,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    action_scores = []\n    exploration_factor = 1.0 - (current_time_slot / total_time_slots)\n    \n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n\n        average_score = np.mean(scores) if score_count > 0 else 0  \n        \n        if total_selection_count > 0:\n            exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count) / (score_count + 1))\n        else:\n            exploration_bonus = 1  # Encourage exploration if no selections have been made\n        \n        adjusted_score = average_score + exploration_bonus\n        action_scores.append(adjusted_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99919917866794,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_factor = 0.1  # Base exploration probability\n    decay_rate = 0.9  # How quickly we shift from exploration to exploitation\n    action_indices = list(score_set.keys())\n    average_scores = []\n    action_counts = []\n\n    for index in action_indices:\n        scores = score_set[index]\n        if scores:\n            average_scores.append(np.mean(scores))\n            action_counts.append(len(scores))\n        else:\n            average_scores.append(0)  # Assign zero if no scores\n            action_counts.append(0)\n\n    # Normalize counts to avoid division by zero\n    normalized_counts = [count / total_selection_count if total_selection_count > 0 else 1 for count in action_counts]\n\n    # Calculate selection probabilities with exploration and exploitation balance\n    adjusted_scores = [(1 - exploration_factor * (1 - (current_time_slot / total_time_slots)))*avg \n                       + exploration_factor * (1 / (1 + count)) \n                       for avg, count in zip(average_scores, normalized_counts)]\n    \n    # Epsilon-greedy selection with adjusted scores\n    action_index = np.random.choice(action_indices) if np.random.rand() < exploration_factor else np.argmax(adjusted_scores)\n    \n    return action_index",
          "objective": -449.99852359957066,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = 0.1 * (1 - current_time_slot / total_time_slots)  # Decay exploration factor over time\n    \n    for action_index in range(8):\n        scores = score_set[action_index]\n        if len(scores) > 0:\n            average_score = np.mean(scores)\n        else:\n            average_score = 0  # If no scores, treat as 0\n\n        # Adjust score with exploration factor\n        exploration_bonus = exploration_factor * (1 / (len(scores) + 1))  # Encourage selection of less selected actions\n        adjusted_score = average_score + exploration_bonus\n        \n        action_scores.append(adjusted_score)\n    \n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.9982999579309,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_factor = 0.1  # Base exploration probability\n    decay_rate = 0.9  # Decay rate for exploitation\n    \n    action_indices = list(score_set.keys())\n    average_scores = []\n    action_counts = []\n\n    for index in action_indices:\n        scores = score_set[index]\n        action_count = len(scores)\n        action_counts.append(action_count)\n        # Calculate the average score, fallback to 0 if no scores are present\n        average_scores.append(np.mean(scores) if action_count > 0 else 0)\n\n    # Prevent division by zero and normalize counts\n    normalized_counts = np.array(action_counts) / total_selection_count if total_selection_count > 0 else np.zeros_like(action_counts)\n\n    # Calculate selection scores based on average scores and exploration encouragement\n    exploitation_scores = decay_rate * np.array(average_scores)\n    exploration_scores = exploration_factor * (1 / (1 + normalized_counts))\n\n    # Combine scores\n    combined_scores = exploitation_scores + exploration_scores\n\n    # Select action index based on the combined scores\n    action_index = np.random.choice(action_indices) if np.random.rand() < exploration_factor else np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -449.9982576833446,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = 1 - (current_time_slot / total_time_slots)\n    exploration_coefficient = 0.5  # This can be tuned\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n        average_score = np.mean(scores) if score_count > 0 else 0\n        \n        # Using a max selector count safeguard\n        selection_count = (score_count if score_count > 0 else 1)\n\n        # Calculate exploitation and exploration components\n        exploitation_score = average_score\n        exploration_score = exploration_factor * exploration_coefficient / selection_count\n        \n        # Combined score\n        adjusted_score = exploitation_score + exploration_score\n        action_scores.append(adjusted_score)\n\n    # Select the action with the highest adjusted score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.9912940882911,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = (1 - current_time_slot / total_time_slots)\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        score_count = len(scores)\n        average_score = np.mean(scores) if score_count > 0 else 0\n        \n        # Upper Confidence Bound (UCB) calculation\n        if score_count > 0:\n            exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (score_count))\n        else:\n            exploration_bonus = np.inf  # Encourage exploration for unchosen actions\n        \n        adjusted_score = average_score + exploration_factor * exploration_bonus\n        action_scores.append(adjusted_score)\n    \n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99119202708937,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_values = []\n    action_counts = []\n    \n    # Calculate average scores and counts of selections for each action\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        avg_score = np.mean(scores) if scores else 0\n        action_counts.append(len(scores))\n        \n        # Calculate a modified score considering the selection count\n        if total_selection_count > 0:\n            exploration_term = (1 / (1 + action_counts[-1]))  # encourage exploration\n            action_values.append(avg_score + exploration_term)\n        else:\n            action_values.append(avg_score)  # if no selections, default to average score\n            \n    # Select action based on the highest modified score\n    action_index = np.argmax(action_values)\n    \n    return action_index",
          "objective": -438.129635796454,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_threshold = 0.2  # Epsilon value for exploration\n    decay_factor = 0.95  # Decay factor for exploration over time\n    action_indices = list(score_set.keys())\n    average_scores = np.zeros(len(action_indices))\n    action_counts = np.zeros(len(action_indices))\n\n    for idx, action in enumerate(action_indices):\n        scores = score_set[action]\n        action_counts[idx] = len(scores)\n        if scores:\n            average_scores[idx] = np.mean(scores)\n\n    # Prevent division by zero for normalization\n    normalized_counts = action_counts / total_selection_count if total_selection_count > 0 else np.ones(len(action_indices))\n\n    # Calculate selection scores with exploration and exploitation balance\n    exploration_bonus = exploration_threshold * (1 / (1 + normalized_counts))\n    adjusted_scores = average_scores + exploration_bonus * (1 - (current_time_slot / total_time_slots))\n\n    # Softmax selection to balance exploration and exploitation\n    exp_scores = np.exp(adjusted_scores - np.max(adjusted_scores))  # Numerical stability\n    selection_probabilities = exp_scores / np.sum(exp_scores)\n    \n    action_index = np.random.choice(action_indices, p=selection_probabilities)\n    \n    return action_index",
          "objective": -436.8312027466074,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Exploration factor and parameters for UCB\n    exploration_factor = 1.0  # Exploration encourages trying less-selected actions\n    exploration_bonus = 1.5  # Boost for exploration in UCB calculation\n\n    action_indices = list(score_set.keys())\n    average_scores = []\n    action_counts = []\n\n    for index in action_indices:\n        scores = score_set[index]\n        action_count = len(scores)\n        action_counts.append(action_count)\n        # Calculate the average score, fallback to 0 if no scores are present\n        average_scores.append(np.mean(scores) if action_count > 0 else 0)\n\n    # Convert counts to numpy array for calculations\n    action_counts = np.array(action_counts)\n\n    # Calculate the selection scores using UCB approach\n    ucb_scores = average_scores + exploration_bonus * np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n\n    # Choose the action index with the highest UCB score\n    action_index = np.argmax(ucb_scores)\n\n    return action_index",
          "objective": -399.0615828224937,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_start = 1.0  # Initial exploration probability\n    exploration_end = 0.05  # Minimum exploration probability\n    decay_steps = total_time_slots  # Time slots after which exploration ends\n    action_indices = list(score_set.keys())\n\n    # Calculate average scores and action counts\n    average_scores = np.array([np.mean(scores) if scores else 0 for scores in score_set.values()])\n    action_counts = np.array([len(scores) for scores in score_set.values()])\n\n    # Normalize counts to avoid division by zero and calculate exploration weight\n    normalized_counts = action_counts / total_selection_count if total_selection_count > 0 else np.zeros_like(action_counts)\n    exploration_weight = (exploration_start - exploration_end) * (1 - current_time_slot / decay_steps) + exploration_end\n\n    # Calculate the adjusted scores for selection\n    adjusted_scores = average_scores * (1 - exploration_weight) + (1 / (1 + normalized_counts + 1e-6)) * exploration_weight\n\n    # Select action using weighted probabilities\n    action_index = np.random.choice(action_indices) if np.random.rand() < exploration_weight else np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 10883.255839228814,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    \n    # Calculate average scores and selection counts\n    average_scores = np.array([np.mean(scores) if scores else 0 for scores in score_set.values()])\n    action_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Normalization to avoid division by zero\n    normalized_counts = action_counts / total_selection_count if total_selection_count > 0 else np.zeros(num_actions)\n    \n    # Exploration mechanism\n    exploration_probability = max(0.05, 1.0 - (current_time_slot / total_time_slots))\n    \n    # Calculate selection weights\n    exploitation = average_scores * (1 - exploration_probability)\n    exploration = (1 / (1 + normalized_counts + 1e-6)) * exploration_probability\n    \n    # Combined score for selection\n    adjusted_scores = exploitation + exploration\n    \n    # Select action based on adjusted scores\n    action_index = np.random.choice(action_indices) if np.random.rand() < exploration_probability else np.argmax(adjusted_scores)\n    \n    return action_index",
          "objective": 14753.601450748538,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    \n    scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        if len(score_set[action]) > 0:\n            scores[idx] = np.mean(score_set[action])\n            selection_counts[idx] = len(score_set[action])\n\n    # Set epsilon to decrease over time and increase exploration in the early stages\n    epsilon = max(0.1 * (1.0 - current_time_slot / total_time_slots), 0.01)\n    \n    # Selection using the epsilon-greedy strategy with UCB consideration\n    if np.random.random() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        with np.errstate(divide='ignore', invalid='ignore'):  # Handling potential division by zero\n            ucb_values = scores + np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n            action_index = action_indices[np.argmax(ucb_values)]\n\n    return action_index",
          "objective": 18455.4215253374,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize variables\n    action_indices = list(score_set.keys())\n    average_scores = []\n    selection_counts = [len(score_set[index]) for index in action_indices]\n\n    # Calculate average scores\n    for index in action_indices:\n        scores = score_set[index]\n        average_scores.append(np.mean(scores) if scores else 0)\n\n    # Calculate UCB values for actions\n    ucb_values = []\n    for i in range(len(action_indices)):\n        if selection_counts[i] == 0:\n            # If the action has never been selected, assign a high value for initial exploration\n            ucb_values.append(float('inf'))\n        else:\n            ucb = average_scores[i] + np.sqrt((2 * np.log(total_selection_count)) / selection_counts[i])\n            ucb_values.append(ucb)\n\n    # Select action using the UCB values\n    action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 30100.723935986578,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate exploration weight based on current time slot\n    exploration_weight = max(0.1, 1 - (current_time_slot / total_time_slots))\n\n    # Prepare variables to calculate selection scores\n    average_scores = np.zeros(8)\n    selection_counts = np.zeros(8)\n\n    for idx in range(8):\n        if score_set[idx]:\n            average_scores[idx] = np.mean(score_set[idx])\n            selection_counts[idx] = len(score_set[idx])\n\n    # Adjust scores to include exploration factor\n    adjusted_scores = average_scores + (exploration_weight * (1.0 / (1 + selection_counts)))\n\n    # Dynamic selection based on exploration vs. exploitation\n    if np.random.rand() < exploration_weight:\n        # Explore: randomly select an action based on selection counts\n        eligible_indices = np.where(selection_counts == 0)[0]\n        if eligible_indices.size > 0:\n            action_index = np.random.choice(eligible_indices)\n        else:\n            action_index = np.random.randint(0, 8)  # fallback to random if everything has been selected\n    else:\n        # Exploit: select action with highest adjusted score\n        action_index = int(np.argmax(adjusted_scores))\n\n    return action_index",
          "objective": 32483.05322205258,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))  # Dynamic epsilon\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, 8)  # Explore: select random action\n    else:\n        average_scores = []\n        for idx in range(8):\n            if score_set[idx]:  # If there are scores for this action\n                avg_score = np.mean(score_set[idx])\n            else:  # If no scores have been recorded for this action\n                avg_score = 0\n            average_scores.append(avg_score)\n        action_index = int(np.argmax(average_scores))  # Exploit: select action with highest average score\n\n    return action_index",
          "objective": 34802.95075435997,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    scores = np.zeros(len(action_indices))\n    selection_counts = np.zeros(len(action_indices))\n\n    # Calculate average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        if len(score_set[action]) > 0:\n            scores[idx] = np.mean(score_set[action])\n            selection_counts[idx] = len(score_set[action])\n\n    # Set epsilon to decrease over time\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01)\n\n    if np.random.random() < epsilon:  # Exploration phase\n        action_index = np.random.choice(action_indices)\n    else:  # Exploitation phase\n        ucb_values = scores + np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))  # Avoid division by zero\n        action_index = action_indices[np.argmax(ucb_values)]\n\n    return action_index",
          "objective": 56798.18185208483,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\nimport random\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration probability\n    action_indices = list(score_set.keys())\n    scores = []\n    \n    for action in action_indices:\n        if len(score_set[action]) == 0:  # If the action has never been selected\n            scores.append(0)  # Can treat unselected actions with a score of 0\n        else:\n            scores.append(np.mean(score_set[action]))  # Average score of selected actions\n    \n    if random.random() < epsilon:  # Exploration phase\n        action_index = random.choice(action_indices)\n    else:  # Exploitation phase\n        # Get the index of the action with the highest average score\n        action_index = action_indices[np.argmax(scores)]\n    \n    return action_index",
          "objective": 57723.2433074563,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon_start = 1.0\n    epsilon_end = 0.1\n    epsilon_decay = total_time_slots\n    \n    # Calculate epsilon for exploration vs exploitation\n    epsilon = epsilon_start - (epsilon_start - epsilon_end) * (current_time_slot / epsilon_decay)\n    epsilon = max(epsilon, epsilon_end)  # Make sure epsilon does not go below epsilon_end\n    \n    # Calculate average scores for each action\n    avg_scores = []\n    for action_index in range(8):  # From 0 to 7\n        scores = score_set.get(action_index, [])\n        if scores:\n            avg_scores.append(np.mean(scores))\n        else:\n            avg_scores.append(0.0)  # Handle case where there are no scores for the action\n            \n    # Choosing action based on epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, 8)  # Randomly select an action\n    else:\n        action_index = np.argmax(avg_scores)  # Select the action with the highest average score\n    \n    return action_index",
          "objective": 82001.5809499548,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores and action counts\n    avg_scores = []\n    action_counts = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_count = len(scores)\n        avg_score = np.mean(scores) if action_count > 0 else 0.0\n        avg_scores.append(avg_score)\n        action_counts.append(action_count)\n    \n    # Normalizing action counts\n    action_counts = np.array(action_counts)\n    total_action_count = np.sum(action_counts) if np.sum(action_counts) > 0 else 1\n    normalized_counts = action_counts / total_action_count\n    \n    # Exploration factor\n    exploration_factor = (1 - current_time_slot / total_time_slots) * (total_selection_count / (total_selection_count + 1))\n    exploration_factor = max(0.1, exploration_factor)\n    \n    # Calculate selection probabilities\n    probabilities = (1 - exploration_factor) * np.array(avg_scores) + exploration_factor * (1 - normalized_counts)\n    \n    # Normalize probabilities\n    probabilities /= np.sum(probabilities)\n    \n    # Select action based on computed probabilities\n    action_index = np.random.choice(np.arange(8), p=probabilities)\n    \n    return action_index",
          "objective": 83258.84152178724,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Decay epsilon over time to encourage exploitation\n    max_epsilon = 0.1  # Maximum exploration factor\n    min_epsilon = 0.01  # Minimum exploration factor\n    epsilon = max(min_epsilon, max_epsilon * (1 - current_time_slot / total_time_slots))\n\n    action_indices = list(score_set.keys())\n    average_scores = []\n    selection_counts = []\n\n    for index in action_indices:\n        scores = score_set[index]\n        if scores:\n            average_scores.append(np.mean(scores))\n            selection_counts.append(len(scores))\n        else:\n            average_scores.append(0)\n            selection_counts.append(0)\n    \n    # Add a small constant to selection counts to avoid division by zero\n    selection_counts = np.array(selection_counts) + 1e-6\n    total_selections = np.sum(selection_counts)\n\n    # Calculate UCB (Upper Confidence Bound) for each action\n    ucb_scores = average_scores + np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts)\n    \n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        best_actions = np.flatnonzero(ucb_scores == np.max(ucb_scores))\n        action_index = np.random.choice(best_actions)\n\n    return action_index",
          "objective": 124642.4357690489,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_factor = 1.0  # Start with max exploration\n    action_indices = list(score_set.keys())\n    average_scores = []\n    selection_counts = []\n    \n    for index in action_indices:\n        scores = score_set[index]\n        average_score = np.mean(scores) if scores else 0\n        average_scores.append(average_score)\n        selection_counts.append(len(scores))  # Count of selections for the action\n\n    # Normalizing selection counts to avoid division by zero\n    normalized_counts = np.array(selection_counts) + 1  # Add 1 for smoothing\n    total_counts = sum(normalized_counts)\n\n    # Calculate probabilities\n    probabilities = np.array(average_scores) / np.array(normalized_counts)\n    probabilities /= probabilities.sum()  # Normalize to get probabilities\n\n    # Epsilon decay based on time slot progression\n    epsilon = exploration_factor * (1 - (current_time_slot / total_time_slots))\n\n    # Epsilon-greedy selection with decreasing exploration\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = np.random.choice(action_indices, p=probabilities)\n\n    return action_index",
          "objective": 128128.60913298602,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for idx, action in enumerate(action_indices):\n        if len(score_set[action]) > 0:\n            scores[idx] = np.mean(score_set[action])\n            selection_counts[idx] = len(score_set[action])\n\n    # Set exploration decay\n    exploration_weight = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01)\n    \n    # Exploration-exploitation balance\n    ucb_values = np.zeros(num_actions)\n    for idx in range(num_actions):\n        if selection_counts[idx] > 0:\n            ucb_values[idx] = scores[idx] + np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[idx])\n        else:\n            ucb_values[idx] = np.inf  # Encourage exploring unselected actions\n\n    # Using an epsilon-greedy approach to balance exploration and exploitation\n    if np.random.random() < exploration_weight:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = action_indices[np.argmax(ucb_values)]\n    \n    return action_index",
          "objective": 132506.0310710088,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    avg_scores = []\n    exploration_factor = 1.0 - (current_time_slot / total_time_slots)\n    \n    for action_index in range(8):  # Actions are indexed from 0 to 7\n        scores = score_set.get(action_index, [])\n        if scores:\n            avg_score = np.mean(scores)\n        else:\n            avg_score = 0\n            \n        # Calculate exploration weight\n        selection_count = len(scores)\n        if total_selection_count > 0:\n            exploration_weight = (1 - (selection_count / total_selection_count))\n        else:\n            exploration_weight = 1\n            \n        # Calculate total value for decision making\n        total_value = avg_score + (exploration_factor * exploration_weight)\n        avg_scores.append(total_value)\n\n    action_index = np.argmax(avg_scores)\n    return action_index",
          "objective": 139474.28783688674,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))  # Dynamic exploration rate\n    exploration_threshold = np.random.rand()\n    \n    if exploration_threshold < epsilon:\n        action_index = np.random.randint(0, 8)  # Explore\n    else:\n        average_scores = []\n        for idx in range(8):\n            num_selections = len(score_set[idx])\n            if num_selections > 0:\n                avg_score = np.mean(score_set[idx])\n                confidence_interval = (avg_score * np.sqrt(num_selections)) / (total_selection_count + 1)\n                adjusted_score = avg_score + confidence_interval  # UCB strategy\n            else:\n                adjusted_score = 0  # No past performance\n            average_scores.append(adjusted_score)\n        \n        action_index = int(np.argmax(average_scores))  # Exploit\n\n    return action_index",
          "objective": 142107.45428240506,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 1.0 - (current_time_slot / total_time_slots)  # Decrease exploration as time slots progress\n    random_value = np.random.random()\n    \n    if random_value < epsilon:  # Exploration\n        action_index = np.random.randint(0, 8)\n    else:  # Exploitation\n        average_scores = []\n        for i in range(8):\n            if len(score_set[i]) > 0:\n                average_scores.append(np.mean(score_set[i]))\n            else:\n                average_scores.append(0)  # Handle zero selection\n        \n        action_index = int(np.argmax(average_scores))  # Select action with highest average score\n    \n    return action_index",
          "objective": 226416.93076159482,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_values = []\n    action_counts = []\n    \n    # Dynamic epsilon for exploration vs exploitation\n    epsilon = 1.0 - (current_time_slot / total_time_slots)\n\n    # Calculate average scores and counts of selections for each action\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        avg_score = np.mean(scores) if scores else 0\n        action_count = len(scores)\n        action_counts.append(action_count)\n        \n        # Compute a modified score; penalizing less selected actions based on epsilon\n        exploration_term = (1 - avg_score) * (1 / (1 + action_count)) if action_count > 0 else 1\n        action_values.append((1 - epsilon) * avg_score + epsilon * exploration_term)\n\n    # Select action based on the highest modified score\n    action_index = np.argmax(action_values)\n    \n    return action_index",
          "objective": 242722.43113229814,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    \n    for action_index in range(num_actions):\n        if action_index in score_set and len(score_set[action_index]) > 0:\n            action_scores[action_index] = np.mean(score_set[action_index])\n        else:\n            action_scores[action_index] = 0  # No score means action not performed yet\n\n    exploration_rate = 1.0 / (1 + total_selection_count)  # Epsilon decreases with more selections\n    if np.random.rand() < exploration_rate:\n        # Explore: Choose a random action\n        action_index = np.random.randint(0, num_actions)\n    else:\n        # Exploit: Use softmax to select the action\n        exp_scores = np.exp(action_scores - np.max(action_scores))  # Stability in softmax\n        probabilities = exp_scores / np.sum(exp_scores)\n        action_index = np.random.choice(range(num_actions), p=probabilities)\n\n    return action_index",
          "objective": 243926.03983374804,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    \n    # Calculate action counts\n    action_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Avoid zero division\n    total_action_count = np.sum(action_counts) if np.sum(action_counts) > 0 else 1\n    normalized_counts = action_counts / total_action_count\n\n    # Exploration factor with an adaptive approach\n    exploration_factor = (1 - current_time_slot / total_time_slots) * (total_selection_count / (total_selection_count + 1))\n    exploration_factor = max(0.1, exploration_factor)\n    \n    # Calculate selection scores\n    scores = (1 - exploration_factor) * avg_scores + exploration_factor * (1 - normalized_counts)\n    \n    # Normalize scores to get selection probabilities\n    probabilities = np.clip(scores, 0, None)  # Ensure no negative probabilities\n    probabilities /= np.sum(probabilities) if np.sum(probabilities) > 0 else 1  # Normalizing\n    \n    # Select action based on computed probabilities\n    action_index = np.random.choice(np.arange(8), p=probabilities)\n    \n    return action_index",
          "objective": 301109.59693101305,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    scores = []\n    \n    # Calculate average scores and selection counts\n    for action in action_indices:\n        if len(score_set[action]) == 0:\n            scores.append(0)  # Default score for unselected actions\n        else:\n            scores.append(np.mean(score_set[action]))  # Average score of selected actions\n    \n    # Normalize scores for softmax probability calculation\n    max_score = np.max(scores) if scores else 0\n    normalized_scores = np.array(scores) - max_score\n    exp_scores = np.exp(normalized_scores)\n    \n    # Calculate the selection probabilities using softmax\n    probabilities = exp_scores / np.sum(exp_scores) if np.sum(exp_scores) > 0 else np.ones(len(action_indices)) / len(action_indices)\n\n    # Epsilon decay based on the current time slot\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.05)  # Minimum epsilon value\n    \n    if np.random.rand() < epsilon:  # Exploration phase\n        action_index = np.random.choice(action_indices)\n    else:  # Exploitation phase\n        action_index = np.random.choice(action_indices, p=probabilities)\n    \n    return action_index",
          "objective": 552777.6110900209,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration factor\n    exploration_threshold = epsilon * (total_time_slots - current_time_slot) / (total_time_slots)\n\n    action_averages = []\n    action_counts = [len(scores) for scores in score_set.values()]\n    \n    for idx in range(8):\n        if action_counts[idx] == 0:\n            action_averages.append(0)  # If not selected, default average to 0\n        else:\n            action_averages.append(np.mean(score_set[idx]))\n\n    if np.random.rand() < exploration_threshold:\n        action_index = np.random.randint(0, 8)  # Explore: select a random action\n    else:\n        action_values = [\n            (action_averages[idx] * (action_counts[idx] / total_selection_count) \n             if total_selection_count > 0 else 0)\n            for idx in range(8)\n        ]\n        action_index = np.argmax(action_values)  # Exploit: select the best action based on calculated values\n\n    return action_index",
          "objective": 765372.3778512229,
          "other_inf": null
     }
]