[
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate averages and counts for each action\n    for index in range(num_actions):\n        scores = score_set.get(index, [])\n        selection_counts[index] = len(scores)\n        avg_scores[index] = np.mean(scores) if scores else 0\n\n    # Smooth selection counts to prevent division by zero\n    smoothed_selection_counts = selection_counts + 1e-5\n    exploration_factor = np.sqrt((2 * np.log(total_selection_count + 1)) / smoothed_selection_counts)\n\n    # Time weighting to emphasize immediate relevance\n    time_weight = (total_time_slots - current_time_slot) / total_time_slots\n\n    # Upper Confidence Bound (UCB) strategy\n    ucb_values = avg_scores + exploration_factor * time_weight\n\n    # Epsilon-Greedy with adaptive epsilon\n    epsilon = max(0.01, 0.1 * (1 - (current_time_slot / total_time_slots)))\n\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(num_actions)\n    else:\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -449.9999999999679,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon = 0.1  # Exploration probability\n    decay_factor = (total_time_slots - current_time_slot) / total_time_slots  # Time decay factor\n\n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for index in range(num_actions):\n        scores = score_set.get(index, [])\n        selection_counts[index] = len(scores)\n        if selection_counts[index] > 0:\n            avg_scores[index] = np.mean(scores)\n        else:\n            avg_scores[index] = 0  # Handle actions never selected\n\n    # Calculate UCB values\n    ucb_values = np.zeros(num_actions)\n    for index in range(num_actions):\n        if selection_counts[index] > 0:\n            confidence_interval = np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts[index])\n            ucb_values[index] = avg_scores[index] + confidence_interval * decay_factor\n        else:\n            ucb_values[index] = float('inf')  # Prioritize unselected actions\n\n    # Epsilon-Greedy strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)  # Explore\n    else:\n        action_index = np.argmax(ucb_values)  # Exploit\n\n    return action_index",
          "objective": -449.9999999999648,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    # Calculate average scores and selection counts\n    for action in range(num_actions):\n        scores = score_set.get(action, [])\n        selection_counts[action] = len(scores)\n        avg_scores[action] = np.mean(scores) if scores else 0.0\n    \n    # Initialize UCB values\n    ucb_values = np.zeros(num_actions)\n    \n    # Calculate UCB values with decay factor for exploration\n    for action in range(num_actions):\n        if selection_counts[action] == 0:\n            ucb_values[action] = float('inf')  # Prioritize unexplored actions\n        else:\n            exploration_term = np.sqrt(np.log(total_selection_count + 1) / (selection_counts[action] + 1e-10))\n            decay_factor = (total_time_slots - current_time_slot) / total_time_slots  # Decay over time\n            ucb_values[action] = avg_scores[action] + decay_factor * exploration_term\n    \n    # Select the action with the highest UCB value\n    action_index = np.argmax(ucb_values)\n    \n    return action_index",
          "objective": -449.9999999999608,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    # Calculate average scores and selection counts\n    for action in range(num_actions):\n        scores = score_set.get(action, [])\n        selection_counts[action] = len(scores)\n        avg_scores[action] = np.mean(scores) if scores else 0.0\n    \n    # Calculate softmax probabilities with exploration adjusted by decay\n    decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = avg_scores + decay_factor * (1 / (selection_counts + 1e-10))\n    exp_scores = np.exp(adjusted_scores - np.max(adjusted_scores))  # Stability for softmax\n    probabilities = exp_scores / np.sum(exp_scores)\n    \n    # Select action based on the calculated probabilities\n    action_index = np.random.choice(num_actions, p=probabilities)\n    \n    return action_index",
          "objective": -449.9999999999582,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for index in range(num_actions):\n        scores = score_set.get(index, [])\n        selection_counts[index] = len(scores)\n        avg_scores[index] = np.mean(scores) if selection_counts[index] > 0 else 0\n\n    # Calculate UCB values\n    ucb_values = np.zeros(num_actions)\n    exploration_weight = 2  # You can tweak this parameter for exploration encouragement\n    stability_factor = 0.1  # Minimum weight for less explored actions\n\n    for index in range(num_actions):\n        if selection_counts[index] == 0:\n            ucb_values[index] = float('inf')  # Encouraging exploration for never selected actions\n        else:\n            exploration_term = exploration_weight * np.sqrt(np.log(total_selection_count) / selection_counts[index])\n            time_factor = (total_time_slots - current_time_slot) / total_time_slots\n            # Prevent the time factor from approaching zero too quickly\n            time_adjusted_exploration = exploration_term * (time_factor + stability_factor)\n            ucb_values[index] = avg_scores[index] + time_adjusted_exploration\n\n    # Select the action with the maximum UCB value\n    action_index = np.argmax(ucb_values)\n    \n    return action_index",
          "objective": -449.99999999994975,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    for index in range(num_actions):\n        scores = score_set.get(index, [])\n        selection_counts[index] = len(scores)\n        avg_scores[index] = np.mean(scores) if selection_counts[index] > 0 else 0\n\n    epsilon = 0.1  # Exploration rate\n    exploration_factor = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n    time_weight = (total_time_slots - current_time_slot) / total_time_slots\n\n    # Epsilon-Greedy Selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(num_actions)\n    else:\n        ucb_values = avg_scores + exploration_factor * time_weight\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -449.999999999941,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon_start = 0.3  # Initial exploration probability\n    epsilon_end = 0.05   # Minimum epsilon for exploration\n    min_selections_for_ucb = 5  # Minimum selections before using UCB formula\n\n    # Compute epsilon\n    epsilon = max(epsilon_end, epsilon_start * (1 - (current_time_slot / total_time_slots)))\n    \n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for index in range(num_actions):\n        scores = score_set.get(index, [])\n        selection_counts[index] = len(scores)\n        if selection_counts[index] > 0:\n            avg_scores[index] = np.mean(scores)\n        else:\n            avg_scores[index] = 0  # Default to 0 for actions never selected\n\n    # Calculate UCB values\n    ucb_values = np.zeros(num_actions)\n    for index in range(num_actions):\n        if selection_counts[index] >= min_selections_for_ucb:\n            confidence_interval = np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts[index])\n            ucb_values[index] = avg_scores[index] + confidence_interval\n        else:\n            ucb_values[index] = float('inf')  # Favor unselected actions\n\n    # Epsilon-Greedy strategy with UCB\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)  # Explore\n    else:\n        action_index = np.argmax(ucb_values)  # Exploit the best UCB action\n\n    return action_index",
          "objective": -449.9999999999393,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon_start = 0.1  # Initial exploration probability\n    epsilon_decay = 0.01  # Decay rate for exploration probability\n    min_epsilon = 0.01  # Minimum epsilon for exploration\n\n    epsilon = max(min_epsilon, epsilon_start * (1 - (current_time_slot / total_time_slots)))\n    \n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for index in range(num_actions):\n        scores = score_set.get(index, [])\n        selection_counts[index] = len(scores)\n        if selection_counts[index] > 0:\n            avg_scores[index] = np.mean(scores)\n        else:\n            avg_scores[index] = 0  # Default to 0 for actions never selected\n\n    # Calculate UCB values\n    ucb_values = np.zeros(num_actions)\n    for index in range(num_actions):\n        if selection_counts[index] > 0:\n            confidence_interval = np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts[index])\n            ucb_values[index] = avg_scores[index] + confidence_interval\n        else:\n            ucb_values[index] = float('inf')  # Favor unselected actions\n\n    # Epsilon-Greedy strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)  # Explore\n    else:\n        action_index = np.argmax(ucb_values)  # Exploit\n\n    return action_index",
          "objective": -449.99999999993275,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action in range(num_actions):\n        scores = score_set.get(action, [])\n        selection_counts[action] = len(scores)\n        avg_scores[action] = np.mean(scores) if scores else 0.0\n\n    # Calculate UCB values with a dynamic exploration factor\n    ucb_values = np.zeros(num_actions)\n\n    for action in range(num_actions):\n        if selection_counts[action] == 0:\n            ucb_values[action] = float('inf')  # Prioritize unexplored actions\n        else:\n            exploration_term = np.sqrt(np.log(total_selection_count) / selection_counts[action])\n            # Dynamic exploration coefficient based on historical average scores\n            avg_score_factor = (avg_scores[action] + 1e-10)  # Small constant to avoid division by zero\n            exploration_coefficient = 1 / (1 + avg_score_factor)\n            ucb_values[action] = avg_scores[action] + exploration_coefficient * exploration_term\n\n    # Select the action with the highest UCB value\n    action_index = np.argmax(ucb_values)\n    \n    return action_index",
          "objective": -449.9999999999311,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action in range(num_actions):\n        scores = score_set.get(action, [])\n        selection_counts[action] = len(scores)\n        avg_scores[action] = np.mean(scores) if scores else 0.0\n\n    # Calculate UCB values with a dynamic exploration factor\n    ucb_values = np.zeros(num_actions)\n    exploration_coefficient = 2  # Coefficient for exploration\n\n    for action in range(num_actions):\n        if selection_counts[action] == 0:\n            ucb_values[action] = float('inf')  # Favor unexplored actions\n        else:\n            exploration_term = exploration_coefficient * np.sqrt(np.log(total_selection_count) / selection_counts[action])\n            # Weight exploration term based on recency\n            time_factor = (total_time_slots - current_time_slot) / total_time_slots\n            decay_factor = (total_time_slots - current_time_slot + 1) / total_time_slots  # Adjust for time\n            ucb_values[action] = avg_scores[action] + exploration_term * time_factor * decay_factor\n\n    # Select the action with the highest UCB value\n    action_index = np.argmax(ucb_values)\n    \n    return action_index",
          "objective": -449.9999999999275,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon = 0.1  # Exploration probability\n    decay_factor = (total_time_slots - current_time_slot) / total_time_slots  # Time decay factor\n\n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for index in range(num_actions):\n        scores = score_set.get(index, [])\n        selection_counts[index] = len(scores)\n        if selection_counts[index] > 0:\n            avg_scores[index] = np.mean(scores)\n        else:\n            avg_scores[index] = 0  # Default to 0 for actions never selected\n\n    # Calculate UCB values\n    ucb_values = np.zeros(num_actions)\n    for index in range(num_actions):\n        if selection_counts[index] > 0:\n            confidence_interval = np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts[index])\n            ucb_values[index] = avg_scores[index] + confidence_interval * decay_factor\n        else:\n            ucb_values[index] = float('inf')  # Favor unselected actions\n\n    # Epsilon-Greedy strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)  # Explore\n    else:\n        action_index = np.argmax(ucb_values)  # Exploit\n\n    return action_index",
          "objective": -449.99999999992514,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    selection_counts = np.zeros(num_actions)\n    avg_scores = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for index in range(num_actions):\n        scores = score_set.get(index, [])\n        selection_counts[index] = len(scores)\n        avg_scores[index] = np.mean(scores) if selection_counts[index] > 0 else 0\n\n    # Initialize UCB values\n    ucb_values = np.zeros(num_actions)\n    exploration_weight = 1.5  # Tuned parameter for exploration encouragement\n    \n    # Calculate UCB values\n    for index in range(num_actions):\n        if selection_counts[index] == 0:\n            ucb_values[index] = float('inf')  # High UCB for unselected actions\n        else:\n            exploration_term = exploration_weight * np.sqrt(np.log(total_selection_count) / selection_counts[index])\n            time_adjusted_exploration = exploration_term * (1 + (total_time_slots - current_time_slot) / total_time_slots)\n            ucb_values[index] = avg_scores[index] + time_adjusted_exploration\n\n    # Select the action with the maximum UCB value\n    action_index = np.argmax(ucb_values)\n    \n    return action_index",
          "objective": -449.999999999496,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon = 0.1  # Exploration probability\n    decay_factor = (total_time_slots - current_time_slot) / total_time_slots  # Time decay factor\n\n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for index in range(num_actions):\n        scores = score_set.get(index, [])\n        selection_counts[index] = len(scores)\n        if selection_counts[index] > 0:\n            avg_scores[index] = np.mean(scores)\n\n    # Calculate UCB values\n    ucb_values = np.zeros(num_actions)\n    for index in range(num_actions):\n        if selection_counts[index] > 0:\n            confidence_interval = np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts[index])\n            ucb_values[index] = avg_scores[index] + confidence_interval * decay_factor\n        else:\n            ucb_values[index] = float('inf')  # Favor unselected actions\n\n    # Epsilon-Greedy strategy with enhanced exploration noise\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)  # Explore\n    else:\n        action_index = np.argmax(ucb_values)  # Exploit\n\n    return action_index",
          "objective": -449.9999999983635,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    successes = np.zeros(num_actions)\n    failures = np.zeros(num_actions)\n\n    # Calculating successes and failures based on score_set\n    for index in range(num_actions):\n        scores = score_set.get(index, [])\n        successes[index] = sum(1 for score in scores if score > 0)  # Count non-zero scores\n        failures[index] = len(scores) - successes[index]  # Total number of selections minus successes\n\n    # Thompson Sampling: Draw samples from Beta distribution based on successes and failures\n    beta_samples = np.random.beta(successes + 1, failures + 1)\n\n    # Time weighting based on recency\n    time_weight = (total_time_slots - current_time_slot) / total_time_slots\n\n    # Multiply beta samples by time weight\n    weighted_samples = beta_samples * time_weight\n\n    # Select action with the maximum weighted sample\n    action_index = np.argmax(weighted_samples)\n\n    return action_index",
          "objective": -449.9999999980005,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon = 0.1  # Exploration probability\n    decay_factor = (total_time_slots - current_time_slot) / total_time_slots  # Time decay factor\n\n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for index in range(num_actions):\n        scores = score_set.get(index, [])\n        selection_counts[index] = len(scores)\n        avg_scores[index] = np.mean(scores) if selection_counts[index] > 0 else 0\n\n    # Calculate UCB values\n    ucb_values = np.zeros(num_actions)\n    for index in range(num_actions):\n        if selection_counts[index] > 0:\n            confidence_interval = np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts[index])\n            ucb_values[index] = avg_scores[index] + confidence_interval * decay_factor\n        else:\n            ucb_values[index] = float('inf')  # Prioritize unselected actions\n\n    # Epsilon-Greedy strategy for exploration\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)  # Explore\n    else:\n        action_index = np.argmax(ucb_values)  # Exploit based on UCB\n\n    return action_index",
          "objective": -449.99999999527336,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon_start = 0.5  # Initial exploration probability\n    epsilon_decay = 0.95  # Decay factor for epsilon\n    min_epsilon = 0.1  # Minimum exploration probability\n\n    # Calculate the current epsilon value based on time\n    epsilon = max(min_epsilon, epsilon_start * (epsilon_decay ** (current_time_slot / total_time_slots)))\n\n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for index in range(num_actions):\n        scores = score_set.get(index, [])\n        selection_counts[index] = len(scores)\n        avg_scores[index] = np.mean(scores) if scores else 0  # Handle actions never selected\n\n    # Calculate confidence intervals\n    confidence_intervals = np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n\n    # Calculate UCB values\n    ucb_values = avg_scores + confidence_intervals\n\n    # Epsilon-Greedy strategy with decay\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)  # Explore\n    else:\n        action_index = np.argmax(ucb_values)  # Exploit\n\n    return action_index",
          "objective": -449.9999999951985,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    # Calculate average scores and selection counts\n    for action in range(num_actions):\n        scores = score_set.get(action, [])\n        selection_counts[action] = len(scores)\n        avg_scores[action] = np.mean(scores) if scores else 0.0\n\n    # Adjust the exploration coefficient based on the current time and total time slots\n    exploration_coefficient = 1.0 / (1 + (current_time_slot / total_time_slots))  # Decrease exploration over time\n\n    # Calculate UCB values\n    ucb_values = np.zeros(num_actions)\n\n    for action in range(num_actions):\n        if selection_counts[action] == 0:\n            ucb_values[action] = float('inf')  # Unexplored actions\n        else:\n            exploration_term = np.sqrt(np.log(total_selection_count + 1) / (selection_counts[action] + 1e-10))  # Smooth to avoid division by zero\n            ucb_values[action] = avg_scores[action] + exploration_coefficient * exploration_term\n\n    # Select the action with the highest UCB value\n    action_index = np.argmax(ucb_values)\n    \n    return action_index",
          "objective": -449.9999999919393,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    # Dynamic exploration-exploitation parameters\n    exploration_bonus_factor = 1.5  # Factor to enhance exploration component\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for index in range(num_actions):\n        scores = score_set.get(index, [])\n        selection_counts[index] = len(scores)\n        avg_scores[index] = np.mean(scores) if scores else 0.0  # Avoid division by zero\n\n    # Initialize UCB values\n    ucb_values = np.zeros(num_actions)\n    for index in range(num_actions):\n        if selection_counts[index] > 0:\n            confidence_interval = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[index])\n            ucb_values[index] = avg_scores[index] + (exploration_bonus_factor * confidence_interval)\n        else:\n            ucb_values[index] = float('inf')  # Unselected actions favored\n\n    # Calculate the exploration probability based on current time slot\n    epsilon = max(0.01, (1 - (current_time_slot / total_time_slots)))  # Decay function for exploration\n\n    # Epsilon-Greedy strategy with UCB\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)  # Explore: choose a random action\n    else:\n        action_index = np.argmax(ucb_values)  # Exploit: choose action with highest UCB value\n\n    return action_index",
          "objective": -449.99999998880156,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    # Calculate average scores and selection counts\n    for index in range(num_actions):\n        scores = score_set.get(index, [])\n        selection_counts[index] = len(scores)\n        avg_scores[index] = np.mean(scores) if selection_counts[index] > 0 else 0\n\n    # Calculate UCB values\n    ucb_values = np.zeros(num_actions)\n    exploration_weight = 2  # Encouragement constant for exploration\n    min_exploration_count = 5  # Ensures a minimum number of observations for meaningful exploration\n\n    for index in range(num_actions):\n        if selection_counts[index] == 0:\n            ucb_values[index] = float('inf')  # Encourage selection of actions not yet taken\n        else:\n            exploration_term = exploration_weight * np.sqrt(np.log(total_selection_count) / selection_counts[index])\n            time_factor = (total_time_slots - current_time_slot) / total_time_slots\n            \n            # Dynamic decay based on how long it's been since the action was last selected\n            time_decay = np.exp(-selection_counts[index] / (total_time_slots / 2))\n            adjusted_exploration = exploration_term * time_decay * time_factor\n            \n            ucb_values[index] = avg_scores[index] + adjusted_exploration\n\n    # Select the action with the maximum UCB value\n    action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -449.9999999451664,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    total_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Collect total scores and selection counts\n    for action_index, scores in score_set.items():\n        total_scores[action_index] = np.sum(scores)  # Sum up the scores\n        selection_counts[action_index] = len(scores)  # Count the selections\n\n    # Calculate averages while handling zero counts\n    avg_scores = np.divide(total_scores, selection_counts, out=np.zeros_like(total_scores), where=selection_counts != 0)\n\n    # Incorporate beta distributions for Thompson Sampling\n    alpha = avg_scores + 1  # Adding 1 to avoid zero count issues\n    beta = (total_selection_count - selection_counts) + 1  # Adding 1 for the other parameter of beta distribution\n\n    # Sample from the beta distribution for each action\n    sampled_values = np.random.beta(alpha, beta)\n\n    # Epsilon for exploration\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n    \n    # Decision to explore or exploit\n    if np.random.rand() < epsilon:  # Explore\n        action_index = np.random.choice(num_actions)\n    else:  # Exploit based on sampled values\n        action_index = np.argmax(sampled_values)\n\n    return action_index",
          "objective": -449.99999993134924,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action in range(num_actions):\n        scores = score_set.get(action, [])\n        selection_counts[action] = len(scores)\n        avg_scores[action] = np.mean(scores) if scores else 0.0\n\n    # Calculate UCB values with weighted exploration\n    ucb_values = np.zeros(num_actions)\n    exploration_coefficient = 2  # Coefficient for exploration\n\n    for action in range(num_actions):\n        if selection_counts[action] == 0:\n            ucb_values[action] = float('inf')  # Give priority to unexplored actions\n        else:\n            exploration_term = exploration_coefficient * np.sqrt(np.log(total_selection_count) / selection_counts[action])\n            time_factor = (total_time_slots - current_time_slot + 1) / total_time_slots\n            decay_factor = np.exp(-0.1 * (total_time_slots - current_time_slot))  # Decay for future relevance\n            ucb_values[action] = avg_scores[action] + (exploration_term * time_factor * decay_factor)\n\n    # Select the action with the highest UCB value\n    action_index = np.argmax(ucb_values)\n    \n    return action_index",
          "objective": -449.9999999280048,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    total_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Collect total scores and selection counts\n    for action_index, scores in score_set.items():\n        total_scores[action_index] = np.sum(scores)\n        selection_counts[action_index] = len(scores)\n\n    # Calculate average scores while avoiding division by zero\n    avg_scores = np.where(selection_counts > 0, total_scores / selection_counts, 0)\n\n    # UCB computation\n    exploration_bonus = np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1))\n    ucb_values = avg_scores + exploration_bonus\n\n    # Dynamic epsilon for exploration, decay over time\n    epsilon = max(0.05, 1 - (current_time_slot / (total_time_slots + 1)))\n\n    # Epsilon-Greedy decision\n    if np.random.rand() < epsilon:  # Explore\n        action_index = np.random.choice(num_actions)\n    else:  # Exploit using UCB\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -449.9999998988854,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    total_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Collect total scores and selection counts\n    for action_index, scores in score_set.items():\n        total_scores[action_index] = np.sum(scores)  # Use np.sum for clarity\n        selection_counts[action_index] = len(scores)\n\n    # Calculate average scores, handle zero counts\n    avg_scores = np.where(selection_counts > 0, total_scores / selection_counts, 0)\n\n    # Upper Confidence Bound (UCB) computation\n    exploration_bonus = np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1))\n    ucb_values = avg_scores + exploration_bonus\n\n    # Dynamic epsilon for exploration, ensuring some exploration\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n    \n    # Deciding whether to explore or exploit\n    if np.random.rand() < epsilon:  # Explore\n        action_index = np.random.choice(num_actions)\n    else:  # Exploit\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -449.9999998772111,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    exploration_proportion = 0.2  # Adjusted exploration factor\n\n    # Initialize arrays for average scores and selection counts\n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for index in range(num_actions):\n        scores = score_set.get(index, [])\n        selection_counts[index] = len(scores)\n        if selection_counts[index] > 0:\n            avg_scores[index] = np.mean(scores)\n\n    # Prevent division by zero for UCB calculations\n    with np.errstate(divide='ignore', invalid='ignore'):\n        ucb_values = avg_scores + np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1))\n\n    # Temporal decay for uncertainty\n    time_decay = (total_time_slots - current_time_slot) / total_time_slots\n\n    # Adjust UCB values based on temporal decay\n    ucb_values *= time_decay\n\n    # Epsilon-Greedy exploration vs. exploitation\n    exploration_threshold = np.random.rand()\n    if exploration_threshold < exploration_proportion:\n        action_index = np.random.randint(num_actions)  # Explore\n    else:\n        action_index = np.argmax(ucb_values)  # Exploit\n\n    return action_index",
          "objective": -449.99999982293053,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for index in range(num_actions):\n        scores = score_set.get(index, [])\n        selection_counts[index] = len(scores)\n        avg_scores[index] = np.mean(scores) if selection_counts[index] > 0 else 0\n\n    # Define epsilon and exploration factor\n    epsilon = 1 / (total_selection_count + 1)  # Decaying epsilon\n    exploration_factor = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n    time_weight = (total_time_slots - current_time_slot) / total_time_slots\n\n    # Enrich UCB with time-weighted exploration and averaging\n    ucb_values = avg_scores + exploration_factor * time_weight\n    \n    # Epsilon-Greedy Selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(num_actions)  # Explore\n    else:\n        action_index = np.argmax(ucb_values)  # Exploit\n\n    return action_index",
          "objective": -449.9999997802265,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    # Calculate average scores and selection counts\n    for index in range(num_actions):\n        scores = score_set.get(index, [])\n        selection_counts[index] = len(scores)\n        avg_scores[index] = np.mean(scores) if selection_counts[index] > 0 else 0\n\n    # Calculate UCB values\n    ucb_values = np.zeros(num_actions)\n    exploration_weight = 2  # Encouragement constant for exploration\n    epsilon = 0.1  # Probability of exploration\n\n    for index in range(num_actions):\n        # Handle actions that have not been selected\n        if selection_counts[index] == 0:\n            ucb_values[index] = float('inf')  # Encourage selection of actions not yet taken\n        else:\n            exploration_term = exploration_weight * np.sqrt(np.log(total_selection_count) / selection_counts[index])\n            ucb_values[index] = avg_scores[index] + exploration_term\n\n    # Epsilon-Greedy implementation for exploration\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)  # Random selection for exploration\n    else:\n        action_index = np.argmax(ucb_values)  # Select based on UCB values\n    \n    return action_index",
          "objective": -449.9999969629713,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon = 0.1  # Exploration probability\n    \n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Populate average scores and selection counts\n    for index in range(num_actions):\n        scores = score_set.get(index, [])\n        selection_counts[index] = len(scores)\n        if selection_counts[index] > 0:\n            avg_scores[index] = np.mean(scores)\n\n    # Calculate UCB values\n    ucb_values = np.zeros(num_actions)\n    confidence_interval = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1))  # Avoid division by zero\n    time_decay = (total_time_slots - current_time_slot) / total_time_slots\n\n    # Adjust UCB calculations\n    for index in range(num_actions):\n        ucb_values[index] = avg_scores[index] + confidence_interval[index] * time_decay\n    \n    # Epsilon-Greedy for exploration-exploitation\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)  # Explore\n    else:\n        action_index = np.argmax(ucb_values)  # Exploit\n\n    return action_index",
          "objective": -449.9999962484144,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    for index in range(num_actions):\n        scores = score_set.get(index, [])\n        selection_counts[index] = len(scores)\n        avg_scores[index] = np.mean(scores) if selection_counts[index] > 0 else 0\n\n    ucb_values = np.zeros(num_actions)\n    \n    for index in range(num_actions):\n        if selection_counts[index] == 0:\n            ucb_values[index] = float('inf')  # Encourage exploration for actions not yet selected\n        else:\n            exploration_term = np.sqrt((2 * np.log(total_selection_count)) / selection_counts[index])\n            time_factor = (total_time_slots - current_time_slot + 1) / total_time_slots\n            ucb_values[index] = avg_scores[index] + exploration_term * time_factor\n\n    action_index = np.argmax(ucb_values)\n    \n    return action_index",
          "objective": -449.9999950801541,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    exploration_proportion = 0.1  # Adjusted exploration factor\n\n    # Initialize arrays for scores, selection counts, and adjusted values\n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for index in range(num_actions):\n        scores = score_set.get(index, [])\n        selection_counts[index] = len(scores)\n        avg_scores[index] = np.mean(scores) if scores else 0  # Avoid division by zero\n\n    # Avoid division by zero in UCB calculations\n    with np.errstate(divide='ignore', invalid='ignore'):\n        ucb_values = avg_scores + np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1))\n\n    # Temporal decay for UCB values\n    time_decay = (total_time_slots - current_time_slot) / total_time_slots\n    ucb_values *= time_decay\n\n    # Epsilon-Greedy exploration vs. exploitation\n    if np.random.rand() < exploration_proportion:\n        action_index = np.random.randint(num_actions)  # Explore\n    else:\n        action_index = np.argmax(ucb_values)  # Exploit\n\n    return action_index",
          "objective": -449.9999842000769,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    total_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        total_scores[action_index] = sum(scores)\n        selection_counts[action_index] = len(scores)\n\n    # Compute average scores while avoiding division by zero\n    avg_scores = np.divide(total_scores, selection_counts, out=np.zeros_like(total_scores), where=selection_counts != 0)\n\n    # Calculate UCB values\n    exploration_bonus = np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1))\n    ucb_values = avg_scores + exploration_bonus\n\n    # Dynamic epsilon for exploration\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n    \n    # Random threshold for epsilon-greedy exploration\n    explore = np.random.rand() < epsilon\n    \n    if explore:  # Explore (Epsilon-Greedy)\n        action_index = np.random.choice(num_actions)\n    else:  # Exploit (UCB)\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -449.99996856389964,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon = 0.1  # Exploration probability\n    alpha = 1.0  # Shape parameter for Beta distribution (Thompson Sampling)\n    \n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    # Calculate average scores and selection counts\n    for index in range(num_actions):\n        scores = score_set.get(index, [])\n        selection_counts[index] = len(scores)\n        if selection_counts[index] > 0:\n            avg_scores[index] = np.mean(scores)\n        \n    ucb_values = np.zeros(num_actions)\n    \n    for index in range(num_actions):\n        if selection_counts[index] == 0:\n            ucb_values[index] = float('inf')  # Unselected actions\n        else:\n            exploration_factor = np.sqrt((2 * np.log(total_selection_count)) / selection_counts[index])\n            time_decay = (total_time_slots - current_time_slot) / total_time_slots\n            ucb_values[index] = avg_scores[index] + exploration_factor * time_decay\n\n    # Thompson Sampling using Beta distribution\n    beta_samples = np.zeros(num_actions)\n    for index in range(num_actions):\n        success_count = len(score_set.get(index, []))\n        failure_count = total_selection_count - success_count\n        beta_samples[index] = np.random.beta(success_count + alpha, failure_count + alpha)\n    \n    # Epsilon-greedy decision making\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)  # Explore\n    else:\n        # Combine UCB and Thompson Sampling for action selection\n        combined_scores = ucb_values + beta_samples\n        action_index = np.argmax(combined_scores)  # Exploit\n\n    return action_index",
          "objective": -449.99993405085985,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for index in range(num_actions):\n        scores = score_set.get(index, [])\n        selection_counts[index] = len(scores)\n        avg_scores[index] = np.mean(scores) if scores else 0.0\n\n    # Calculate UCB with a focus on exploitation and exploration\n    ucb_values = np.zeros(num_actions)\n    for index in range(num_actions):\n        if selection_counts[index] == 0:\n            ucb_values[index] = float('inf')  # Encourage exploration\n        else:\n            exploration_term = np.sqrt((2 * np.log(total_selection_count)) / selection_counts[index])\n            time_factor = (total_time_slots - current_time_slot + 1) / total_time_slots\n            ucb_values[index] = avg_scores[index] + exploration_term * time_factor\n\n    # Use softmax to derive probabilities for action selection based on UCB values\n    exp_ucb = np.exp(ucb_values - np.max(ucb_values))  # For numerical stability\n    probabilities = exp_ucb / np.sum(exp_ucb)\n\n    # Sample action based on computed probabilities\n    action_index = np.random.choice(num_actions, p=probabilities)\n    \n    return action_index",
          "objective": -449.999911439538,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    total_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate total scores and selection counts\n    for action_index, scores in score_set.items():\n        total_scores[action_index] = sum(scores)  # Sum of scores for the action\n        selection_counts[action_index] = len(scores)  # Number of times the action has been selected\n\n    # Handle zero selections gracefully\n    avg_scores = np.where(selection_counts > 0, total_scores / selection_counts, 0)  # Avoid division by zero\n\n    # Initialize UCB values\n    ucb_values = np.zeros(num_actions)\n    exploration_bonus = np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1))  # Improved exploration bonus\n\n    ucb_values = avg_scores + exploration_bonus  # Calculate UCB values\n\n    # Dynamic epsilon exploration strategy\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))  # Ensure a minimum exploration rate\n    if np.random.rand() < epsilon:  # Explore\n        action_index = np.random.choice(num_actions)\n    else:  # Exploit\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -449.9993851435312,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon = 0.1  # Exploration probability\n    \n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    # Populate average scores and selection counts\n    for index in range(num_actions):\n        scores = score_set.get(index, [])\n        selection_counts[index] = len(scores)\n        if selection_counts[index] > 0:\n            avg_scores[index] = np.mean(scores)\n\n    # Calculate UCB values\n    ucb_values = np.zeros(num_actions)\n    confidence_interval = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))  # Avoiding division by zero\n    time_decay = (total_time_slots - current_time_slot) / total_time_slots\n\n    # Adjust UCB values\n    for index in range(num_actions):\n        ucb_values[index] = avg_scores[index] + confidence_interval[index] * time_decay\n    \n    # Combine UCB method with Epsilon-Greedy for exploration-exploitation\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)  # Explore\n    else:\n        action_index = np.argmax(ucb_values)  # Exploit\n\n    return action_index",
          "objective": -449.99911949299013,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action in range(num_actions):\n        scores = score_set.get(action, [])\n        selection_counts[action] = len(scores)\n        avg_scores[action] = np.mean(scores) if scores else 0.0\n\n    # Calculate UCB values\n    ucb_values = np.zeros(num_actions)\n    exploration_coefficient = 2  # Coefficient for exploration\n\n    for action in range(num_actions):\n        if selection_counts[action] == 0:\n            ucb_values[action] = float('inf')  # Prioritize unexplored actions\n        else:\n            # UCB formula\n            exploration_term = exploration_coefficient * np.sqrt(np.log(total_selection_count) / selection_counts[action])\n            time_factor = (total_time_slots - current_time_slot + 1) / total_time_slots\n            ucb_values[action] = avg_scores[action] + exploration_term * time_factor\n\n    # Select the action with the highest UCB value\n    action_index = np.argmax(ucb_values)\n    \n    return action_index",
          "objective": -449.99819980795274,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon = 0.1  # Exploration probability\n    temperature = 1.0  # Softmax temperature\n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for index in range(num_actions):\n        scores = score_set.get(index, [])\n        selection_counts[index] = len(scores)\n        avg_scores[index] = np.mean(scores) if selection_counts[index] > 0 else 0\n\n    # Calculate UCB values\n    ucb_values = np.zeros(num_actions)\n    for index in range(num_actions):\n        if selection_counts[index] == 0:\n            ucb_values[index] = float('inf')  # Assign a high value for unselected actions\n        else:\n            exploration_factor = np.sqrt((2 * np.log(total_selection_count)) / selection_counts[index])\n            time_decay = (total_time_slots - current_time_slot) / total_time_slots\n            ucb_values[index] = avg_scores[index] + exploration_factor * time_decay\n\n    # Combine UCB and average scores for a balanced approach\n    combined_values = (1 - epsilon) * ucb_values + epsilon * avg_scores\n\n    # Softmax selection\n    exp_values = np.exp(combined_values / temperature)\n    softmax_probs = exp_values / np.sum(exp_values)\n    \n    # Select action based on calculated probabilities\n    action_index = np.random.choice(num_actions, p=softmax_probs)\n\n    return action_index",
          "objective": -449.9979956481095,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    total_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate total scores and selection counts\n    for action_index, scores in score_set.items():\n        total_scores[action_index] = sum(scores)\n        selection_counts[action_index] = len(scores)\n\n    # Handle zero selections gracefully\n    avg_scores = np.where(selection_counts > 0, total_scores / selection_counts, 0)\n\n    # Adjust the UCB confidence term based on the current time slot\n    ucb_values = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        exploration_bonus = np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts[action_index] + 1)) if selection_counts[action_index] > 0 else np.inf\n        ucb_values[action_index] = avg_scores[action_index] + exploration_bonus\n\n    # Prioritize non-selected actions when they have a lower count\n    if total_selection_count < num_actions:  # to ensure we select all actions initially\n        action_index = total_selection_count\n    else:\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -449.9978814080521,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    total_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate total scores and selection counts\n    for action_index, scores in score_set.items():\n        total_scores[action_index] = sum(scores)\n        selection_counts[action_index] = len(scores)\n\n    # Handle zero selections gracefully\n    avg_scores = np.where(selection_counts > 0, total_scores / selection_counts, 0)\n\n    # Initialize UCB values\n    ucb_values = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        # Adjust exploration bonus based on the time slot\n        exploration_bonus = np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts[action_index] + 1)) if selection_counts[action_index] > 0 else np.inf\n        ucb_values[action_index] = avg_scores[action_index] + exploration_bonus\n\n    # Dynamic epsilon exploration strategy\n    epsilon = 1 - (current_time_slot / total_time_slots)\n    if np.random.rand() < epsilon:  # Explore\n        action_index = np.random.choice(num_actions)\n    else:  # Exploit\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -449.995919636139,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon = 0.1  # Exploration probability\n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    # Calculate average scores and selection counts\n    for index in range(num_actions):\n        scores = score_set.get(index, [])\n        selection_counts[index] = len(scores)\n        if selection_counts[index] > 0:\n            avg_scores[index] = np.mean(scores)\n    \n    # Calculate UCB values\n    ucb_values = np.zeros(num_actions)\n    for index in range(num_actions):\n        if selection_counts[index] == 0:\n            ucb_values[index] = float('inf')  # Assign a high value for unselected actions\n        else:\n            exploration_factor = np.sqrt((2 * np.log(total_selection_count)) / selection_counts[index])\n            time_decay = (total_time_slots - current_time_slot) / total_time_slots\n            ucb_values[index] = avg_scores[index] + exploration_factor * time_decay\n\n    # Epsilon-Greedy decision making\n    if np.random.rand() < epsilon:  # Explore\n        action_index = np.random.randint(num_actions)\n    else:  # Exploit\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -449.9951911133332,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    avg_scores = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n\n    # Compute average scores and counts\n    for index in range(action_count):\n        scores = score_set.get(index, [])\n        selection_counts[index] = len(scores)\n        if selection_counts[index] > 0:\n            avg_scores[index] = np.mean(scores)\n\n    # Calculate effective exploration-exploitation value\n    exploration_bonus = 1.5  # Hyperparameter for exploration strength\n    ucb_values = np.zeros(action_count)\n\n    for index in range(action_count):\n        if selection_counts[index] == 0:\n            ucb_values[index] = float('inf')  # Encourage exploration of untried actions\n        else:\n            ucb_values[index] = avg_scores[index] + np.sqrt((2 * np.log(total_selection_count)) / selection_counts[index])\n\n    # Time-sensitive decay\n    time_decay = (total_time_slots - current_time_slot + 1) / total_time_slots\n    ucb_values *= time_decay\n\n    # Epsilon-Greedy selection mechanism\n    epsilon = 0.1  # Probability for exploration\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)  # Randomly select one action\n    else:\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -449.9922978183303,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon = 0.1  # Exploration probability\n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    # Calculate average scores and selection counts\n    for index in range(num_actions):\n        scores = score_set.get(index, [])\n        selection_counts[index] = len(scores)\n        if selection_counts[index] > 0:\n            avg_scores[index] = np.mean(scores)\n    \n    # Calculate UCB values with time decay\n    ucb_values = np.zeros(num_actions)\n    for index in range(num_actions):\n        if selection_counts[index] == 0:\n            ucb_values[index] = float('inf')  # Assign high value for unselected actions\n        else:\n            exploration_factor = np.sqrt((2 * np.log(total_selection_count)) / selection_counts[index])\n            time_decay = (total_time_slots - current_time_slot) / total_time_slots\n            ucb_values[index] = avg_scores[index] + exploration_factor * time_decay\n    \n    # Combine exploration and exploitation\n    max_ucb = np.max(ucb_values)\n    exp_ucb_values = np.exp(ucb_values - max_ucb)  # Softmax trick for balancing\n    softmax_probabilities = exp_ucb_values / np.sum(exp_ucb_values)\n\n    # Epsilon-Greedy decision making\n    if np.random.rand() < epsilon:  # Explore\n        action_index = np.random.randint(num_actions)\n    else:  # Exploit\n        action_index = np.random.choice(np.arange(num_actions), p=softmax_probabilities)\n\n    return action_index",
          "objective": -449.99178842877325,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for index in range(n_actions):\n        scores = score_set.get(index, [])\n        selection_counts[index] = len(scores)\n        if selection_counts[index] > 0:\n            avg_scores[index] = np.mean(scores)\n\n    # Handle case with no previous selections\n    if total_selection_count == 0:\n        return np.random.randint(0, n_actions)\n\n    # Compute the UCB values with a time-based component\n    time_factor = np.sqrt(np.log(total_time_slots + 1) / (current_time_slot + 1))\n    exploration_bonus = np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n\n    ucb_values = avg_scores + time_factor * exploration_bonus\n    \n    # Select the action with the highest UCB value\n    action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -449.9874598084971,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize variables for average scores and selection counts\n    avg_scores = np.zeros(8)\n    selection_counts = np.zeros(8)\n\n    # Calculate average scores and selection counts\n    for index in range(8):\n        scores = score_set.get(index, [])\n        selection_counts[index] = len(scores)\n        avg_scores[index] = np.mean(scores) if selection_counts[index] > 0 else 0\n\n    # Calculate exploration confidence with a decay factor\n    exploration_strength = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))  # Prevent division by zero\n\n    # Apply a time-based decay factor to encourage exploration in earlier time slots, while favoring high rewards in later slots\n    time_decay = (total_time_slots - current_time_slot) / total_time_slots\n\n    # Combine average scores with exploration confidence and time decay\n    # Adding weighted components to balance between exploitation and exploration\n    combined_scores = avg_scores + exploration_strength * time_decay\n\n    # Select action with the maximum combined score\n    action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": -449.9815778899753,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for index in range(num_actions):\n        scores = score_set.get(index, [])\n        selection_counts[index] = len(scores)\n        if selection_counts[index] > 0:\n            avg_scores[index] = np.mean(scores)\n\n    # Calculate UCB values\n    ucb_values = np.zeros(num_actions)\n    for index in range(num_actions):\n        if selection_counts[index] == 0:\n            ucb_values[index] = float('inf')  # Prioritize unselected actions\n        else:\n            exploration_factor = np.sqrt((2 * np.log(total_selection_count)) / selection_counts[index])\n            time_decay = (total_time_slots - current_time_slot) / total_time_slots\n            ucb_values[index] = avg_scores[index] + exploration_factor * time_decay\n\n    # Softmax-based decision for balanced exploration and exploitation\n    exp_values = np.exp(ucb_values - np.max(ucb_values))  # For numerical stability\n    probabilities = exp_values / np.sum(exp_values)\n    \n    # Select action based on calculated probabilities\n    action_index = np.random.choice(num_actions, p=probabilities)\n\n    return action_index",
          "objective": -449.9692630309067,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_scores = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n\n    # Compute total scores and counts\n    for index in range(action_count):\n        scores = score_set.get(index, [])\n        selection_counts[index] = len(scores)\n        if selection_counts[index] > 0:\n            action_scores[index] = np.sum(scores)  # Total score instead of mean for better accumulation\n\n    # Calculate average scores\n    avg_scores = np.where(selection_counts > 0, action_scores / selection_counts, 0)\n\n    # Calculate effective exploration-exploitation value using UCB formula\n    ucb_values = np.zeros(action_count)\n    confidence_level = 2  # Hyperparameter to define exploration strength\n\n    for index in range(action_count):\n        if selection_counts[index] == 0:\n            ucb_values[index] = float('inf')  # Encourage exploration of untried actions\n        else:\n            ucb_values[index] = avg_scores[index] + confidence_level * np.sqrt(np.log(total_selection_count) / selection_counts[index])\n\n    # Time-sensitive decay to account for remaining time slots\n    time_decay = (total_time_slots - current_time_slot) / total_time_slots if total_time_slots > 0 else 0\n    ucb_values *= time_decay\n\n    # Epsilon-Greedy selection mechanism for robustness\n    epsilon = 0.1  # Probability for exploration\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)  # Random exploration\n    else:\n        action_index = np.argmax(ucb_values)  # Exploit the best action based on UCB values\n\n    return action_index",
          "objective": -449.96535550796807,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    avg_scores = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n\n    # Calculate average scores and selection counts\n    for index in range(action_count):\n        scores = score_set.get(index, [])\n        selection_counts[index] = len(scores)\n        if selection_counts[index] > 0:\n            avg_scores[index] = np.mean(scores)\n\n    # Calculate UCB values\n    ucb_values = np.zeros(action_count)\n    for index in range(action_count):\n        if selection_counts[index] == 0:\n            ucb_values[index] = float('inf')  # Encouraging exploration for untried actions\n        else:\n            ucb_values[index] = avg_scores[index] + np.sqrt((2 * np.log(total_selection_count)) / selection_counts[index])\n\n    # Softmax scaling for exploration-exploitation balance\n    softmax_scores = np.exp(ucb_values) / np.sum(np.exp(ucb_values))\n    \n    # Epsilon-Greedy selection mechanism\n    epsilon = 0.1  # Exploration probability\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)  # Explore randomly\n    else:\n        # Choose action based on the weighted probabilities from softmax scores\n        action_index = np.random.choice(action_count, p=softmax_scores)\n\n    return action_index",
          "objective": -449.94121090550345,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize arrays for average scores and counts\n    avg_scores = np.zeros(8)\n    selection_counts = np.zeros(8)\n\n    # Compute average scores and selection counts\n    for index in range(8):\n        scores = score_set.get(index, [])\n        selection_counts[index] = len(scores)\n        if selection_counts[index] > 0:\n            avg_scores[index] = np.mean(scores)\n\n    # Calculate effective exploration-exploitation value\n    exploration_bonus = 1.5  # Hyperparameter to tune exploration strength\n    epsilon = 0.1  # Probability for exploration\n    action_probs = np.zeros(8)\n\n    for index in range(8):\n        if selection_counts[index] == 0:\n            action_probs[index] = float('inf')  # Encourage exploration of untried actions\n        else:\n            # UCB component\n            ucb_value = avg_scores[index] + np.sqrt((2 * np.log(total_selection_count)) / selection_counts[index])\n            # Time-sensitive decay\n            time_decay = (total_time_slots - current_time_slot + 1) / total_time_slots\n            action_probs[index] = ucb_value * time_decay\n\n    # Epsilon-Greedy selection mechanism for introducing randomness\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, 8)  # Randomly select one action\n    else:\n        # Select action based on calculated probabilities\n        action_index = np.argmax(action_probs)\n\n    return action_index",
          "objective": -449.91863335350786,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize variables for average scores and counts\n    avg_scores = np.zeros(8)\n    selection_counts = np.zeros(8)\n\n    # Calculate average scores and selection counts\n    for index in range(8):\n        scores = score_set.get(index, [])\n        selection_counts[index] = len(scores)\n        if selection_counts[index] > 0:\n            avg_scores[index] = np.mean(scores)\n\n    # Calculate UCB values with time decay\n    ucb_values = np.zeros(8)\n    \n    for index in range(8):\n        if selection_counts[index] == 0:\n            ucb_values[index] = float('inf')  # Exploratory value for unselected actions\n        else:\n            exploration_factor = np.sqrt((2 * np.log(total_selection_count)) / selection_counts[index])\n            time_decay = (total_time_slots - current_time_slot + 1) / total_time_slots  # Decay factor\n            ucb_values[index] = avg_scores[index] + exploration_factor * time_decay\n\n    # Select action with the maximum UCB value\n    action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -449.91581375211956,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Probability of random selection for exploration\n    num_actions = 8\n    \n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for index in range(num_actions):\n        scores = score_set.get(index, [])\n        selection_counts[index] = len(scores)\n        avg_scores[index] = np.mean(scores) if selection_counts[index] > 0 else 0.0\n\n    # Calculate UCB values\n    ucb_values = np.zeros(num_actions)\n    exploration_factor = np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1))\n    time_decay = (total_time_slots - current_time_slot + 1) / total_time_slots\n    ucb_values = avg_scores + exploration_factor * time_decay\n\n    # Epsilon-Greedy decision making\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -449.91172365335404,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action in range(num_actions):\n        scores = score_set.get(action, [])\n        selection_counts[action] = len(scores)\n        avg_scores[action] = np.mean(scores) if scores else 0.0\n\n    # Calculate exploration probability based on time\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))  # Decay epsilon over time\n    \n    # Select action based on epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        # Explore: Choose an action randomly\n        action_index = np.random.choice(np.arange(num_actions))\n    else:\n        # Exploit: Choose the action with the highest average score\n        action_index = np.argmax(avg_scores)\n\n    return action_index",
          "objective": -449.9059581653917,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    exploration_probability = 0.1  # A low probability for exploration\n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Compute average scores and counts of selections\n    for index in range(num_actions):\n        scores = score_set.get(index, [])\n        selection_counts[index] = len(scores)\n        if selection_counts[index] > 0:\n            avg_scores[index] = np.mean(scores)\n    \n    # Calculate UCB values\n    ucb_values = np.zeros(num_actions)\n    for index in range(num_actions):\n        if selection_counts[index] == 0:\n            ucb_values[index] = float('inf')  # Ensure unselected actions are prioritized\n        else:\n            # UCB calculation with time decay\n            exploration_factor = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[index])\n            time_decay = (total_time_slots - current_time_slot) / total_time_slots\n            ucb_values[index] = avg_scores[index] + exploration_factor * time_decay\n    \n    # Epsilon-Greedy decision making with UCB\n    if np.random.rand() < exploration_probability:  # Explore\n        action_index = np.random.randint(num_actions)\n    else:  # Exploit\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -449.89325617335135,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon = 0.1  # Exploration probability\n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    # Calculate average scores and selection counts\n    for index in range(num_actions):\n        scores = score_set.get(index, [])\n        selection_counts[index] = len(scores)\n        if selection_counts[index] > 0:\n            avg_scores[index] = np.mean(scores)\n\n    # Calculate UCB values\n    ucb_values = np.zeros(num_actions)\n    for index in range(num_actions):\n        if selection_counts[index] == 0:\n            ucb_values[index] = float('inf')  # High value for unselected actions\n        else:\n            exploration_factor = np.sqrt((2 * np.log(total_selection_count)) / selection_counts[index])\n            time_decay = (total_time_slots - current_time_slot) / total_time_slots\n            ucb_values[index] = avg_scores[index] + exploration_factor * time_decay\n\n    # Epsilon-Greedy decision making\n    if np.random.rand() < epsilon:  # Explore\n        action_index = np.random.randint(num_actions)\n    else:  # Exploit using UCB values\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -449.8314798938243,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for index in range(n_actions):\n        scores = score_set.get(index, [])\n        selection_counts[index] = len(scores)\n        if selection_counts[index] > 0:\n            avg_scores[index] = np.mean(scores)\n\n    # If total_selection_count is zero, all actions are equally likely\n    if total_selection_count == 0:\n        action_index = np.random.randint(0, n_actions)\n    else:\n        ucb_values = avg_scores + np.sqrt((2 * np.log(total_selection_count)) / (selection_counts + 1e-5))\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -449.3101223570911,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Configuration constants\n    epsilon = 0.1  # Exploration probability\n    num_actions = 8\n    \n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for index in range(num_actions):\n        scores = score_set.get(index, [])\n        selection_counts[index] = len(scores)\n        if selection_counts[index] > 0:\n            avg_scores[index] = np.mean(scores)\n    \n    # Calculate UCB values\n    ucb_values = np.zeros(num_actions)\n    for index in range(num_actions):\n        if selection_counts[index] > 0:\n            exploration_factor = np.sqrt((2 * np.log(total_selection_count)) / selection_counts[index])\n            time_decay = (total_time_slots - current_time_slot + 1) / total_time_slots\n            ucb_values[index] = avg_scores[index] + exploration_factor * time_decay\n        else:\n            ucb_values[index] = float('inf')  # High value for unselected actions\n\n    # Epsilon-Greedy decision making\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)  # Explore\n    else:\n        action_index = np.argmax(ucb_values)  # Exploit\n\n    return action_index",
          "objective": -449.27694488564066,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Constants for exploration\n    epsilon = 0.1  # Probability of random selection for exploration\n    num_actions = 8\n    \n    # Initialize average scores, counts and UCB values\n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    # Calculate average scores and selection counts\n    for index in range(num_actions):\n        scores = score_set.get(index, [])\n        selection_counts[index] = len(scores)\n        if selection_counts[index] > 0:\n            avg_scores[index] = np.mean(scores)\n    \n    # Calculate UCB values\n    ucb_values = np.zeros(num_actions)\n    for index in range(num_actions):\n        if selection_counts[index] == 0:\n            # Assign a high UCB value for unselected actions\n            ucb_values[index] = float('inf')\n        else:\n            exploration_factor = np.sqrt((2 * np.log(total_selection_count)) / selection_counts[index])\n            time_decay = (total_time_slots - current_time_slot + 1) / total_time_slots\n            ucb_values[index] = avg_scores[index] + exploration_factor * time_decay\n    \n    # Epsilon-Greedy decision making\n    if np.random.rand() < epsilon:  # Explore: select a random action\n        action_index = np.random.randint(num_actions)\n    else:  # Exploit: select the action with the maximum UCB value\n        action_index = np.argmax(ucb_values)\n    \n    return action_index",
          "objective": -449.2459992701545,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_counts = np.zeros(num_actions)\n    average_scores = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        if action_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Calculate exploration term using UCB\n    exploration_bonus = np.sqrt(2 * np.log(total_selection_count + 1) / (action_counts + 1e-5))\n    ucb_values = average_scores + exploration_bonus\n\n    # Select action based on UCB values\n    action_index = np.argmax(ucb_values)\n    \n    return action_index",
          "objective": -449.14129820902104,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    # Calculate average scores and counts\n    for index in range(num_actions):\n        scores = score_set.get(index, [])\n        selection_counts[index] = len(scores)\n        if selection_counts[index] > 0:\n            avg_scores[index] = np.mean(scores)\n\n    # Compute UCB values incorporating exploration\n    ucb_values = np.zeros(num_actions)\n    exploration_bonus = 0.1  # Exploration factor to encourage trying unchosen actions\n\n    for index in range(num_actions):\n        if selection_counts[index] == 0:\n            ucb_values[index] = float('inf')  # Unexplored actions have infinite UCB value\n        else:\n            exploration_factor = np.sqrt((2 * np.log(total_selection_count)) / selection_counts[index])\n            ucb_values[index] = avg_scores[index] + exploration_factor + exploration_bonus\n\n    # Select action with the maximum UCB value\n    action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -448.95631427690347,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    total_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    # Calculate total scores and selection counts\n    for action_index, scores in score_set.items():\n        total_scores[action_index] = sum(scores)\n        selection_counts[action_index] = len(scores)\n    \n    # Handle zero selections gracefully\n    avg_scores = np.where(selection_counts > 0, total_scores / selection_counts, 0)\n    \n    # Initialize UCB values; UCB uses a confidence interval approach\n    ucb_values = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        if selection_counts[action_index] > 0:\n            ucb_values[action_index] = avg_scores[action_index] + np.sqrt(2 * np.log(total_selection_count) / selection_counts[action_index])\n        else:\n            # Assign a high UCB value to unexplored actions\n            ucb_values[action_index] = np.inf\n\n    # Select action based on UCB values\n    action_index = np.argmax(ucb_values)\n    \n    return action_index",
          "objective": -442.1219012066626,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots, epsilon=0.1):\n    n_actions = 8\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for index in range(n_actions):\n        scores = score_set.get(index, [])\n        selection_counts[index] = len(scores)\n        if selection_counts[index] > 0:\n            avg_scores[index] = np.mean(scores)\n\n    if np.random.rand() < epsilon:  # Explore with probability \u03b5\n        action_index = np.random.randint(0, n_actions)\n    else:  # Exploit based on average scores\n        action_index = np.argmax(avg_scores)\n\n    return action_index",
          "objective": -440.24152585332286,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon = 0.1  # Exploration probability\n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for index in range(num_actions):\n        scores = score_set.get(index, [])\n        selection_counts[index] = len(scores)\n        if selection_counts[index] > 0:\n            avg_scores[index] = np.mean(scores)\n\n    # Calculate UCB values\n    ucb_values = np.zeros(num_actions)\n    for index in range(num_actions):\n        if selection_counts[index] == 0:\n            ucb_values[index] = float('inf')  # High value for unselected actions\n        else:\n            exploration_factor = np.sqrt((2 * np.log(total_selection_count)) / selection_counts[index])\n            time_decay = (total_time_slots - current_time_slot) / total_time_slots\n            ucb_values[index] = avg_scores[index] + exploration_factor * time_decay\n\n    # Epsilon-Greedy decision making\n    if np.random.rand() < epsilon:  # Explore\n        action_index = np.random.randint(num_actions)\n    else:  # Exploit\n        action_index = np.argmax(ucb_values)\n    \n    return action_index",
          "objective": -434.6588102246084,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_counts = np.zeros(num_actions)\n    average_scores = np.zeros(num_actions)\n\n    # Calculate average scores and counts\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        if action_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Calculate the exploration rate based on the current time slot\n    exploration_probability = max(0.1, (total_time_slots - current_time_slot) / total_time_slots)\n\n    # Generate a random number to decide between exploration and exploitation\n    if np.random.rand() < exploration_probability:\n        # Exploration: select randomly from all actions\n        action_index = np.random.randint(num_actions)\n    else:\n        # Exploitation: select based on average scores\n        action_index = np.argmax(average_scores)\n\n    return action_index",
          "objective": -433.08073612814275,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for index in range(n_actions):\n        scores = score_set.get(index, [])\n        selection_counts[index] = len(scores)\n        if selection_counts[index] > 0:\n            avg_scores[index] = np.mean(scores)\n\n    # Epsilon-Greedy parameter\n    epsilon = 0.1\n    if total_selection_count == 0 or np.random.rand() < epsilon:\n        action_index = np.random.randint(0, n_actions)  # Explore\n    else:\n        # UCB Calculation\n        ucb_values = avg_scores + np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n        action_index = np.argmax(ucb_values)  # Exploit\n\n    return action_index",
          "objective": -302.19908783645724,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    total_scores = [0] * num_actions\n    selection_counts = [0] * num_actions\n    \n    # Calculate total scores and selection counts\n    for action_index in score_set:\n        total_scores[action_index] = sum(score_set[action_index])\n        selection_counts[action_index] = len(score_set[action_index])\n    \n    action_scores = []\n    \n    # UCB strategy for action selection\n    for i in range(num_actions):\n        if selection_counts[i] == 0:\n            # Encourage exploration of actions that have never been selected\n            action_score = float('inf')  # or a large arbitrary number\n        else:\n            avg_score = total_scores[i] / selection_counts[i]\n            exploration_term = np.sqrt((2 * np.log(total_selection_count)) / selection_counts[i])\n            action_score = avg_score + exploration_term\n        \n        action_scores.append(action_score)\n    \n    action_index = np.argmax(action_scores)  # Select action with the highest UCB score\n    return action_index",
          "objective": -299.4159720442082,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    total_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate total scores and selection counts\n    for action_index, scores in score_set.items():\n        total_scores[action_index] = sum(scores)\n        selection_counts[action_index] = len(scores)\n    \n    # Normalize scores\n    avg_scores = np.where(selection_counts > 0, total_scores / selection_counts, 0)\n    \n    # Epsilon-greedy strategy with decay\n    epsilon_start = 1.0\n    epsilon_end = 0.01\n    epsilon_decay_duration = total_time_slots\n    epsilon = max(epsilon_end, epsilon_start - (current_time_slot / epsilon_decay_duration) * (epsilon_start - epsilon_end))\n    \n    if np.random.rand() < epsilon:\n        # Exploration\n        action_index = np.random.choice(num_actions)\n    else:\n        # Exploitation\n        action_index = np.argmax(avg_scores)\n\n    return action_index",
          "objective": -250.9427034040609,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_counts = np.zeros(num_actions)\n    average_scores = np.zeros(num_actions)\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        if action_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n    \n    # To avoid division by zero and to manage unselected actions\n    scaled_action_counts = action_counts + 1e-5\n    softmax_scores = np.exp(average_scores) / np.sum(np.exp(average_scores))\n\n    # Incorporating exploration term based on selection counts\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / scaled_action_counts)\n    exploration_contribution = exploration_bonus / np.sum(exploration_bonus)\n\n    # Combining the exploitation and exploration via weighted sum\n    combined_scores = 0.7 * softmax_scores + 0.3 * exploration_contribution\n\n    # Select the action based on combined scores\n    action_index = np.random.choice(num_actions, p=combined_scores)\n    \n    return action_index",
          "objective": -191.16090406184526,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize variables\n    num_actions = 8\n    action_counts = np.zeros(num_actions)\n    average_scores = np.zeros(num_actions)\n    \n    # Calculate average scores and counts\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        if action_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n        else:\n            average_scores[action_index] = 0\n\n    # UCB calculation for each action\n    total_selected = np.sum(action_counts)\n    ucb_scores = np.zeros(num_actions)\n    \n    for i in range(num_actions):\n        if action_counts[i] > 0:\n            confidence = np.sqrt((2 * np.log(total_selected)) / action_counts[i])\n            ucb_scores[i] = average_scores[i] + confidence\n        else:\n            # Assign a high score to unselected actions to encourage their selection\n            ucb_scores[i] = float('inf')\n    \n    # Select action based on UCB Scores\n    action_index = np.argmax(ucb_scores)\n\n    return action_index",
          "objective": -1.3425837815241835,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for index in range(num_actions):\n        scores = score_set.get(index, [])\n        selection_counts[index] = len(scores)\n        if selection_counts[index] > 0:\n            avg_scores[index] = np.mean(scores)\n\n    # Define exploration decay\n    if total_selection_count > 0:\n        epsilon = max(0.1, 1 - (total_selection_count / (total_time_slots * 2)))\n    else:\n        epsilon = 1  # Explore fully if no selections have been made\n\n    # Select action with epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        # Explore: select a random action\n        action_index = np.random.choice(num_actions)\n    else:\n        # Exploit: select the action with the highest average score\n        action_index = np.argmax(avg_scores)\n\n    return action_index",
          "objective": 9.349698453412827,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\nimport random\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration factor\n    exploration = random.random() < epsilon\n    \n    if exploration:\n        action_index = random.randint(0, 7)\n    else:\n        avg_scores = []\n        for index in range(8):\n            scores = score_set.get(index, [])\n            if len(scores) > 0:\n                avg_score = np.mean(scores)\n            else:\n                avg_score = 0  # Handle actions that have never been selected\n            # Add weighted term for exploration\n            if total_selection_count > 0:\n                selection_count = len(scores)\n                weighting = selection_count / total_selection_count\n            else:\n                weighting = 0\n            \n            # Use the average score adjusted by a factor based on selection frequency\n            adjusted_score = avg_score * (1 - weighting) + weighting * (1 / (1 + selection_count))\n            avg_scores.append(adjusted_score)\n\n        action_index = np.argmax(avg_scores)\n\n    return action_index",
          "objective": 212.08464068448825,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    avg_scores = np.zeros(8)\n    selection_counts = np.zeros(8)\n\n    for index in range(8):\n        scores = score_set.get(index, [])\n        selection_counts[index] = len(scores)\n        if selection_counts[index] > 0:\n            avg_scores[index] = np.mean(scores)\n\n    # Calculate temperature for Softmax\n    temperature = np.clip(total_time_slots / (current_time_slot + 1), 0.1, 1.0)\n\n    # Compute the Softmax values\n    exp_scores = np.exp(avg_scores / temperature)\n    softmax_values = exp_scores / np.sum(exp_scores)\n\n    # Sample action index based on Softmax probabilities\n    action_index = np.random.choice(range(8), p=softmax_values)\n\n    return action_index",
          "objective": 229.8579347138459,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize variables to keep track of averages and counts\n    avg_scores = np.zeros(8)\n    selection_counts = np.zeros(8)\n\n    # Compute the average scores and selection counts\n    for index in range(8):\n        scores = score_set.get(index, [])\n        selection_counts[index] = len(scores)\n        if selection_counts[index] > 0:\n            avg_scores[index] = np.mean(scores)\n\n    # Initialize UCB values\n    ucb_values = np.zeros(8)\n    \n    for index in range(8):\n        if selection_counts[index] == 0:\n            # If an action has never been selected, give it a high UCB value\n            ucb_values[index] = float('inf')\n        else:\n            # Calculate UCB\n            ucb_values[index] = avg_scores[index] + np.sqrt((2 * np.log(total_selection_count)) / selection_counts[index])\n\n    # Select action based on maximum UCB value\n    action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 248.99523006074503,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    total_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    # Calculate total scores and selection counts from the score_set\n    for action_index, scores in score_set.items():\n        total_scores[action_index] = np.sum(scores)\n        selection_counts[action_index] = len(scores)\n    \n    # Define exploration probability and set epsilon value\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))  # Dynamic epsilon\n    action_index = None\n    \n    if np.random.rand() < epsilon:\n        # Exploration: Select a random action\n        action_index = np.random.randint(num_actions)\n    else:\n        # Exploitation: Use UCB for action selection\n        action_scores = np.zeros(num_actions)\n        \n        for i in range(num_actions):\n            if selection_counts[i] == 0:\n                action_scores[i] = float('inf')  # Encourage exploration of unselected actions\n            else:\n                avg_score = total_scores[i] / selection_counts[i]\n                exploration_term = np.sqrt((2 * np.log(total_selection_count)) / selection_counts[i])\n                action_scores[i] = avg_score + exploration_term\n        \n        action_index = np.argmax(action_scores)  # Select action with highest UCB score\n    \n    return action_index",
          "objective": 422.3046376877892,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize variables\n    num_actions = 8\n    action_counts = np.zeros(num_actions)  # Track counts of each action\n    average_scores = np.zeros(num_actions)  # Track average scores of each action\n\n    # Calculate average scores and counts\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        if action_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n        else:\n            average_scores[action_index] = 0  # Default for actions not selected\n\n    # Define exploration factor based on total selections\n    alpha = 1.0 / (1 + total_selection_count)  # Diminishing exploration factor\n    exploration_rate = max(0.1, 1.0 - (current_time_slot / total_time_slots))\n    \n    # Calculate probabilities using a softmax approach for balanced exploration/exploitation\n    exp_scores = np.exp(average_scores / alpha)  # Scale scores\n    action_probabilities = exp_scores / np.sum(exp_scores)\n\n    # Apply exploration strategy\n    action_probabilities = (1 - exploration_rate) * action_probabilities + (exploration_rate / num_actions)\n\n    # Select action based on calculated probabilities\n    action_index = np.random.choice(range(num_actions), p=action_probabilities)\n    \n    return action_index",
          "objective": 2280.0369372654804,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    avg_scores = np.zeros(8)\n    selection_counts = np.zeros(8)\n\n    # Calculate average scores and selection counts\n    for index in range(8):\n        scores = score_set.get(index, [])\n        selection_counts[index] = len(scores)\n        if selection_counts[index] > 0:\n            avg_scores[index] = np.mean(scores)\n        else:\n            avg_scores[index] = 0  # Handle actions that have never been selected\n\n    # Compute Softmax temperature based on exploration/exploitation balance\n    temperature = np.clip(total_time_slots / (current_time_slot + 1), 0.1, 1.0)\n\n    # Normalize selection counts to prevent division by zero\n    normalized_counts = (selection_counts + 1) / (total_selection_count + 8)  # Add 1 for Laplace smoothing\n    exploration_factor = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-6))  # To avoid div by zero\n    adjusted_scores = avg_scores * normalized_counts * exploration_factor\n\n    # Softmax calculation on adjusted scores\n    exp_adjusted_scores = np.exp(adjusted_scores / temperature)\n    softmax_values = exp_adjusted_scores / np.sum(exp_adjusted_scores)\n\n    # Sample action index based on Softmax probabilities\n    action_index = np.random.choice(range(8), p=softmax_values)\n\n    return action_index",
          "objective": 3155.737540494872,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores for each action\n    avg_scores = []\n    for action_index in range(8):\n        if len(score_set[action_index]) > 0:\n            avg_score = np.mean(score_set[action_index])\n        else:\n            avg_score = 0  # No scores yet\n        avg_scores.append(avg_score)\n    \n    # Exploration factor based on early exploration\n    if total_selection_count > 0:\n        exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n    else:\n        exploration_factor = 1  # Explore fully in the first slot\n    \n    # Selection of action with balance between exploitation and exploration\n    action_scores = [\n        avg_scores[i] + exploration_factor * (1 - avg_scores[i]) for i in range(8)\n    ]\n    \n    action_index = np.argmax(action_scores)  # Select action with the highest computed score\n    return action_index",
          "objective": 3867.566490313233,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    avg_scores = np.zeros(num_actions)\n\n    # Calculate average scores for each action\n    for action_index in range(num_actions):\n        if len(score_set[action_index]) > 0:\n            avg_scores[action_index] = np.mean(score_set[action_index])\n    \n    # Exploration rate \u03b5, which decreases over time\n    epsilon = 1.0 / (1 + current_time_slot)\n    \n    # Choose to explore or exploit\n    if np.random.rand() < epsilon:\n        # Explore: Select a random action\n        action_index = np.random.choice(num_actions)\n    else:\n        # Exploit: Select the action with the highest average score\n        action_index = np.argmax(avg_scores)\n    \n    return action_index",
          "objective": 4205.5219683709565,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in score_set:\n        scores = score_set[action_index]\n        avg_scores[action_index] = np.mean(scores) if scores else 0\n        selection_counts[action_index] = len(scores)\n\n    exploration_factor = 1 / (1 + selection_counts)\n    \n    # Calculate combined score considering exploration and exploitation\n    combined_scores = avg_scores + exploration_factor\n\n    # Select action based on the combined scores\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": 5616.848104396307,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_counts = np.zeros(num_actions)  # Track counts of each action\n    average_scores = np.zeros(num_actions)  # Track average scores of each action\n\n    # Calculate average scores and counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        average_scores[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Define exploration factor based on the total selections and time\n    epsilon = max(0.1, 0.1 * (1 - (current_time_slot / total_time_slots)))  # Decaying epsilon\n    exploration_choices = np.ones(num_actions) * epsilon / num_actions  # Equal probability for exploration\n\n    # Calculate the exploitation probabilities using softmax for average scores\n    exp_scores = np.exp(average_scores - np.max(average_scores))  # To avoid overflow\n    exploitation_probabilities = exp_scores / np.sum(exp_scores)\n\n    # Combine exploration and exploitation\n    final_probabilities = (1 - epsilon) * exploitation_probabilities + exploration_choices\n\n    # Select action based on calculated probabilities\n    action_index = np.random.choice(range(num_actions), p=final_probabilities)\n\n    return action_index",
          "objective": 5999.773608354299,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_counts = np.zeros(num_actions)\n    average_scores = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        if action_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Handle zero counts to avoid division errors\n    exploration_bonus = np.sqrt(2 * np.log(total_selection_count + 1) / (action_counts + 1e-5))\n    \n    # Incorporate temporal factor for gradual exploration based on time slot proximity\n    time_factor = (current_time_slot + 1) / (total_time_slots + 1)\n    action_weights = 1.0 + time_factor  # weight increases over time slots\n    \n    # Calculate UCB values while considering time-based weighting\n    ucb_values = average_scores + exploration_bonus * action_weights\n    action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 6177.1704994338725,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores for each action\n    avg_scores = []\n    for action in range(8):\n        if score_set[action]:\n            avg_scores.append(np.mean(score_set[action]))\n        else:\n            avg_scores.append(0)  # If no scores, assume average score is 0\n\n    # Compute exploration factor\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n    \n    # Compute selection probabilities\n    selection_probabilities = []\n    for action in range(8):\n        # Get count of historical selections for the action\n        selections_count = len(score_set[action])\n        # Calculate selection score with exploration factor\n        if total_selection_count > 0:\n            prob = avg_scores[action] * (selections_count / total_selection_count) ** (1-exploration_factor) * (1 + exploration_factor)\n        else:\n            prob = 1 / 8  # Uniform distribution if no selection has been made yet\n        selection_probabilities.append(prob)\n\n    # Normalize probabilities\n    selection_probabilities = np.array(selection_probabilities)\n    selection_probabilities /= selection_probabilities.sum()\n\n    # Select an action based on the calculated probabilities\n    action_index = np.random.choice(range(8), p=selection_probabilities)\n    \n    return action_index",
          "objective": 7973.6841570353645,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize an array to hold the average scores of each action.\n    average_scores = []\n    selection_counts = []\n    \n    # Calculate average scores and selection counts for each action.\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        if selection_count > 0:\n            average_score = np.mean(scores)\n        else:\n            average_score = 0.0\n        \n        average_scores.append(average_score)\n        selection_counts.append(selection_count)\n    \n    # Calculate exploration factor based on the current selection count.\n    exploration_factor = 1 / (1 + total_selection_count)  # Simple exploration factor\n    \n    # Scores adjusted with exploration factor\n    adjusted_scores = np.array(average_scores) + exploration_factor * (1 - np.array(selection_counts) / total_selection_count)\n    \n    # Normalize the scores using softmax for selection probability\n    exp_scores = np.exp(adjusted_scores - np.max(adjusted_scores))  # Prevent overflow\n    probabilities = exp_scores / np.sum(exp_scores)\n    \n    # Select an action based on the calculated probabilities\n    action_index = np.random.choice(range(8), p=probabilities)\n    \n    return action_index",
          "objective": 9343.70354014074,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    total_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    # Calculate total scores and selection counts\n    for action_index, scores in score_set.items():\n        total_scores[action_index] = sum(scores)\n        selection_counts[action_index] = len(scores)\n    \n    # Set exploration rate (epsilon)\n    epsilon = 1.0 - (current_time_slot / total_time_slots)  # Decrease exploration over time\n    epsilon = max(0.1, epsilon)  # Ensure a minimum exploration rate\n    \n    if np.random.rand() < epsilon:\n        # Explore: Randomly select an action\n        action_index = np.random.choice(num_actions)\n    else:\n        # Exploit: Use average scores for selection\n        action_scores = np.zeros(num_actions)\n        \n        for i in range(num_actions):\n            if selection_counts[i] > 0:\n                action_scores[i] = total_scores[i] / selection_counts[i]  # Average score\n            else:\n                action_scores[i] = float('inf')  # Encourage exploration of unselected actions\n        \n        action_index = np.argmax(action_scores)  # Select action with the highest average score\n\n    return action_index",
          "objective": 14065.471287936207,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_counts = np.zeros(num_actions)\n    average_scores = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        if action_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Avoid division by zero\n    exploration_bonus = np.sqrt(2 * np.log(total_selection_count + 1) / (action_counts + 1e-5))\n    \n    # Adjust exploration based on the current time slot\n    exploration_factor = np.sqrt(current_time_slot / total_time_slots)\n    ucb_values = average_scores + exploration_bonus * (1 + exploration_factor)\n\n    # Select action based on UCB values\n    action_index = np.argmax(ucb_values)\n    \n    return action_index",
          "objective": 22162.37006286095,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_counts = np.zeros(num_actions)  # Track counts of each action\n    average_scores = np.zeros(num_actions)  # Track average scores of each action\n\n    # Calculate total counts and average scores\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        average_scores[action_index] = np.mean(scores) if scores else 0\n\n    # Define exploration rate\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n    \n    # Choose action via \u03b5-greedy strategy\n    if np.random.rand() < epsilon:  # Explore\n        action_index = np.random.choice(range(num_actions))\n    else:  # Exploit\n        action_index = np.argmax(average_scores)\n\n    return action_index",
          "objective": 36352.01087214249,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_counts = np.zeros(num_actions)  # Track counts of each action\n    average_scores = np.zeros(num_actions)  # Track average scores of each action\n    \n    # Calculate average scores and counts for each action\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        average_scores[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Calculate UCB for each action, adding a small constant to avoid division by zero\n    ucb_term = np.sqrt((2 * np.log(total_selection_count + 1)) / (action_counts + 1e-5))\n    ucb_scores = average_scores + ucb_term\n    \n    # Linearly decay exploration factor from 1 to 0 over time slots\n    exploration_rate = 1.0 - (current_time_slot / total_time_slots)\n    \n    # Combine exploration and exploitation\n    adjusted_scores = (1 - exploration_rate) * ucb_scores + exploration_rate * np.random.random(num_actions)\n    \n    # Select action based on adjusted scores\n    action_index = np.argmax(adjusted_scores)\n    \n    return action_index",
          "objective": 47972.73617056815,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration rate\n    \n    action_scores = []\n    for action_index, scores in score_set.items():\n        if scores:\n            average_score = np.mean(scores)\n        else:\n            average_score = 0.0\n        action_scores.append(average_score)\n\n    if total_selection_count == 0 or (np.random.rand() < epsilon):\n        action_index = np.random.randint(0, 8)\n    else:\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": 50688.39796548876,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8  # Number of actions available\n    epsilon = max(0.1, 0.9 - (current_time_slot / total_time_slots) * 0.8)  # Decreasing epsilon\n    \n    # Calculate average scores for each action\n    average_scores = []\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        if scores:\n            average_scores.append(np.mean(scores))\n        else:\n            average_scores.append(0.0)  # No history leads to a score of 0\n\n    # Explore with probability epsilon, otherwise exploit\n    if np.random.rand() < epsilon:\n        # Exploration: choose a random action\n        action_index = np.random.choice(range(num_actions))\n    else:\n        # Exploitation: choose the action with the highest average score\n        action_index = int(np.argmax(average_scores))\n    \n    return action_index",
          "objective": 58035.68348166262,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize arrays for average scores and selection counts\n    avg_scores = np.zeros(8)\n    selection_counts = np.zeros(8)\n\n    # Calculate average scores and selection counts\n    for index in range(8):\n        scores = score_set.get(index, [])\n        selection_counts[index] = len(scores)\n        if selection_counts[index] > 0:\n            avg_scores[index] = np.mean(scores)\n\n    # Calculate exploration factor (\u03b5) based on total selections\n    exploration_factor = 1.0 / (1 + total_selection_count)\n    \n    # Calculate Upper Confidence Bound (UCB)\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n    ucb_scores = avg_scores + exploration_factor * exploration_bonus\n\n    # Normalize UCB scores for soft selection\n    exp_ucb = np.exp(ucb_scores)\n    softmax_ucb_values = exp_ucb / np.sum(exp_ucb)\n\n    # Sample action index based on UCB probabilities\n    action_index = np.random.choice(range(8), p=softmax_ucb_values)\n\n    return action_index",
          "objective": 83020.89257452686,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize variables\n    action_counts = np.zeros(8)  # To track how many times each action has been selected\n    average_scores = np.zeros(8)  # To store the average scores of each action\n    \n    # Calculate average scores and selection counts\n    for action_index, scores in score_set.items():\n        if scores:\n            action_counts[action_index] = len(scores)\n            average_scores[action_index] = sum(scores) / len(scores)\n    \n    # Define exploration factor (epsilon) based on total selection count\n    if total_selection_count > 0:\n        exploration_rate = max(0.1, 1.0 - (current_time_slot / total_time_slots))\n    else:\n        exploration_rate = 1.0  # Start with full exploration if no actions have been selected yet\n    \n    # Generate probabilities for actions\n    if total_selection_count == 0:\n        # If no selections have been made, explore all actions equally\n        action_probabilities = np.ones(8) / 8\n    else:\n        # Convert average scores to probabilities\n        exp_scores = np.exp(average_scores)\n        action_probabilities = exp_scores / np.sum(exp_scores)\n        \n        # Apply exploration strategy\n        action_probabilities = (1 - exploration_rate) * action_probabilities + (exploration_rate / 8)\n    \n    # Select action based on calculated probabilities\n    action_index = np.random.choice(range(8), p=action_probabilities)\n    \n    return action_index",
          "objective": 93425.02580869778,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = max(0.1, 1 - current_time_slot / total_time_slots)  # Decrease exploration over time\n    action_indices = list(score_set.keys())\n    \n    if total_selection_count == 0:\n        return np.random.choice(action_indices)  # Explore all options equally if nothing has been selected\n\n    avg_scores = {index: (np.mean(scores) if scores else 0) for index, scores in score_set.items()}\n    selection_counts = {index: len(scores) for index, scores in score_set.items()}\n\n    if np.random.rand() < epsilon:  # Exploration: select an action randomly\n        return np.random.choice(action_indices)\n\n    # Exploitation: select action with highest average score considering selection counts\n    adjusted_scores = {\n        index: avg_scores[index] + (1 / (selection_counts[index] + 1))  # Bonus for less frequent selections\n        for index in action_indices\n    }\n\n    return action_index",
          "objective": 97617.8432908645,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize lists to hold average scores and selection counts.\n    average_scores = []\n    selection_counts = []\n    \n    # Calculate average scores and selection counts for each action.\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        if selection_count > 0:\n            average_score = np.mean(scores)\n        else:\n            average_score = 0.0\n            \n        average_scores.append(average_score)\n        selection_counts.append(selection_count)\n    \n    # Calculate exploration probability based on the current time slot.\n    exploration_prob = max(0.1, (1 - current_time_slot / total_time_slots))  # Decrease exploration over time\n    \n    # Choose a random number to decide between exploration and exploitation.\n    if np.random.rand() < exploration_prob:\n        # Exploration: randomly select an action\n        action_index = np.random.choice(range(8))\n    else:\n        # Exploitation: select action based on average scores\n        adjusted_scores = np.array(average_scores) + (1 / (1 + np.array(selection_counts)))  # Encourage selection of less often chosen actions\n        exp_scores = np.exp(adjusted_scores - np.max(adjusted_scores))  # Softmax trick for stability\n        probabilities = exp_scores / np.sum(exp_scores)\n        \n        # Select action based on calculated probabilities\n        action_index = np.random.choice(range(8), p=probabilities)\n    \n    return action_index",
          "objective": 110963.26143502831,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize variables\n    num_actions = 8\n    action_counts = np.zeros(num_actions)  # Track counts of each action\n    average_scores = np.zeros(num_actions)  # Track average scores of each action\n    \n    # Calculate average scores and counts\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        if action_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n        else:\n            average_scores[action_index] = 0  # Default for actions not selected\n\n    # Calculate UCB for each action\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / (action_counts + 1e-5))\n    ucb_scores = average_scores + exploration_bonus\n    \n    # Define exploration factor that decreases over time\n    exploration_rate = 1.0 - (current_time_slot / total_time_slots)\n\n    # Mix UCB scores with exploration rate for decision making\n    adjusted_scores = (1 - exploration_rate) * ucb_scores + exploration_rate * np.random.random(num_actions)\n\n    # Select action based on adjusted scores\n    action_index = np.argmax(adjusted_scores)\n    \n    return action_index",
          "objective": 126874.93161622269,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Define parameters for exploration\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)  # Decreasing exploration rate\n    \n    # Calculate average scores and selection counts\n    action_scores = []\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        average_score = np.mean(scores) if scores else 0.0\n        action_scores.append((average_score, len(scores)))\n\n    # Normalize action scores to avoid over-reliance on any single metric\n    max_score = max(action_scores, key=lambda x: x[0])[0] + 1e-5  # prevent division by zero\n    normalized_scores = [(avg_score / max_score, count) for avg_score, count in action_scores]\n    \n    # Implement a softmax-like selection mechanism to balance exploration and exploitation\n    if total_selection_count == 0 or (np.random.rand() < epsilon):\n        action_index = np.random.randint(0, 8)  # Explore\n    else:\n        action_probs = np.array([normal_score for normal_score, _ in normalized_scores])\n        action_probs /= action_probs.sum()  # Normalize to sum to 1\n        action_index = np.random.choice(range(8), p=action_probs)  # Select based on probabilities\n\n    return action_index",
          "objective": 136078.20821357897,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize parameters\n    num_actions = 8\n    action_counts = np.zeros(num_actions)  # Track counts of each action\n    average_scores = np.zeros(num_actions)  # Track average scores of each action\n    \n    # Calculate average scores and counts\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        if action_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n        else:\n            average_scores[action_index] = 0  # Default for actions with no selections\n\n    # Handle the case where no action has been selected yet\n    if total_selection_count == 0:\n        return np.random.choice(num_actions)\n\n    # Calculate UCB for each action\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count)) / (action_counts + 1e-5))\n    ucb_scores = average_scores + exploration_bonus\n\n    # Softmax function for action selection probabilities\n    def softmax(scores):\n        exp_scores = np.exp(scores - np.max(scores))  # For numerical stability\n        return exp_scores / np.sum(exp_scores)\n\n    # Compute the probabilities using softmax\n    action_probabilities = softmax(ucb_scores)\n\n    # Select action based on the calculated probabilities\n    action_index = np.random.choice(num_actions, p=action_probabilities)\n    \n    return action_index",
          "objective": 137202.38482344488,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores for each action\n    avg_scores = []\n    for i in range(8):\n        if i in score_set and len(score_set[i]) > 0:\n            avg_scores.append(np.mean(score_set[i]))\n        else:\n            avg_scores.append(0.0)\n    \n    # Convert avg_scores to a NumPy array for easier manipulation\n    avg_scores = np.array(avg_scores)\n\n    # Define exploration factor (epsilon), which can be tuned\n    epsilon = 0.1\n\n    # Select action based on epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        # Explore: Randomly select an action\n        action_index = np.random.randint(0, 8)  # Choose one of the 8 actions\n    else:\n        # Exploit: Select action with the highest average score\n        action_index = np.argmax(avg_scores)\n\n    return action_index",
          "objective": 138879.5807842647,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\nimport random\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores for each action\n    avg_scores = []\n    for action_index in range(8):\n        if action_index in score_set and score_set[action_index]:\n            avg_score = np.mean(score_set[action_index])\n        else:\n            avg_score = 0\n        avg_scores.append(avg_score)\n\n    # Epsilon for exploration\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n    \n    # Epsilon-greedy selection\n    if random.random() < epsilon:  # Explore (random choice)\n        action_index = random.randint(0, 7)\n    else:  # Exploit (select action with highest average score)\n        action_index = np.argmax(avg_scores)\n\n    return action_index",
          "objective": 173835.18545051364,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_counts = np.zeros(num_actions)\n    average_scores = np.zeros(num_actions)\n\n    # Gather counts and average scores for each action\n    for action_index in range(num_actions):\n        if action_index in score_set:\n            scores = score_set[action_index]\n            action_counts[action_index] = len(scores)\n            if action_counts[action_index] > 0:\n                average_scores[action_index] = np.mean(scores)\n\n    # Calculate exploration bonus and avoid division by zero\n    exploration_bonus = np.sqrt(2 * np.log(total_selection_count + 1) / (action_counts + 1e-5))\n    \n    # Dynamic exploration factor based on current time slot\n    exploration_factor = np.sqrt(current_time_slot / total_time_slots)\n    ucb_values = average_scores + exploration_bonus * (1 + exploration_factor)\n\n    # Select action with the highest UCB value\n    action_index = np.argmax(ucb_values)\n    \n    return action_index",
          "objective": 277826.7081574549,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        if scores:\n            avg_scores[action_index] = np.mean(scores)\n            selection_counts[action_index] = len(scores)\n\n    # To avoid division by zero, use a small constant when counts are zero\n    epsilon = 1e-5\n    selection_counts += epsilon\n\n    # Calculate exploration factors based on upper confidence bound\n    exploration_factors = np.sqrt(np.log(total_selection_count) / selection_counts)\n\n    # Combined score: exploitation + exploration\n    combined_scores = avg_scores + exploration_factors\n\n    # Epsilon-greedy adjustment based on time slot\n    epsilon_greedy_probability = (current_time_slot / total_time_slots) * 0.1  # 10% exploration at the end\n    if np.random.rand() < epsilon_greedy_probability:\n        action_index = np.random.randint(num_actions)  # randomly select an action\n    else:\n        action_index = np.argmax(combined_scores)  # select the action with the highest score\n\n    return action_index",
          "objective": 369248.3598619483,
          "other_inf": null
     }
]