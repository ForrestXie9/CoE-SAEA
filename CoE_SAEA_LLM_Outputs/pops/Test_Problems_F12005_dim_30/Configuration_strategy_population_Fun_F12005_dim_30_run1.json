[
     {
          "algorithm": null,
          "code": "import numpy as np\nimport random\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration probability\n    action_indices = list(score_set.keys())\n    \n    # Calculate average scores for each action\n    avg_scores = []\n    for action_index in action_indices:\n        scores = score_set[action_index]\n        if scores:  # To handle case of empty scores\n            avg_score = sum(scores) / len(scores)\n        else:\n            avg_score = 0  # If no scores, treat as zero\n        avg_scores.append(avg_score)\n\n    # Epsilon-greedy selection strategy\n    if random.random() < epsilon:\n        # Random action for exploration\n        action_index = random.choice(action_indices)\n    else:\n        # Select action with the highest average score for exploitation\n        max_avg_score = max(avg_scores)\n        best_actions = [i for i, score in enumerate(avg_scores) if score == max_avg_score]\n        action_index = random.choice(best_actions)  # In case of ties\n    \n    return action_index",
          "objective": -449.99999999995345,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    # Adaptive exploration factor\n    base_exploration = 0.1\n    decay_factor = 0.99\n    exploration_probability = max(base_exploration * (decay_factor ** current_time_slot), 0.01)  # Minimum exploration probability\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index, scores in score_set.items():\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n\n    # Handle unselected actions\n    unselected_actions = np.where(selection_counts == 0)[0]\n    if len(unselected_actions) > 0:\n        return np.random.choice(unselected_actions)  # Randomly choose an unselected action\n\n    # Calculate UCB values\n    confidence_bounds = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n    ucb_values = avg_scores + confidence_bounds\n\n    # Exploration vs Exploitation decision\n    if np.random.rand() < exploration_probability:\n        action_index = np.random.choice(num_actions)  # Explore\n    else:\n        action_index = np.argmax(ucb_values)  # Exploit\n\n    return action_index",
          "objective": -449.99999999995293,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon = 1.0 / (1 + total_selection_count)  # Decreasing exploration rate\n\n    # Calculate average scores and selection counts\n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            avg_scores[action_index] = sum(scores) / selection_counts[action_index]\n    \n    # Calculate UCB values\n    ucb_values = avg_scores + np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n    \n    # Decide whether to explore or exploit\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(num_actions)  # Explore\n    else:\n        action_index = np.argmax(ucb_values)  # Exploit\n\n    return action_index",
          "objective": -449.99999999995015,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n    \n    # Calculate means and counts for each action, handling cases with no selections\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        if scores:\n            action_means[action_index] = np.mean(scores)\n    \n    # Define the prior parameters for the Beta distribution\n    alpha = action_means * 100 + 1  # Upward bias towards higher scores\n    beta = (1 - action_means) * 100 + 1  # Upward bias towards lower scores\n    \n    # Sample from the Beta distribution for each action\n    sampled_values = np.random.beta(alpha, beta)\n    \n    # Select action with the highest sampled value\n    action_index = np.argmax(sampled_values)\n\n    return action_index",
          "objective": -449.9999999999488,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Action indices\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    for index, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        if scores:\n            avg_scores[index] = np.mean(scores)\n            selection_counts[index] = len(scores)\n\n    # Calculate exploration probability based on time\n    exploration_factor = max(0, (total_time_slots - current_time_slot) / total_time_slots)\n\n    # Apply Upper Confidence Bound (UCB) for action selection\n    ucb_values = avg_scores + np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n\n    # Use exploration factor to blend UCB with random exploration\n    if np.random.rand() < exploration_factor:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = action_indices[np.argmax(ucb_values)]\n\n    return action_index",
          "objective": -449.99999999993776,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    successes = np.zeros(num_actions)\n    failures = np.zeros(num_actions)\n\n    # Calculate successes and failures based on historical scores\n    for action_index, scores in score_set.items():\n        if scores:\n            successes[action_index] = sum(scores)  # Total score for the action\n            failures[action_index] = len(scores) - successes[action_index]  # Count of unsuccessful selections\n    \n    # Use a Bayesian approach for action selection (Thompson Sampling)\n    sampled_theta = np.random.beta(successes + 1, failures + 1)  # Beta distribution for each action\n    action_index = np.argmax(sampled_theta)  # Select action with maximum sampled value\n\n    return action_index",
          "objective": -449.99999999993713,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n    \n    # Calculate means and counts for each action\n    for action_index, scores in score_set.items():\n        if scores:\n            action_means[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n    \n    # Epsilon-Greedy hyperparameter\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n\n    # Explore or exploit based on epsilon\n    if np.random.rand() < epsilon:\n        # Explore: Select a random action\n        action_index = np.random.randint(0, num_actions)\n    else:\n        # Exploit: Calculate UCB for each action\n        ucb_values = action_means + np.sqrt((2 * np.log(total_selection_count + 1)) / (action_counts + 1e-5))\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -449.999999999929,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    total_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate total scores and counts for each action\n    for action_index, scores in score_set.items():\n        total_scores[action_index] = np.sum(scores)\n        selection_counts[action_index] = len(scores)\n\n    # Calculate average scores, ensuring no division by zero\n    average_scores = np.where(selection_counts > 0, total_scores / selection_counts, 0)\n\n    # Dynamic epsilon based on the current time slot\n    epsilon = max(0.1, 1.0 - (current_time_slot / total_time_slots))\n\n    # Selection strategy: Epsilon-Greedy with UCB\n    if np.random.rand() < epsilon:\n        # Explore: select a random action\n        action_index = np.random.randint(num_actions)\n    else:\n        ucb_values = np.zeros(num_actions)\n        for i in range(num_actions):\n            if selection_counts[i] > 0:\n                ucb_values[i] = average_scores[i] + np.sqrt((2 * np.log(total_selection_count)) / selection_counts[i])\n            else:\n                ucb_values[i] = float('inf')  # Prioritize unselected actions for exploration\n\n        # Select the action with the highest UCB value\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -449.9999999999206,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon = 1.0 / (1 + total_selection_count)  # Decaying exploration rate\n    action_indices = np.arange(num_actions)\n\n    # Calculate average scores and counts of each action\n    averages = np.zeros(num_actions)\n    counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            averages[action_index] = np.mean(scores)\n            counts[action_index] = len(scores)\n\n    # Epsilon-Greedy choice\n    if np.random.rand() < epsilon:  # Exploration\n        action_index = np.random.choice(action_indices)\n    else:  # Exploitation\n        action_index = np.argmax(averages)\n\n    return action_index",
          "objective": -449.9999999999136,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    total_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    # Calculate total scores and counts for each action\n    for action_index, scores in score_set.items():\n        total_scores[action_index] = np.sum(scores)\n        selection_counts[action_index] = len(scores)\n\n    # Calculate average scores, ensuring no division by zero\n    average_scores = np.divide(total_scores, selection_counts, out=np.zeros_like(total_scores), where=selection_counts > 0)\n\n    # Dynamic epsilon based on the current time slot, ensuring exploration\n    epsilon = max(0.1, 1.0 - (current_time_slot / total_time_slots))\n\n    # Epsilon-Greedy decision\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)  # Explore\n    else:\n        # Calculate UCB for exploitation\n        ucb_values = np.zeros(num_actions)\n        for i in range(num_actions):\n            if selection_counts[i] > 0:\n                ucb_values[i] = average_scores[i] + np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[i])\n            else:\n                ucb_values[i] = np.inf  # Prioritize unselected actions for exploration\n\n        action_index = np.argmax(ucb_values)  # Exploit\n\n    return action_index",
          "objective": -449.9999999999069,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate means and counts based on historical scores\n    for action_index, scores in score_set.items():\n        if scores:\n            action_means[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    total_actions = np.sum(action_counts)\n\n    # Parameters for exploration vs exploitation\n    epsilon = 1.0 if total_actions < num_actions else max(0.1, 1 - (total_selection_count / (total_time_slots * 5)))\n    exploration = np.random.rand()\n\n    if exploration < epsilon:\n        # Exploration: Select a random action (ensure each action is selected at least once)\n        action_index = np.random.choice(num_actions)\n    else:\n        # Exploitation: Utilize Upper Confidence Bound (UCB)\n        ucb_values = np.zeros(num_actions)\n        for i in range(num_actions):\n            if action_counts[i] > 0:\n                ucb_values[i] = action_means[i] + np.sqrt(2 * np.log(total_selection_count) / action_counts[i])\n            else:\n                ucb_values[i] = float('inf')  # Ensures unselected actions have a high UCB\n\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -449.99999999978445,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    beta = 0.1  # Controls the level of exploration\n    exploration_probability = min(1.0, beta * (total_time_slots - current_time_slot) / total_time_slots)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index, scores in score_set.items():\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n\n    # Handle the case where actions have never been selected\n    unselected_actions = np.where(selection_counts == 0)[0]\n    if len(unselected_actions) > 0:\n        # Select one of the unselected actions randomly to explore\n        action_index = np.random.choice(unselected_actions)\n        return action_index\n\n    # Calculate UCB values\n    ucb_values = avg_scores + np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n\n    # Decide whether to explore or exploit\n    if np.random.rand() < exploration_probability:\n        action_index = np.random.choice(num_actions)  # Explore\n    else:\n        action_index = np.argmax(ucb_values)  # Exploit\n\n    return action_index",
          "objective": -449.99999999938393,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate means and counts based on historical scores\n    for action_index, scores in score_set.items():\n        if scores:\n            action_means[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n    \n    # Initialize parameters for epsilon-greedy exploration vs exploitation\n    epsilon = max(0.1, 1 - (total_selection_count / (total_time_slots * 5)))  # Decaying epsilon\n    exploration = np.random.rand()\n\n    if exploration < epsilon:\n        # Exploration: Select a random action\n        action_index = np.random.randint(num_actions)\n    else:\n        # Exploitation: Utilize Upper Confidence Bound (UCB)\n        ucb_values = action_means + np.sqrt(2 * np.log(total_selection_count + 1) / (action_counts + 1e-5))\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -449.9999999993177,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    total_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate total scores and counts for each action\n    for action_index, scores in score_set.items():\n        total_scores[action_index] = sum(scores)\n        selection_counts[action_index] = len(scores)\n\n    # Calculate average scores, avoiding division by zero\n    average_scores = np.where(selection_counts > 0, total_scores / selection_counts, 0)\n\n    # Epsilon-Greedy strategy with dynamic epsilon\n    exploration_factor = 1.0 / (current_time_slot + 1)  # Decay exploration over time\n    epsilon = max(0.1, exploration_factor)\n\n    if np.random.rand() < epsilon:\n        # Explore: select a random action\n        action_index = np.random.randint(num_actions)\n    else:\n        # Exploit: use UCB to select the best action\n        ucb_values = np.zeros(num_actions)\n        for i in range(num_actions):\n            if selection_counts[i] > 0:\n                ucb_values[i] = average_scores[i] + np.sqrt((2 * np.log(total_selection_count)) / selection_counts[i])\n            else:\n                # If action has never been selected, give it a high value\n                ucb_values[i] = float('inf')\n\n        # Select the action with the highest UCB value\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -449.9999999991649,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate means and counts for each action\n    for action_index, scores in score_set.items():\n        if scores:\n            action_means[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n    \n    # Select action using UCB method\n    if total_selection_count == 0:\n        # If no selection has been made yet, explore all actions equally\n        action_index = np.random.randint(0, num_actions)\n    else:\n        # Calculate UCB for each action\n        ucb_values = action_means + np.sqrt((2 * np.log(total_selection_count + 1)) / (action_counts + 1e-5))\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -449.9999999987853,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_indices = np.arange(num_actions)\n\n    # Calculate average scores and counts of each action\n    averages = np.zeros(num_actions)\n    n = np.zeros(num_actions)  # Count of selections for each action\n\n    for action_index, scores in score_set.items():\n        if scores:\n            averages[action_index] = np.mean(scores)\n            n[action_index] = len(scores)\n\n    # If an action has never been selected, we will treat that as having infinite potential\n    # Initialize with a very high UCB value to encourage exploration\n    ucb_values = np.zeros(num_actions) + np.inf\n    for action_index in action_indices:\n        if n[action_index] > 0:\n            ucb_values[action_index] = averages[action_index] + np.sqrt(2 * np.log(total_selection_count) / n[action_index])\n\n    # Select the action with the highest UCB value\n    action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -449.9999999981811,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores and selection counts\n    action_indices = list(score_set.keys())\n    avg_scores = []\n    selection_counts = []\n    \n    for action_index in action_indices:\n        scores = score_set[action_index]\n        selection_count = len(scores)\n        selection_counts.append(selection_count)\n        avg_score = sum(scores) / selection_count if selection_count > 0 else 0\n        avg_scores.append(avg_score)\n\n    # Calculate UCB for each action\n    ucb_scores = []\n    for i in range(len(action_indices)):\n        if selection_counts[i] == 0:  # If the action has never been selected, give it a high UCB\n            ucb_score = float('inf')  # Encourage exploration of untried actions\n        else:\n            ucb_score = avg_scores[i] + np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[i])\n        ucb_scores.append(ucb_score)\n\n    # Choose the action with the highest UCB score\n    action_index = np.argmax(ucb_scores)\n    \n    return action_index",
          "objective": -449.9999999974142,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize action indices\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    \n    # Calculate average scores and counts for each action\n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    for action_index, scores in score_set.items():\n        if scores:  # If there are historical scores\n            avg_scores[action_index] = np.mean(scores)\n            selection_counts[action_index] = len(scores)\n    \n    # Handle the case where no action has been selected yet\n    if total_selection_count == 0:\n        return np.random.choice(action_indices)  # Explore all actions initially\n    \n    # Calculate the UCB for each action\n    ucb_values = avg_scores + np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n    \n    # Select the action with the highest UCB value\n    action_index = np.argmax(ucb_values)\n    \n    return action_index",
          "objective": -449.99999999073333,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Calculate average scores and selection counts for each action\n    avg_scores = [np.mean(score_set[action_index]) if score_set[action_index] else 0 for action_index in action_indices]\n    selection_counts = [len(score_set[action_index]) for action_index in action_indices]\n    \n    # If total_selection_count is zero, explore all actions evenly\n    if total_selection_count == 0:\n        return np.random.choice(action_indices)\n    \n    # Adaptive exploration probability based on time\n    exploration_probability = max(0.01, (total_time_slots - current_time_slot) / total_time_slots)\n    \n    # Select action using Upper Confidence Bound (UCB)\n    ucb_values = []\n    for i in range(n_actions):\n        if selection_counts[i] == 0:\n            ucb_value = float('inf')  # Prioritize unselected actions initially\n        else:\n            ucb_value = avg_scores[i] + np.sqrt((2 * np.log(total_selection_count)) / selection_counts[i])\n        ucb_values.append(ucb_value)\n    \n    # Epsilon-greedy approach combined with UCB\n    if np.random.rand() < exploration_probability:\n        action_index = np.random.choice(action_indices)\n    else:\n        max_ucb_value = max(ucb_values)\n        best_actions = [i for i, ucb in enumerate(ucb_values) if ucb == max_ucb_value]\n        action_index = np.random.choice(best_actions)  # In case of ties\n\n    return action_index",
          "objective": -449.9999999889631,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    total_actions = len(action_indices)\n    \n    # Initialize average scores and counts\n    avg_scores = np.zeros(total_actions)\n    selection_counts = np.zeros(total_actions)\n    \n    # Calculate average scores and selection counts\n    for action_index in action_indices:\n        scores = score_set[action_index]\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n    \n    # Epsilon-decay exploration factor\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))  # Decay exploration with time\n    \n    # Randomly select to explore with probability epsilon\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n        return action_index\n    \n    # UCB calculation for exploitation\n    ucb_values = np.zeros(total_actions)\n    for i in action_indices:\n        if selection_counts[i] == 0:\n            ucb_values[i] = float('inf')\n        else:\n            ucb_values[i] = avg_scores[i] + np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[i])\n    \n    # Select action with the maximum UCB\n    action_index = np.argmax(ucb_values)\n    return action_index",
          "objective": -449.9999999780189,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    total_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate total scores and counts for each action\n    for action_index, scores in score_set.items():\n        if scores:\n            total_scores[action_index] = sum(scores)\n            selection_counts[action_index] = len(scores)\n    \n    # Calculate average scores, avoiding division by zero\n    average_scores = np.where(selection_counts > 0, total_scores / selection_counts, 0)\n\n    # UCB calculation\n    exploration_factors = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-10))\n    ucb_values = average_scores + exploration_factors\n    \n    # Select the action with the highest UCB value\n    action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -449.99999993854726,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    total_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate total scores and counts for each action\n    for action_index, scores in score_set.items():\n        total_scores[action_index] = sum(scores)\n        selection_counts[action_index] = len(scores)\n\n    # Calculate average scores, avoiding division by zero\n    average_scores = np.where(selection_counts > 0, total_scores / selection_counts, 0)\n    \n    # Use the UCB formula: average + sqrt((2 * log(total_selection_count)) / count)\n    ucb_values = np.zeros(num_actions)\n    for i in range(num_actions):\n        if selection_counts[i] > 0:\n            ucb_values[i] = average_scores[i] + np.sqrt((2 * np.log(total_selection_count)) / selection_counts[i])\n        else:\n            # If the action has never been selected, set a high UCB value to encourage exploration\n            ucb_values[i] = float('inf')\n\n    # Select the action with the highest UCB value\n    action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -449.99999993022044,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    total_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate total scores and counts for each action\n    for action_index, scores in score_set.items():\n        total_scores[action_index] = sum(scores)\n        selection_counts[action_index] = len(scores)\n\n    # Calculate average scores, avoiding division by zero\n    average_scores = np.where(selection_counts > 0, total_scores / selection_counts, 0)\n    \n    # Calculate epsilon value (decaying)\n    epsilon = max(0.1, 1.0 - (current_time_slot / total_time_slots))\n\n    # If random number is less than epsilon, explore: select a random action\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)\n    else:\n        # Otherwise, exploit: select the action with the highest average score\n        action_index = np.argmax(average_scores)\n\n    return action_index",
          "objective": -449.99999992940957,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    \n    # Initialize average scores and counts for each action\n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    for action_index, scores in score_set.items():\n        selection_counts[action_index] = len(scores)\n        if scores:  # If there are historical scores\n            avg_scores[action_index] = np.mean(scores)\n    \n    # Handle the case of zero total selections (initial exploration)\n    if total_selection_count == 0:\n        return np.random.choice(action_indices)\n    \n    # Calculate Epsilon-Greedy parameter based on the current time slot\n    epsilon = np.clip(1.0 - (current_time_slot / total_time_slots), 0.05, 1.0)\n    \n    # Random exploration or exploitation decision\n    if np.random.rand() < epsilon:\n        # Exploration: Select a random action\n        return np.random.choice(action_indices)\n    \n    # Exploitation: Calculate UCB for each action\n    ucb_values = avg_scores + np.sqrt(2 * np.log(total_selection_count) / (selection_counts + 1e-5))\n    \n    # Select the action with the highest UCB value\n    action_index = np.argmax(ucb_values)\n    \n    return action_index",
          "objective": -449.99999984628914,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate the exploration probability (epsilon) that decreases with time\n    epsilon = max(0.1 * (total_time_slots - current_time_slot) / total_time_slots, 0.01)\n    \n    action_indices = list(score_set.keys())\n    avg_scores = []\n    \n    # Calculate average scores for each action\n    for action_index in action_indices:\n        scores = score_set[action_index]\n        if scores:  # Handle non-empty score lists\n            avg_score = np.mean(scores)\n        else:\n            avg_score = 0  # Handle actions with no scores\n        avg_scores.append(avg_score)\n\n    # Epsilon-greedy selection strategy\n    if np.random.random() < epsilon:\n        # Explore: choose a random action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploit: choose the action with the highest average score\n        max_avg_score = max(avg_scores)\n        best_actions = [i for i, score in enumerate(avg_scores) if score == max_avg_score]\n        action_index = np.random.choice(best_actions)  # In case of ties\n    \n    return action_index",
          "objective": -449.99999975885845,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    total_actions = len(action_indices)  # 8 actions as per description\n    avg_scores = np.zeros(total_actions)\n    selection_counts = np.zeros(total_actions)\n\n    # Calculate average scores and selection counts for each action\n    for action_index in action_indices:\n        scores = score_set[action_index]\n        selection_counts[action_index] = len(scores)\n        \n        if selection_counts[action_index] > 0:  # Check if action has been selected before\n            avg_scores[action_index] = np.mean(scores)\n    \n    # UCB calculation\n    ucb_values = np.zeros(total_actions)\n    for i in action_indices:\n        if selection_counts[i] == 0:  # If an action hasn't been selected, give it a high initial UCB\n            ucb_values[i] = float('inf')  # Infinite UCB for untried actions\n        else:\n            # Calculate UCB\n            ucb_values[i] = avg_scores[i] + np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[i])\n\n    # Select action with the maximum UCB\n    action_index = np.argmax(ucb_values)\n    \n    return action_index",
          "objective": -449.999999425966,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    total_actions = len(action_indices)\n    \n    # Initialize lists to hold average scores and selection counts\n    avg_scores = np.zeros(total_actions)\n    selection_counts = np.zeros(total_actions)\n    \n    # Calculate average scores and selection counts for each action\n    for action_index in action_indices:\n        scores = score_set[action_index]\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n    \n    # UCB calculation\n    ucb_values = np.zeros(total_actions)\n    for action_index in action_indices:\n        if selection_counts[action_index] > 0:\n            ucb_values[action_index] = avg_scores[action_index] + np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[action_index])\n        else:\n            # If an action was never selected, assign a high value to encourage exploration\n            ucb_values[action_index] = float('inf')\n    \n    # Select action with the highest UCB value\n    action_index = np.argmax(ucb_values)\n    return action_index",
          "objective": -449.99999932322675,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Calculate average scores and counts for each action\n    avg_scores = np.array([np.mean(score_set[action_index]) if score_set[action_index] else 0 for action_index in action_indices])\n    selection_counts = np.array([len(score_set[action_index]) for action_index in action_indices])\n    \n    # Initialize UCB values\n    ucb_values = np.zeros(n_actions)\n\n    # Calculate the UCB for each action\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            ucb_values[i] = avg_scores[i] + np.sqrt((2 * np.log(total_selection_count)) / selection_counts[i])\n        else:\n            # If an action has never been selected, assign a high UCB value to encourage exploration\n            ucb_values[i] = float('inf')  # Encourages exploration of unselected actions\n\n    # Select the action with the highest UCB value\n    action_index = np.argmax(ucb_values)\n    \n    return action_index",
          "objective": -449.9999993122075,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    total_actions = len(action_indices)\n    \n    # Initialize lists to hold average scores and selection counts\n    avg_scores = np.zeros(total_actions)\n    selection_counts = np.zeros(total_actions)\n    \n    # Calculate average scores and selection counts for each action\n    for action_index in action_indices:\n        scores = score_set[action_index]\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n    \n    # Dynamic epsilon calculation\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))  # Decrease epsilon over time\n    \n    # Explore or exploit\n    if np.random.rand() < epsilon:\n        # Exploration: Select a random action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploitation: Select the action with the highest average score\n        action_index = np.argmax(avg_scores)\n    \n    return action_index",
          "objective": -449.99999905446623,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    total_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate total scores and counts for each action\n    for action_index, scores in score_set.items():\n        if scores:\n            total_scores[action_index] = sum(scores)\n            selection_counts[action_index] = len(scores)\n    \n    # Avoid division by zero for average score calculation\n    average_scores = total_scores / (selection_counts + 1e-10)\n\n    # Calculate an exploration factor that decreases over time\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n\n    # Calculate confidence intervals using the Upper Confidence Bound approach\n    ucb_values = average_scores + exploration_factor * np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-10))\n\n    # Select action with the highest UCB value\n    action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -449.9999982161154,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    total_actions = len(action_indices)\n    \n    # Initialize arrays for average scores and selection counts\n    avg_scores = np.zeros(total_actions)\n    selection_counts = np.zeros(total_actions)\n\n    # Calculate average scores and selection counts for each action\n    for action_index in action_indices:\n        scores = score_set[action_index]\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n    \n    # Dynamic epsilon calculation to balance exploration and exploitation\n    epsilon = max(0.1, 1.0 - (current_time_slot / total_time_slots))\n\n    # Upper Confidence Bound (UCB) for uncertainty bonus calculation\n    total_selections = np.sum(selection_counts)\n    confidence_bounds = np.sqrt(np.log(total_selections + 1) / (selection_counts + 1e-5))\n    \n    # Calculate UCB scores\n    ucb_scores = avg_scores + confidence_bounds\n\n    # Explore or exploit\n    if np.random.rand() < epsilon:\n        # Exploration: Select an action randomly\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploitation: Select the action with the highest upper confidence bound\n        action_index = np.argmax(ucb_scores)\n    \n    return action_index",
          "objective": -449.9999964090478,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts for each action\n    for action_index, scores in score_set.items():\n        if scores:\n            average_scores[action_index] = np.mean(scores)  # Calculate the average score\n            selection_counts[action_index] = len(scores)      # Count of times the action was selected\n\n    # Define exploration factor\n    exploration_factor = np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n    \n    # Calculate UCB values for each action\n    ucb_values = average_scores + exploration_factor\n\n    # Select the action with the highest UCB value\n    action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -449.999995676903,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    \n    # Calculate average scores and selection counts\n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        if scores:\n            avg_scores[i] = sum(scores) / len(scores)\n            selection_counts[i] = len(scores)\n\n    # UCB calculation for exploration-exploitation balance\n    if total_selection_count == 0:  # Handling first selection\n        # Choose uniformly if no actions have been selected yet\n        action_index = np.random.choice(action_indices)\n    else:\n        confidence_bounds = np.sqrt((2 * np.log(total_selection_count)) / (selection_counts + 1e-5))\n        upper_confidence_bounds = avg_scores + confidence_bounds\n        action_index = np.argmax(upper_confidence_bounds)\n    \n    return action_index",
          "objective": -449.9999934153713,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate the exploration probability (epsilon) that decreases with time\n    epsilon = max(0.1, (total_time_slots - current_time_slot) / total_time_slots)\n    \n    action_indices = list(score_set.keys())\n    avg_scores = []\n    selection_counts = []\n\n    # Calculate average scores and selection counts for each action\n    for action_index in action_indices:\n        scores = score_set[action_index]\n        selection_count = len(scores)\n        avg_score = np.mean(scores) if selection_count > 0 else 0\n        avg_scores.append(avg_score)\n        selection_counts.append(selection_count)\n\n    # Epsilon-greedy selection strategy\n    if np.random.random() < epsilon:\n        # Explore: choose a random action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploit: use UCB (Upper Confidence Bound)\n        total_count = total_selection_count + 1  # Ensure denominator is not zero\n        ucb_values = [\n            avg_scores[i] + np.sqrt(2 * np.log(total_count) / (selection_counts[i] + 1e-5 if selection_counts[i] > 0 else 1))\n            for i in range(len(action_indices))\n        ]\n        max_ucb_value = max(ucb_values)\n        best_actions = [i for i, ucb in enumerate(ucb_values) if ucb == max_ucb_value]\n        action_index = np.random.choice(best_actions)  # In case of ties\n\n    return action_index",
          "objective": -449.99998834850845,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    avg_scores = []\n    selection_counts = []\n    \n    # Calculate average scores and selection counts for each action\n    for action_index in action_indices:\n        scores = score_set[action_index]\n        selection_count = len(scores)\n        if scores:  # Handle non-empty score lists\n            avg_score = np.mean(scores)\n        else:\n            avg_score = 0  # Handle actions with no scores\n        avg_scores.append(avg_score)\n        selection_counts.append(selection_count)\n    \n    # UCB selection mechanism\n    ucb_values = []\n    for i in range(len(action_indices)):\n        if selection_counts[i] == 0:  # Action has not been selected yet\n            ucb_value = float('inf')  # Assign infinite value to unselected actions\n        else:\n            ucb_value = avg_scores[i] + np.sqrt((2 * np.log(total_selection_count)) / selection_counts[i])\n        ucb_values.append(ucb_value)\n    \n    action_index = np.argmax(ucb_values)  # Select the action with the highest UCB value\n\n    return action_index",
          "objective": -449.99997339034826,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    total_actions = len(action_indices)\n\n    # Initialize lists for average scores and selection counts\n    avg_scores = np.zeros(total_actions)\n    selection_counts = np.zeros(total_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in action_indices:\n        scores = score_set[action_index]\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n    \n    # If no actions have been selected yet, select randomly\n    if total_selection_count == 0:\n        action_index = np.random.choice(action_indices)\n    else:\n        # Implement Upper Confidence Bound (UCB) for action selection\n        ucb_values = np.zeros(total_actions)\n        for i in range(total_actions):\n            if selection_counts[i] > 0:\n                ucb_values[i] = avg_scores[i] + np.sqrt((2 * np.log(total_selection_count)) / selection_counts[i])\n            else:\n                # If action was never selected, give it a high initial value\n                ucb_values[i] = float('inf')\n        \n        # Select the action with the highest UCB value\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -449.99994000666754,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    selection_counts = []\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0.0\n        \n        # Calculate exploration term\n        if selection_count == 0:\n            exploration_term = float('inf')  # Give priority to unselected actions\n        else:\n            exploration_term = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_count)\n        \n        # Combine average score with exploration term\n        final_score = average_score + exploration_term\n        action_scores.append(final_score)\n        selection_counts.append(selection_count)\n\n    # Adjust scores with a dynamic time factor\n    time_factor = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = np.array(action_scores) * time_factor\n    \n    # Select the action with the highest adjusted score\n    action_index = int(np.argmax(adjusted_scores))\n    \n    return action_index",
          "objective": -449.99988052046865,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    total_actions = len(action_indices)\n\n    # Initialize lists for average scores and selection counts\n    avg_scores = np.zeros(total_actions)\n    selection_counts = np.zeros(total_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in action_indices:\n        scores = score_set[action_index]\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n\n    # Epsilon decay for exploration\n    epsilon_start = 1.0\n    epsilon_end = 0.1\n    decay_rate = (epsilon_start - epsilon_end) / total_time_slots\n    epsilon = max(epsilon_end, epsilon_start - decay_rate * current_time_slot)\n\n    # Exploration vs Exploitation decision\n    if np.random.rand() < epsilon:\n        # Explore: choose a random action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploit: choose the best action\n        action_index = np.argmax(avg_scores)\n\n    return action_index",
          "objective": -449.99987629380456,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize lists to store average scores and selection counts\n    action_scores = []\n    selection_counts = []\n\n    # Calculate scores and counts for each action\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        average_score = np.mean(scores) if selection_count > 0 else 0.0\n        \n        # Store the selection count for later use\n        selection_counts.append(selection_count)\n        \n        # Calculate exploration factor with a diminishing exploration term\n        if selection_count == 0:\n            exploration_factor = float('inf')  # Prioritize unselected actions\n        else:\n            exploration_factor = np.sqrt(np.log(total_selection_count + 1) / selection_count)\n        \n        # Combine average score with exploration factor\n        final_score = average_score + exploration_factor\n        action_scores.append(final_score)\n\n    # Apply diminishing exploration over time\n    time_factor = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = np.array(action_scores) * time_factor\n    \n    # Select the action with the highest adjusted score\n    action_index = int(np.argmax(adjusted_scores))\n    \n    return action_index",
          "objective": -449.99977605247426,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon_max = 0.9  # Maximum exploration rate\n    epsilon_min = 0.1  # Minimum exploration rate\n    epsilon_decay_time_slots = total_time_slots // 2  # When to start decaying epsilon\n    time_slot_fraction = min(current_time_slot / epsilon_decay_time_slots, 1)\n    \n    # Dynamically calculate epsilon based on time slot\n    epsilon = epsilon_max * (1 - time_slot_fraction) + epsilon_min * time_slot_fraction\n    \n    # Compute the mean scores for each action\n    action_means = np.zeros(num_actions)\n    for action_index, scores in score_set.items():\n        if scores:  # If there are historical scores\n            action_means[action_index] = np.mean(scores)\n        else:  # No scores means this action has never been selected\n            action_means[action_index] = 0\n    \n    # Epsilon-Greedy action selection\n    if np.random.rand() < epsilon:\n        # Exploration: select a random action\n        action_index = np.random.choice(num_actions)\n    else:\n        # Exploitation: select the action with the highest mean score\n        action_index = np.argmax(action_means)\n\n    return action_index",
          "objective": -449.9994547280216,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    total_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate total scores and counts for each action\n    for action_index, scores in score_set.items():\n        if scores:\n            total_scores[action_index] = sum(scores)\n            selection_counts[action_index] = len(scores)\n\n    # Calculate average scores while avoiding division by zero\n    average_scores = total_scores / (selection_counts + 1e-10)\n    \n    # Epsilon decreasing method for exploration\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n    \n    # Randomly decide whether to explore or exploit\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)  # Explore\n    else:\n        action_index = np.argmax(average_scores)  # Exploit\n\n    return action_index",
          "objective": -449.99924102343175,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Dynamic epsilon calculation: decays as time slots progress\n    max_epsilon = 1.0\n    min_epsilon = 0.1\n    decay_rate = 0.9\n    epsilon = max(min_epsilon, max_epsilon * (decay_rate ** (current_time_slot / total_time_slots)))\n\n    action_indices = list(score_set.keys())\n    \n    # Calculate average scores for each action, handling cases with zero scores\n    avg_scores = np.array([\n        np.mean(score_set[action_index]) if score_set[action_index] else 0\n        for action_index in action_indices\n    ])\n\n    # UCB calculation to balance exploration and exploitation\n    ucb_values = np.zeros(len(action_indices))\n    for i, action_index in enumerate(action_indices):\n        n = len(score_set[action_index])  # count of selections for this action\n        if n == 0:\n            ucb_values[i] = float('inf')  # prioritize unexplored actions\n        else:\n            avg_score = avg_scores[i]\n            ucb_values[i] = avg_score + np.sqrt(2 * np.log(total_selection_count) / n)\n\n    # Epsilon-greedy selection strategy with UCB\n    if np.random.rand() < epsilon:\n        # Random action for exploration\n        action_index = np.random.choice(action_indices)\n    else:\n        # Select action with the highest UCB value for exploitation\n        action_index = np.argmax(ucb_values)\n    \n    return action_index",
          "objective": -449.9988955869049,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores and selection counts for each action\n    action_indices = list(score_set.keys())\n    avg_scores = []\n    selection_counts = []\n\n    for action_index in action_indices:\n        scores = score_set[action_index]\n        if scores:  # Calculate mean score\n            avg_score = sum(scores) / len(scores)\n        else:  # No scores yet, treat as zero\n            avg_score = 0\n        avg_scores.append(avg_score)\n        selection_counts.append(len(scores))\n\n    # UCB selection formula composed of average score and exploration term\n    exploration_factor = np.sqrt(2 * np.log(total_selection_count + 1) / (np.array(selection_counts) + 1e-5))\n\n    ucb_values = np.array(avg_scores) + exploration_factor\n\n    # Select action with the highest UCB value\n    action_index = np.argmax(ucb_values)\n    \n    return action_index",
          "objective": -449.99855045221176,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    total_actions = len(action_indices)\n\n    # Initialize lists for average scores and selection counts\n    avg_scores = np.zeros(total_actions)\n    selection_counts = np.zeros(total_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in action_indices:\n        scores = score_set[action_index]\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n\n    # Apply UCB formula\n    if total_selection_count > 0:\n        ucb_values = avg_scores + np.sqrt(2 * np.log(total_selection_count) / (selection_counts + 1e-5))\n    else:\n        ucb_values = np.ones(total_actions)  # Ensure all actions have a baseline value when no selections have been made\n\n    # Select the action with the highest UCB value\n    action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -449.9982753024565,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    total_actions = len(action_indices)\n    \n    # Initialize variables to hold average scores and selection counts\n    avg_scores = np.zeros(total_actions)\n    selection_counts = np.zeros(total_actions)\n    \n    # Calculate average scores and selection counts for each action\n    for action_index in action_indices:\n        scores = score_set[action_index]\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n    \n    # Dynamic exploration rate\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n    \n    # Generate uniform random values for exploration\n    if np.random.rand() < epsilon:\n        # Randomly explore an action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Use a modified UCB strategy\n        ucb_values = np.zeros(total_actions)\n        for action_index in action_indices:\n            if selection_counts[action_index] > 0:\n                ucb_values[action_index] = (\n                    avg_scores[action_index] + \n                    np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[action_index])\n                )\n            else:\n                # Value for unselected actions\n                ucb_values[action_index] = float('inf')\n        \n        # Select action with the highest UCB value\n        action_index = np.argmax(ucb_values)\n    \n    return action_index",
          "objective": -449.99755995902626,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_counts = np.zeros(num_actions)\n    action_scores = np.zeros(num_actions)\n    \n    # Calculate action statistics\n    for action_index, scores in score_set.items():\n        if scores:\n            action_counts[action_index] = len(scores)  # Count of selections for this action\n            action_scores[action_index] = np.mean(scores)  # Average score for the action\n\n    # If total_selection_count is less than num_actions, ensure each action is selected at least once\n    if total_selection_count < num_actions:\n        return total_selection_count  # Select the action by its total selection count as index\n\n    # UCB calculation: mean reward + confidence interval\n    ucb_values = action_scores + np.sqrt((2 * np.log(total_selection_count + 1)) / (action_counts + 1e-5))\n    \n    # Handle case where action_counts is zero to avoid taking log of zero\n    ucb_values[np.isnan(ucb_values)] = float('inf')  # Treat any NaN as an infinitely better option\n\n    # Select action with maximum UCB value\n    action_index = np.argmax(ucb_values)\n    \n    return action_index",
          "objective": -449.9970854592276,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    \n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    for action_index in action_indices:\n        scores = score_set[action_index]\n        selection_counts[action_index] = len(scores)\n        if scores:  \n            avg_scores[action_index] = sum(scores) / len(scores)\n    \n    # Handle actions not selected yet\n    for i in range(num_actions):\n        if selection_counts[i] == 0:\n            return i  # Explore untried actions first\n    \n    # Calculate UCB for each action\n    ucb_values = avg_scores + np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts)\n    \n    # Select the action with the highest UCB\n    action_index = np.argmax(ucb_values)\n    \n    return action_index",
          "objective": -449.99639886376843,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_weight = 1.0 - (current_time_slot / total_time_slots)  # Decaying exploration factor\n    \n    for action_index in range(8):\n        if action_index in score_set:\n            scores = score_set[action_index]\n            selection_count = len(scores)\n            average_score = np.mean(scores) if selection_count > 0 else 0.0\n        else:\n            selection_count = 0\n            average_score = 0.0\n        \n        # UCB calculation: average score + exploration term\n        if selection_count > 0:\n            exploration_factor = exploration_weight * np.sqrt(np.log(total_selection_count) / selection_count)\n        else:\n            exploration_factor = float('inf')  # Encourages selection of untried actions\n\n        final_score = average_score + exploration_factor\n        action_scores.append(final_score)\n    \n    # Select the action with the highest score\n    action_index = int(np.argmax(action_scores))\n    \n    return action_index",
          "objective": -449.9948616429335,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    \n    for action_index in range(8):\n        # Calculate the average score for the action\n        if action_index in score_set and len(score_set[action_index]) > 0:\n            average_score = np.mean(score_set[action_index])\n            selection_count = len(score_set[action_index])\n        else:\n            average_score = 0.0\n            selection_count = 0\n        \n        # Exploration term varies based on the number of selections\n        if selection_count > 0:\n            exploration_factor = np.sqrt((np.log(total_selection_count) / selection_count))\n        else:\n            exploration_factor = float('inf')  # Encourage selection of less tried actions\n        \n        # Combine exploitation and exploration\n        final_score = average_score + exploration_factor\n        \n        action_scores.append(final_score)\n    \n    # Choose the action with the highest score\n    action_index = int(np.argmax(action_scores))\n    \n    return action_index",
          "objective": -449.9939849854795,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    average_scores = []\n    selection_counts = []\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        average_score = np.mean(scores) if selection_count > 0 else 0.0\n        \n        average_scores.append(average_score)\n        selection_counts.append(selection_count)\n\n    # Calculate UCB values\n    ucb_values = []\n    for action_index in range(num_actions):\n        if selection_counts[action_index] == 0:\n            ucb_value = float('inf')  # Force selection of untried actions\n        else:\n            # Confidence factor based on selection count\n            confidence_factor = np.sqrt(np.log(total_selection_count) / selection_counts[action_index])\n            ucb_value = average_scores[action_index] + confidence_factor\n        \n        ucb_values.append(ucb_value)\n\n    # Adjust scores with diminishing exploration over time\n    decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = np.array(ucb_values) * decay_factor\n\n    # Select the action with the highest adjusted score\n    action_index = int(np.argmax(adjusted_scores))\n\n    return action_index",
          "objective": -449.9773443424562,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    \n    # Initialize arrays for average scores and selection counts\n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for idx, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[idx] = len(scores)  # Number of times the action has been selected\n        if selection_counts[idx] > 0:\n            avg_scores[idx] = sum(scores) / selection_counts[idx]  # Average score\n\n    # UCB calculation\n    if total_selection_count > 0:\n        confidence_bounds = np.sqrt(2 * np.log(total_selection_count) / (selection_counts + 1e-5))  # Small epsilon to avoid division by zero\n        ucb_values = avg_scores + confidence_bounds\n    else:\n        ucb_values = np.ones(num_actions)  # If no selections made, initialize equally for exploration\n\n    # Select action with the highest UCB value\n    action_index = np.argmax(ucb_values)\n    \n    return action_index",
          "objective": -449.96801658283334,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    averages = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate averages and selection counts from the historical scores\n    for action_index, scores in score_set.items():\n        if scores:\n            averages[action_index] = np.mean(scores)  # Average score for the action\n            selection_counts[action_index] = len(scores)  # Count of selections for the action\n\n    # If selection_count is zero, all actions are treated equally\n    if total_selection_count == 0:\n        return np.random.randint(num_actions)  # Randomly select an action\n\n    # Calculate UCB for each action\n    ucb_values = np.zeros(num_actions)\n    for i in range(num_actions):\n        if selection_counts[i] > 0:\n            ucb_values[i] = averages[i] + np.sqrt(2 * np.log(total_selection_count) / selection_counts[i])\n        else:\n            ucb_values[i] = float('inf')  # Consider unselected actions as highest UCB\n\n    action_index = np.argmax(ucb_values)  # Select action with maximum UCB value\n    return action_index",
          "objective": -449.80539549893314,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))  # Dynamic exploration rate\n    \n    action_indices = list(score_set.keys())\n    total_actions = len(action_indices)\n\n    # Initialize lists to hold average scores and selection counts\n    avg_scores = np.zeros(total_actions)\n    selection_counts = np.zeros(total_actions)\n\n    # Calculate average scores and selection counts for each action\n    for action_index in action_indices:\n        scores = score_set[action_index]\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n\n    if np.random.rand() < epsilon:\n        # Epsilon-Greedy: explore\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploit: select the action with the highest average score\n        action_index = np.argmax(avg_scores)\n\n    return action_index",
          "objective": -449.55124499838024,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate the average score for the action\n        if selection_count > 0:\n            average_score = np.mean(scores)\n        else:\n            average_score = 0.0  # If the action has never been selected\n        \n        # Exploration term using UCB with a time-dependent factor\n        exploration_factor = np.sqrt((2 * np.log(total_selection_count + 1) / (selection_count + 1))) if selection_count > 0 else float('inf')\n        \n        # Combine exploitation and exploration\n        final_score = average_score + exploration_factor\n        \n        action_scores.append(final_score)\n    \n    # Choose the action with the highest score\n    action_index = int(np.argmax(action_scores))\n    \n    return action_index",
          "objective": -449.4538036398777,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and action selection counts\n    for action_index, scores in score_set.items():\n        if scores:\n            selection_counts[action_index] = len(scores)\n            avg_scores[action_index] = np.mean(scores)\n\n    # Calculate exploration factor based on time slot, encouraging exploration in early slots\n    epsilon = max(0.1, 0.5 * (1 - (current_time_slot / total_time_slots)))\n\n    # Calculate upper confidence bounds\n    ucb_values = np.zeros(num_actions)\n    for i in range(num_actions):\n        if selection_counts[i] == 0:\n            ucb_values[i] = np.inf  # Assign infinite UCB to encourage exploration\n        else:\n            ucb_values[i] = avg_scores[i] + np.sqrt((2 * np.log(total_selection_count)) / selection_counts[i])\n\n    # Combine average scores with exploration probabilities\n    combined_scores = (1 - epsilon) * ucb_values + epsilon * np.random.rand(num_actions)\n\n    # Select action based on combined scores\n    action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": -437.8895457737467,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    \n    for action_index in range(8):\n        if action_index in score_set and len(score_set[action_index]) > 0:\n            average_score = np.mean(score_set[action_index])\n        else:\n            average_score = 0.0\n        \n        # Explore less-selected actions when total_selection_count is low\n        exploration_factor = np.sqrt(np.log(total_selection_count + 1) / (len(score_set[action_index]) + 1)) if len(score_set[action_index]) > 0 else float('inf')\n        \n        # Compute final score with exploration\n        final_score = average_score + exploration_factor\n        action_scores.append(final_score)\n    \n    # Choose the action with the highest score\n    action_index = int(np.argmax(action_scores))\n    \n    return action_index",
          "objective": -437.58405925190795,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    average_scores = np.zeros(num_actions)\n    selections = np.zeros(num_actions)\n    \n    # Calculate average scores and selections for each action\n    for action_index, scores in score_set.items():\n        selections[action_index] = len(scores)\n        if selections[action_index] > 0:\n            average_scores[action_index] = sum(scores) / selections[action_index]\n    \n    # Define exploration factor\n    exploration_factor = np.sqrt((2 * np.log(total_selection_count + 1)) / (selections + 1e-5))\n    \n    # Calculate UCB for each action\n    ucb_values = average_scores + exploration_factor\n    \n    # If selected count is less than the total actions, prefer exploring unselected actions\n    if total_selection_count < num_actions:\n        action_index = total_selection_count\n    else:\n        action_index = np.argmax(ucb_values)\n    \n    return action_index",
          "objective": -436.0772237285688,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    total_actions = len(action_indices)\n\n    # Prepare arrays for average scores and selection counts\n    avg_scores = np.zeros(total_actions)\n    selection_counts = np.zeros(total_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in action_indices:\n        scores = score_set[action_index]\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n\n    # Epsilon-Greedy exploration factor that decays over time \n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n\n    # Randomly explore actions based on epsilon\n    if np.random.rand() < epsilon or total_selection_count == 0:\n        action_index = np.random.choice(action_indices)\n    else:\n        # Calculate UCB for exploitation\n        confidence_bounds = np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n        upper_bounds = avg_scores + confidence_bounds\n        action_index = np.argmax(upper_bounds)\n\n    return action_index",
          "objective": -435.48460644260274,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    total_actions = len(action_indices)\n    \n    # Handling the case when no actions have been selected\n    if total_selection_count == 0:\n        return np.random.choice(action_indices)  # Explore randomly on the first choice\n    \n    avg_scores = np.zeros(total_actions)\n    selection_counts = np.zeros(total_actions)\n\n    for action_index in action_indices:\n        scores = score_set[action_index]\n        selection_counts[action_index] = len(scores)\n        avg_scores[action_index] = np.mean(scores) if selection_counts[action_index] > 0 else 0\n        \n    # Epsilon-Greedy parameters\n    epsilon = 0.1  # Exploration rate\n    if np.random.rand() < epsilon:\n        return np.random.choice(action_indices)  # Explore with epsilon probability\n\n    # Compute confidence bounds using UCB\n    confidence_bounds = np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n    upper_bounds = avg_scores + confidence_bounds\n\n    # Select the action with the highest upper confidence bound\n    action_index = np.argmax(upper_bounds)\n    \n    return action_index",
          "objective": -430.545544506854,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    total_actions = len(action_indices)\n    \n    # Calculate average scores and counts\n    avg_scores = np.zeros(total_actions)\n    selection_counts = np.zeros(total_actions)\n    \n    for action_index in action_indices:\n        scores = score_set[action_index]\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n\n    # Applying UCB to select action\n    if total_selection_count == 0:  # If no selections have been made yet, explore all actions\n        action_index = np.random.choice(action_indices)\n    else:\n        confidence_bounds = np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n        upper_bounds = avg_scores + confidence_bounds\n        action_index = np.argmax(upper_bounds)\n\n    return action_index",
          "objective": -420.05506170538496,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(8)\n    action_counts = np.zeros(8)\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        if action_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n\n    # Calculate upper confidence bounds\n    ucb_values = np.zeros(8)\n    for action_index in range(8):\n        if action_counts[action_index] > 0:\n            confidence = np.sqrt(2 * np.log(total_selection_count + 1) / action_counts[action_index])\n            ucb_values[action_index] = avg_scores[action_index] + confidence\n        else:\n            # If an action has not been selected, give it the maximum potential score to encourage exploration\n            ucb_values[action_index] = np.inf\n\n    # Select action based on UCB values\n    action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -403.2988015609949,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    total_actions = len(action_indices)\n\n    # Initialize arrays for average scores and selection counts\n    avg_scores = np.zeros(total_actions)\n    selection_counts = np.zeros(total_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in action_indices:\n        scores = score_set[action_index]\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n\n    # Epsilon-Greedy exploration factor decays over time\n    epsilon = max(0.1, 1.0 - (current_time_slot / total_time_slots))\n\n    # Compute upper confidence bounds\n    confidence_bounds = np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n    upper_bounds = avg_scores + confidence_bounds\n\n    # Select action based on exploration or exploiting the best UCB\n    if np.random.rand() < epsilon or total_selection_count == 0:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        action_index = np.argmax(upper_bounds)  # Exploit\n\n    return action_index",
          "objective": -395.0960194719469,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    \n    # Calculate average scores and selection counts\n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    for idx, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[idx] = len(scores)\n        if selection_counts[idx] > 0:\n            avg_scores[idx] = sum(scores) / selection_counts[idx]\n    \n    # Calculate exploration rate (epsilon)\n    if total_time_slots > 1:\n        epsilon = (total_time_slots - current_time_slot) / total_time_slots\n    else:\n        epsilon = 0.1  # Default epsilon if only one time slot\n\n    # Randomly explore with probability epsilon, otherwise exploit\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        action_index = np.argmax(avg_scores)  # Exploit\n    \n    return action_index",
          "objective": -353.5688339649656,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    total_actions = len(action_indices)\n    \n    # Calculate average scores for each action and counts\n    avg_scores = np.zeros(total_actions)\n    selection_counts = np.zeros(total_actions)\n    \n    for action_index in action_indices:\n        scores = score_set[action_index]\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n    \n    # Determine the exploration rate based on the current time slot\n    epsilon = max(0.01, 1 - (current_time_slot / total_time_slots))\n    \n    # Decide whether to explore or exploit\n    if np.random.rand() < epsilon:\n        # Exploration: Select a random action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploitation: Select the action with the highest average score\n        action_index = np.argmax(avg_scores)\n    \n    return action_index",
          "objective": -318.83825374100184,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(8)\n    action_counts = np.zeros(8)\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        if action_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n\n    # Define a dynamic epsilon based on the current time slot\n    epsilon = max(0.1, (total_time_slots - current_time_slot) / total_time_slots)\n    \n    # Randomly explore with probability epsilon\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, 8)\n    else:\n        # Apply UCB-based exploitation\n        ucb_values = np.zeros(8)\n        \n        for action_index in range(8):\n            if action_counts[action_index] > 0:\n                confidence = np.sqrt(2 * np.log(total_selection_count) / action_counts[action_index])\n                ucb_values[action_index] = avg_scores[action_index] + confidence\n            else:\n                ucb_values[action_index] = np.inf  # Encourage exploration of unselected actions\n\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -308.2117287401856,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    avg_scores = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        avg_scores[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Calculate the dynamic exploration rate using a decaying epsilon\n    epsilon = max(0.1, 0.5 * (1 - (current_time_slot / total_time_slots)))\n\n    # Compute UCB values with enhanced exploration for lesser-selected actions\n    ucb_values = np.zeros(num_actions)\n    for i in range(num_actions):\n        if action_counts[i] > 0:\n            ucb_values[i] = avg_scores[i] + np.sqrt((2 * np.log(total_selection_count)) / action_counts[i])\n        else:\n            ucb_values[i] = np.inf  # Encourage exploration for untried actions\n\n    # Generate random values for epsilon-greedy decision making\n    random_exploration = np.random.rand(num_actions)\n    combined_scores = (1 - epsilon) * ucb_values + epsilon * random_exploration\n\n    # Select the action with the highest combined score\n    action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": -9.468259082064776,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    successes = np.zeros(num_actions)\n    counts = np.zeros(num_actions)\n    \n    # Calculate successes and counts based on historical scores\n    for action_index, scores in score_set.items():\n        if scores:\n            successes[action_index] = sum(scores)  # Total score for the action\n            counts[action_index] = len(scores)  # Count of times action was selected\n    \n    # Prevent division by zero and handle eventualities\n    averages = np.divide(successes, counts, out=np.zeros_like(successes), where=counts!=0)\n\n    # Define Epsilon for exploration\n    epsilon = 1.0 / (1 + current_time_slot)  # Decaying exploration rate\n    \n    # Upper Confidence Bound (UCB) calculation\n    ucb_values = np.zeros(num_actions)\n    \n    for i in range(num_actions):\n        if counts[i] > 0:\n            ucb_values[i] = averages[i] + np.sqrt((2 * np.log(total_selection_count + 1)) / counts[i])\n        else:\n            ucb_values[i] = np.inf  # If action has never been selected, assign infinite UCB\n    \n    # Epsilon-Greedy selection\n    if np.random.rand() < epsilon:\n        # Explore: Select a random action\n        action_index = np.random.randint(num_actions)\n    else:\n        # Exploit: Select action with the highest UCB value\n        action_index = np.argmax(ucb_values)\n    \n    return action_index",
          "objective": 77.64390554507963,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    # Calculate average scores and selection counts\n    for action_index, scores in score_set.items():\n        if scores:\n            selection_counts[action_index] = len(scores)\n            avg_scores[action_index] = np.mean(scores)\n\n    # Handling cases where selection_counts is zero\n    selection_counts += 1e-5  # Small constant to avoid division by zero\n\n    # Calculate exploration term based on UCB\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count)) / selection_counts)\n    \n    # Calculate adjusted scores considering UCB\n    adjusted_scores = avg_scores + exploration_bonus\n\n    # Select action based on adjusted scores\n    action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 79.24646905958662,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts for each action\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        count = len(scores)\n        selection_counts[action_index] = count\n        avg_scores[action_index] = np.mean(scores) if count > 0 else 0\n\n    # Initialize UCB scores\n    ucb_scores = np.zeros(n_actions)\n\n    for action_index in range(n_actions):\n        # Use a high exploration factor for actions that haven't been selected yet\n        if selection_counts[action_index] == 0:\n            ucb_scores[action_index] = float('inf')  # Assigning infinity to ensure it gets selected\n        else:\n            ucb_scores[action_index] = avg_scores[action_index] + np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[action_index])\n\n    # Select action with the highest UCB score\n    action_index = np.argmax(ucb_scores)\n    \n    return action_index",
          "objective": 258.7984623770849,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(8)\n    action_counts = np.zeros(8)\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        if action_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n\n    # Epsilon-decaying schedule \n    epsilon = max(0.01, (total_time_slots - current_time_slot) / total_time_slots)  # Minimum epsilon is 0.01\n    exploration_choice = np.random.rand() < epsilon\n    \n    if exploration_choice:\n        # Explore: random selection among all actions\n        action_index = np.random.randint(0, 8)\n    else:\n        # Exploit: select the action with the highest average score\n        action_index = np.argmax(avg_scores)\n\n    return action_index",
          "objective": 369.82720826057664,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Define the epsilon decay rate\n    initial_epsilon = 1.0\n    final_epsilon = 0.1\n    decay_rate = (initial_epsilon - final_epsilon) / total_time_slots\n    \n    # Compute the current epsilon\n    current_epsilon = max(final_epsilon, initial_epsilon - decay_rate * current_time_slot)\n    \n    action_scores = []\n    \n    for action_index in range(8):\n        if action_index in score_set and len(score_set[action_index]) > 0:\n            average_score = np.mean(score_set[action_index])\n            selection_count = len(score_set[action_index])\n        else:\n            average_score = 0.0\n            selection_count = 0\n        \n        # Compute an exploration term\n        exploration_term = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combine average score and exploration term\n        final_score = average_score + exploration_term\n        action_scores.append(final_score)\n    \n    # Decide whether to explore or exploit\n    if np.random.rand() < current_epsilon:\n        # Exploration: select a random action\n        action_index = np.random.randint(0, 8)\n    else:\n        # Exploitation: select the best action\n        action_index = int(np.argmax(action_scores))\n    \n    return action_index",
          "objective": 535.0778099057068,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate the average scores for each action\n    avg_scores = np.zeros(8)\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        if scores:\n            avg_scores[action_index] = sum(scores) / len(scores)\n        else:\n            avg_scores[action_index] = 0.0  # No scores mean average score is 0\n\n    # Compute temperature parameter for softmax exploration\n    exploration_strength = np.log(total_selection_count + 1) / (current_time_slot + 1)  # Diminishing exploration factor\n    softmax_scores = avg_scores / exploration_strength\n\n    # Calculate the softmax probabilities\n    exp_scores = np.exp(softmax_scores - np.max(softmax_scores))  # subtract max for numerical stability\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Select action index based on probabilities\n    action_index = np.random.choice(range(8), p=probabilities)\n\n    return action_index",
          "objective": 536.8616035012961,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate the average scores for each action\n    avg_scores = np.zeros(8)\n    selection_counts = np.zeros(8)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            selection_counts[action_index] = len(scores)\n            avg_scores[action_index] = sum(scores) / len(scores)\n\n    # Handle exploration probability\n    epsilon = max(0.1, 0.5 * (1 - (current_time_slot / total_time_slots)))  # Linearly decrease \u03b5 over time\n\n    # Decide whether to explore or exploit\n    if np.random.rand() < epsilon:\n        # Exploration: randomly select an action\n        action_index = np.random.choice(range(8))\n    else:\n        # Exploitation: select action based on average scores\n        # Use softmax for probabilities based on avg_scores\n        exp_scores = np.exp(avg_scores - np.max(avg_scores))  # For numerical stability\n        probabilities = exp_scores / np.sum(exp_scores)\n        action_index = np.random.choice(range(8), p=probabilities)\n\n    return action_index",
          "objective": 536.9026609094741,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize arrays for average scores and selection counts\n    avg_scores = np.zeros(8)\n    selection_counts = np.zeros(8)\n\n    # Calculate average scores and selection counts for each action\n    for action_index, scores in score_set.items():\n        if scores:\n            selection_counts[action_index] = len(scores)\n            avg_scores[action_index] = sum(scores) / len(scores)\n\n    # Calculate exploration probability based on time slot\n    epsilon = max(0.1, 0.5 * (1 - (current_time_slot / total_time_slots)))\n\n    # Calculate exploration bonus for less-selected actions\n    exploration_bonus = np.log(total_selection_count / (selection_counts + 1) + 1)  # Add 1 to avoid division by zero\n    \n    # Combine average scores and exploration bonus\n    adjusted_scores = avg_scores + exploration_bonus\n\n    # Decide whether to explore or exploit\n    if np.random.rand() < epsilon:\n        # Explore: randomly select an action\n        action_index = np.random.choice(range(8))\n    else:\n        # Exploit: select action based on adjusted scores\n        exp_scores = np.exp(adjusted_scores - np.max(adjusted_scores))  # For numerical stability\n        probabilities = exp_scores / np.sum(exp_scores)\n        action_index = np.random.choice(range(8), p=probabilities)\n\n    return action_index",
          "objective": 609.3441890585004,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    avg_scores = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n    \n    # Calculate average scores and selection counts\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        if action_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n\n    # Dynamic exploration rate (epsilon) decaying over time\n    epsilon = max(0.1, 0.5 * (1 - (current_time_slot / total_time_slots)))\n    \n    # Calculate upper confidence bounds (UCB)\n    ucb_values = np.zeros(num_actions)\n    for i in range(num_actions):\n        if action_counts[i] == 0:\n            ucb_values[i] = np.inf  # Encourage exploration for untried actions\n        else:\n            ucb_values[i] = avg_scores[i] + np.sqrt((2 * np.log(total_selection_count)) / action_counts[i])\n\n    # Combine UCB with epsilon-greedy exploration\n    random_exploration = np.random.rand(num_actions)\n    combined_scores = (1 - epsilon) * ucb_values + epsilon * random_exploration\n    \n    # Return the index of the action with the highest combined score\n    action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": 615.4977703854754,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize constants for the exploration-exploitation balance\n    initial_temperature = 2.0\n    final_temperature = 0.1\n    decay_rate = (initial_temperature - final_temperature) / total_time_slots\n\n    # Compute the current temperature for softmax exploration\n    current_temperature = max(final_temperature, initial_temperature - decay_rate * current_time_slot)\n\n    action_scores = []\n    \n    for action_index in range(8):\n        if action_index in score_set and score_set[action_index]:\n            average_score = np.mean(score_set[action_index])\n            selection_count = len(score_set[action_index])\n        else:\n            average_score = 0.0\n            selection_count = 0\n            \n        # Encourage exploration for less selected actions\n        exploration_value = np.log(total_selection_count + 1) / (selection_count + 1) if selection_count > 0 else float('inf')\n        \n        # Final score combining average score and exploration value\n        final_score = average_score + exploration_value\n        action_scores.append(final_score)\n\n    # Apply softmax to action scores to get probabilities\n    exp_scores = np.exp(np.array(action_scores) / current_temperature)\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Select action based on computed probabilities\n    action_index = np.random.choice(range(8), p=probabilities)\n    \n    return action_index",
          "objective": 2694.930371388537,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize variables\n    avg_scores = []\n    exploration_factor = 0.1  # Exploration factor (epsilon)\n    \n    # Calculate average scores for each action and handle division by zero\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        if scores:\n            avg_score = np.mean(scores)\n        else:\n            avg_score = 0\n        avg_scores.append(avg_score)\n    \n    # Calculate exploration bonus for actions that have been selected fewer times\n    exploration_bonuses = [\n        exploration_factor * (1 / (1 + len(score_set.get(action_index, [])))) \n        for action_index in range(8)\n    ]\n    \n    # Calculate the final scores considering both average scores and exploration bonuses\n    final_scores = [\n        avg_scores[action_index] + exploration_bonuses[action_index] \n        for action_index in range(8)\n    ]\n    \n    # Normalize the final scores to sum to 1\n    final_scores = np.array(final_scores)\n    final_scores /= final_scores.sum()\n    \n    # Select an action based on the computed probabilities\n    action_index = np.random.choice(range(8), p=final_scores)\n    \n    return action_index",
          "objective": 6242.204868490122,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # exploration factor\n    action_indices = list(range(8))  # Possible action indices from 0 to 7\n    \n    # Calculate average scores for each action\n    average_scores = {\n        index: np.mean(scores) if scores else 0\n        for index, scores in score_set.items()\n    }\n    \n    # Handle the exploration-exploitation trade-off\n    if np.random.rand() < epsilon or total_selection_count == 0:\n        # Exploration: select a random action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploitation: select the action with the highest average score\n        action_index = max(action_indices, key=lambda index: average_scores[index])\n    \n    return action_index",
          "objective": 6911.197764884922,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n\n    # Calculate average scores and counts for each action\n    avg_scores = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n    \n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        action_counts[i] = len(scores)\n        if action_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n    \n    # Dynamic exploration factor\n    exploration_factor = max(0.1, (total_time_slots - current_time_slot) / total_time_slots)\n\n    # UCB selection strategy\n    ucb_values = avg_scores + np.sqrt((2 * np.log(total_selection_count + 1)) / (action_counts + 1e-5))\n    \n    # Select an action based on exploration-exploitation balance\n    if np.random.rand() < exploration_factor:\n        # Randomly select an underexplored action\n        if total_selection_count < num_actions:  # Ensure every action gets at least one exploration\n            action_index = np.random.randint(num_actions)\n        else:\n            underexplored_actions = [i for i in action_indices if action_counts[i] < np.mean(action_counts)]\n            if underexplored_actions:\n                action_index = np.random.choice(underexplored_actions)\n            else:\n                action_index = np.random.choice(action_indices)\n    else:\n        # Select action with the highest UCB value\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 10238.56373112324,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize arrays for average scores and selection counts\n    avg_scores = np.zeros(8)\n    selection_counts = np.zeros(8)\n\n    # Calculate average scores and selection counts for each action\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        count = len(scores)\n        selection_counts[action_index] = count\n        avg_scores[action_index] = np.mean(scores) if count > 0 else 0\n\n    # Calculate epsilon based on the time slot (decreasing exploration over time)\n    epsilon = max(0.1, 0.5 * (1 - (current_time_slot / total_time_slots)))\n\n    # Compute exploration term based on selection counts\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1))\n    \n    # Combine average scores with exploration bonus\n    adjusted_scores = avg_scores + exploration_bonus\n\n    # Choose whether to explore or exploit action selection\n    if np.random.rand() < epsilon:\n        # Explore: randomly select an action\n        action_index = np.random.choice(np.arange(8))\n    else:\n        # Exploit: select action based on adjusted scores\n        probabilities = np.exp(adjusted_scores - np.max(adjusted_scores))  # For numerical stability\n        probabilities /= np.sum(probabilities)  # Normalize to get probabilities\n        action_index = np.random.choice(np.arange(8), p=probabilities)\n\n    return action_index",
          "objective": 10274.75481359585,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    \n    # Calculate average scores for each action\n    avg_scores = np.array([\n        np.mean(scores) if scores else 0.0 \n        for scores in (score_set.get(i, []) for i in range(action_count))\n    ])\n    \n    # To promote underexploration, we can use a simple heuristic based on selection counts\n    selection_counts = np.array([\n        len(scores) for scores in (score_set.get(i, []) for i in range(action_count))\n    ])\n    \n    # Minimum selection count to avoid division by zero\n    selection_counts[selection_counts == 0] = 1\n\n    # Calculate a bonus for less selected actions\n    exploration_bonus = (1 / selection_counts) ** 0.5\n\n    # Combine average scores with exploration bonuses\n    combined_scores = avg_scores + exploration_bonus\n\n    # Normalize combined scores to get probabilities\n    probabilities = combined_scores / np.sum(combined_scores)\n\n    # Epsilon strategy for exploration\n    epsilon = max(0.1, 1.0 - (current_time_slot / total_time_slots))\n\n    # Randomly select an action based on probabilities or explore randomly\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(range(action_count))\n    else:\n        action_index = np.random.choice(range(action_count), p=probabilities)\n\n    return action_index",
          "objective": 11573.459971541095,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Dynamic epsilon based on the current time slot\n    epsilon = 0.1 * (1 - current_time_slot / total_time_slots)\n\n    action_indices = list(score_set.keys())\n    avg_scores = []\n    \n    # Calculate average scores for each action\n    for action_index in action_indices:\n        scores = score_set[action_index]\n        if scores:  # Calculate average if scores are not empty\n            avg_score = np.mean(scores)\n        else:\n            avg_score = 0  # Treat empty score list as zero\n        avg_scores.append(avg_score)\n\n    # Epsilon-greedy selection strategy\n    if np.random.rand() < epsilon:\n        # Random action for exploration\n        action_index = np.random.choice(action_indices)\n    else:\n        # Select action with the highest average score for exploitation\n        max_avg_score = max(avg_scores)\n        best_actions = [i for i, score in enumerate(avg_scores) if score == max_avg_score]\n        action_index = np.random.choice(best_actions)  # In case of ties\n    \n    return action_index",
          "objective": 14377.88521729099,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    action_counts = np.zeros(len(action_indices))  # Counts for each action\n    avg_scores = np.zeros(len(action_indices))  # Average scores for each action\n\n    # Calculate average scores and selection counts\n    for idx, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        action_counts[idx] = len(scores)\n        avg_scores[idx] = np.mean(scores) if scores else 0  # Handle empty scores\n\n    # Implement UCB strategy\n    if total_selection_count > 0:\n        ucb_values = avg_scores + np.sqrt(2 * np.log(total_selection_count) / (action_counts + 1e-5))\n    else:\n        ucb_values = np.ones(len(action_indices))  # Initial selection if counts are zero\n\n    # Select action with the highest UCB value\n    action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 16675.000937260513,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores for each action\n    average_scores = []\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        if scores:\n            average_score = np.mean(scores)\n        else:\n            average_score = 0\n        average_scores.append(average_score)\n    \n    # Calculate exploration factor\n    exploration_factor = max(1.0 - (current_time_slot / total_time_slots), 0.1)\n    \n    # Calculate the selection probabilities\n    selection_probs = (1 - exploration_factor) * (np.array(average_scores) / np.sum(average_scores)) + \\\n                       (exploration_factor / 8)\n    \n    # Choose an action based on the selection probabilities\n    action_index = np.random.choice(range(8), p=selection_probs)\n    \n    return action_index",
          "objective": 25385.730724161964,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize parameters\n    num_actions = 8\n    avg_scores = np.zeros(num_actions)\n    exploration_factor = 0.1\n    \n    # Calculate average scores for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        avg_scores[action_index] = np.mean(scores) if scores else 0\n\n    # Calculate selection frequencies\n    selection_counts = np.array([len(score_set.get(action_index, [])) for action_index in range(num_actions)])\n    \n    # Calculate the exploration bonuses\n    exploration_bonuses = exploration_factor / (1 + selection_counts)\n    \n    # Combine average scores with exploration bonuses\n    combined_scores = avg_scores + exploration_bonuses\n    \n    # Softmax function to convert scores into probabilities\n    probabilities = np.exp(combined_scores) / np.sum(np.exp(combined_scores))\n    \n    # Select an action based on the computed probabilities\n    action_index = np.random.choice(range(num_actions), p=probabilities)\n    \n    return action_index",
          "objective": 48160.20881025072,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores and selection counts\n    avg_scores = []\n    selection_counts = []\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        if scores:\n            avg_score = sum(scores) / len(scores)\n            count = len(scores)\n        else:\n            avg_score = 0.0\n            count = 0\n        avg_scores.append(avg_score)\n        selection_counts.append(count)\n\n    # Calculate UCB values for exploration\n    ucb_values = []\n    for i in range(8):\n        if selection_counts[i] > 0:\n            ucb = avg_scores[i] + np.sqrt((2 * np.log(total_selection_count)) / selection_counts[i])\n        else:\n            ucb = float('inf')  # Assign high value to unselected actions\n        ucb_values.append(ucb)\n\n    # Select action index by choosing the action with the highest UCB value\n    action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 66359.93199713636,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    scores = np.array([np.mean(score_set[i]) if score_set[i] else 0 for i in action_indices])\n    selection_counts = np.array([len(score_set[i]) for i in action_indices])\n\n    # Using UCB1 strategy\n    if total_selection_count > 0:\n        bonuses = np.sqrt((2 * np.log(total_selection_count)) / (selection_counts + 1e-5))\n        ucb_scores = scores + bonuses\n    else:\n        # If no selections have been made, select randomly one of the actions\n        return np.random.choice(action_indices)\n\n    action_index = np.argmax(ucb_scores)\n    return action_index",
          "objective": 68617.20629891955,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    action_scores = np.array([np.mean(score_set[i]) if score_set[i] else 0 for i in action_indices])\n    selection_counts = np.array([len(score_set[i]) for i in action_indices])\n    \n    # Dynamic \u03b5 value based on current time slot\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n    \n    if np.random.rand() < epsilon:\n        # Exploration: Choose a random action\n        return np.random.choice(action_indices)\n    else:\n        # Exploitation: Compute the action scores\n        # Handling cases with zero selections\n        bonuses = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n        \n        # UCB score to encourage exploration of less-selected actions\n        ucb_scores = action_scores + bonuses\n        action_index = np.argmax(ucb_scores)\n        return action_index",
          "objective": 71311.67007253831,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores for each action considering cases where selection could be zero\n    avg_scores = []\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        if len(scores) > 0:\n            avg_score = np.mean(scores)\n        else:\n            avg_score = 0\n        avg_scores.append(avg_score)\n\n    # Compute selection counts for each action\n    selection_counts = np.array([len(score_set.get(i, [])) for i in range(8)])\n    \n    # Define exploration vs exploitation parameters\n    epsilon = max(1 - (current_time_slot / total_time_slots), 0.1)  # Decreasing epsilon over time\n    random_action_chance = np.random.rand()\n\n    if random_action_chance < epsilon:\n        # Exploration: choose a random action\n        action_index = np.random.randint(0, 8)\n    else:\n        # Exploitation: choose action with maximum average score, breaking ties randomly\n        max_score = np.max(avg_scores)\n        best_actions = np.where(avg_scores == max_score)[0]\n        action_index = np.random.choice(best_actions)\n    \n    return action_index",
          "objective": 95562.04318985714,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))  # Epsilon decay\n    action_index = None\n\n    # Collect scores and selection counts\n    averages = []\n    for action_index in range(8):\n        if action_index in score_set:\n            scores = score_set[action_index]\n            if scores:\n                avg_score = np.mean(scores)  # Calculate average score\n            else:\n                avg_score = 0  # No scores yet\n        else:\n            avg_score = 0  # Action not in the set\n        \n        averages.append(avg_score)\n\n    # Decide on action using epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, 8)  # Explore: Select random action\n    else:\n        action_index = np.argmax(averages)  # Exploit: Select action with highest average score\n\n    return action_index",
          "objective": 99446.38192002189,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores for each action\n    avg_scores = []\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        if scores:\n            avg_score = sum(scores) / len(scores)\n        else:\n            avg_score = 0.0  # No scores means average score is 0\n        avg_scores.append(avg_score)\n    \n    # Exploration factor, decaying over the time slots\n    epsilon = 1.0 - (current_time_slot / total_time_slots)\n    epsilon = max(epsilon, 0.1)  # Ensure there's a minimum exploration factor\n\n    # Generate probabilities considering both exploration and exploitation\n    probabilities = []\n    for i in range(8):\n        probabilities.append((1 - epsilon) * avg_scores[i] + epsilon * (1 / 8))\n\n    # Normalize probabilities\n    probabilities = np.array(probabilities) / np.sum(probabilities)\n\n    # Select action index based on probabilities\n    action_index = np.random.choice(range(8), p=probabilities)\n\n    return action_index",
          "objective": 160623.7895579838,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize variables\n    action_scores = []\n    \n    # Calculate mean scores for each action\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        if scores:\n            mean_score = np.mean(scores)\n        else:\n            mean_score = 0\n            \n        action_scores.append(mean_score)\n\n    # Exploration factor (decay over time)\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n\n    # Calculate selection probabilities based on scores and an exploration term\n    selection_values = []\n    for idx, score in enumerate(action_scores):\n        if total_selection_count > 0:\n            exploration_term = np.sqrt(np.log(total_selection_count) / (1 + len(score_set.get(idx, []))))\n        else:\n            exploration_term = 1  # If no selections have been made yet, explore\n        selection_value = score + exploration_factor * exploration_term\n        selection_values.append(selection_value)\n\n    # Choose action with highest value\n    action_index = np.argmax(selection_values)\n    return action_index",
          "objective": 241727.11200200804,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate the average scores for each action\n    avg_scores = np.zeros(8)\n    selection_counts = np.zeros(8)\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            avg_scores[action_index] = sum(scores) / selection_counts[action_index]\n\n    # Dynamic \u03b5-greedy strategy\n    # Set \u03b5 to decrease over time, allowing more exploration in earlier stages\n    epsilon = max(0.1, (total_time_slots - current_time_slot) / total_time_slots)\n    \n    # Select action based on \u03b5-greedy approach\n    if np.random.rand() < epsilon:\n        # Exploration: choose a random action\n        action_index = np.random.choice(range(8))\n    else:\n        # Exploitation: choose the action with the highest average score\n        action_index = np.argmax(avg_scores)\n\n    return action_index",
          "objective": 244434.9345334415,
          "other_inf": null
     }
]