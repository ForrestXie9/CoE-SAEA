[
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_avgs = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        action_avgs[action_index] = np.mean(scores) if scores else 0.0\n\n    # Dynamic exploration-exploitation balancing using Upper Confidence Bound\n    exploration_bonus = np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n    effective_scores = action_avgs + exploration_bonus\n\n    # Encouraging underutilized actions by scaling effective scores\n    scaling_factor = 1 + (np.max(selection_counts) - selection_counts) / (np.max(selection_counts) + 1e-5)\n    combined_scores = effective_scores * scaling_factor\n\n    # Probabilistic selection\n    combined_scores -= np.min(combined_scores)  # shift to avoid negative values\n    if np.sum(combined_scores) > 0:\n        combined_scores /= np.sum(combined_scores)\n\n    # Sample action based on the scaled scores\n    action_index = np.random.choice(num_actions, p=combined_scores)\n\n    return action_index",
          "objective": 9722.419796147507,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_avgs = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        action_avgs[action_index] = np.mean(scores) if scores else 0.0\n\n    # Calculate exploration factor (Epsilon-Greedy concept, varies with time)\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))  # Gradually reduce exploration\n    if np.random.rand() < epsilon:\n        # Randomly select an action to explore\n        return np.random.randint(num_actions)\n\n    # Adjust selection counts to prevent division by zero\n    selection_counts_safe = selection_counts + 1e-5\n    # Compute combined scores integrating historical averages with exploration incentives\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / selection_counts_safe)\n    combined_scores = action_avgs + exploration_bonus\n\n    # Scale combined scores to [0, 1] for probabilistic selection\n    combined_scores -= np.min(combined_scores)\n    if np.sum(combined_scores) > 0:\n        combined_scores /= np.sum(combined_scores)\n\n    # Sample action based on scores\n    action_index = np.random.choice(num_actions, p=combined_scores)\n\n    return action_index",
          "objective": 10118.916294225151,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8  # Fixed number of actions\n\n    # Calculate average scores and selection counts\n    avg_scores = []\n    selection_counts = []\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        average_score = np.mean(scores) if scores else 0.0\n        avg_scores.append(average_score)\n        selection_counts.append(len(scores))\n\n    # Compute exploration bonuses based on selection counts\n    exploration_bonus = np.array([1 / (count + 1) for count in selection_counts])  # Favor underselected actions\n\n    # Calculate total selections and define the exploration rate\n    exploration_weight = np.clip(1 - (total_selection_count / (total_time_slots + 1)), 0, 1)\n\n    # Composite score combining average score and exploration bonus\n    composite_scores = np.array(avg_scores) + exploration_weight * exploration_bonus\n\n    # Select action based on composite scores\n    action_index = np.argmax(composite_scores)\n\n    # Implement probabilistic selection to promote exploration\n    probabilities = composite_scores / np.sum(composite_scores)\n    if np.random.rand() < exploration_weight:  # Explore with a probability related to selection count\n        action_index = np.random.choice(num_actions, p=probabilities)\n\n    return action_index",
          "objective": 10270.769527579725,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_avgs = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        action_avgs[action_index] = np.mean(scores) if scores else 0.0\n\n    # Add a small constant to avoid division by zero when calculating UCB\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n\n    # Calculate the final UCB score for each action\n    ucb_scores = action_avgs + exploration_bonus\n\n    # Normalize scores to [0, 1] for a probabilistic selection\n    ucb_scores -= np.min(ucb_scores)\n    if np.sum(ucb_scores) > 0:\n        ucb_scores /= np.sum(ucb_scores)\n\n    # Sample an action based on the normalized scores\n    action_index = np.random.choice(num_actions, p=ucb_scores)\n\n    return action_index",
          "objective": 10339.68101982822,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8  # Fixed number of actions\n\n    # Calculate average scores and selection counts\n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        avg_scores[action_index] = np.mean(scores) if scores else 0.0\n\n    # Exploration incentive based on selection frequency\n    exploration_bonus = np.log((total_selection_count + 1) / (selection_counts + 1))  # Log bonus\n    exploration_bonus[selection_counts == 0] = 1.0  # Full bonus for never selected actions\n\n    # Scale exploration weight based on the current time slot\n    exploration_weight = 1 - (current_time_slot / total_time_slots)  # Decreasing exploration\n\n    # Calculate composite scores\n    composite_scores = avg_scores + exploration_weight * exploration_bonus\n\n    # Normalize scores to create probabilities for stochastic selection\n    probabilities = composite_scores / np.sum(composite_scores) if np.sum(composite_scores) > 0 else np.ones(num_actions) / num_actions\n\n    # Implement stochastic selection with an epsilon factor for exploration\n    epsilon = 0.1  # Epsilon for exploration\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(num_actions, p=probabilities)\n    else:\n        action_index = np.argmax(composite_scores)\n\n    return action_index",
          "objective": 10356.681536575528,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_avgs = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        action_avgs[action_index] = np.mean(scores) if scores else 0.0\n\n    # Calculate exploration term based on the Upper Confidence Bound (UCB)\n    selection_counts_safe = selection_counts + 1e-5  # Safe division\n    ucb_term = np.sqrt(np.log(total_selection_count + 1) / selection_counts_safe)\n\n    # Use a combined score metric that influences the probability distribution\n    combined_scores = action_avgs + ucb_term\n\n    # Normalize the combined scores for probability distribution\n    max_score = np.max(combined_scores)  # To prevent overflow\n    scaled_scores = combined_scores - max_score  # Shift to prevent excess\n    if np.sum(np.exp(scaled_scores)) > 0:  # Prevent division by zero\n        probabilities = np.exp(scaled_scores) / np.sum(np.exp(scaled_scores))\n    else:\n        probabilities = np.ones(num_actions) / num_actions  # Uniform distribution if all scores are equal\n\n    # Randomly select an action with the computed probabilities\n    action_index = np.random.choice(num_actions, p=probabilities)\n\n    return action_index",
          "objective": 10357.723196515697,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters for dynamic exploration-exploitation strategy\n    initial_epsilon = 0.9  # Initial exploration factor\n    min_epsilon = 0.01      # Minimum exploration factor\n    decay_factor = 0.001    # Decay factor for exploration rate\n    \n    num_actions = 8  # Fixed number of actions\n    action_scores = np.zeros(num_actions)\n    \n    # Calculate dynamic epsilon\n    epsilon = max(min_epsilon, initial_epsilon * (1 - total_selection_count / (total_time_slots * decay_factor + 1)))\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        average_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n        \n        # Introduce a bias favoring underutilized actions\n        # Using a small constant to prevent division by zero\n        exploration_bias = np.log((total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else 1.0\n        \n        # Calculating the final action score\n        action_scores[action_index] = average_score + epsilon * exploration_bias\n    \n    # Select the action index with the highest score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": 10449.222611832158,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_avgs = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        action_avgs[action_index] = np.mean(scores) if scores else 0.0\n    \n    # Define exploration factor (epsilon) dynamically decreasing over time\n    epsilon = 1.0 - (current_time_slot / total_time_slots) * 0.9  # Reduces exploration over time\n    exploration_factor = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n\n    # Calculate composite scores\n    composite_scores = (1 - epsilon) * action_avgs + epsilon * exploration_factor\n\n    # Normalize scores to probabilities\n    if np.sum(composite_scores) > 0:\n        composite_scores -= np.min(composite_scores)\n        composite_scores /= np.sum(composite_scores)\n\n    # Sample action based on normalized composite scores\n    action_index = np.random.choice(num_actions, p=composite_scores)\n\n    return action_index",
          "objective": 11000.972049908713,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8  # Fixed number of actions\n    \n    # Calculate average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        average_scores[action_index] = np.mean(scores) if scores else 0.0\n    \n    # Compute exploration bonuses (UCB)\n    total_actions = np.sum(selection_counts)\n    exploration_boost = np.sqrt(np.log(total_actions + 1) / (selection_counts + 1e-5))\n\n    # Calculate composite scores combining average scores and exploration bonuses\n    composite_scores = average_scores + exploration_boost\n    \n    # Epsilon-Greedy parameter\n    epsilon = max(0.1, 0.9 * (1 - current_time_slot / total_time_slots))\n    \n    # Generate probabilities for actions based on composite scores\n    exp_scores = np.exp(composite_scores - np.max(composite_scores))  # For numerical stability\n    probabilities = exp_scores / np.sum(exp_scores)\n    \n    # Incorporate exploration using epsilon-greedy approach\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(num_actions)  # Explore\n    else:\n        action_index = np.argmax(probabilities)  # Exploit\n    \n    return action_index",
          "objective": 11127.548225825436,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8  # Fixed number of actions\n\n    # Calculate average scores and selection counts\n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        avg_scores[action_index] = np.mean(scores) if scores else 0.0\n        selection_counts[action_index] = len(scores)\n\n    # Exploration bonus: Encourage selection of underexplored actions\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1))  # UCB-like\n\n    # Calculate composite scores\n    composite_scores = avg_scores + exploration_bonus\n\n    # Implement probabilistic selection to promote exploration\n    probabilities = np.exp(composite_scores) / np.sum(np.exp(composite_scores))  # Softmax for probability distribution\n\n    # Explore with epsilon probability\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))  # Decrease exploration over time\n    if np.random.rand() < epsilon:  # Explore\n        action_index = np.random.choice(num_actions, p=probabilities)\n    else:  # Exploit based on the best composite score\n        action_index = np.argmax(composite_scores)\n\n    return action_index",
          "objective": 11520.74629126492,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters for exploration-exploitation strategy\n    initial_epsilon = 0.9  # Initial exploration factor\n    min_epsilon = 0.1       # Minimum exploration factor\n    decay_factor = 0.002    # Decay factor for exploration rate\n\n    # Calculate current exploration rate based on total selections and current time\n    epsilon = max(min_epsilon, initial_epsilon * (1 - (total_selection_count / (total_time_slots * decay_factor + 1)) * (current_time_slot / total_time_slots)))\n\n    action_scores = []\n    num_actions = 8  # Fixed number of actions\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n\n        # Calculate average score and selection count\n        average_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n        \n        # Compute exploration bonus; avoid division by zero\n        exploration_bonus = epsilon / (selection_count + 1)\n\n        # Combine average score and exploration bonus\n        action_score = average_score + exploration_bonus\n        action_scores.append(action_score)\n\n    # Select the action index with the highest action score\n    action_index = np.argmax(action_scores)\n\n    # Adding a probabilistic exploration mechanism\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(num_actions, p=np.array(action_scores) / np.sum(action_scores))\n\n    return action_index",
          "objective": 11996.320735881518,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters for exploration-exploitation strategy\n    initial_epsilon = 0.9  # Initial exploration factor\n    min_epsilon = 0.1       # Minimum exploration factor\n    decay_factor = 0.001    # Decay factor for exploration rate\n\n    # Calculate adaptive exploration rate based on total selections\n    epsilon = max(min_epsilon, initial_epsilon * (1 - (total_selection_count / (total_time_slots * decay_factor + 1))))\n    \n    action_scores = []\n    num_actions = 8  # Fixed number of actions\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        \n        # Calculate average score and selection count\n        average_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n        \n        # Calculate exploration factor\n        exploration_value = epsilon / (selection_count + 1)  # Adding 1 to avoid division by zero\n\n        # Calculate final score considering exploitation and exploration\n        action_score = average_score + exploration_value\n        action_scores.append(action_score)\n\n    # Select the action index with the highest action score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": 12420.059098973126,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    \n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        average_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n        \n        # Incentivize less frequently selected actions\n        selection_bonus = 1 / (selection_count + 1) if selection_count > 0 else 1.0\n\n        # Dynamic exploration strategy - Epsilon-Greedy approach\n        epsilon = max(0.1, (total_time_slots - current_time_slot) / total_time_slots)  # Decrease epsilon over time\n        exploration_score = np.random.binomial(1, epsilon)  # 1 with probability epsilon, 0 otherwise\n\n        # Comprehensive scoring combining average score, exploration bonus, and selection bonus\n        action_scores[action_index] = (1 - exploration_score) * average_score + exploration_score * selection_bonus\n\n    # Normalize scores to convert to probabilities\n    probabilities = np.exp(action_scores - np.max(action_scores))  # Subtract max for numerical stability\n    probabilities /= np.sum(probabilities)  # Normalize\n\n    # Stochastic selection based on probabilities\n    action_index = np.random.choice(num_actions, p=probabilities)\n\n    return action_index",
          "objective": 12557.335624906893,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8  # Fixed number of actions\n\n    # Calculate average scores and selection counts\n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        avg_scores[action_index] = np.mean(scores) if scores else 0.0\n        selection_counts[action_index] = len(scores)\n\n    # Exploration bonus: use upper confidence bound approach\n    exploration_bonus = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        if selection_counts[action_index] > 0:\n            exploration_bonus[action_index] = np.sqrt((2 * np.log(total_selection_count)) / selection_counts[action_index])\n        else:\n            exploration_bonus[action_index] = np.inf  # Infinite bonus for unselected actions\n\n    # Composite scores as the sum of average scores and exploration bonuses\n    composite_scores = avg_scores + exploration_bonus\n\n    # Implement softmax for probabilistic selection based on composite scores\n    exp_scores = np.exp(composite_scores - np.max(composite_scores))  # Stability trick\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Epsilon-greedy strategy for exploration\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))  # Decaying exploration rate\n\n    if np.random.rand() < epsilon:  # Explore\n        action_index = np.random.choice(num_actions, p=probabilities)\n    else:  # Exploit based on the highest composite score\n        action_index = np.argmax(composite_scores)\n\n    return action_index",
          "objective": 12575.600800723185,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8  # Fixed number of actions\n\n    # Initialize arrays to hold average scores and selection counts\n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n\n    # Compute exploration bonuses based on selection counts using UCB\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-8))  # Avoid division by zero\n\n    # Composite score: mean scores + exploration bonus\n    composite_scores = avg_scores + exploration_bonus\n    \n    # Normalize the composite scores for probabilistic selection\n    probabilities = composite_scores / np.sum(composite_scores)\n    \n    # Stochastic selection based on probabilities\n    action_index = np.random.choice(num_actions, p=probabilities)\n\n    return action_index",
          "objective": 12582.838612132156,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8  # Fixed number of actions\n    action_scores = np.zeros(num_actions)\n    \n    # Calculate average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        average_scores[action_index] = np.mean(scores) if scores else 0.0\n\n    # Compute exploration bonuses based on selection frequency\n    exploration_bonus = np.log((total_selection_count + num_actions) / (selection_counts + 1))\n\n    # Calculate final scores combining average scores and exploration bonus\n    action_scores = average_scores + exploration_bonus\n\n    # Normalize the scores to a probability distribution\n    prob_scores = np.exp(action_scores - np.max(action_scores))  # For numerical stability\n    prob_scores /= np.sum(prob_scores)\n\n    # Use random choice based on the computed probabilities\n    action_index = np.random.choice(num_actions, p=prob_scores)\n\n    return action_index",
          "objective": 12691.489406529645,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_avgs = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        action_avgs[action_index] = np.mean(scores) if scores else 0.0\n\n    # Dynamic exploration versus exploitation balance\n    exploration_factor = 1 / (1 + (current_time_slot / total_time_slots))  # Decrease exploration over time\n\n    # Calculate exploration bonus\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n\n    # Enhancing selection of underutilized actions\n    underutilized_bonus = (1.0 / (selection_counts + 1e-5))\n\n    # Unified scoring approach: Combining averages, exploration bonus, and underutilized bonus\n    scores = (action_avgs * (1 - exploration_factor)) + (exploration_bonus * exploration_factor) + underutilized_bonus\n\n    # Normalize scores to create a probability distribution for stochastic selection\n    scores -= np.min(scores)\n    if np.sum(scores) > 0:\n        scores /= np.sum(scores)\n\n    # Sample an action based on the normalized scores\n    action_index = np.random.choice(num_actions, p=scores)\n\n    return action_index",
          "objective": 13072.971962349638,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    exploration_factor = 1.0  # Adjust this factor to change the exploration incentive\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        average_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n\n        # Exploration bonus increases for under-selected actions\n        if selection_count > 0:\n            exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count) / selection_count)\n        else:\n            exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count + 1))  # encourage new actions\n        \n        # Calculate composite score with a preference towards historical performance and exploration\n        action_scores[action_index] = average_score + exploration_bonus\n\n    # Convert scores to probabilities\n    probabilities = np.exp(action_scores - np.max(action_scores))  # for numerical stability\n    probabilities /= np.sum(probabilities)  # Normalize to sum to 1\n\n    # Randomly select an action based on computed probabilities\n    action_index = np.random.choice(num_actions, p=probabilities)\n\n    return action_index",
          "objective": 13183.90072692313,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    \n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        average_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n        \n        # Explore more early; less as time goes on\n        epsilon = 1.0 / (current_time_slot + 1)\n        \n        # Calculate exploration bonus using UCB approach\n        exploration_bonus = np.sqrt(2 * np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else np.inf\n        \n        # Composite score\n        action_scores[action_index] = average_score + (epsilon * exploration_bonus)\n\n    # Normalize scores to create probabilities\n    probabilities = np.exp(action_scores - np.max(action_scores))  # For numerical stability\n    probabilities /= np.sum(probabilities)  # Normalize to sum to 1\n\n    # Stochastic selection based on probabilities\n    action_index = np.random.choice(num_actions, p=probabilities)\n\n    return action_index",
          "objective": 13433.461568773375,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8  # There are 8 actions, indexed from 0 to 7\n    action_scores = np.zeros(num_actions)\n\n    # Compute mean scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        average_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n        \n        # Exploration bonus based on UCB-style\n        exploration_bonus = 0.0\n        if selection_count > 0:\n            exploration_bonus = np.sqrt((2 * np.log(total_selection_count)) / selection_count)\n        else:\n            exploration_bonus = 1.0  # Full bonus for unselected actions\n        \n        # Underexploration incentive\n        underexploration_incentive = 1 / (selection_count + 1) if selection_count > 0 else 1.0\n        \n        # Calculate composite action score\n        action_scores[action_index] = average_score + exploration_bonus + underexploration_incentive\n\n    # Normalize scores to obtain probabilities\n    probabilities = action_scores - np.max(action_scores)  # For numerical stability\n    probabilities = np.exp(probabilities)  # Exponential of scores\n    probabilities /= np.sum(probabilities)  # Normalize to get probabilities\n\n    # Stochastic selection based on computed probabilities\n    action_index = np.random.choice(num_actions, p=probabilities)\n\n    return action_index",
          "objective": 13482.811873591405,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8  # Fixed number of actions\n    action_scores = np.zeros(num_actions)\n    \n    # Calculate average scores and scores count for actions\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        average_scores[action_index] = np.mean(scores) if scores else 0.0\n    \n    # Dynamic epsilon based on total selections\n    initial_epsilon = 1.0  # Starting with a high exploration rate\n    min_epsilon = 0.01      # Minimum exploration factor\n    decay_rate = 0.9        # Rate of decay for epsilon over time\n    epsilon = max(min_epsilon, initial_epsilon * (decay_rate ** (current_time_slot / total_time_slots)))\n    \n    # Calculating the exploration bonus based on selection counts\n    for action_index in range(num_actions):\n        # Assign exploration bias favoring underutilized actions\n        exploration_bonus = np.log((total_selection_count + 1) / (selection_counts[action_index] + 1)) if selection_counts[action_index] > 0 else 1.0\n        \n        # Combined action score\n        action_scores[action_index] = average_scores[action_index] + (epsilon * exploration_bonus)\n    \n    # Probabilistic selection based on action scores\n    action_probabilities = np.exp(action_scores) / np.sum(np.exp(action_scores))  # Softmax to generate probabilities\n    action_index = np.random.choice(num_actions, p=action_probabilities)  # Select action according to probabilistic choice\n\n    return action_index",
          "objective": 14528.680230863481,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    \n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        average_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n        \n        # Incentivize less frequently selected actions\n        selection_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else 1.0\n        \n        # Combined score for action\n        action_scores[action_index] = average_score + selection_bonus\n\n    # Normalize scores to convert to probabilities\n    probabilities = np.exp(action_scores - np.max(action_scores))  # Subtract max for numerical stability\n    probabilities /= np.sum(probabilities)  # Normalize\n\n    # Stochastic selection based on probabilities\n    action_index = np.random.choice(num_actions, p=probabilities)\n\n    return action_index",
          "objective": 15017.478741643841,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8  # Fixed number of actions\n    action_scores = []\n    \n    # Define exploration factor parameters\n    initial_exploration_bonus = 1.0  # Initial exploration value\n    exploration_decay = 0.05          # Decay rate for exploration bonus\n    \n    # Calculate exploration bonus based on the selection count and time slot\n    exploration_bonus = initial_exploration_bonus * (1 - (total_selection_count / (total_time_slots + 1)))\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        \n        # Calculate average score and selection count\n        average_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n\n        # UCB based exploration bonus\n        if selection_count > 0:\n            exploration_value = exploration_bonus * np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1))\n        else:\n            exploration_value = exploration_bonus * np.sqrt(np.log(total_selection_count + 1))  # Give all exploration to untried actions\n\n        # Calculate composite score\n        composite_score = average_score + exploration_value\n        action_scores.append(composite_score)\n\n    # Select action based on composite scores\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": 15036.384315077117,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Define parameters for exploration-exploitation strategy\n    initial_epsilon = 1.0  # Initial exploration factor\n    min_epsilon = 0.05      # Minimum exploration factor\n    decay_factor = 0.001    # Decay factor for exploration rate\n\n    # Update epsilon based on the selection progress\n    epsilon = max(min_epsilon, initial_epsilon * (1 - (total_selection_count / (total_time_slots * decay_factor + 1))))\n\n    action_scores = []\n    num_actions = 8  # Fixed number of actions\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        \n        # Calculate average score and selection count\n        average_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n        \n        # Exploration bonus is inversely proportional to selection count\n        exploration_bonus = np.log(total_selection_count + 1) / (selection_count + 1) if selection_count > 0 else 1.0\n\n        # Combine average score and exploration bonus into a composite score\n        composite_score = average_score + (epsilon * exploration_bonus)\n        action_scores.append(composite_score)\n\n    # Normalize scores to create a probability distribution\n    action_probabilities = np.array(action_scores) / np.sum(action_scores)\n    \n    # Select action based on probabilities (probabilistic selection approach)\n    action_index = np.random.choice(num_actions, p=action_probabilities)\n\n    return action_index",
          "objective": 15060.435623033156,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initial parameters for epsilon-greedy strategy\n    min_epsilon = 0.05\n    max_epsilon = 1.0\n    decay_rate = 0.99\n    base_explore_factor = 0.1  # Impact of exploration based on selection count\n    \n    # Calculate dynamic epsilon based on current time slot and total selections\n    epsilon = max(\n        min_epsilon,\n        max_epsilon * (decay_rate ** (total_selection_count / (total_time_slots + 1)))\n    )\n    \n    # Initialize scores and gather statistics\n    action_scores = []\n    total_actions = 8  # Total actions from 0 to 7\n\n    for action_index in range(total_actions):\n        scores = score_set.get(action_index, [])\n        mean_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n\n        # Effective exploitation term\n        exploration_bonus = (base_explore_factor / (selection_count + 1)) if selection_count > 0 else base_explore_factor\n        adjusted_score = mean_score + (exploration_bonus * epsilon)\n\n        action_scores.append(adjusted_score)\n\n    # Select the action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 15230.937474350734,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    \n    # Calculate the average score for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        average_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n        \n        # Upper Confidence Bound (UCB) calculation\n        exploration_bonus = 0.0\n        if selection_count > 0:\n            exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_count)\n        \n        # Composite score combining average score and exploration bonus\n        action_scores[action_index] = average_score + exploration_bonus\n\n    # Probabilistic selection based on composite scores\n    probabilities = np.exp(action_scores) / np.sum(np.exp(action_scores))  # Softmax probabilities\n    action_index = np.random.choice(num_actions, p=probabilities)\n\n    return action_index",
          "objective": 15425.651705994787,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    \n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        average_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n        \n        # Adaptive exploration bonus\n        exploration_bonus = 0.0\n        if selection_count > 0:\n            exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_count)\n        else:\n            # For actions not selected yet, encourage exploration\n            exploration_bonus = np.sqrt(2 * np.log(total_selection_count + 1))  # Stronger exploration for unselected actions\n\n        # Composite score: We give more weight to exploration as time progresses\n        exploration_weight = (total_time_slots - current_time_slot) / total_time_slots  # Decrease exploration weight over time\n        action_scores[action_index] = (1 - exploration_weight) * average_score + exploration_weight * exploration_bonus\n    \n    # Normalize scores to convert to probabilities\n    probabilities = np.exp(action_scores - np.max(action_scores))  # Subtract max for numerical stability\n    probabilities /= np.sum(probabilities)  # Normalize\n\n    # Stochastic selection based on probabilities\n    action_index = np.random.choice(num_actions, p=probabilities)\n\n    return action_index",
          "objective": 15621.216003855263,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters for dynamic exploration-exploitation strategy\n    initial_epsilon = 1.0  # Start with high exploration\n    min_epsilon = 0.1       # Minimum exploration factor\n    decay_factor = 0.001    # Slow decay for exploration rate\n\n    # Calculate the current exploration rate\n    epsilon = max(min_epsilon, initial_epsilon * (1 - (total_selection_count / (total_time_slots * decay_factor + 1)) * (current_time_slot / total_time_slots)))\n\n    num_actions = 8  # Fixed number of actions\n    action_scores = []\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        average_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n        \n        # Compute exploration bonus using selection count\n        exploration_bonus = epsilon / (selection_count + 1) \n        \n        # Tune the score based on exploration incentive for underexplored actions\n        composite_score = average_score + exploration_bonus * (1 + np.log(1 + selection_count))\n        \n        action_scores.append(composite_score)\n\n    # Normalize action scores to create a probability distribution\n    action_probs = np.array(action_scores) / np.sum(action_scores)\n    \n    # Select an action based on the probabilities\n    action_index = np.random.choice(num_actions, p=action_probs)\n    \n    return action_index",
          "objective": 15699.037679159555,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Configuration for exploration\n    min_epsilon = 0.05  # Minimum exploration probability\n    initial_epsilon = 1.0   # Initial exploration probability\n    decay_rate = 0.999   # Decay rate for exploration\n\n    # Epsilon calculation\n    epsilon = max(min_epsilon, initial_epsilon * (decay_rate ** current_time_slot))\n    \n    total_actions = 8  # There are 8 actions indexed from 0 to 7\n    action_scores = []\n\n    for action_index in range(total_actions):\n        scores = score_set.get(action_index, [])\n        mean_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n        \n        # Exploration adjustment: Higher emphasis on under-selected actions\n        exploration_bonus = 1.0 / (selection_count + 1) if selection_count < 10 else 0.0  # Bonus for under-selected actions\n        adjusted_mean = mean_score + exploration_bonus\n\n        # Weighted score based on exploration vs exploitation\n        final_score = (1 - epsilon) * adjusted_mean + epsilon * np.random.rand()\n\n        action_scores.append(final_score)\n    \n    # Select the action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 16071.42191075781,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters for modified epsilon-greedy strategy\n    initial_epsilon = 0.9  # Initial exploration factor\n    min_epsilon = 0.1       # Minimum exploration factor\n    decay_factor = 0.001    # Decay factor for exploration rate\n\n    # Calculate adaptive exploration rate based on total selections\n    epsilon = max(min_epsilon, initial_epsilon * (1 - total_selection_count / (total_time_slots * decay_factor + 1)))\n    \n    num_actions = 8  # Fixed number of actions\n    action_scores = []\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        \n        # Compute average score\n        average_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n        \n        # Compute exploration incentive\n        exploration_value = epsilon * (1 / (selection_count + 1))\n        \n        # Calculate the combined score for the action\n        action_score = average_score + exploration_value\n        action_scores.append(action_score)\n\n    # Select the action with the highest score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": 16319.808314795857,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8  # Fixed number of actions\n\n    # Initialize lists to store average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    # Calculate average scores and selection counts\n    for action_index, scores in score_set.items():\n        selection_counts[action_index] = len(scores)\n        average_scores[action_index] = np.mean(scores) if scores else 0.0\n\n    # Total number of selections so far\n    total_selections = total_selection_count\n    \n    # UCB calculation\n    exploration_bonus = np.sqrt((2 * np.log(total_selections + 1)) / (selection_counts + 1e-5)) # avoid division by zero\n    action_scores = average_scores + exploration_bonus\n\n    # Probabilistic selection based on action scores\n    action_probabilities = action_scores / np.sum(action_scores)  # Normalize scores to create probabilities\n    action_index = np.random.choice(num_actions, p=action_probabilities)\n\n    return action_index",
          "objective": 16327.916932582611,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        average_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n        action_counts[action_index] = selection_count\n        \n        # Upper Confidence Bound approach\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else np.sqrt(np.log(total_selection_count + 1))\n        \n        # Enhanced score combining historical average score and exploration bonus\n        action_scores[action_index] = average_score + exploration_bonus\n\n    # Normalize scores to convert to probabilities\n    probabilities = action_scores - np.max(action_scores)  # For numerical stability\n    probabilities = np.exp(probabilities)  # Exponential to emphasize higher scores\n    probabilities /= np.sum(probabilities)  # Normalize\n\n    # Stochastic selection based on probabilities\n    action_index = np.random.choice(num_actions, p=probabilities)\n\n    return action_index",
          "objective": 16373.839153423738,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        average_scores[action_index] = np.mean(scores) if scores else 0.0\n        selection_counts[action_index] = len(scores)\n\n    # Calculate exploration bonuses\n    exploration_bonuses = np.zeros(num_actions)\n    exploration_weight = max(0.1, (total_time_slots - current_time_slot) / total_time_slots)\n    \n    for action_index in range(num_actions):\n        if selection_counts[action_index] > 0:\n            exploration_bonuses[action_index] = np.sqrt(np.log(total_selection_count) / selection_counts[action_index])\n        else:\n            exploration_bonuses[action_index] = 1.0  # Encourage exploration for unselected actions\n\n    # Compute composite scores\n    composite_scores = average_scores + exploration_weight * exploration_bonuses\n    \n    # Probabilistic selection based on composite scores\n    probabilities = np.exp(composite_scores - np.max(composite_scores))  # For numerical stability\n    probabilities /= np.sum(probabilities)  # Normalize probabilities\n\n    # Select action based on calculated probabilities\n    action_index = np.random.choice(num_actions, p=probabilities)\n\n    return action_index",
          "objective": 16638.254163269397,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    \n    # Calculate average scores for each action\n    action_avgs = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_avgs[action_index] = np.mean(scores) if scores else 0.0\n\n    # Dynamic exploration-exploitation balance using declining epsilon\n    initial_epsilon = 0.9\n    min_epsilon = 0.01\n    epsilon = max(min_epsilon, initial_epsilon * (1 - current_time_slot / total_time_slots))\n\n    action_scores = np.zeros(num_actions)\n    \n    # Assess selection frequencies and calculate composite score\n    for action_index in range(num_actions):\n        selection_count = len(score_set.get(action_index, []))\n        \n        # Exploration incentive: greater penalty for actions chosen more frequently\n        exploration_incentive = np.log((total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else 1.0\n        \n        # Composite score: combine average score with exploration incentive\n        action_scores[action_index] = action_avgs[action_index] + epsilon * exploration_incentive\n\n    # Probability distribution for actions based on composite scores\n    probabilities = action_scores / np.sum(action_scores) if np.sum(action_scores) > 0 else np.ones(num_actions) / num_actions\n    \n    # Stochastic selection based on calculated probabilities\n    action_index = np.random.choice(range(num_actions), p=probabilities)\n\n    return action_index",
          "objective": 16848.790125380016,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8  # Fixed number of actions\n    action_scores = []\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        \n        # Calculate average score and selection count\n        average_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n\n        # Calculate exploration factor using UCB\n        if selection_count > 0:\n            exploration_term = np.sqrt((2 * np.log(total_selection_count)) / selection_count)\n        else:\n            exploration_term = float('inf')  # Encourage exploration for unselected actions\n        \n        # Combine exploitation and exploration\n        total_action_score = average_score + exploration_term\n        action_scores.append(total_action_score)\n\n    # Normalize scores and compute selection probabilities\n    exp_scores = np.exp(action_scores - np.max(action_scores))\n    selection_probabilities = exp_scores / np.sum(exp_scores)\n\n    # Randomly select action based on computed probabilities\n    action_index = np.random.choice(num_actions, p=selection_probabilities)\n\n    return action_index",
          "objective": 16995.586778698133,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8  # Fixed number of actions\n    action_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_scores[action_index] = np.mean(scores) if scores else 0.0\n        selection_counts[action_index] = len(scores)\n\n    # Dynamic exploration factor using UCB\n    exploration_factor = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n    \n    # Composite scores: average score + exploration factor\n    composite_scores = action_scores + exploration_factor\n\n    # Normalize to create probabilities\n    probabilities = composite_scores / np.sum(composite_scores)\n\n    # Stochastic selection based on composite scores\n    action_index = np.random.choice(num_actions, p=probabilities)\n\n    return action_index",
          "objective": 17442.90682977877,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_avgs = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        action_avgs[action_index] = np.mean(scores) if scores else 0.0\n\n    # Define epsilon based on the current time slot to encourage exploration\n    epsilon = 1.0 - (current_time_slot / total_time_slots)\n\n    # Generate a unified score combining averages and selection counts\n    # We use an exploration bonus that favors less selected actions\n    exploration_bonus = (1 / (selection_counts + 1e-5))\n    combined_scores = action_avgs + exploration_bonus\n\n    # Softmax normalization for better scoring distribution\n    exp_scores = np.exp(combined_scores - np.max(combined_scores))  # Stability improvement\n    softmax_scores = exp_scores / np.sum(exp_scores)\n\n    # Epsilon-Greedy approach\n    if np.random.rand() < epsilon:\n        # Exploration: Select a random action\n        action_index = np.random.choice(num_actions)\n    else:\n        # Exploitation: Select based on the softmax scores\n        action_index = np.random.choice(num_actions, p=softmax_scores)\n\n    return action_index",
          "objective": 17603.51185717527,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8  # Fixed number of actions\n    min_selection_threshold = 1  # Minimum number of selections to consider\n    \n    # Calculate average scores and selection counts\n    average_scores = []\n    selection_counts = []\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        average_scores.append(np.mean(scores) if scores else 0.0)\n        selection_counts.append(len(scores))\n    \n    # Calculate exploration factor\n    exploration_term = np.array(selection_counts) + 1  # To avoid division by zero\n    exploration_boost = (total_selection_count + 1) / exploration_term  # Encourage infrequent actions\n    \n    # Create a composite score combining average scores and exploration incentives\n    composite_scores = np.array(average_scores) * exploration_boost\n    \n    # Epsilon-greedy approach with a dynamic exploration probability\n    epsilon = max(0.1, 0.9 * (1 - current_time_slot / total_time_slots))\n    \n    # Generate probabilities for actions based on composite scores\n    exp_scores = np.exp(composite_scores)  # For probabilistic selection\n    probabilities = exp_scores / np.sum(exp_scores)\n    \n    # Incorporate exploration: generate random value for epsilon-greedy\n    random_value = np.random.rand()\n    if random_value < epsilon:\n        action_index = np.random.choice(num_actions)  # Explore: select random action\n    else:\n        action_index = np.argmax(probabilities)  # Exploit: select based on probabilities\n    \n    return action_index",
          "objective": 17759.95517456955,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters for exploration-exploitation strategy\n    initial_epsilon = 0.9  # Initial exploration factor\n    min_epsilon = 0.1       # Minimum exploration factor\n    decay_factor = 0.001    # Decay factor for exploration rate\n\n    # Calculate adaptive exploration rate based on total selections\n    epsilon = max(min_epsilon, initial_epsilon * (1 - (total_selection_count / (total_time_slots * decay_factor + 1))))\n    \n    action_scores = []\n    num_actions = 8  # Fixed number of actions\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        \n        # Compute average score and selection count safely\n        average_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n\n        # Compute exploration term to balance exploration and exploitation\n        exploration_value = epsilon / (selection_count + 1)  # Ensure no division by zero\n        \n        # Calculate the combined score for the action\n        action_score = average_score + exploration_value\n        action_scores.append(action_score)\n\n    # Select the action with the highest score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": 17971.598671416796,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    action_scores = np.zeros(num_actions)\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        average_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n        \n        # Incentivize less frequently selected actions\n        if selection_count > 0:\n            selection_bonus = 1 / (selection_count + 1)  # Bonus inversely proportional to selection count\n        else:\n            selection_bonus = 1.0  # Give a full bonus for actions that have never been selected\n\n        # Calculate an exploration term using UCB strategy\n        if total_selection_count > 0:\n            exploration_term = np.sqrt((2 * np.log(total_selection_count)) / (selection_count + 1e-5))  # Small constant to avoid division by zero\n        else:\n            exploration_term = 0.0  # No prior selections to explore from\n\n        # Compose the final action score\n        action_scores[action_index] = average_score + selection_bonus + exploration_term\n\n    # Convert scores to probabilities for selection\n    probabilities = np.exp(action_scores - np.max(action_scores))  # Subtract max for numerical stability\n    probabilities /= np.sum(probabilities)  # Normalize\n\n    # Stochastic selection based on computed probabilities\n    action_index = np.random.choice(num_actions, p=probabilities)\n\n    return action_index",
          "objective": 18122.52307184511,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    initial_epsilon = 0.9  # Initial exploration factor\n    min_epsilon = 0.1       # Minimum exploration factor\n    decay_factor = 0.001    # Decay factor for exploration rate\n\n    # Calculate adaptive exploration rate based on total selections\n    epsilon = max(min_epsilon, initial_epsilon * (1 - (total_selection_count / (total_time_slots * decay_factor + 1))))\n    \n    action_scores = []\n    num_actions = 8  # Fixed number of actions\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        \n        # Calculate average score and selection count\n        average_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n        \n        # Explore using a probabilistic factor tied to epsilon\n        bonus = epsilon / (selection_count + 1)  # Added 1 to avoid division by zero\n\n        # Compute the final score blending exploitation and exploration components\n        action_score = average_score + bonus\n        action_scores.append(action_score)\n\n    # Select the action with the maximum calculated score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": 18124.915636389564,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    \n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        average_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n\n        # Calculate exploration bonus: UCB-like term\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else 1.0\n\n        # Dynamic exploration strategy\n        exploration_weight = max(0.1, (total_time_slots - current_time_slot) / total_time_slots)  # Gradually decrease exploration\n        action_scores[action_index] = average_score + exploration_weight * exploration_bonus\n\n    # Stochastic selection based on scores\n    probabilities = np.exp(action_scores - np.max(action_scores))  # For numerical stability\n    probabilities /= np.sum(probabilities)  # Normalize probabilities\n\n    # Select action based on calculated probabilities\n    action_index = np.random.choice(num_actions, p=probabilities)\n\n    return action_index",
          "objective": 18276.706974883058,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters for the exploration-exploitation strategy\n    initial_epsilon = 0.9\n    min_epsilon = 0.1\n    decay_factor = 0.001\n    temperature_scale = 1.0  # Scaling for temperature (exploration)\n    \n    # Compute adaptive epsilon\n    epsilon = max(min_epsilon, initial_epsilon * (1 - (total_selection_count / (total_time_slots * decay_factor + 1))))\n    \n    num_actions = 8\n    action_scores = []\n    \n    # Calculate scores for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        average_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n\n        # Penalty term for frequent selections to encourage exploration\n        penalty = (selection_count ** 0.5) / (total_selection_count + 1) if selection_count > 0 else 0\n        \n        # Compute final action score with a temperature-based exploration term\n        exploration_value = epsilon * (1 - average_score) * temperature_scale / (selection_count + 1)\n        action_score = average_score - penalty + exploration_value\n        action_scores.append(action_score)\n\n    # Select the action with the highest score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": 18314.38941402583,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    exploration_factor = 2.0  # Exploration-enhancing factor\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        average_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n\n        # Calculate exploration bonus\n        if selection_count > 0:\n            exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count) / selection_count)\n        else:\n            exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count + 1))  # Encourage exploration of new actions\n\n        # Calculate composite score as a blend of average score and exploration bonus\n        action_scores[action_index] = average_score + exploration_bonus\n\n    # Normalize scores for probability distribution, introducing a controlled randomness for selection\n    max_score = np.max(action_scores)\n    adjusted_scores = action_scores - max_score\n    probabilities = np.exp(adjusted_scores)\n    \n    # Stochastic adjustment to maintain exploration\n    stochastic_adjustment = np.random.rand(num_actions) * 0.05  # Small random noise\n    probabilities += stochastic_adjustment\n    probabilities /= np.sum(probabilities)  # Normalize to sum to 1\n\n    # Select action based on computed probabilities\n    action_index = np.random.choice(num_actions, p=probabilities)\n\n    return action_index",
          "objective": 18774.292116546312,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters for epsilon-greedy strategy with dynamic exploration bonus\n    initial_epsilon = 0.8  # Initial exploration factor\n    min_epsilon = 0.1       # Minimum exploration factor\n    decay_factor = total_time_slots  # Decay factor based on total time slots\n    \n    # Calculate current epsilon value\n    epsilon = max(min_epsilon, initial_epsilon * (1 - total_selection_count / (decay_factor + 1)))\n    \n    num_actions = 8  # Total actions\n    action_scores = []\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        \n        # Compute average score and selection count\n        average_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n        \n        # Exploration incentive, adjust based on selection count\n        exploration_value = epsilon * (1 / (selection_count + 1))  # Adding 1 to avoid division by zero\n        \n        # Calculate combined score for selection\n        action_score = average_score + exploration_value\n        action_scores.append(action_score)\n\n    # Select action with the highest score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": 19291.624713644927,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_avgs = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        action_avgs[action_index] = np.mean(scores) if scores else 0.0\n\n    # Calculate dynamic exploration factor\n    exploration_factor = np.sqrt((np.log(total_selection_count + 1) / (selection_counts + 1e-5)))\n\n    # Epsilon for exploration strategy\n    epsilon = 1 / (current_time_slot + 1)\n\n    # Create effective scores based on exploration-exploitation balance\n    effective_scores = action_avgs + exploration_factor\n\n    # Introduce an exploration term to promote less utilized actions\n    underutilization_bonus = (num_actions - selection_counts) / (num_actions + 1e-5)\n    combined_scores = effective_scores + underutilization_bonus\n\n    # Normalize combined scores for probabilistic selection\n    combined_scores -= np.min(combined_scores)  # Shift to avoid negatives\n    if np.sum(combined_scores) > 0:\n        combined_scores /= np.sum(combined_scores)\n\n    # Epsilon-Greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(num_actions)\n    else:\n        action_index = np.random.choice(num_actions, p=combined_scores)\n\n    return action_index",
          "objective": 19419.643165185535,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        average_score = np.mean(scores) if scores else 0.0\n\n        # Exploration bonus calculation\n        exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else np.inf\n        \n        # Composite score calculation\n        action_scores[action_index] = average_score + exploration_bonus\n\n    # Dynamic scaling of exploration based on time\n    epsilon = max(0.1, (1 - current_time_slot / total_time_slots))  # Decrease exploration over time\n    action_probs = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        if np.random.rand() < epsilon:\n            # Exploration: uniform distribution\n            action_probs[action_index] = 1 / num_actions\n        else:\n            # Exploitation: based on composite scores\n            action_probs[action_index] = action_scores[action_index]\n\n    # Normalize the probabilities to sum to 1\n    action_probs /= np.sum(action_probs)\n\n    # Select action based on the calculated probabilities\n    action_index = np.random.choice(num_actions, p=action_probs)\n\n    return action_index",
          "objective": 20415.47793881713,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Ensure that we have a fixed number of actions\n    num_actions = 8\n    \n    # 1. Calculate average scores for each action and manage selection counts\n    average_scores = []\n    selection_counts = []\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        average_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n        \n        average_scores.append(average_score)\n        selection_counts.append(selection_count)\n    \n    # 2. Implement the Upper Confidence Bound (UCB) for exploration-exploitation\n    action_scores = []\n    for action_index in range(num_actions):\n        average_score = average_scores[action_index]\n        selection_count = selection_counts[action_index]\n        \n        if selection_count > 0:\n            ucb_value = average_score + np.sqrt((2 * np.log(total_selection_count)) / selection_count)\n        else:\n            ucb_value = 1.0  # Assign a high score to unselected actions for exploration\n        \n        action_scores.append(ucb_value)\n    \n    # 3. Incorporate selection bias\n    selection_bias = 0.1\n    action_scores = [score + selection_bias * (1 / (count + 1)) for score, count in zip(action_scores, selection_counts)]\n    \n    # 4. Probabilistic action selection based on normalized scores\n    action_scores_array = np.array(action_scores)\n    probabilities = action_scores_array / action_scores_array.sum()\n    \n    action_index = np.random.choice(num_actions, p=probabilities)\n    \n    return action_index",
          "objective": 20453.83242236127,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        average_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n        \n        # Update selection counts\n        selection_counts[action_index] = selection_count\n\n        # Dynamic exploration bonus (UCB)\n        if selection_count > 0:\n            ucb_value = average_score + np.sqrt(2 * np.log(total_selection_count + 1) / selection_count)\n        else:\n            ucb_value = 1.0  # A high initial value for untried actions to encourage exploration\n        \n        action_scores[action_index] = ucb_value\n\n    # Normalize scores to convert to probabilities\n    probabilities = np.exp(action_scores - np.max(action_scores))  # for numerical stability\n    probabilities /= np.sum(probabilities)  # Normalize to sum to 1\n\n    # Stochastic selection based on probabilities\n    action_index = np.random.choice(num_actions, p=probabilities)\n\n    return action_index",
          "objective": 20489.0501339295,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters for adaptive epsilon-greedy strategy\n    initial_epsilon = 0.9\n    min_epsilon = 0.1\n    decay_factor = 1000  # Modulation factor for decay\n\n    # Calculate adaptive exploration rate\n    epsilon = max(min_epsilon, initial_epsilon * (1 - total_selection_count / max(1, total_time_slots * decay_factor)))\n    \n    num_actions = 8\n    action_scores = []\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        \n        # Calculate average score and selection count\n        average_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n        \n        # Exploration adjustment: inversely proportional to selection count\n        exploration_value = epsilon / (selection_count + 1) if selection_count > 0 else epsilon\n        \n        # Combined score for action\n        action_score = average_score + exploration_value\n        action_scores.append(action_score)\n\n    # Select the action with the maximum score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": 20590.211958924094,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8  # Total number of actions\n    action_scores = np.zeros(num_actions)\n    \n    # Historical Performance Evaluation\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        average_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n        \n        # Dynamic exploration-exploitation mechanism\n        # Epsilon is inversely proportional to the current selection frequency\n        exploration_bonus = 1.0 / (selection_count + 1) if selection_count > 0 else 1.0\n        \n        # Minimum selection threshold for lesser-explored actions\n        # Encourage exploration of poorly selected actions\n        rarity_bonus = np.log((total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else 1.0\n        \n        # Composite action score integrating average score, exploration, and rarity bonus\n        action_scores[action_index] = average_score + exploration_bonus + rarity_bonus\n    \n    # Softmax function for probabilistic selection\n    exp_scores = np.exp(action_scores - np.max(action_scores))  # Stability improvement\n    probabilities = exp_scores / np.sum(exp_scores)\n    \n    # Sample action index based on probabilities\n    action_index = np.random.choice(num_actions, p=probabilities)\n\n    return action_index",
          "objective": 21025.36809528071,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Configuration for exploration\n    min_epsilon = 0.05  # Minimum exploration probability\n    initial_epsilon = 1.0  # Initial exploration probability\n    decay_factor = 0.95  # Decay factor for exploration\n\n    # Epsilon calculation\n    epsilon = max(min_epsilon, \n                  initial_epsilon * (decay_factor ** (total_selection_count // total_time_slots)))\n\n    total_actions = 8  # There are 8 actions indexed from 0 to 7\n    action_scores = []\n\n    for action_index in range(total_actions):\n        scores = score_set.get(action_index, [])\n        mean_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n        \n        # Exploration adjustment: Higher emphasis on less frequently selected actions\n        selection_bonus = 1 / (selection_count + 1) if selection_count < 5 else 0.0\n        adjusted_mean = mean_score + selection_bonus\n\n        # Weighted score based on exploration (epsilon) and exploitation (mean score)\n        exploration_component = epsilon * np.random.rand()\n        exploitation_component = (1 - epsilon) * adjusted_mean\n\n        final_score = exploitation_component + exploration_component\n        action_scores.append(final_score)\n    \n    # Select the action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 21690.490474898976,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters for exploration-exploitation balance\n    initial_epsilon = 1.0\n    min_epsilon = 0.05\n    decay_rate = 0.99\n    \n    # Epsilon decay based on total selections and time slots\n    epsilon = max(min_epsilon, initial_epsilon * (decay_rate ** (total_selection_count / (total_time_slots + 1))))\n    \n    action_scores = []\n    total_actions = 8\n\n    for action_index in range(total_actions):\n        scores = score_set.get(action_index, [])\n        mean_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n        \n        # Penalty for frequently selected actions to encourage exploration\n        selection_penalty = 1 / (selection_count + 1) if selection_count > 0 else 1.0\n        \n        # Combined score formula\n        combined_score = mean_score - selection_penalty\n        \n        exploration = np.random.rand() < epsilon\n        \n        # Choose either to explore or exploit\n        if exploration:\n            action_scores.append(combined_score + np.random.rand() * (1.0 - mean_score))  # Explore with some noise\n        else:\n            action_scores.append(combined_score)\n\n    # Select the action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 21873.17343368212,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters for epsilon-greedy strategy\n    min_epsilon = 0.05\n    max_epsilon = 1.0\n    epsilon_decay_rate = 0.95\n    \n    # Calculate dynamic epsilon based on total selection count\n    epsilon = max(min_epsilon, max_epsilon * (epsilon_decay_rate ** (total_selection_count / (total_time_slots + 1))))\n    \n    total_actions = 8\n    action_scores = []\n    \n    for action_index in range(total_actions):\n        scores = score_set.get(action_index, [])\n        mean_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n        \n        # Weighted exploration factor\n        exploration_factor = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else 1.0\n        \n        # Calculate the adjusted score considering exploration\n        adjusted_score = mean_score + (exploration_factor * epsilon)\n        action_scores.append(adjusted_score)\n\n    # Select the action with the highest adjusted score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 21925.88253000933,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Configuration for exploration\n    min_epsilon = 0.05  # Minimum exploration probability\n    max_epsilon = 1.0   # Maximum exploration probability\n    decay_rate = 0.99   # Decay rate for exploration\n\n    # Epsilon calculation based on total selection and current time slot\n    exploration_fraction = current_time_slot / total_time_slots\n    epsilon = max(min_epsilon, max_epsilon * (decay_rate ** (total_selection_count / (total_time_slots + 1))))\n\n    action_scores = []\n    total_actions = 8  # There are 8 actions indexed from 0 to 7\n\n    for action_index in range(total_actions):\n        scores = score_set.get(action_index, [])\n        mean_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n        \n        # Exploration bonus: increase exploration for underselected actions\n        exploration_bonus = 1.0 / (selection_count + 1)  # Encourages exploration of under-selected actions\n        \n        # Calculate the final score for each action\n        if np.random.rand() < epsilon:\n            # Exploration phase\n            action_score = mean_score + exploration_bonus\n        else:\n            # Exploitation phase\n            action_score = mean_score\n        \n        action_scores.append(action_score)\n    \n    # Select the action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 22158.71098536449,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8  # Fixed number of actions\n    action_scores = np.zeros(num_actions)\n    \n    # Epsilon for exploration\n    epsilon = 0.1  # Exploration probability\n    \n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        average_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n        \n        # Exploration bonus calculation\n        if selection_count > 0:\n            # Assign a bonus based on the number of selections\n            exploration_value = np.sqrt((np.log(total_selection_count) / selection_count))\n        else:\n            # For untried actions, give a significant exploration bonus\n            exploration_value = np.sqrt(np.log(total_selection_count + 1))\n        \n        # Combined score calculation\n        action_scores[action_index] = average_score + exploration_value\n\n    # Create a probability distribution for actions\n    if np.random.rand() < epsilon:\n        # Explore: select a random action\n        action_index = np.random.randint(num_actions)\n    else:\n        # Exploit: select the action with the highest score\n        action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": 22457.966229750233,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters for exploration-exploitation strategy\n    initial_temperature = 2.0  # Initial temperature parameter for Boltzmann exploration\n    min_temperature = 0.1      # Minimum temperature parameter\n    decay_factor = 0.001       # Decay factor for temperature\n    \n    # Calculate adaptive temperature based on total selections\n    temperature = max(min_temperature, initial_temperature * (1 - (total_selection_count / (total_time_slots * decay_factor + 1))))\n    \n    action_scores = []\n    num_actions = 8  # Fixed number of actions\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        \n        # Calculate average score and selection count\n        average_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n        \n        # Apply a selection count penalty to encourage exploration of less-selected actions\n        selection_penalty = np.log(selection_count + 1)  # Logarithmic penalty to reduce impact gradually\n        \n        # Calculate final score considering exploitation and exploration\n        action_score = average_score - (selection_penalty / 10)  # Adjusting penalty magnitude\n        action_scores.append(action_score)\n\n    # Compute probabilities for each action based on the scores transformed by the temperature\n    exp_scores = np.exp(np.array(action_scores) / temperature)\n    probabilities = exp_scores / np.sum(exp_scores)  # Normalize to get probabilities\n    \n    # Select action based on computed probabilities\n    action_index = np.random.choice(range(num_actions), p=probabilities)\n    \n    return action_index",
          "objective": 22659.7593380842,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    \n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        average_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n        \n        # We use a modified UCB strategy to reward exploration\n        exploration_bonus = np.sqrt(2 * np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else 2 * np.log(total_selection_count + 1)\n\n        # Combined score for action\n        action_scores[action_index] = average_score + exploration_bonus\n\n    # Normalizing scores to create probabilities\n    probabilities = np.exp(action_scores - np.max(action_scores))  # For numerical stability\n    probabilities /= np.sum(probabilities)  # Normalize to sum to 1\n\n    # Stochastic selection based on probabilities\n    action_index = np.random.choice(num_actions, p=probabilities)\n\n    return action_index",
          "objective": 22752.02214841338,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Constants\n    total_actions = 8  # There are 8 actions indexed from 0 to 7\n    exploration_weight = 0.5  # Weighing factor for exploration vs exploitation\n    min_selection_count = 5  # Minimum selections before discarding exploration bonus\n\n    action_scores = []\n    \n    for action_index in range(total_actions):\n        scores = score_set.get(action_index, [])\n        mean_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n\n        # Calculate the exploration bonus\n        if selection_count > 0:\n            exploration_bonus = np.sqrt((2 * np.log(total_selection_count)) / selection_count)\n        else:\n            exploration_bonus = float('inf')  # Give highest priority to actions never selected\n\n        # Calculate composite score combining exploitation and exploration\n        composite_score = (1 - exploration_weight) * mean_score + exploration_weight * exploration_bonus\n        \n        action_scores.append(composite_score)\n    \n    # Convert scores to probabilities\n    action_probs = np.exp(action_scores) / np.sum(np.exp(action_scores))\n    \n    # Randomly select an action based on computed probabilities\n    action_index = np.random.choice(total_actions, p=action_probs)\n    \n    return action_index",
          "objective": 22847.545581174356,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Set parameters for exploration-exploitation balance\n    epsilon_initial = 1.0\n    epsilon_min = 0.1\n    total_duration = total_time_slots\n\n    # Compute the exploration parameter epsilon using a more gradual decay based on total selections\n    epsilon = max(epsilon_min, epsilon_initial * (1 - (current_time_slot / total_duration)))\n\n    action_scores = []\n    \n    for action_index in range(8):\n        # Get historical scores for the action\n        scores = score_set.get(action_index, [])\n        average_score = np.mean(scores) if scores else 0.0  # Average score\n        \n        # Count of selections\n        selection_count = len(scores)\n\n        # Use a softer exploration bonus function that encourages exploration of less selected actions\n        exploration_bonus = np.log(total_selection_count + 1) / (selection_count + 1) if selection_count > 0 else 1.0\n        \n        # Adjust the score with a normalized exploration factor\n        adjusted_score = (1 - epsilon) * average_score + epsilon * exploration_bonus\n        \n        action_scores.append(adjusted_score)\n\n    # Select the action index with the highest adjusted score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 23101.144001370558,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters for exploration-exploitation balance\n    epsilon_initial = 1.0\n    epsilon_min = 0.1\n    decay_rate = 0.1\n\n    # Calculate epsilon based on the current time slot and total selections\n    epsilon = max(epsilon_min, epsilon_initial * np.exp(-decay_rate * current_time_slot))\n\n    action_scores = []\n    \n    for action_index in range(8):\n        # Get historical scores for the action\n        scores = score_set.get(action_index, [])\n        average_score = np.mean(scores) if scores else 0.0  # Average score\n\n        # Count of selections\n        selection_count = len(scores)\n\n        # Calculate exploration factor\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else 1.0\n        \n        # Combine scores with exploration factor\n        adjusted_score = (1 - epsilon) * average_score + epsilon * exploration_bonus\n        \n        action_scores.append(adjusted_score)\n\n    # Select the action index with the highest adjusted score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 23590.605034090477,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Configurable parameters\n    min_epsilon = 0.1\n    max_epsilon = 1.0\n    decay_rate = 0.95\n    base_explore_factor = 0.2  # Exploration impact factor\n    rare_action_bonus = 0.1     # Bonus for rarely selected actions\n    \n    # Calculate dynamic epsilon\n    epsilon = min_epsilon + (max_epsilon - min_epsilon) * (current_time_slot / total_time_slots)\n    \n    action_scores = []\n    total_actions = 8  # Total actions from 0 to 7\n    \n    for action_index in range(total_actions):\n        scores = score_set.get(action_index, [])\n        mean_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n\n        # Adjust exploration effectively\n        exploration_bonus = base_explore_factor / (selection_count + 1) if selection_count > 0 else base_explore_factor\n        # Add bonus for less frequently selected actions\n        rare_action_bonus_score = rare_action_bonus if selection_count < (total_selection_count / total_actions) else 0\n        \n        # Composite score combining mean score and exploration incentives\n        adjusted_score = mean_score + (exploration_bonus * epsilon) + rare_action_bonus_score\n        action_scores.append(adjusted_score)\n    \n    # Convert scores to probabilities\n    probabilities = np.array(action_scores) / np.sum(action_scores)\n    \n    # Stochastic selection based on the probabilities\n    action_index = np.random.choice(np.arange(total_actions), p=probabilities)\n    \n    return action_index",
          "objective": 24046.95875207706,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Hyperparameters for exploration-exploitation trade-off\n    initial_epsilon = 0.9\n    min_epsilon = 0.1\n    decay_factor = 0.002\n    \n    # Compute the current epsilon\n    epsilon = max(min_epsilon, initial_epsilon * (1 - total_selection_count / (total_time_slots * decay_factor + 1)))\n    \n    action_scores = []\n    num_actions = 8\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        \n        # Calculate average score and selection count\n        average_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n        \n        # Exploration bonus to encourage selection of less frequently chosen actions\n        exploration_bonus = (epsilon / (selection_count + 1)) if selection_count > 0 else epsilon\n        \n        # Calculate total score for action\n        action_score = average_score + exploration_bonus\n        action_scores.append(action_score)\n\n    # Select the action index with the highest action score\n    action_index = np.argmax(action_scores)\n\n    # Introduce a stochastic choice with respect to the action scores\n    if np.random.rand() < epsilon:\n        probabilities = np.array(action_scores) / np.sum(action_scores)\n        action_index = np.random.choice(num_actions, p=probabilities)\n\n    return action_index",
          "objective": 24324.132143811836,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Define epsilon parameters for exploration\n    epsilon_initial = 1.0\n    epsilon_min = 0.1\n    epsilon_decay = 0.95\n\n    # Compute exploration parameter epsilon\n    epsilon = max(epsilon_min, epsilon_initial * (epsilon_decay ** current_time_slot))\n\n    action_scores = []\n\n    # Calculate average scores and selection counts\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        average_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n\n        # Apply a penalty for actions that are less frequently selected\n        selection_penalty = (1.0 / (selection_count + 1)) if selection_count > 0 else 1.0\n        \n        # Calculate adjusted score\n        adjusted_score = (1 - epsilon) * average_score - epsilon * selection_penalty\n        \n        action_scores.append(adjusted_score)\n\n    # Select the action index with the highest adjusted score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": 25332.14862641116,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8  # Fixed number of actions\n    action_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        average_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n        \n        action_scores[action_index] = average_score\n        selection_counts[action_index] = selection_count\n\n    # Calculate exploration bonuses\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))  # Avoid division by zero\n\n    # Composite scores: average score + exploration bonus\n    composite_scores = action_scores + exploration_bonus\n\n    # Normalize probabilities\n    probabilities = composite_scores / np.sum(composite_scores)\n\n    # Stochastic selection based on composite scores\n    action_index = np.random.choice(num_actions, p=probabilities)\n\n    return action_index",
          "objective": 25618.10650746009,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8  # Fixed number of actions\n    action_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        average_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n        \n        action_scores[action_index] = average_score\n        selection_counts[action_index] = selection_count\n\n    # Adaptive exploration factor\n    epsilon = max(0.1, 0.9 * (1 - current_time_slot / total_time_slots))\n\n    # Exploration bonus based on selection counts\n    exploration_bonus = 1 / (selection_counts + 1)  # Avoid division by zero\n    exploration_bonus *= epsilon\n\n    # Composite scores: average score + exploration bonus\n    composite_scores = action_scores + exploration_bonus\n\n    # Stochastic selection based on composite scores\n    probabilities = composite_scores / np.sum(composite_scores)\n    action_index = np.random.choice(num_actions, p=probabilities)\n\n    return action_index",
          "objective": 26245.495268862527,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters for exploration-exploitation strategy\n    exploration_base = 0.9  # Initial exploration ratio\n    decay_factor = 0.01  # Decay factor for exploration rate\n\n    # Calculate dynamic exploration rate\n    exploration_rate = max(0.1, exploration_base * np.exp(-decay_factor * total_selection_count))\n\n    action_scores = []\n    num_actions = 8  # Fixed number of actions\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        \n        # Handle edge cases for selection count\n        average_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n        \n        if selection_count == 0:\n            # If the action hasn't been selected, we give it a high potential score for exploration\n            exploration_bonus = exploration_rate * 2  # Encourage exploration for untried actions\n        else:\n            exploration_bonus = exploration_rate / selection_count  # Standard exploration adjustment\n        \n        # Calculate final score considering exploration\n        action_score = average_score + exploration_bonus\n        action_scores.append(action_score)\n\n    # Select the action index with the highest action score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": 26836.44856053823,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters for exploration-exploitation strategy\n    initial_epsilon = 0.9  # Initial exploration factor\n    min_epsilon = 0.1       # Minimum exploration factor\n    decay_factor = 0.002    # Decay factor for exploration rate\n    \n    # Calculate current exploration rate\n    epsilon = max(min_epsilon, initial_epsilon * (1 - (total_selection_count / (total_time_slots * decay_factor + 1)) * (current_time_slot / total_time_slots)))\n\n    num_actions = 8  # Fixed number of actions\n    action_scores = np.zeros(num_actions)\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        \n        # Compute average score\n        average_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n\n        # Compute UCB based exploration bonus\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1e-5)) if selection_count > 0 else np.inf\n        \n        # Calculate composite score\n        action_scores[action_index] = average_score + (epsilon * exploration_bonus)\n\n    # Robust stochastic selection\n    probabilities = np.exp(action_scores - np.max(action_scores))  # Avoid overflow in exp\n    probabilities /= np.sum(probabilities)  # Normalize to create a probability distribution\n    \n    # Randomly select an action based on the computed probabilities\n    action_index = np.random.choice(num_actions, p=probabilities)\n    \n    return action_index",
          "objective": 30707.28729801452,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Constants for epsilon decay and exploration\n    INITIAL_EPSILON = 1.0\n    FINAL_EPSILON = 0.05\n    DECAY_RATE = 0.99\n    \n    # Adaptive epsilon calculation based on total selections and current time slot\n    exploration_fraction = current_time_slot / total_time_slots\n    epsilon = max(FINAL_EPSILON, INITIAL_EPSILON * (DECAY_RATE ** total_selection_count))\n    \n    action_scores = []\n    total_actions = 8\n\n    # Calculate mean score and selection counts\n    for action_index in range(total_actions):\n        scores = score_set.get(action_index, [])\n        mean_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n        \n        # Exploration bonus for less frequently selected actions\n        exploration_bonus = 1.0 / (selection_count + 1) if selection_count > 0 else 1.0\n        \n        # Combine the mean score and exploration bonus\n        action_score = mean_score + exploration_bonus\n        \n        # Add random exploration based on epsilon\n        if np.random.rand() < epsilon:\n            action_score += np.random.rand() * exploration_bonus\n        \n        action_scores.append(action_score)\n\n    # Select the action with the highest computed score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 30723.386498128893,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Define exploration parameters\n    epsilon_initial = 1.0\n    epsilon_min = 0.1\n    epsilon_decay = max(1 - (total_selection_count / (total_time_slots * 8 + 1e-5)), 0.0)\n    \n    # Compute the exploration parameter epsilon\n    epsilon = epsilon_initial * epsilon_decay + epsilon_min * (1 - epsilon_decay)\n    \n    action_scores = []\n    \n    for action_index in range(8):\n        # Get historical scores for the action\n        scores = score_set.get(action_index, [])\n        average_score = np.mean(scores) if scores else 0.0  # Average score\n        \n        # Count of selections for handling exploration\n        selection_count = len(scores)\n        \n        # Avoid division by zero and compute exploration bonus\n        exploration_bonus = 1.0 / (selection_count + 1) if selection_count > 0 else 1.0\n        \n        # Calculate adjusted score\n        adjusted_score = (1 - epsilon) * average_score + epsilon * exploration_bonus\n        \n        action_scores.append(adjusted_score)\n\n    # Select the action index with the highest adjusted score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 32677.00128325008,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Set parameters for epsilon-greedy strategy\n    min_epsilon = 0.05  # Minimum exploration probability\n    max_epsilon = 1.0   # Initial maximum exploration probability\n    decay_rate = 0.99   # Decay rate for exploration\n\n    # Adaptive epsilon calculation based on total selections and current time slot\n    exploration_fraction = current_time_slot / total_time_slots\n    epsilon = max(min_epsilon, max_epsilon * (decay_rate ** total_selection_count))\n\n    action_scores = []\n    total_actions = 8  # Action indices from 0 to 7\n\n    for action_index in range(total_actions):\n        scores = score_set.get(action_index, [])\n        mean_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n        \n        # Exploration bonus for less frequently selected actions\n        exploration_bonus = 1.0 / (selection_count + 1) if selection_count > 0 else 1.0\n        \n        # Combining scores\n        action_score = mean_score + exploration_bonus\n        \n        # Add exploration factor\n        if np.random.rand() < epsilon:\n            action_score += np.random.rand() * exploration_bonus  # Add random exploration\n        \n        action_scores.append(action_score)\n\n    # Select the action with the highest computed score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 33170.69314433224,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8  # Fixed number of actions\n    action_scores = []\n    \n    # Calculate exploration rate\n    max_exploration = 0.9\n    min_exploration = 0.1\n    exploration_decay = 0.001\n    exploration_rate = max(min_exploration, max_exploration * (1 - (total_selection_count / (total_time_slots * exploration_decay + 1)) * (current_time_slot / total_time_slots)))\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        \n        # Calculate average score and selection count\n        average_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n        \n        # Exploration bonus\n        exploration_bonus = 1 / (selection_count + 1)  # Add one to avoid division by zero\n        \n        # Total score combines average score and exploration bonus\n        total_action_score = average_score + exploration_rate * exploration_bonus\n        action_scores.append(total_action_score)\n\n    # Softmax selection for better exploration\n    exp_scores = np.exp(action_scores - np.max(action_scores))  # Stable softmax computation\n    selection_probabilities = exp_scores / np.sum(exp_scores)\n\n    # Randomly select action based on computed probabilities\n    action_index = np.random.choice(num_actions, p=selection_probabilities)\n\n    return action_index",
          "objective": 33345.37492634616,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8  # Total number of actions\n    action_scores = np.zeros(num_actions)\n    \n    # Average score calculation and selection count\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        average_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n\n        # Adaptive exploration-exploitation strategy\n        exploration_factor = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else 1.0\n        \n        # Promotion of underused actions\n        rarity_factor = (total_time_slots - current_time_slot) / (total_time_slots + 1) * (1.0 / (selection_count + 1)) if selection_count > 0 else 1.0\n        \n        # Composite score: balance between performance and exploration\n        action_scores[action_index] = average_score + exploration_factor + rarity_factor\n\n    # Softmax function for probabilistic selection\n    exp_scores = np.exp(action_scores - np.max(action_scores))\n    probabilities = exp_scores / np.sum(exp_scores)\n    \n    # Sample action index based on probabilities\n    action_index = np.random.choice(num_actions, p=probabilities)\n\n    return action_index",
          "objective": 33346.15988488539,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Configuration for exploration\n    min_epsilon = 0.05  # Minimum exploration probability\n    initial_epsilon = 1.0  # Initial exploration probability\n    # Dynamic decay based on total selections to promote exploration initially\n    decay_rate = 0.95  # Decay rate for exploration\n\n    # Epsilon calculation based on the number of selections made\n    epsilon = max(min_epsilon, initial_epsilon * (decay_rate ** (total_selection_count / total_time_slots)))\n    \n    total_actions = 8  # Total actions from 0 to 7\n    action_scores = np.zeros(total_actions)\n    \n    for action_index in range(total_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Mean score calculation, handle division by zero\n        mean_score = np.mean(scores) if selection_count > 0 else 0.0\n        \n        # Exploration bonus for under-selected actions\n        exploration_bonus = (1.0 / (selection_count + 1)) if selection_count < 10 else 0.0\n        \n        # Calculate the adjusted scoring with exploration factor\n        adjusted_score = mean_score + exploration_bonus\n        \n        # Final score combining exploration and exploitation\n        action_scores[action_index] = (1 - epsilon) * adjusted_score + epsilon * np.random.rand()\n    \n    # Select the action with the highest computed score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 33818.49182883912,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Adaptive epsilon settings\n    min_epsilon = 0.05\n    max_epsilon = 1.0\n    decay_factor = 0.99\n\n    # Calculate dynamic epsilon based on current time slot and total selections\n    epsilon = max(min_epsilon, max_epsilon * (decay_factor ** (current_time_slot / (total_time_slots + 1))))\n\n    # Initialize scores and gather statistics\n    action_count = 8  # Total actions from 0 to 7\n    action_scores = []\n    \n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        mean_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n\n        # Exploration bonus considering selection count\n        exploration_bonus = (1 / (selection_count + 1)) if selection_count > 0 else 1.0  # safer factor for untried actions\n        \n        # Adjusted score combining historical performance and exploration\n        adjusted_score = mean_score + (epsilon * exploration_bonus)\n        action_scores.append(adjusted_score)\n\n    # Select the action with the highest adjusted score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 34152.90697704816,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters for exploration and exploitation\n    initial_epsilon = 1.0  # Initial exploration rate\n    final_epsilon = 0.1    # Final exploration rate\n    epsilon_decay = 0.001   # Decay rate for exploration\n    num_actions = 8         # Number of actions\n\n    # Calculate current exploration rate\n    epsilon = max(final_epsilon, initial_epsilon * np.exp(-epsilon_decay * total_selection_count))\n    \n    action_scores = []\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        average_score = np.mean(scores) if scores else 0.0\n        \n        # Selection count for normalization\n        selection_count = len(scores) if scores else 0\n        normalized_count = selection_count + 1e-5  # Avoid division by zero\n        \n        # Incorporate selection count into the action score, promoting less frequent actions\n        exploration_bonus = 1 / normalized_count\n        \n        # Total action score with exploration and average score\n        action_score = average_score + (epsilon * exploration_bonus)\n        \n        action_scores.append(action_score)\n\n    # Select action index with the highest action score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": 34293.81248838675,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters for exploration and exploitation\n    initial_epsilon = 1.0  # Initial exploration rate\n    final_epsilon = 0.05   # Final exploration rate\n    epsilon_decay_rate = 0.005  # Decay rate combining time slots and selections\n    \n    num_actions = 8  # Number of actions\n    \n    # Calculate current exploration rate based on total selections and time slots\n    epsilon = max(final_epsilon, initial_epsilon * (1 - (current_time_slot / total_time_slots)))\n    \n    action_scores = []\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        average_score = np.mean(scores) if scores else 0.0\n        \n        # Selection count for normalization\n        selection_count = len(scores)\n        normalized_count = selection_count + 1e-5  # Avoid division by zero\n        \n        # Exploration bonus inversely related to the selection count\n        exploration_bonus = 1.0 / normalized_count\n        \n        # Total action score combining average score and exploration bonus\n        action_score = average_score + (epsilon * exploration_bonus)\n        \n        action_scores.append(action_score)\n\n    # Select action index with the highest action score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": 34327.17846626154,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters for epsilon-greedy strategy\n    initial_epsilon = 0.9  # Initial exploration factor\n    min_epsilon = 0.1       # Minimum exploration factor\n    decay_factor = 0.001    # Decay factor for exploration rate\n\n    # Calculate adaptive exploration rate based on total selections\n    epsilon = max(min_epsilon, initial_epsilon * (1 - total_selection_count / (total_time_slots * decay_factor + 1)))\n    \n    num_actions = 8  # Fixed number of actions\n    action_scores = []\n    \n    # Calculate the action scores while handling empty score lists\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Compute average score and handle cases where selection_count is zero\n        average_score = np.mean(scores) if selection_count > 0 else 0.0\n        \n        # Exploration component: Incorporate the upper confidence bound\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combine average score and exploration value\n        action_score = average_score + exploration_value\n        action_scores.append(action_score)\n    \n    # Epsilon-greedy selection process\n    if np.random.rand() < epsilon:\n        # Randomly explore one of the actions\n        action_index = np.random.randint(num_actions)\n    else:\n        # Exploit the best-known action\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": 34644.10218633446,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters for exploration-exploitation strategy\n    exploration_min = 0.1  # Minimum exploration rate\n    exploration_max = 1.0  # Maximum exploration rate\n    decay_rate = 0.005  # Rate of decay for exploration\n\n    # Calculate exploration factor based on total selection count and current time slot\n    exploration_rate = exploration_max * np.exp(-decay_rate * total_selection_count)\n    exploration_rate = max(exploration_min, exploration_rate * (1 - (current_time_slot / total_time_slots)))\n\n    action_scores = []\n    num_actions = 8  # Fixed number of actions\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        \n        # Calculate average score and handle edge case for selection_count = 0\n        average_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n        \n        # Dynamic exploration adjustment based on action selection count\n        normalized_count = selection_count + 1e-5  # Prevent division by zero\n        exploration_adjustment = exploration_rate / normalized_count\n        \n        # Calculate final score for each action\n        action_score = average_score + exploration_adjustment\n        \n        action_scores.append(action_score)\n\n    # Select action index with the highest action score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": 34723.95085503254,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Set parameters for dynamic exploration-exploitation balance\n    epsilon_initial = 1.0\n    epsilon_min = 0.1\n    decay_rate = 0.99  # Decay rate for epsilon reduction over time\n    \n    # Compute the exploration parameter epsilon\n    if total_selection_count == 0:  # Before any action is selected\n        epsilon = epsilon_initial\n    else:\n        epsilon = max(epsilon_min, epsilon_initial * (decay_rate ** current_time_slot))\n    \n    action_scores = []\n    \n    for action_index in range(8):\n        # Get historical scores for the action\n        scores = score_set.get(action_index, [])\n        average_score = np.mean(scores) if scores else 0.0  # Average score\n        \n        # Count of selections to handle exploration\n        selection_count = len(scores)\n        \n        # Exploration bonus calculation\n        exploration_bonus = (1 / (selection_count + 1e-5)) if selection_count > 0 else 1.0\n        \n        # Adjusted score calculation\n        adjusted_score = (1 - epsilon) * average_score + epsilon * exploration_bonus\n        action_scores.append(adjusted_score)\n\n    # Select the action index with the highest adjusted score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 35303.499692143276,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Set parameters for exploration\n    min_epsilon = 0.05  # Minimum exploration probability\n    max_epsilon = 1.0   # Maximum exploration probability\n    decay_rate = 0.99   # Decay rate for exploration\n\n    # Adaptive epsilon calculation\n    exploration_fraction = current_time_slot / total_time_slots\n    epsilon = max(min_epsilon, max_epsilon * (decay_rate ** (total_selection_count / (total_time_slots + 1))))\n\n    action_scores = []\n    \n    total_actions = 8  # Since action indices are from 0 to 7 \n\n    for action_index in range(total_actions):\n        scores = score_set.get(action_index, [])\n        mean_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n        variance_score = np.var(scores) if scores else 0.0  # Calculate variance if there are scores\n        \n        # Exploration bonus for less frequently selected actions\n        exploration_bonus = (1.0 / (selection_count + 1)) if selection_count > 0 else 1.0\n        \n        # Compute the bonus based on variance\n        variance_bonus = variance_score ** 0.5  # Square root of variance\n        \n        # Total action score combining mean score, exploration bonus, and variance bonus\n        action_score = mean_score + (exploration_bonus if np.random.rand() < epsilon else 0) + variance_bonus\n        action_scores.append(action_score)\n\n    # Select the action with the highest computed score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 36008.51175806626,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize exploration rate with a value that diminishes over time\n    epsilon_initial = 1.0  # Start with full exploration\n    epsilon_min = 0.1      # Minimum exploration rate\n    decay_factor = min(current_time_slot / total_time_slots, 1.0)\n\n    # Calculate epsilon for exploration\n    epsilon = epsilon_initial * (1 - decay_factor) + epsilon_min * decay_factor\n\n    action_scores = []\n    \n    for action_index in range(8):\n        # Get historical scores for the action\n        scores = score_set.get(action_index, [])\n        average_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores) if scores else 1  # Prevent division by zero\n\n        # Calculating exploration bonus and action score\n        selection_bonus = 1 / selection_count  # Lower selection count increases bonus\n        adjusted_score = (1 - epsilon) * average_score + epsilon * selection_bonus\n\n        action_scores.append(adjusted_score)\n\n    # Select the action index with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 36161.150233397464,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    \n    # Calculate average scores and selection counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        average_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n        \n        # Calculate exploration bonus using UCB\n        exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else np.sqrt(2 * np.log(total_selection_count + 1))\n        \n        # Adjust exploration bonus for actions that have been selected less frequently\n        underutilization_bonus = max(0, (1.0 / (selection_count + 1)))\n        \n        # Composite score\n        action_scores[action_index] = average_score + exploration_bonus + underutilization_bonus\n\n    # Probabilistic selection based on composite scores via softmax\n    probabilities = np.exp(action_scores) / np.sum(np.exp(action_scores))\n    action_index = np.random.choice(num_actions, p=probabilities)\n\n    return action_index",
          "objective": 36665.906099610256,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters for epsilon\n    epsilon_min = 0.1\n    epsilon_max = 1.0\n    epsilon_decay_rate = 0.01\n\n    # Dynamic exploration factor (epsilon)\n    epsilon = max(epsilon_min, epsilon_max * np.exp(-epsilon_decay_rate * total_selection_count))\n\n    action_scores = []\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        average_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n\n        # Adjust epsilon based on the current time slot proportion\n        epsilon_adjusted = epsilon * (current_time_slot / total_time_slots)\n\n        # Exploration adjustment inversely proportional to selection count\n        exploration_adjustment = epsilon_adjusted / (selection_count + 1e-5)\n\n        # Compute total score\n        total_score = average_score + exploration_adjustment\n\n        # Apply normalization based on selection frequency\n        normalization_factor = 1 + (1 - (selection_count / (total_selection_count + 1e-5)))\n        action_scores.append(total_score * normalization_factor)\n\n    # Select the action index with the highest computed score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 36666.68895169225,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Define exploration parameters\n    min_epsilon = 0.01  # Minimum exploration probability\n    max_epsilon = 1.0   # Maximum exploration probability\n    decay_rate = 0.99   # Decay rate for exploration\n    \n    # Dynamic epsilon calculation based on selection count and time slot progress\n    exploration_fraction = current_time_slot / total_time_slots\n    epsilon = max(min_epsilon, max_epsilon * (decay_rate ** (total_selection_count / (total_time_slots + 1))))\n\n    action_scores = []\n    \n    total_actions = 8  # There are 8 actions indexed from 0 to 7\n\n    for action_index in range(total_actions):\n        scores = score_set.get(action_index, [])\n        mean_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n        \n        # Handling under-selection with a bonus\n        exploration_bonus = (1.0 / (selection_count + 1)) if selection_count > 0 else 2.0  # Encourage exploration\n        \n        # Compute the total action score with exploration component\n        if np.random.rand() < epsilon:\n            action_score = mean_score + exploration_bonus\n        else:\n            action_score = mean_score\n        \n        action_scores.append(action_score)\n\n    # Select the action with the highest computed score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 36704.9068371241,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Set parameters for exploration\n    min_epsilon = 0.05  # Minimum exploration probability\n    max_epsilon = 1.0   # Maximum exploration probability\n    decay_rate = 0.99   # Decay rate for exploration\n\n    # Calculate dynamic epsilon based on selections and time slots\n    exploration_fraction = current_time_slot / total_time_slots\n    epsilon = max(min_epsilon, max_epsilon * (decay_rate ** (total_selection_count / (total_time_slots + 1))))\n\n    action_scores = []\n    total_actions = 8  # Since action indices are from 0 to 7 \n\n    for action_index in range(total_actions):\n        scores = score_set.get(action_index, [])\n        mean_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n        explore_bonus = (1.0 / (selection_count + 1)) if selection_count > 0 else 1.0\n\n        # Combining mean score with bonuses\n        adjusted_score = mean_score + explore_bonus * epsilon\n\n        # Normalize score to be between 0 and 1\n        action_scores.append(adjusted_score)\n\n    # Select the action with the highest computed score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 37205.238033523805,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Exploration parameters\n    min_epsilon = 0.01  # Minimum exploration probability\n    max_epsilon = 1.0   # Maximum exploration probability\n    decay_factor = 0.9  # Decay factor for exploration\n\n    # Dynamic epsilon calculation based on time slot progress\n    epsilon = max(min_epsilon, max_epsilon * (decay_factor ** (total_selection_count / (total_time_slots + 1))))\n\n    action_scores = []\n    \n    total_actions = 8  # Total number of actions, indexed from 0 to 7\n\n    for action_index in range(total_actions):\n        scores = score_set.get(action_index, [])\n        mean_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n\n        # Handling under-selection with a bonus\n        exploration_bonus = 1.0 / (selection_count + 1) if selection_count > 0 else 2.0\n        \n        # Compute total action score with exploration component\n        if np.random.rand() < epsilon:\n            action_score = mean_score + exploration_bonus\n        else:\n            action_score = mean_score\n\n        action_scores.append(action_score)\n\n    # Select the action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 38107.698166125716,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Set parameters for exploration-exploitation balance\n    epsilon_initial = 1.0\n    epsilon_min = 0.1\n    decay_factor = min(current_time_slot / total_time_slots, 1.0)\n    \n    # Compute the exploration parameter epsilon\n    epsilon = epsilon_initial * (1 - decay_factor) + epsilon_min * decay_factor\n    \n    action_scores = []\n    \n    for action_index in range(8):\n        # Get historical scores for the action\n        scores = score_set.get(action_index, [])\n        average_score = np.mean(scores) if scores else 0.0  # Average score\n        \n        # Count of selections for handling exploration\n        selection_count = len(scores) if scores else 0\n        \n        # Compute exploration bonus\n        if selection_count == 0:\n            exploration_bonus = 1.0  # Encourage exploration of unselected actions\n        else:\n            exploration_bonus = (1 / (selection_count + 1e-5))\n        \n        # Adjusted score calculation\n        adjusted_score = (1 - epsilon) * average_score + epsilon * exploration_bonus\n        \n        action_scores.append(adjusted_score)\n\n    # Select the action index with the highest adjusted score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 38689.2700905515,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Exploration parameters\n    min_epsilon = 0.01  # Minimum exploration probability\n    initial_epsilon = 1.0  # Initial exploration probability\n    decay_rate = 0.95   # Decay rate for exploration over time\n    \n    # Compute epsilon based on total selection count and time slot progress\n    exploration_fraction = current_time_slot / total_time_slots\n    epsilon_decay = decay_rate ** exploration_fraction\n    epsilon = max(min_epsilon, initial_epsilon * epsilon_decay)\n\n    action_scores = []\n    total_actions = 8  # Number of available actions\n\n    for action_index in range(total_actions):\n        scores = score_set.get(action_index, [])\n        mean_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n        \n        # Exploration bonus to encourage selection of less explored actions\n        exploration_bonus = (1.0 / (selection_count + 1)) if selection_count > 0 else 2.0\n\n        # Combine mean score and exploration component\n        action_score = mean_score + (exploration_bonus if np.random.rand() < epsilon else 0)\n        action_scores.append(action_score)\n\n    # Select the action with the highest computed score\n    action_index = int(np.argmax(action_scores))\n    return action_index",
          "objective": 40443.10859039255,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    exploration_factor = 1.0  # Exploring underselected actions\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        average_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n\n        # Exploration bonus using UCB-style calculation\n        if selection_count > 0:\n            # UCB: Explore based on selection count\n            exploration_bonus = exploration_factor * np.sqrt((np.log(total_selection_count)) / selection_count)\n        else:\n            # New actions get a higher exploration score\n            exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count + 1))\n\n        # Composite score calculation\n        action_scores[action_index] = average_score + exploration_bonus\n\n    # Normalize scores to probabilities\n    max_score = np.max(action_scores)\n    probabilities = np.exp(action_scores - max_score)  # for numerical stability\n    probabilities /= np.sum(probabilities)\n\n    # Stochastic selection based on probabilities\n    action_index = np.random.choice(num_actions, p=probabilities)\n\n    return action_index",
          "objective": 41286.02728929056,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Constants for exploration\n    base_exploration_rate = 0.5  # Initial higher exploration\n    min_exploration_rate = 0.05    # Minimum exploration rate\n    decay_factor = total_time_slots / (total_time_slots + total_selection_count)  # Decay factor\n\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        average_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n\n        # Normalizing selection count to avoid division by zero\n        normalized_count = selection_count + 1e-5\n        \n        # Dynamic exploration rate based on current time slot \n        exploration_rate = max(min_exploration_rate, base_exploration_rate * decay_factor)\n        \n        # Boost actions with lower selection counts\n        selection_bonus = 1 / normalized_count\n        \n        # Calculate the final action score\n        action_score = average_score + exploration_rate * selection_bonus\n        \n        action_scores.append(action_score)\n\n    # Select action index with the highest action score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 41787.99867515457,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters for exploration\n    epsilon_initial = 1.0\n    epsilon_min = 0.1\n    decay_factor = min(current_time_slot / total_time_slots, 1.0)\n\n    # Compute the exploration parameter epsilon\n    epsilon = epsilon_initial * (1 - decay_factor) + epsilon_min * decay_factor\n\n    action_scores = []\n    \n    for action_index in range(8):\n        # Get historical scores for the action\n        scores = score_set.get(action_index, [])\n        average_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores) if scores else 1  # Prevent division by zero\n        \n        # Calculating exploration bonus and action score\n        exploration_bonus = (1 / (selection_count + 1e-5))  # Adding a small constant to avoid division by zero\n        adjusted_score = (1 - epsilon) * average_score + epsilon * exploration_bonus\n\n        action_scores.append(adjusted_score)\n\n    # Select the action index with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 42052.31180227113,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters for exploration-exploitation strategy\n    min_epsilon = 0.05  # Minimum exploration probability\n    initial_epsilon = 1.0  # Initial exploration probability\n    decay_rate = 0.999  # Decay rate for exploration\n    beta = 1.0  # Exploration weight\n\n    # Calculate exploration probability based on the current selection count and time slot\n    epsilon = max(min_epsilon, initial_epsilon * (decay_rate ** (current_time_slot + total_selection_count)))\n\n    # Initialize action scores array\n    action_scores = np.zeros(8)\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        mean_score = np.mean(scores) if scores else 0  # Handle empty scores\n        selection_count = len(scores)\n        \n        # Calculate exploration bonus: higher if selected less\n        exploration_bonus = beta / (selection_count + 1) if selection_count > 0 else beta\n        \n        # Store combined score\n        action_scores[action_index] = mean_score + exploration_bonus\n\n    # Epsilon-greedy selection strategy\n    if np.random.rand() < epsilon:\n        # Select an action randomly for exploration\n        action_index = np.random.randint(0, 8)\n    else:\n        # Select the action with the highest score\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": 42633.88779873077,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters for dynamic epsilon\n    min_epsilon = 0.05\n    max_epsilon = 1.0\n    epsilon_decay_factor = 0.1\n    \n    # Calculate dynamic epsilon based on the current time slot and total time slots\n    epsilon = max(min_epsilon, max_epsilon * (1 - (current_time_slot / total_time_slots)))\n    \n    total_actions = 8\n    action_scores = []\n    \n    for action_index in range(total_actions):\n        scores = score_set.get(action_index, [])\n        mean_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n        \n        # Prioritize exploration of less selected actions\n        exploration_bonus = (total_selection_count / (selection_count + 1)) if selection_count > 0 else total_selection_count + 1\n        \n        # Combined score with exploration balance\n        adjusted_score = mean_score + (epsilon * exploration_bonus)\n        action_scores.append(adjusted_score)\n\n    # Select action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 44019.788379785874,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Set parameters for exploration\n    min_epsilon = 0.1  # Minimum exploration probability\n    max_epsilon = 1.0  # Initial exploration probability\n    decay_rate = 0.99  # Decay rate for exploration\n\n    # Calculate adaptive epsilon\n    exploration_fraction = current_time_slot / total_time_slots\n    epsilon = max(min_epsilon, max_epsilon * (decay_rate ** total_selection_count))\n    \n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        mean_score = np.mean(scores) if scores else 0\n        selection_count = len(scores)\n        \n        # Calculate exploration bonus for less frequently selected actions\n        exploration_bonus = (1 / (selection_count + 1)) if selection_count > 0 else 1\n        \n        # Combine exploration and exploitation\n        if np.random.rand() < epsilon:\n            action_score = mean_score + exploration_bonus\n        else:\n            action_score = mean_score\n        \n        action_scores.append(action_score)\n\n    # Select the action with the highest computed score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 46542.56277214333,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Define epsilon parameters for controlling exploration\n    epsilon_initial = 1.0\n    epsilon_min = 0.1\n    decay_factor = min(current_time_slot / total_time_slots, 1.0)\n    \n    # Compute the exploration parameter epsilon\n    epsilon = epsilon_initial * (1 - decay_factor) + epsilon_min * decay_factor\n    \n    action_scores = []\n    \n    for action_index in range(8):\n        # Get historical scores for the action\n        scores = score_set.get(action_index, [])\n        average_score = np.mean(scores) if scores else 0.0  # Average score\n        \n        # Count of selections\n        selection_count = len(scores)\n        \n        # Compute exploration bonus\n        exploration_bonus = (1.0 / (selection_count + 1e-5)) if selection_count > 0 else 1.0\n        \n        # Calculate adjusted score combining exploration and exploitation\n        adjusted_score = (1 - epsilon) * average_score + epsilon * exploration_bonus\n        \n        action_scores.append(adjusted_score)\n\n    # Select the action index with the highest adjusted score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": 47106.96972598945,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Define epsilon parameters\n    min_epsilon = 0.1\n    max_epsilon = 1.0\n    epsilon_decay_rate = 0.99\n\n    # Calculate the dynamic epsilon based on total selection count and time slot\n    epsilon = max(min_epsilon, max_epsilon * (epsilon_decay_rate ** (total_selection_count / (total_time_slots + 1))))\n\n    action_count = 8  # Total actions from 0 to 7\n    action_scores = np.zeros(action_count)\n    \n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate mean score, handling the case where there are no selections\n        mean_score = np.mean(scores) if scores else 0.0\n\n        # Explore unseen actions with a safer exploration bonus\n        exploration_bonus = (1 / (selection_count + 1)) if selection_count > 0 else 1.0\n        \n        # Calculate adjusted score\n        adjusted_score = mean_score + (epsilon * exploration_bonus)\n        action_scores[action_index] = adjusted_score\n\n    # Normalize scores to handle stability, avoiding very high values from exploration bonuses\n    normalized_scores = (action_scores - np.min(action_scores)) / \\\n                        (np.max(action_scores) - np.min(action_scores)) if np.max(action_scores) > np.min(action_scores) else action_scores\n\n    # Select an action probabilistically based on normalized scores\n    action_probabilities = normalized_scores / np.sum(normalized_scores)\n    action_index = np.random.choice(np.arange(action_count), p=action_probabilities)\n\n    return action_index",
          "objective": 47457.56860680993,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon_min = 0.1\n    epsilon_max = 1.0\n    epsilon_decay_factor = 0.01\n    \n    # Calculate dynamic epsilon based on total selection count\n    epsilon = max(epsilon_min, epsilon_max * np.exp(-epsilon_decay_factor * total_selection_count))\n    \n    # Dynamic adjustment based on current time slot\n    exploration_factor = epsilon * (current_time_slot / total_time_slots)\n    \n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        average_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n\n        # Normalize selection count impact\n        if total_selection_count > 0:\n            exploration_bonus = exploration_factor / (selection_count + 1e-5)\n        else:\n            exploration_bonus = exploration_factor  # No selections yet, explore fully\n        \n        total_score = average_score + exploration_bonus\n        \n        action_scores.append(total_score)\n\n    # Random selection for exploration based on epsilon\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, 8)\n    else:\n        action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": 48191.27491320271,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        average_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n\n        # Exploration factor\n        if total_selection_count > 0:\n            exploration_factor = 1.0 / (selection_count + 1)\n        else:\n            exploration_factor = 1.0\n\n        # Epsilon for dynamic exploration\n        epsilon = (total_time_slots - current_time_slot) / total_time_slots\n        exploration_component = exploration_factor * epsilon\n\n        # Composite score\n        action_scores[action_index] = average_score + exploration_component\n\n    # Probabilistic selection based on composite scores\n    probabilities = np.exp(action_scores - np.max(action_scores))  # Stability in softmax\n    probabilities /= np.sum(probabilities)\n    \n    action_index = np.random.choice(num_actions, p=probabilities)\n\n    return action_index",
          "objective": 48370.30568398194,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Set parameters for exploration\n    min_epsilon = 0.01  # Minimum exploration probability\n    max_epsilon = 1.0   # Maximum exploration probability\n    decay_rate = 0.99   # Decay rate for exploration\n\n    # Adaptive epsilon calculation\n    exploration_fraction = current_time_slot / total_time_slots\n    epsilon = max(min_epsilon, max_epsilon * (decay_rate ** exploration_fraction))\n\n    action_scores = []\n    total_actions = 8  # Since action indices are from 0 to 7 \n\n    for action_index in range(total_actions):\n        scores = score_set.get(action_index, [])\n        mean_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n        exploration_bonus = (1.0 / (selection_count + 1)) if selection_count > 0 else 1.0\n\n        # No variance bonus for actions with no selection\n        variance_bonus = np.sqrt(np.var(scores)) if selection_count > 1 else 0.0\n        \n        # Total action score combining mean score and exploration/variance bonuses\n        if np.random.rand() < epsilon:\n            # Explore: Favor actions that have been selected less frequently\n            effective_exploration_score = exploration_bonus * (1.0 / (total_selection_count + 1))\n            action_score = mean_score + effective_exploration_score\n        else:\n            # Exploit: Favor actions with higher mean scores\n            action_score = mean_score + variance_bonus\n\n        action_scores.append(action_score)\n\n    # Select the action with the highest computed score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 49255.703545376746,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters for exploration and exploitation\n    exploration_min = 0.1  # Minimum exploration rate\n    exploration_max = 1.0  # Maximum exploration rate\n    decay_rate = 0.005  # Rate at which exploration decreases\n\n    # Calculate exploration factor based on total selection count\n    exploration_rate = max(exploration_min, exploration_max * np.exp(-decay_rate * total_selection_count))\n    \n    action_scores = []\n    num_actions = 8  # Fixed number of actions\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        average_score = np.mean(scores) if scores else 0.0\n        \n        # Selection count\n        selection_count = len(scores) if scores else 0\n        normalized_count = selection_count + 1e-5  # Avoid division by zero\n\n        # Adjusting exploration based on current time slot\n        exploration_adjustment = exploration_rate * (1 - (current_time_slot / total_time_slots))\n        \n        # Incorporate the exploration factor into action score\n        action_score = average_score + exploration_adjustment / normalized_count\n        \n        action_scores.append(action_score)\n\n    # Select action index with the highest action score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": 49390.99782219004,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon_min = 0.05\n    epsilon_max = 1.0\n    epsilon_decay_rate = 0.005\n\n    # Calculate dynamic exploration factor (epsilon)\n    epsilon = max(epsilon_min, epsilon_max * np.exp(-epsilon_decay_rate * total_selection_count))\n\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        average_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n        \n        # Calculate exploration adjustment based on selection count\n        exploration_adjustment = epsilon / (selection_count + 1e-5)\n        \n        # Calculate total score as a combination of average score and exploration factor\n        total_score = average_score + exploration_adjustment\n\n        # For normalization, we also take into account the overall number of actions selected\n        action_scores.append(total_score * (1 + (1 - (selection_count / (total_selection_count + 1e-5)))))\n\n    # Select the action index with the highest computed score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 53143.18258328765,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Set parameters for exploration\n    min_epsilon = 0.05  # Minimum exploration probability\n    max_epsilon = 1.0   # Initial exploration probability\n    decay_rate = 0.99   # Decay rate for exploration\n    \n    # Adaptive epsilon calculation\n    exploration_fraction = current_time_slot / total_time_slots\n    epsilon = max(min_epsilon, max_epsilon * (decay_rate ** (total_selection_count / (total_time_slots + 1))))\n\n    action_scores = []\n    total_actions = len(score_set)\n\n    for action_index in range(total_actions):\n        scores = score_set.get(action_index, [])\n        mean_score = np.mean(scores) if scores else 0\n        selection_count = len(scores)\n\n        # Exploration bonus for less frequently selected actions\n        if selection_count == 0:\n            exploration_bonus = 1.0  # Give a full bonus if never selected\n        else:\n            exploration_bonus = 1 / (selection_count ** 0.5)\n\n        # Compute action score for exploitation and exploration\n        action_score = mean_score + (exploration_bonus if np.random.rand() < epsilon else 0)\n        action_scores.append(action_score)\n\n    # Select the action with the highest computed score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 53961.96540698114,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_min = 0.1  # Minimum exploration rate\n    exploration_max = 1.0  # Maximum exploration rate\n    decay_rate = 0.01  # Rate at which exploration decreases\n\n    # Calculate current exploration factor\n    exploration_rate = max(exploration_min, exploration_max * np.exp(-decay_rate * total_selection_count))\n    \n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        average_score = np.mean(scores) if scores else 0.0\n        \n        selection_count = len(scores)\n        normalized_count = selection_count + 1e-5  # Avoid division by zero\n\n        # Adjusted exploration factor based on current time slot\n        exploration_adjustment = exploration_rate * (1 - (current_time_slot / total_time_slots))\n        \n        # Incorporate exploration into action score calculation\n        action_score = average_score + exploration_adjustment / normalized_count\n        \n        action_scores.append(action_score)\n\n    # Select action index with the highest action score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 55794.51337984255,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Set epsilon bounds and decay parameters\n    min_epsilon = 0.05\n    max_epsilon = 1.0\n    decay_rate = 0.995  # Decay rate for exploration\n\n    # Calculate the exploration probability (epsilon)\n    exploration_fraction = current_time_slot / total_time_slots\n    epsilon = max(min_epsilon, max_epsilon * (decay_rate ** total_selection_count))\n\n    action_scores = []\n\n    for action_index in range(8):  # There are 8 actions from 0 to 7\n        scores = score_set.get(action_index, [])\n        mean_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n        variance_score = np.var(scores) if scores else 0.0\n        \n        # Exploration bonus\n        exploration_bonus = (1.0 / (selection_count + 1)) if selection_count > 0 else 1.0\n        \n        # Variance-based factor\n        variance_bonus = np.sqrt(variance_score)\n        \n        # Total score calculation\n        if np.random.rand() < epsilon:\n            total_action_score = mean_score + exploration_bonus + variance_bonus\n        else:\n            total_action_score = mean_score\n        \n        action_scores.append(total_action_score)\n\n    # Select the action with the highest computed score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 56656.999426404414,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters for exploration-exploitation strategy\n    min_epsilon = 0.05  # Minimum exploration probability\n    initial_epsilon = 1.0  # Initial exploration probability\n    decay_factor = 0.95  # Decay factor for exploration based on total selections and time slots\n\n    # Calculate decay for exploration based on the fraction of time slots completed\n    epsilon_decay = decay_factor ** (current_time_slot / total_time_slots)\n    epsilon = max(min_epsilon, initial_epsilon * epsilon_decay)\n\n    # Initialize action scores array\n    action_scores = np.zeros(8)\n    selection_counts = np.zeros(8)\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)  # Count of selections for the action\n        \n        if selection_counts[action_index] > 0:\n            mean_score = np.mean(scores)  # Calculate the mean score for the action\n            action_scores[action_index] = mean_score\n        else:\n            action_scores[action_index] = 0  # Unselected actions get a score of 0\n\n    # Add exploration bonus inversely based on selection counts\n    exploration_bonus = np.where(selection_counts > 0, 1 / (selection_counts + 1), 1.0)\n    action_scores += exploration_bonus\n\n    # Normalize scores to a probability distribution\n    probabilities = action_scores / np.sum(action_scores)\n\n    # Epsilon-greedy selection from this probability distribution\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(np.arange(8), p=probabilities)\n    else:\n        # Select the action with the highest adjusted score\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": 56812.61715001684,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Dynamic exploration factor\n    exploration_min = 0.1\n    exploration_max = 1.0\n    decay_rate = 0.01\n    exploration_rate = max(exploration_min, exploration_max * np.exp(-decay_rate * total_selection_count))\n    \n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        average_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n\n        # Avoid division by zero\n        normalized_count = selection_count + 1e-5\n        exploration_adjustment = exploration_rate * (1 - (current_time_slot / total_time_slots))\n        \n        # Calculate the weighted action score\n        action_score = (average_score + exploration_adjustment / normalized_count) * (1 + (1 - (selection_count / total_selection_count + 1e-5)))\n\n        action_scores.append(action_score)\n\n    # Select the action index with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 57836.443983960904,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Constants for exploration parameters\n    min_exploration_factor = 0.1\n    max_exploration_factor = 1.0\n    exploration_decay_rate = 1.0\n    \n    # Calculate exploration probability based on the time slot progression\n    exploration_scale = (max_exploration_factor - min_exploration_factor) * (current_time_slot / total_time_slots) + min_exploration_factor\n    \n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        average_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n        \n        # Calculate exploration adjustment based on how many times the action has been selected\n        exploration_adjustment = exploration_scale / (selection_count + 1)\n        \n        # Calculate total estimated score for each action\n        total_score = average_score + exploration_adjustment\n        \n        # Normalize the score further if this action has been selected fewer times\n        if total_selection_count > 0:\n            selection_weight = (1 + (1 - (selection_count / total_selection_count)))  # Encourages selection of less chosen actions\n            total_score *= selection_weight\n        \n        action_scores.append(total_score)\n\n    # Select the action index with the highest computed score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 61104.39854413271,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Define parameters for exploration-exploitation balancing\n    min_epsilon = 0.05  # Minimum exploration probability\n    max_epsilon = 0.95   # Maximum exploration probability to ensure sufficient exploration\n    decay_rate = 0.98    # Decay rate of exploration probability\n\n    # Calculate epsilon based on time slots and selection count\n    exploration_fraction = current_time_slot / total_time_slots\n    epsilon = max(min_epsilon, max_epsilon * (decay_rate ** (total_selection_count / max(total_selection_count, 1))))\n\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        mean_score = np.mean(scores) if scores else 0\n        selection_count = len(scores)\n        \n        # Avoid division by zero and compute exploration bonus\n        exploration_bonus = (1 / (selection_count + 1)) * np.log(1 + total_selection_count) if selection_count > 0 else 1\n        \n        # Use randomized decision to balance exploration and exploitation\n        if np.random.rand() < epsilon:\n            action_score = mean_score + exploration_bonus\n        else:\n            action_score = mean_score\n        \n        action_scores.append(action_score)\n\n    # Select the action with the highest computed score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 62876.54810330072,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters for exploration-exploitation strategy\n    min_epsilon = 0.1  # Minimum exploration probability\n    initial_epsilon = 1.0  # Initial exploration probability\n    decay_rate = 0.99  # Decay rate for exploration\n\n    # Calculate exploration probability based on the current time slot\n    epsilon = max(min_epsilon, initial_epsilon * (decay_rate ** total_selection_count))\n\n    # Initialize action scores array\n    action_scores = np.zeros(8)\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        if scores:\n            mean_score = np.mean(scores)  # Calculate the mean score for the action\n            selection_count = len(scores)  # Count of selections for the action\n            \n            # Calculate exploration bonus\n            exploration_bonus = 1 / (selection_count + 1)  # Encourage selection of less frequently chosen actions\n            \n            # Combine mean score with exploration bonus for the score\n            action_scores[action_index] = mean_score + exploration_bonus\n        else:\n            # If no scores exist, give a high initial score for exploration\n            action_scores[action_index] = 1.0  # Max for untested actions\n\n    # Apply epsilon-greedy strategy to balance exploration and exploitation\n    if np.random.rand() < epsilon:\n        # Introduce randomness by selecting an action uniformly at random\n        action_index = np.random.randint(0, 8)\n    else:\n        # Select the action with the highest computed score\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": 63966.18570807584,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Define constants\n    min_epsilon = 0.1\n    max_epsilon = 1.0\n    initial_explore_prob = 0.2\n    \n    # Calculate dynamic epsilon based on the time slot ratio to encourage exploration earlier\n    epsilon = max(min_epsilon, max_epsilon * (1 - (current_time_slot / total_time_slots)))\n    \n    action_count = 8  # Total actions from 0 to 7\n    action_scores = np.zeros(action_count)\n    \n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Handle mean calculation\n        mean_score = np.mean(scores) if scores else 0.0\n        \n        # Exploration adjustment based on selection count\n        exploration_bonus = (initial_explore_prob / (selection_count + 1)) if selection_count > 0 else initial_explore_prob\n        \n        # Calculate adjusted score with exploration consideration\n        adjusted_score = mean_score + (epsilon * exploration_bonus)\n        action_scores[action_index] = adjusted_score\n\n    # Normalize scores\n    score_range = np.max(action_scores) - np.min(action_scores)\n    normalized_scores = (action_scores - np.min(action_scores)) / score_range if score_range > 0 else action_scores\n    \n    # Define selection probabilities with a consistent baseline for exploration\n    exploration_probabilities = np.full(action_count, epsilon / action_count)\n    selection_probabilities = normalized_scores * (1 - epsilon)  # Exploitation part\n    \n    action_probabilities = exploration_probabilities + selection_probabilities\n    action_probabilities /= np.sum(action_probabilities)  # Normalize to sum to 1\n    \n    # Select an action probabilistically\n    action_index = np.random.choice(np.arange(action_count), p=action_probabilities)\n\n    return action_index",
          "objective": 64905.026825163055,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    epsilon_min = 0.1  # Minimum exploration rate\n    epsilon_max = 1.0  # Maximum exploration rate\n    decay_rate = 0.01  # Rate at which exploration decreases\n\n    # Calculate current exploration factor\n    epsilon = max(epsilon_min, epsilon_max * np.exp(-decay_rate * total_selection_count))\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        average_score = np.mean(scores) if scores else 0.0\n        \n        # Number of times the action has been selected\n        selection_count = len(scores)\n        normalized_selection_count = selection_count + 1e-5  # Avoid division by zero\n        \n        # Dynamic adjustment based on current time slot\n        exploration_factor = epsilon * (1 - (current_time_slot / total_time_slots))\n        \n        # Adjusted score incorporating exploration\n        action_score = average_score + exploration_factor * (1 / normalized_selection_count)\n\n        action_scores.append(action_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 69850.04766965519,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores and their selection counts\n    action_means = []\n    action_counts = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        mean_score = np.mean(scores) if scores else 0.0\n        action_means.append(mean_score)\n        action_counts.append(len(scores))\n    \n    # Calculate epsilon for exploration based on total selection count and current time slot\n    decay_factor = 0.1  # Controls the rate of decay for exploration\n    epsilon = max(0.01, 1.0 - (total_selection_count / (total_time_slots * decay_factor)))\n    \n    # Calculate selection scores\n    action_scores = []\n    \n    for action_index in range(8):\n        if action_counts[action_index] == 0:\n            exploration_bonus = 1.0  # Give new actions high exploration score\n        else:\n            exploration_bonus = np.sqrt(np.log(total_selection_count) / action_counts[action_index])\n        \n        action_score = (1 - epsilon) * action_means[action_index] + (epsilon * exploration_bonus)\n        action_scores.append(action_score)\n    \n    # Select the action with the highest calculated score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 70484.08954457383,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Adaptive exploration factor based on the current time slot\n    exploration_factor = min(1.0, 2 * (current_time_slot / total_time_slots))\n    \n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        mean_score = np.mean(scores) if scores else 0  # Default score for untried actions\n        \n        selection_count = len(scores) + 1e-6  # Add a small number to avoid division by zero\n        \n        # Upper Confidence Bound strategy\n        exploration_term = np.sqrt(np.log(total_selection_count) / selection_count)\n        action_score = mean_score + exploration_factor * exploration_term\n        \n        action_scores.append(action_score)\n    \n    # Select the action with the highest calculated score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 71711.76373115304,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters for balancing exploration and exploitation\n    min_exploration_prob = 0.1  # Minimum exploration probability\n    max_exploration_prob = 1.0   # Initial exploration probability\n    exploration_decay = 0.9       # Decay factor for exploration probability\n\n    # Calculate exploration probability\n    exploration_fraction = current_time_slot / total_time_slots\n    exploration_prob = max(min_exploration_prob, max_exploration_prob * (exploration_decay ** (total_selection_count / total_time_slots)))\n\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        mean_score = np.mean(scores) if scores else 0\n        selection_count = len(scores)\n\n        # Explore less frequently chosen actions by adding a bonus based on selection count\n        exploration_bonus = (1 - exploration_fraction) * np.sqrt(np.log(total_time_slots + 1) / (selection_count + 1)) if selection_count > 0 else 1.0\n\n        # Exploration adjustment\n        final_score = mean_score + (exploration_bonus if np.random.rand() < exploration_prob else 0)\n        action_scores.append(final_score)\n\n    # Select the action with the highest calculated score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 76819.50543719497,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters for exploration-exploitation strategy\n    min_epsilon = 0.05  # Minimum exploration probability\n    initial_epsilon = 1.0  # Initial exploration probability\n    decay_rate = 0.95  # Decay rate for exploration\n    \n    # Calculate exploration probability based on the current time slot\n    epsilon = max(min_epsilon, initial_epsilon * (decay_rate ** total_selection_count))\n    \n    # Initialize action scores array\n    action_scores = np.zeros(8)\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate mean score\n        mean_score = np.mean(scores) if scores else 0  # Default to 0 for untested actions\n        \n        # Calculate exploration bonus\n        exploration_bonus = (0.5 / (selection_count + 1)) if selection_count > 0 else 0.5  # Encourage exploration\n        \n        # Combine mean score with exploration bonus\n        action_scores[action_index] = mean_score + exploration_bonus\n\n    # Apply epsilon-greedy strategy to balance exploration and exploitation\n    if np.random.rand() < epsilon:\n        # Random action selection for exploration\n        action_index = np.random.randint(0, 8)\n    else:\n        # Select the action with the highest computed score\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": 77574.39333131463,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters\n    min_epsilon = 0.05  # Minimum exploration rate\n    max_epsilon = 1.0  # Maximum exploration rate\n    decay_rate = 0.9  # Rate at which exploration decreases over time\n    \n    # Calculate dynamic epsilon based on current time slot\n    exploration_prob = max(min_epsilon, max_epsilon * (1 - (current_time_slot / total_time_slots) ** decay_rate))\n\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        mean_score = np.mean(scores) if scores else 0  # Handle untried actions\n        selection_count = len(scores)\n        \n        # Calculate exploration term with a more refined approach\n        if selection_count > 0:\n            exploration_term = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1))\n        else:\n            exploration_term = np.sqrt(np.log(total_time_slots + 1))  # Encourage exploration for untried actions\n\n        # Combine exploration and exploitation\n        action_score = mean_score + exploration_prob * exploration_term\n        action_scores.append(action_score)\n\n    # Select the action with the highest calculated score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 78639.7291688395,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Configuration for exploration\n    min_epsilon = 0.05  # Minimum exploration probability\n    initial_epsilon = 1.0  # Initial exploration probability\n    decay_rate = 0.95  # Decay rate for exploration\n\n    # Epsilon calculation based on total selection count\n    epsilon = max(min_epsilon, initial_epsilon * (decay_rate ** total_selection_count))\n\n    action_scores = []\n    total_actions = 8  # There are 8 actions indexed from 0 to 7\n\n    for action_index in range(total_actions):\n        scores = score_set.get(action_index, [])\n        mean_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n\n        # Exploration bonus: increase exploration for underselected actions\n        exploration_bonus = 1.0 / (selection_count + 1) if selection_count > 0 else 1.0\n        \n        # Combine mean score and exploration\n        action_scores.append(mean_score + exploration_bonus)\n\n    # Apply softmax to balance exploration and exploitation\n    action_probs = np.exp(action_scores) / np.sum(np.exp(action_scores))\n\n    # Explore with probability epsilon; otherwise, choose based on softmax probabilities\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(total_actions)\n    else:\n        action_index = np.random.choice(total_actions, p=action_probs)\n    \n    return action_index",
          "objective": 79120.75011232652,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_weight = 2.0  # Exploration weight\n    exploitation_weight = 3.0  # Exploitation weight\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        mean_score = np.mean(scores) if scores else 0\n        \n        selection_count = len(scores)\n\n        # Dynamic adjustment of exploration and exploitation weights based on time slot\n        exploration_factor = (1 - (current_time_slot / total_time_slots))\n        exploitation_factor = current_time_slot / total_time_slots\n        \n        # Calculate adjusted mean score\n        adjusted_mean_score = mean_score if total_selection_count > 0 else 0\n        \n        # Explore less-selected actions\n        if selection_count > 0:\n            exploration_term = np.sqrt(np.log(total_selection_count) / selection_count)\n        else:\n            exploration_term = np.sqrt(np.log(total_time_slots) + 1)  # Encourage new actions\n\n        # Combine mean score and exploration term for action score\n        action_score = (exploitation_weight * adjusted_mean_score) + \\\n                       (exploration_weight * exploration_factor * exploration_term)\n        \n        action_scores.append(action_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 81578.255593238,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Define exploration parameters\n    exploration_initial = 1.0  # Starting exploration factor\n    exploration_min = 0.1      # Minimum exploration factor\n    decay_rate = 0.001         # Rate of exploration decay\n\n    # Calculate the current exploration rate\n    exploration_rate = max(exploration_min, \n                           exploration_initial * np.exp(-decay_rate * total_selection_count))\n    \n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        average_score = np.mean(scores) if scores else 0.0\n        \n        # Normalize the exploration factor based on how many times the action has been selected\n        selection_count = len(scores)\n        normalized_count = selection_count + 1e-5  # Small constant to avoid division by zero\n        \n        # Calculate the exploration adjustment based on current time\n        adjusted_exploration = exploration_rate / (normalized_count + 1e-5)\n\n        # Combine average score with exploration adjustment\n        action_score = average_score + adjusted_exploration\n        \n        action_scores.append(action_score)\n\n    # Implement epsilon-greedy strategy for action selection\n    if np.random.rand() < exploration_rate:  # Exploration phase\n        action_index = np.random.choice(range(8))  # Random selection among actions\n    else:  # Exploitation phase\n        action_index = np.argmax(action_scores)  # Select the action with the highest score\n\n    return action_index",
          "objective": 83648.27780666391,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_weight = 1.0  # Initial exploration weight\n    exploitation_weight = 1.0  # Initial exploitation weight\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        mean_score = np.mean(scores) if scores else 0\n        \n        selection_count = len(scores)\n        \n        # Normalizing mean score based on total selections\n        normalized_mean_score = mean_score * (selection_count / total_selection_count) if total_selection_count > 0 else mean_score\n        \n        # Dynamic adjustment of exploration and exploitation weights\n        exploration_factor = (current_time_slot / total_time_slots) ** 0.5  # Encourage exploration in earlier time slots\n        exploitation_factor = 1 - exploration_factor  # Reduce exploitation in earlier slots\n\n        # Calculate action score combining normalized mean score and exploration component\n        if selection_count > 0:\n            exploration_term = np.sqrt(np.log(total_selection_count) / selection_count)\n            action_score = (exploitation_weight * normalized_mean_score) + (exploration_weight * exploration_factor * exploration_term)\n        else:\n            action_score = exploration_weight * exploration_factor * np.sqrt(np.log(total_time_slots) + 1)\n\n        action_scores.append(action_score)\n    \n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 84104.34636498595,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    min_epsilon = 0.1  # Minimum exploration probability\n    initial_epsilon = 1.0  # Initial exploration probability\n    epsilon_decay = 0.99  # Decay rate for exploration probability\n    \n    # Calculate average scores for each action\n    average_scores = np.array([\n        np.mean(scores) if scores else 0 for scores in score_set.values()\n    ])\n    \n    # Count of selections for each action\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Maximum average score for immediate exploitation\n    best_action_index = np.argmax(average_scores)\n    \n    # Dynamic epsilon using total_selection_count\n    epsilon = max(min_epsilon, initial_epsilon * (epsilon_decay ** current_time_slot))\n    \n    # Choose action: exploration or exploitation\n    if np.random.rand() < epsilon:\n        # Explore: prefer actions with fewer selections\n        probabilities = (1 / (selection_counts + 1e-5))  # Adding a small value to avoid division by zero\n        probabilities /= np.sum(probabilities)  # Normalize to form a probability distribution\n        action_index = np.random.choice(range(8), p=probabilities)\n    else:\n        # Exploit: select the best action based on average scores\n        action_index = best_action_index\n\n    return action_index",
          "objective": 85131.86895052898,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize parameters\n    num_actions = 8\n    min_epsilon = 0.1  # Minimum exploration rate\n    \n    # Calculate the average score for each action\n    action_means = np.array([np.mean(score_set.get(i, [])) if score_set.get(i) else 0 for i in range(num_actions)])\n    \n    # Adjustment for selection counts\n    action_counts = np.array([len(score_set.get(i, [])) for i in range(num_actions)])\n    \n    # Compute epsilon based on total selection count and current time slot\n    epsilon = max(min_epsilon, 1.0 - (current_time_slot / total_time_slots) * (total_selection_count / (total_selection_count + 1)))\n    \n    # Select an action based on exploration/exploitation strategy\n    if np.random.rand() < epsilon:  # Exploration\n        # Explore by choosing with uniform probability\n        action_index = np.random.choice(num_actions)\n    else:  # Exploitation\n        # Exploit the best-known action\n        adjusted_scores = action_means + np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))  # Added small constant for stability\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 91700.58729957593,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_factor = 0.5  # Balancing factor for exploration vs exploitation\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        mean_score = np.mean(scores) if scores else 0  # Handle cases with no previous scores\n        selection_count = len(scores)\n        \n        # Using UCB (Upper Confidence Bound) strategy for balancing exploration and exploitation\n        if selection_count > 0:\n            exploration_term = np.sqrt(np.log(total_selection_count) / selection_count)\n            action_score = mean_score + exploration_factor * exploration_term\n        else:\n            # Completely untried actions get a base score (high exploration)\n            action_score = exploration_factor * np.sqrt(np.log(total_time_slots) + 1)\n        \n        action_scores.append(action_score)\n    \n    # Select the action with the highest calculated score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 92054.52171556842,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Define exploration-exploitation parameters\n    min_epsilon = 0.05  # Minimum exploration probability\n    max_epsilon = 1.0   # Initial exploration probability\n    decay_rate = 0.98   # Decay rate for exploration\n\n    # Calculate adaptive epsilon based on the current time slot\n    exploration_fraction = current_time_slot / total_time_slots\n    epsilon = max(min_epsilon, max_epsilon * (decay_rate ** (total_selection_count / max(total_selection_count, 1))))\n    \n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        mean_score = np.mean(scores) if scores else 0\n        selection_count = len(scores)\n        \n        # Normalized exploration bonus for less frequently selected actions\n        exploration_bonus = (1 / (selection_count + 1)) * np.log(1 + total_selection_count) if selection_count > 0 else 1\n        \n        # Compute combined score with exploration and exploitation\n        if np.random.rand() < epsilon:\n            action_score = mean_score + exploration_bonus\n        else:\n            action_score = mean_score\n        \n        action_scores.append(action_score)\n\n    # Select the action with the highest computed score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 95540.29623159388,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize variables\n    action_scores = []\n    n_actions = 8\n    min_epsilon = 0.1  # Minimum exploration rate\n    \n    # Calculate average scores and counts for each action\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        mean_score = np.mean(scores) if scores else 0\n        selection_count = len(scores)\n\n        # Epsilon adjustment based on selection count and time slot\n        epsilon = max(min_epsilon, 1 - (total_selection_count / (total_selection_count + 1)))\n        \n        # If action has never been selected, we encourage exploration\n        if selection_count == 0:\n            action_scores.append(1)  # High exploratory score for unselected actions\n            continue\n\n        # Calculate action score\n        exploration_term = np.sqrt(np.log(total_selection_count + 1) / selection_count)\n        action_score = (1 - epsilon) * mean_score + epsilon * exploration_term\n        action_scores.append(action_score)\n    \n    # Select action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 95652.8065230847,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8  # Total actions available\n    min_epsilon = 0.01\n    max_epsilon = 0.5\n    epsilon_decay = 0.99  # Decay factor for epsilon\n\n    # Compute average scores and selection counts\n    avg_scores = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        avg_scores[action_index] = np.mean(scores) if scores else 0.0\n\n    # Dynamic epsilon calculation\n    epsilon = max(min_epsilon, max_epsilon * (epsilon_decay ** (total_selection_count / action_count)))\n    \n    if np.random.rand() < epsilon:\n        # Exploration: Select action based on softmax probabilities of average scores\n        scaling_factors = np.exp(avg_scores - np.max(avg_scores))  # Subtracting max for numerical stability\n        probabilities = scaling_factors / np.sum(scaling_factors)\n        action_index = np.random.choice(action_count, p=probabilities)\n    else:\n        # Exploitation: Select the action with the highest average score\n        action_index = np.argmax(avg_scores)\n\n    return action_index",
          "objective": 95828.88659936527,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    min_epsilon = 0.1  # Minimum exploration probability\n    initial_epsilon = 1.0  # Start with full exploration\n    decay_rate = 0.99  # Rate at which epsilon decreases\n    \n    # Calculate epsilon based on total selections\n    epsilon = max(min_epsilon, initial_epsilon * (decay_rate ** current_time_slot))\n    \n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        mean_score = np.mean(scores) if scores else 0\n        selection_count = len(scores)\n        \n        # Calculate the action's score\n        if selection_count > 0:\n            action_score = mean_score\n        else:\n            # Use a high base score for untried actions to encourage exploration\n            action_score = 1.0\n        \n        # Store scores for final decision making\n        action_scores.append((action_score, selection_count))\n    \n    # Epsilon-Greedy choice\n    if np.random.rand() < epsilon:\n        # Explore: select a random action\n        action_index = np.random.choice(8)\n    else:\n        # Exploit: select the action with the highest score\n        action_index = np.argmax([score[0] for score in action_scores])\n    \n    return action_index",
          "objective": 99840.6741107924,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters\n    min_epsilon = 0.1  # Minimum exploration rate\n    max_epsilon = 1.0  # Maximum exploration rate\n    decay_rate = 0.9  # Rate at which exploration decreases over time\n\n    # Calculate dynamic epsilon based on current time slot\n    exploration_prob = max(min_epsilon, max_epsilon * (1 - (current_time_slot / total_time_slots) ** decay_rate))\n\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        mean_score = np.mean(scores) if scores else 0  # Handle untried actions\n        selection_count = len(scores)\n        \n        # Calculate exploration score\n        if selection_count > 0:\n            exploration_term = np.sqrt(np.log(total_selection_count) / selection_count)\n            action_score = mean_score + exploration_prob * exploration_term\n        else:\n            # Completely untried actions get a high exploratory score\n            action_score = exploration_prob * np.sqrt(np.log(total_time_slots) + 1)\n\n        action_scores.append(action_score)\n\n    # Select the action with the highest calculated score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 103765.41109723302,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Compute mean scores for all actions, handling cases with no historical data\n    action_scores = []\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        mean_score = np.mean(scores) if scores else 0\n        \n        # Calculate selection count with a small offset to prevent division by zero\n        selection_count = len(scores) + 1e-6\n        \n        # Update exploration factor based on time slot and total selections\n        # More weight on exploitation as time progresses\n        exploration_factor = 1.0 / (1 + np.sqrt(total_selection_count))\n        \n        # Compute the exploration term for UCB\n        exploration_term = np.sqrt(np.log(total_selection_count + 1) / selection_count)\n        \n        # Combine mean score with exploration term\n        action_score = mean_score + exploration_factor * exploration_term\n        \n        action_scores.append(action_score)\n    \n    # Use softmax for probability distribution of actions to maintain diversity\n    action_probabilities = np.exp(action_scores) / np.sum(np.exp(action_scores))\n    \n    # Sample an action based on calculated probabilities\n    action_index = np.random.choice(range(8), p=action_probabilities)\n    \n    return action_index",
          "objective": 112387.31221010367,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8  # Total actions available\n    min_epsilon = 0.01\n    max_epsilon = 0.5\n    epsilon_decay = 0.99  # Decay factor for epsilon\n\n    # Initialize arrays for average scores and selection counts\n    avg_scores = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n\n    # Calculate average scores and the number of selections for each action\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        avg_scores[action_index] = np.mean(scores) if scores else 0.0\n\n    # Dynamic epsilon calculation based on total selection count\n    epsilon = max(min_epsilon, max_epsilon * (epsilon_decay ** (total_selection_count / action_count)))\n\n    if np.random.rand() < epsilon:\n        # Exploration: Select action using softmax weighted by average scores\n        scaled_scores = np.exp(avg_scores - np.max(avg_scores))  # Stability adjustment\n        probabilities = scaled_scores / np.sum(scaled_scores) if np.sum(scaled_scores) > 0 else np.ones(action_count) / action_count\n        action_index = np.random.choice(action_count, p=probabilities)\n    else:\n        # Exploitation: Select the action with the highest average score\n        action_index = np.argmax(avg_scores)\n\n    return action_index",
          "objective": 113309.85310553684,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters for exploration\n    min_epsilon = 0.05  # Minimum exploration probability\n    max_epsilon = 1.0   # Initial exploration probability\n    decay_rate = 0.95   # Rate at which epsilon decays\n    \n    # Calculate epsilon based on the total selections and time slots\n    exploration_fraction = current_time_slot / total_time_slots\n    epsilon = max(min_epsilon, max_epsilon * (decay_rate ** (total_selection_count / total_time_slots)))\n\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        mean_score = np.mean(scores) if scores else 0\n        selection_count = len(scores)\n\n        # Exploration adjustment\n        exploration_bonus = (1 - exploration_fraction) * np.sqrt(np.log(total_time_slots + 1) / (selection_count + 1)) if selection_count > 0 else 1.0\n        exploration_part = exploration_bonus if np.random.rand() < epsilon else 0\n\n        # Calculate the final action score\n        action_score = mean_score + exploration_part\n        action_scores.append(action_score)\n\n    # Select the action with the highest calculated score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 114787.50739853432,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    base_epsilon = 0.5\n    min_epsilon = 0.01\n    exploration_decay = 0.99  # Decay factor for epsilon\n    action_count = 8  # There are a total of 8 actions (0-7)\n    \n    avg_scores = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        avg_scores[action_index] = np.mean(scores) if scores else 0.0\n    \n    # Dynamic epsilon calculation\n    epsilon = max(min_epsilon, base_epsilon * (exploration_decay ** (total_selection_count / action_count)))\n    \n    if np.random.rand() < epsilon:\n        # Explore with softmax selection\n        scaling_factors = np.exp(avg_scores)  # Exponentiate scores for softmax\n        probabilities = scaling_factors / scaling_factors.sum()\n        action_index = np.random.choice(action_count, p=probabilities)\n    else:\n        # Exploit by selecting the best average score\n        action_index = np.argmax(avg_scores)\n    \n    return action_index",
          "objective": 118658.57565836144,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Define the minimum epsilon value for exploration\n    min_epsilon = 0.1\n    # Set the maximum epsilon value for exploration\n    max_epsilon = 1.0\n    # Calculate the decayed epsilon based on total selections\n    epsilon = max(min_epsilon, max_epsilon * (1 - (total_selection_count / total_time_slots)))\n\n    action_scores = []\n    \n    for action_index in range(8):\n        # Fetch historical scores for the action; default to empty list if none\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate mean score or set to zero if no scores are available\n        mean_score = np.mean(scores) if scores else 0.0\n        \n        # Normalization factor for mean score based on total selections\n        normalization_factor = (selection_count / total_selection_count) if total_selection_count > 0 else 1.0\n        normalized_mean_score = mean_score * normalization_factor\n        \n        # Calculate exploration term (bonus for being less selected)\n        exploration_term = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else np.sqrt(np.log(total_time_slots + 1) + 1)\n        \n        # Compute the final action score\n        action_score = normalized_mean_score + (1 - epsilon) * exploration_term\n        \n        action_scores.append(action_score)\n    \n    # Epsilon-greedy selection: choose randomly with probability epsilon, otherwise choose the best score\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, 8)  # Randomly select an action\n    else:\n        action_index = np.argmax(action_scores)  # Select the action with the highest score\n    \n    return action_index",
          "objective": 125431.48314919873,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Define parameters for exploration\n    min_epsilon = 0.1  # Minimum value for epsilon\n    max_epsilon = 1.0  # Initial exploration probability\n    decay_rate = 0.9   # Rate at which epsilon decays\n\n    # Calculate epsilon based on total selections and time slots\n    exploration_fraction = current_time_slot / total_time_slots\n    epsilon = max(min_epsilon, max_epsilon * (decay_rate ** (total_selection_count / (total_time_slots * 0.1))))\n\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        mean_score = np.mean(scores) if scores else 0\n        \n        selection_count = len(scores)\n\n        # Adjust action scores for exploiting known data and exploring less known actions\n        if np.random.rand() < epsilon:\n            # Exploration: Assign a high score to untried actions\n            exploration_bonus = exploration_fraction * np.sqrt(np.log(total_time_slots + 1)) \n            action_score = mean_score + exploration_bonus if selection_count == 0 else mean_score\n        else:\n            # Exploitation: Use the mean score directly\n            action_score = mean_score\n\n        action_scores.append(action_score)\n\n    # Select the action with the highest calculated score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 128415.12277542162,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and update selection counts\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            average_scores[i] = np.mean(scores)\n\n    # Dynamic exploration factor based on time slot\n    exploration_factor = np.maximum(0.1, 1.0 - (current_time_slot / total_time_slots))\n    \n    # Calculate UCB values to balance exploration and exploitation\n    uncertainty = np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n    ucb_values = average_scores + exploration_factor * uncertainty\n    \n    # Select action based on the highest UCB value\n    action_index = action_indices[np.argmax(ucb_values)]\n    \n    return action_index",
          "objective": 133191.39709223222,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters for exploration-exploitation strategy\n    min_epsilon = 0.05  # Minimum exploration probability\n    initial_epsilon = 1.0  # Initial exploration probability\n    decay_rate = 0.99  # Decay rate for exploration based on total selection count\n    \n    # Calculate exploration probability based on the current time slot\n    epsilon = max(min_epsilon, initial_epsilon * (decay_rate ** total_selection_count))\n    \n    action_scores = np.zeros(8)\n    action_variances = np.zeros(8)\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate mean score\n        mean_score = np.mean(scores) if scores else 0.0\n        \n        # Calculate variance if there are multiple selections\n        if selection_count > 1:\n            action_variance = np.var(scores)\n        else:\n            action_variance = 0.0  # No variance for never-selected or single-selection actions\n\n        # Exploration bonus based on variance\n        exploration_bonus = 0.5 / (selection_count + 1) if selection_count > 0 else 0.5\n        \n        # Combine mean score with exploration bonus and variance\n        action_scores[action_index] = mean_score + exploration_bonus + action_variance * 0.1\n\n    # Apply epsilon-greedy strategy to balance exploration and exploitation\n    if np.random.rand() < epsilon:\n        # Random action selection for exploration\n        action_index = np.random.randint(0, 8)\n    else:\n        # Select the action with the highest computed score\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": 134003.65599092856,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Dynamically adjusted exploration factor\n    exploration_factor = 1 + (current_time_slot / total_time_slots)\n    \n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        \n        # Calculate mean score and selection count\n        mean_score = np.mean(scores) if scores else 0\n        selection_count = len(scores) + 1e-6  # To avoid division by zero\n        \n        # Exploration term using Thompson Sampling approach\n        if total_selection_count > 0:\n            exploration_term = np.sqrt((2 * np.log(total_selection_count)) / selection_count)\n        else:\n            exploration_term = 0\n        \n        action_score = mean_score + exploration_factor * exploration_term\n        action_scores.append(action_score)\n    \n    # Select the action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 136237.49405581848,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8  # There are a total of 8 actions (0-7)\n\n    # Initialize arrays for average scores and selection counts\n    avg_scores = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n\n    # Calculate average scores and selection counts for each action\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        avg_scores[action_index] = np.mean(scores) if scores else 0.0\n\n    # Normalization factor for the exploration strategy\n    normalized_time = current_time_slot / total_time_slots if total_time_slots > 0 else 1\n    exploration_bonus = 1.0 - normalized_time  # Decrease exploration as time progresses\n\n    # Adjust the average scores with exploration bonus\n    adjusted_scores = avg_scores + (exploration_bonus * (1.0 / (selection_counts + 1e-5)))  # Avoid division by zero\n\n    # Select action based on adjusted scores\n    action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 152220.06594113482,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n\n    # Initialize arrays to hold average scores and selection counts\n    average_scores = np.zeros(len(action_indices))\n    selection_counts = np.zeros(len(action_indices))\n    \n    # Calculate average scores and selection counts\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if scores:\n            average_scores[i] = np.mean(scores)\n    \n    # Define epsilon parameters\n    min_epsilon = 0.05\n    max_epsilon = 1.0\n    decay_rate = 0.001\n\n    # Epsilon calculation based on the total selection count\n    epsilon = max(min_epsilon, max_epsilon * (1 - (total_selection_count / (total_time_slots + 1))))\n\n    # Epsilon-greedy strategy for action selection\n    if np.random.rand() < epsilon:\n        # Exploration: choose a random action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploitation: calculate UCB values\n        ucb_values = average_scores + np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n        action_index = action_indices[np.argmax(ucb_values)]\n\n    return action_index",
          "objective": 156828.0724506692,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    \n    # Calculate the average scores for each action\n    average_scores = []\n    for action_index in action_indices:\n        scores = score_set[action_index]\n        if scores:\n            average_scores.append(np.mean(scores))\n        else:\n            average_scores.append(0.0)  # Assign 0 if action hasn't been selected yet\n\n    # Define epsilon, starting from a higher value and decaying over time\n    min_epsilon = 0.1\n    max_epsilon = 1.0\n    decay_factor = 0.01  # Adjust this factor to change rate of decay\n    epsilon = max(min_epsilon, max_epsilon * (1 - (total_selection_count / (total_time_slots + 1))))\n\n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        # Exploration: randomly select an action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploitation: select the action with the highest average score\n        action_index = action_indices[np.argmax(average_scores)]\n    \n    return action_index",
          "objective": 160028.4296933725,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    \n    # Initialize alpha and beta for Beta distribution\n    alpha = np.zeros(len(action_indices))\n    beta = np.zeros(len(action_indices))\n    \n    # Calculate alpha and beta for each action based on historical scores\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        successes = sum(scores)  # Total score accumulated\n        failures = len(scores) - successes  # Number of selections - successful scores\n        \n        # Avoid division by zero and handle unselected actions\n        alpha[i] = successes + 1  # Adding 1 for prior\n        beta[i] = failures + 1     # Adding 1 for prior\n\n    # Sample from Beta distribution for each action\n    sampled_scores = np.random.beta(alpha, beta)\n    \n    # Choose the action with the highest sampled score\n    action_index = np.argmax(sampled_scores)\n    \n    return action_index",
          "objective": 162876.6152622269,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    \n    # Define a minimum epsilon threshold\n    min_epsilon = 0.1\n    # Calculate epsilon based on the time slot\n    epsilon = max(min_epsilon, 1.0 - (current_time_slot / total_time_slots))\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        mean_score = np.mean(scores) if scores else 0\n        \n        selection_count = len(scores)\n        \n        # Calculate exploration term for rarely selected actions\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else np.sqrt(np.log(total_time_slots + 1))\n        \n        # Combine mean score and exploration term\n        action_score = (1 - epsilon) * mean_score + epsilon * exploration_bonus\n        \n        action_scores.append(action_score)\n    \n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 166031.60929975926,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    \n    # Calculate average scores for each action\n    avg_scores = np.array([np.mean(score_set[action_index]) if score_set[action_index] else 0 for action_index in action_indices])\n    \n    # Calculate the number of actions\n    num_actions = len(action_indices)\n\n    # Dynamic exploration probability (epsilon)\n    min_exploration = 0.1\n    max_exploration = 1.0\n    \n    # Epsilon based on current time slot and selection count\n    epsilon = max(min_exploration, \n                  max_exploration * (1 - (current_time_slot / total_time_slots)) * (1 - (total_selection_count / (10 * total_time_slots))))\n\n    # Select actions based on epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        # Exploration: select a random action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploitation: select the action with the highest average score\n        # We add a small random noise to break ties\n        noise = np.random.rand(num_actions) * 1e-8\n        action_index = action_indices[np.argmax(avg_scores + noise)]\n        \n    return action_index",
          "objective": 168305.84554026276,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters for epsilon-greedy exploration strategy\n    epsilon_max = 1.0  # Maximum exploration rate at the beginning\n    epsilon_min = 0.1  # Minimum exploration rate for later time slots\n    epsilon_decay = 0.99  # Decay rate for epsilon\n\n    # Calculate dynamic epsilon based on the current time slot\n    epsilon = max(epsilon_min, epsilon_max * (epsilon_decay ** (current_time_slot / total_time_slots)))\n    \n    action_scores = []\n    num_actions = 8  # Fixed number of actions\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        average_score = np.mean(scores) if scores else 0.0\n        \n        # Selection count\n        selection_count = len(scores) if scores else 0\n        normalized_count = selection_count + 1e-5  # Avoid division by zero\n\n        # Exploration term inversely proportional to the number of times action has been chosen\n        exploration_bonus = (1 - np.clip(selection_count / (total_selection_count + 1e-5), 0, 1)) * (1 - epsilon)\n        \n        # Combine average score with exploration factor\n        action_score = average_score + exploration_bonus\n        \n        action_scores.append(action_score)\n\n    # Epsilon-greedy action selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)  # Explore: randomly select an action\n    else:\n        action_index = np.argmax(action_scores)  # Exploit: select the best action based on computed scores\n    \n    return action_index",
          "objective": 174711.0100637181,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters for exploration-exploitation trade-off\n    base_epsilon = 0.5\n    min_epsilon = 0.01\n    decay_factor = 0.95\n    action_count = 8  # Total actions available (0 - 7)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n    \n    # Calculate average scores and selection counts for each action\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        avg_scores[action_index] = np.mean(scores) if scores else 0.0\n\n    # Calculate dynamic epsilon\n    epsilon = max(\n        min_epsilon,\n        base_epsilon * (decay_factor ** (total_selection_count / action_count))\n    )\n    \n    # Adjust selection probabilities based on historical performance and epsilon\n    if np.random.rand() < epsilon:\n        # Exploration: use softmax based on average scores\n        scaling_factors = np.exp(avg_scores - np.max(avg_scores))  # Subtract max for numerical stability\n        probabilities = scaling_factors / np.sum(scaling_factors)\n        action_index = np.random.choice(action_count, p=probabilities)\n    else:\n        # Exploitation: select the action with the highest average score\n        action_index = np.argmax(avg_scores)\n\n    return action_index",
          "objective": 181110.0649296851,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_weight = 1.5  # Adjusted exploration weight\n    exploitation_weight = 4.0  # Adjusted exploitation weight\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        mean_score = np.mean(scores) if scores else 0.0\n        \n        selection_count = len(scores)\n        normalized_selection_count = selection_count + 1e-5  # Avoid division by zero\n\n        # Dynamic adjustment of exploration and exploitation factors\n        exploration_factor = 1 - (current_time_slot / total_time_slots)\n        exploitation_factor = current_time_slot / total_time_slots\n\n        # Calculate adjusted mean score, normalizing for total selections\n        adjusted_mean_score = (mean_score * normalized_selection_count) / (total_selection_count + 1e-5)\n\n        # Calculate exploration term considering the historical selection count\n        exploration_term = np.sqrt(np.log(total_selection_count + 1) / normalized_selection_count)\n\n        # Combine mean score and exploration term for action score\n        action_score = (exploitation_weight * adjusted_mean_score) + \\\n                       (exploration_weight * exploration_factor * exploration_term)\n        \n        action_scores.append(action_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 182392.26698163865,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n\n    # Initialize average scores and counts\n    average_scores = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate average scores and counts for each action\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        action_counts[i] = len(scores)\n        if action_counts[i] > 0:\n            average_scores[i] = np.mean(scores)\n\n    # Calculate exploration factor based on the current time\n    exploration_factor = np.sqrt((current_time_slot + 1) / (total_time_slots + 1))\n    \n    # Create an exploration-adjusted score\n    adjusted_scores = average_scores + exploration_factor * (1 / (action_counts + 1e-5))\n\n    # Choose the action with the highest adjusted score\n    action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 191523.86388442278,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters for exploration\n    initial_exploration_probability = 1.0\n    min_exploration_probability = 0.1\n    decay_rate = 0.01  # Decay rate for exploration probability\n    \n    # Calculate the exploration probability based on the selection count\n    epsilon = max(min_exploration_probability, \n                  initial_exploration_probability * np.exp(-decay_rate * total_selection_count))\n    \n    # Calculate average scores and selection counts\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        mean_score = np.mean(scores) if scores else 0  # Handle cases with no previous scores\n        selection_count = len(scores)\n        \n        # Calculate action score\n        if np.random.rand() < epsilon:  # Explore\n            # Encouraging exploration by using a base score or random score\n            action_score = np.random.rand()  # Random score for exploration\n        else:  # Exploit\n            action_score = mean_score\n\n        action_scores.append(action_score)\n    \n    # Select the action with the highest calculated score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 194964.88786209206,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters for epsilon-greedy strategy\n    epsilon_start = 0.1  # Initial exploration probability\n    epsilon_end = 0.01   # Final exploration probability\n    decay_rate = 0.01    # Rate of decay for epsilon\n    \n    # Calculate the current epsilon value based on time slot\n    epsilon = max(epsilon_end, epsilon_start - decay_rate * current_time_slot)\n    \n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        mean_score = np.mean(scores) if scores else 0\n        \n        # Use a score metric with a minor penalty for untried actions\n        selection_count = len(scores)\n        bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else 1\n        action_score = mean_score + bonus\n        \n        action_scores.append(action_score)\n    \n    if np.random.rand() < epsilon:\n        # Exploration: choose a random action\n        action_index = np.random.randint(0, 8)\n    else:\n        # Exploitation: choose the action with the highest score\n        action_index = np.argmax(action_scores)\n        \n    return action_index",
          "objective": 200243.40861942622,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters for exploration and exploitation\n    initial_epsilon = 1.0  # Initial exploration rate\n    final_epsilon = 0.05   # Final exploration rate\n    epsilon_decay = 0.99    # Exploration decay factor\n\n    num_actions = 8  # Number of actions\n\n    # Calculate current exploration rate using decay based on total selections\n    epsilon = max(final_epsilon, initial_epsilon * (epsilon_decay ** (total_selection_count / total_time_slots)))\n\n    action_scores = []\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        average_score = np.mean(scores) if scores else 0.0\n        \n        # Using selection count to formulate exploration bonus\n        selection_count = len(scores)\n        normalized_count = selection_count + 1e-5  # Avoid division by zero\n        exploration_bonus = 1.0 / normalized_count\n        \n        # Combining average score and exploration bonus\n        action_score = average_score * (1 - epsilon) + (epsilon * exploration_bonus)\n        action_scores.append(action_score)\n\n    # Select action index with the highest score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": 211149.46350050034,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8  # There are a total of 8 actions (0-7)\n\n    # Initialize arrays for average scores and selection counts\n    avg_scores = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n\n    # Calculate average scores and selection counts for each action\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        avg_scores[action_index] = np.mean(scores) if scores else 0.0\n\n    # Calculate epsilon dynamically\n    epsilon = max(0.1, 0.5 * (1 - current_time_slot / total_time_slots) * (1 - total_selection_count / (total_selection_count + 1)))\n    \n    # Generate a random number to decide exploration vs exploitation\n    if np.random.rand() < epsilon:\n        # Explore: select a random action\n        action_index = np.random.choice(action_count)\n    else:\n        # Exploit: use a weighted scoring based on average scores and selection counts\n        adjusted_scores = avg_scores + (1.0 / (selection_counts + 1e-5))  # Encourage selection of less explored actions\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 241663.05875883857,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Hyperparameters for the strategy\n    base_epsilon = 0.1    # Initial exploration probability\n    min_epsilon = 0.01     # Minimum exploration probability\n    decay_factor = 0.001   # Decay factor for epsilon, adjusted for total time slots\n\n    # Calculate the epsilon value for exploration-exploitation trade-off\n    epsilon = max(min_epsilon, base_epsilon * (1 - current_time_slot / total_time_slots))\n    \n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        mean_score = np.mean(scores) if scores else 0\n        selection_count = len(scores)\n        \n        # UCB-based exploration bonus\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else 1\n        action_score = mean_score + exploration_bonus\n        \n        action_scores.append(action_score)\n\n    # Stochastic choice between exploration and exploitation\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(8)  # Exploration\n    else:\n        action_index = np.argmax(action_scores)  # Exploitation\n        \n    return action_index",
          "objective": 284474.02432193246,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    base_epsilon = 0.1\n    min_epsilon = 0.01\n    exploration_factor = 0.5  # Factor to scale exploration\n    action_count = len(score_set)\n    \n    avg_scores = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        avg_scores[action_index] = np.mean(scores) if scores else 0.0\n    \n    # Dynamic epsilon calculation\n    exploration_weight = exploration_factor * (1 - total_selection_count / (total_selection_count + 1))\n    epsilon = max(min_epsilon, base_epsilon * exploration_weight)\n\n    if np.random.rand() < epsilon:\n        # Explore\n        probabilities = (1 / (selection_counts + 1))  # Favor actions with fewer selections\n        probabilities /= probabilities.sum()  # Normalize\n        action_index = np.random.choice(action_count, p=probabilities)\n    else:\n        # Exploit\n        action_index = np.argmax(avg_scores)\n    \n    return action_index",
          "objective": 300352.1971303773,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n\n    # Initialize average scores and selection counts \n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for action in range(n_actions):\n        scores = score_set.get(action, [])\n        selection_counts[action] = len(scores)  # Count of times action has been selected\n        if selection_counts[action] > 0:\n            avg_scores[action] = np.mean(scores)  # Calculate the mean of scores if available\n\n    # Calculate exploration bonuses based on selection counts\n    exploration_bonuses = np.log(total_selection_count + 1) / (selection_counts + 1e-10)\n    avg_scores += exploration_bonuses  # Encourage exploration for less selected actions\n    \n    # Define epsilon dynamically based on the time slot\n    epsilon = np.max([0.1, (1.0 - (current_time_slot / total_time_slots))])\n\n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(np.arange(n_actions))  # Explore\n    else:\n        action_index = np.argmax(avg_scores)  # Exploit\n\n    return action_index",
          "objective": 300540.65175288275,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    \n    # Calculate average scores and selection counts for each action\n    avg_scores = np.array([\n        np.mean(score_set[action_index]) if score_set[action_index] else 0 \n        for action_index in action_indices\n    ])\n    \n    selection_counts = np.array([len(score_set[action_index]) for action_index in action_indices])\n    \n    # Calculate exploration probability\n    min_exploration = 0.05\n    exploration_factor = 1.0 - (current_time_slot / total_time_slots) - (total_selection_count / (10 * total_time_slots))\n    exploration_factor = np.clip(exploration_factor, min_exploration, 1.0)\n    \n    # Calculate exploration bonus based on selection counts\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n    \n    # Combined score with exploration\n    adjusted_scores = avg_scores + exploration_factor * exploration_bonus\n    \n    # Select an action based on the highest adjusted score\n    action_index = action_indices[np.argmax(adjusted_scores)]\n\n    return action_index",
          "objective": 305713.87736042123,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    \n    # Initialize arrays for average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for action in range(n_actions):\n        if action in score_set and score_set[action]:\n            avg_scores[action] = np.mean(score_set[action])\n            selection_counts[action] = len(score_set[action])\n\n    # Handle division by zero by introducing a small number\n    selection_adjustment = np.where(selection_counts > 0, selection_counts, 1)\n    \n    # Compute exploration factor\n    exploration_factor = np.sqrt(total_selection_count / selection_adjustment)\n\n    # Combine average scores with exploration factor for balanced scoring\n    combined_scores = avg_scores * exploration_factor\n\n    # Define adaptive exploration rate\n    exploration_rate = np.clip(1 - (current_time_slot / max(total_time_slots, 1)), 0.05, 0.5)\n\n    # Epsilon-greedy selection based on exploring and exploiting\n    if np.random.rand() < exploration_rate:\n        action_index = np.random.choice(np.arange(n_actions))  # Explore\n    else:\n        action_index = np.argmax(combined_scores)  # Exploit\n\n    return action_index",
          "objective": 311982.6661907674,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = max(0.1, 1 - (total_selection_count / total_time_slots))  # Decreasing exploration rate\n    epsilon = min(epsilon, 0.5)  # Cap the exploration rate at 0.5 to ensure exploration degrades slowly\n\n    action_scores = []\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        mean_score = np.mean(scores) if scores else 0\n        action_scores.append((mean_score, len(scores)))\n\n    # Epsilon-greedy decision\n    if np.random.rand() < epsilon:\n        # Exploration: randomly select an action from actions not fully explored\n        unexplored_actions = [i for i in range(8) if action_scores[i][1] == 0]\n        if unexplored_actions:\n            action_index = np.random.choice(unexplored_actions)\n        else:\n            action_index = np.random.choice(range(8))\n    else:\n        # Exploitation: select the action with the highest average score\n        action_index = np.argmax([score for score, _ in action_scores])\n    \n    return action_index",
          "objective": 315691.358344158,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Calculate average scores and selection counts\n    average_scores = []\n    selection_counts = []\n    \n    for action_index in action_indices:\n        scores = score_set[action_index]\n        selection_count = len(scores)\n        average_score = np.mean(scores) if scores else 0.0\n        average_scores.append(average_score)\n        selection_counts.append(selection_count)\n\n    # Calculate exploration factor based on a decreasing function of time\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n\n    # Compute upper confidence bounds\n    ucb_values = [\n        avg_score + exploration_weight * np.sqrt(np.log(total_selection_count + 1) / (count + 1))\n        for avg_score, count in zip(average_scores, selection_counts)\n    ]\n    \n    # Select action based on UCB\n    action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 317992.7904304358,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    \n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for action in range(n_actions):\n        if action in score_set and score_set[action]:\n            avg_scores[action] = np.mean(score_set[action])\n            selection_counts[action] = len(score_set[action])\n\n    # Handle zero selection counts to avoid div by zero in exploration bonus\n    exploration_bonus = np.where(selection_counts > 0, 1 / (selection_counts + 1e-10), 1.0)\n    \n    # Incorporate exploration bonus into average scores\n    avg_scores += exploration_bonus\n\n    # Adaptive epsilon-greedy strategy\n    base_exploration_rate = 0.5\n    exploration_decay = base_exploration_rate * (current_time_slot / max(total_time_slots, 1))\n    exploration_rate = base_exploration_rate - exploration_decay\n    exploration_rate = np.clip(exploration_rate, 0.05, 0.5)\n\n    # Epsilon-greedy selection\n    if np.random.rand() < exploration_rate:\n        action_index = np.random.choice(range(n_actions))  # Explore\n    else:\n        action_index = np.argmax(avg_scores)  # Exploit\n    \n    return action_index",
          "objective": 337911.5378133939,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    \n    # Calculate average scores for each action\n    avg_scores = []\n    for action_index in action_indices:\n        scores = score_set[action_index]\n        if scores:  # If there are scores for this action\n            avg_score = np.mean(scores)\n        else:  # If no scores have been recorded, assign a score of 0\n            avg_score = 0\n        avg_scores.append(avg_score)\n    \n    # Calculate exploration factor\n    exploration_factor = max(0, 1 - (current_time_slot / total_time_slots)) * max(0, 1 - (total_selection_count / (10 * total_time_slots)))\n    \n    # Weighted scores combining average scores and random exploration\n    weighted_scores = []\n    for i, avg_score in enumerate(avg_scores):\n        # Introduce randomness into the selection process\n        weighted_score = avg_score + exploration_factor * np.random.rand()\n        weighted_scores.append(weighted_score)\n        \n    # Choose the action with the highest weighted score\n    action_index = np.argmax(weighted_scores)\n    \n    return action_index",
          "objective": 357910.52855017,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    exploration_rate = 1.0 / (1 + total_selection_count)  # Decay exploration over time\n\n    # Initialize average scores and selection counts \n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for action in range(n_actions):\n        if action in score_set and score_set[action]:  # Avoid empty lists\n            avg_scores[action] = np.mean(score_set[action])\n            selection_counts[action] = len(score_set[action])\n    \n    # Calculate exploration bonuses inversely proportional to selection counts\n    with np.errstate(divide='ignore', invalid='ignore'):\n        exploration_bonuses = 1 / (selection_counts + 1e-10)\n        avg_scores += exploration_bonuses\n    \n    # Adaptive epsilon for exploration\n    epsilon = exploration_rate * (1 - (current_time_slot / total_time_slots))\n\n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(range(n_actions))  # Explore\n    else:\n        action_index = np.argmax(avg_scores)  # Exploit\n\n    return action_index",
          "objective": 363378.98089535325,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for action in range(n_actions):\n        if action in score_set and score_set[action]:  # Avoid empty score lists\n            avg_scores[action] = np.mean(score_set[action])\n            selection_counts[action] = len(score_set[action])\n\n    # Handle division by zero gracefully: add a small constant to counts\n    selection_counts = np.clip(selection_counts, 1e-10, None)\n    \n    # Calculate exploration bonus\n    exploration_bonus = 1 / selection_counts\n    avg_scores += exploration_bonus\n\n    # Epsilon decay logic\n    decay_rate = 0.05 * (1 - (total_selection_count / max(total_time_slots, 1)))\n    epsilon = max(0.1, min(0.5, decay_rate))\n\n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(range(n_actions))  # Explore\n    else:\n        best_action_indices = np.flatnonzero(avg_scores == np.max(avg_scores))\n        action_index = np.random.choice(best_action_indices)  # Exploit\n\n    return action_index",
          "objective": 377508.007590257,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    min_epsilon = 0.05  # Minimum exploration probability\n    initial_epsilon = 1.0  # Initial exploration probability\n    epsilon_decay_factor = 5  # Decay factor for exploration probability\n    \n    # Calculate average scores for each action\n    average_scores = np.array([\n        np.mean(scores) if scores else 0 for scores in score_set.values()\n    ])\n    \n    # Count the number of selections for each action\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n\n    # Calculate exploration probability based on time and selection counts\n    epsilon = max(min_epsilon, initial_epsilon * (1 - current_time_slot / total_time_slots))\n    \n    # Compute an exploration bonus inversely proportional to selection counts\n    exploration_bonus = np.log(total_selection_count + 1) / (selection_counts + 1)  # Avoid division by zero\n    exploration_scores = exploration_bonus / np.max(exploration_bonus)  # Normalize exploration scores\n\n    # Combined scores for action selection with exploration bonuses\n    combined_scores = average_scores + exploration_scores\n    \n    # Choose action: exploration or exploitation\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(range(8), p=exploration_scores / np.sum(exploration_scores))\n    else:\n        action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": 390517.2215042634,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize parameters\n    action_indices = list(score_set.keys())\n    action_count = len(action_indices)\n    exploration_factor = np.sqrt(np.log(total_selection_count + 1) / (np.array([len(score_set[i]) for i in action_indices]) + 1))\n    \n    # Calculate average scores for each action\n    average_scores = [\n        np.mean(score_set[action_index]) if score_set[action_index] else 0\n        for action_index in action_indices\n    ]\n    \n    # Upper Confidence Bound (UCB) for exploration-exploitation balance\n    ucb_scores = np.array(average_scores) + exploration_factor\n\n    # Select action index with the highest UCB score\n    action_index = action_indices[np.argmax(ucb_scores)]\n\n    return action_index",
          "objective": 391441.89442035154,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    \n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for action in range(n_actions):\n        if action in score_set and score_set[action]:\n            avg_scores[action] = np.mean(score_set[action])\n            selection_counts[action] = len(score_set[action])\n\n    # Use a small number (1e-10) to avoid division by zero\n    selection_counts += 1e-10\n    exploration_factor = (total_selection_count + 1) / selection_counts\n\n    # Combining average scores with exploration factor\n    combined_scores = avg_scores * exploration_factor\n\n    # Dynamic exploration rate\n    exploration_rate = 0.5 * (1 - (current_time_slot / total_time_slots))\n    exploration_rate = np.clip(exploration_rate, 0.05, 0.5)\n\n    # Epsilon-greedy selection\n    if np.random.rand() < exploration_rate:\n        # Select a random action (explore)\n        action_index = np.random.choice(np.arange(n_actions))\n    else:\n        # Select the best action based on combined scores (exploit)\n        action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": 419849.979108599,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Define parameters for exploration and exploitation\n    initial_exploration_probability = 1.0  # Start with high exploration\n    min_exploration_probability = 0.1  # Minimum exploration probability\n    exploration_decay = (initial_exploration_probability - min_exploration_probability) / total_time_slots\n    \n    # Calculate the current exploration probability based on time slot\n    exploration_probability = initial_exploration_probability - exploration_decay * current_time_slot\n    exploration_probability = max(exploration_probability, min_exploration_probability)\n    \n    # If random chance, select a random action\n    if np.random.rand() < exploration_probability:\n        return np.random.randint(0, 8)  # Randomly select an action from 0 to 7\n    \n    # Calculate action scores based on historical data\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        mean_score = np.mean(scores) if scores else 0  # Average score for the action\n        selection_count = len(scores)\n        \n        # Adding exploration term to boost lesser-tried actions\n        if selection_count > 0:\n            exploration_term = np.sqrt(np.log(total_selection_count) / selection_count)\n            action_score = mean_score + exploration_term\n        else:\n            action_score = 1.0  # Assign a high score for untried actions\n            \n        action_scores.append(action_score)\n    \n    # Select the action with the highest calculated score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 421211.1139609543,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    \n    for index in range(8):\n        # Retrieve scores for the action\n        scores = score_set.get(index, [])\n        action_count = len(scores)\n        average_score = np.mean(scores) if action_count > 0 else 0\n        \n        # Use total_selection_count to normalize the score\n        if total_selection_count > 0:\n            exploration_bonus = (1 / (action_count + 1)) * (1 / (total_selection_count + 1))\n        else:\n            exploration_bonus = 1  # Favor exploration if nothing selected yet\n        \n        action_scores.append((average_score + exploration_bonus, action_count))\n    \n    # An adaptive exploration rate that encourages exploration early and shifts to exploitation later\n    exploration_rate = max(0.1, (total_time_slots - current_time_slot) / total_time_slots)\n\n    # Select action based on exploration vs exploitation\n    if np.random.rand() < exploration_rate:\n        # Exploration: Select an action with an increased likelihood for less selected actions\n        action_probabilities = [1 / (count + 1) for _, count in action_scores]\n        action_probabilities = np.array(action_probabilities) / sum(action_probabilities)  # Normalize probabilities\n        action_index = np.random.choice(range(8), p=action_probabilities)\n    else:\n        # Exploitation: Select action with the highest adjusted score\n        action_index = np.argmax([avg_score for avg_score, _ in action_scores])\n\n    return action_index",
          "objective": 442240.8702158345,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    \n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Compute average scores and selection counts\n    for action in range(n_actions):\n        if action in score_set and score_set[action]:\n            avg_scores[action] = np.mean(score_set[action])\n            selection_counts[action] = len(score_set[action])\n\n    # Handling unselected actions: assign a low average score to encourage exploration\n    avg_scores[selection_counts == 0] = 0  # Set average score for unselected actions to zero\n\n    # Calculate exploration bonus\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-10))  # Encourage exploration\n    exploration_bonus[selection_counts == 0] = np.inf  # Infinite exploration for unselected actions\n\n    # Combine scores\n    combined_scores = avg_scores + exploration_bonus\n\n    # Dynamic epsilon-greedy selection\n    epsilon = max(0.1, 1 - (total_selection_count / (total_time_slots + 1e-10)))  # Inverse relation to selection count\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(np.arange(n_actions))  # Explore\n    else:\n        action_index = np.argmax(combined_scores)  # Exploit\n\n    return action_index",
          "objective": 451168.3990822878,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    \n    # Calculate the average scores and selection counts for each action\n    average_scores = np.zeros(len(action_indices))\n    selection_counts = np.zeros(len(action_indices))\n    \n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if scores:\n            average_scores[i] = np.mean(scores)\n    \n    # Define decay for epsilon with a minimum exploration parameter\n    min_epsilon = 0.1\n    max_epsilon = 1.0\n    decay_rate = 0.01\n    \n    epsilon = max(min_epsilon, max_epsilon * (1 - min(total_selection_count / (total_time_slots + 1), 1)))\n    \n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        # Exploration strategy\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploitation strategy, prefer actions with fewer selections\n        ucb_values = average_scores + np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n        action_index = action_indices[np.argmax(ucb_values)]\n    \n    return action_index",
          "objective": 452132.8812727114,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores and selection counts\n    action_scores = []\n    exploration_rate = max(0.1, (total_time_slots - current_time_slot) / total_time_slots)  # Decay exploration\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        mean_score = np.mean(scores) if scores else 0  # Mean score handling\n        \n        # Calculate the probability of exploration for the action\n        if len(scores) > 0:\n            exploration_adjustment = np.sqrt(np.log(total_selection_count) / len(scores))\n            adjusted_score = mean_score + exploration_adjustment  # UCB adjustment\n        else:\n            adjusted_score = 1.0  # High score for untried action to encourage exploration\n        \n        # Incorporate exploration rate into final scoring\n        final_score = (1 - exploration_rate) * adjusted_score + exploration_rate * np.random.rand()\n        action_scores.append(final_score)\n    \n    # Select the action with the highest calculated score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 453219.9852699008,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Compute average scores and selection counts\n    for action in range(n_actions):\n        if action in score_set and score_set[action]:\n            avg_scores[action] = np.mean(score_set[action])\n            selection_counts[action] = len(score_set[action])\n\n    # Handling unselected actions\n    avg_scores[selection_counts == 0] = 0\n\n    # Calculate exploration bonus\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1) / (selection_counts + 1e-10)))\n    exploration_bonus[selection_counts == 0] = np.inf  # Infinite exploration for unselected actions\n\n    # Combine scores\n    combined_scores = avg_scores + exploration_bonus\n\n    # Dynamic epsilon-greedy selection\n    epsilon = 0.1 + ((total_selection_count / total_time_slots) * (0.9 - 0.1))  # Adjust epsilon based on selection count\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(np.arange(n_actions))  # Explore\n    else:\n        action_index = np.argmax(combined_scores)  # Exploit\n\n    return action_index",
          "objective": 457958.63395917503,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    base_epsilon = 0.1\n    min_epsilon = 0.01\n    decay_rate = 0.001  # Adjust this rate to control how fast exploration decays\n    action_count = len(score_set)\n    \n    # Calculate average scores and selection counts\n    avg_scores = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        avg_scores[action_index] = np.mean(scores) if scores else 0.0\n\n    # Dynamic epsilon calculation based on total selection count\n    exploration_weight = max(min_epsilon, base_epsilon * np.exp(-decay_rate * total_selection_count))\n    \n    # Further adjustment based on time, encouraging exploration at early time slots\n    time_factor = current_time_slot / total_time_slots\n    epsilon = exploration_weight * (1 - time_factor)\n\n    if np.random.rand() < epsilon:\n        # Explore: Balance between less selected actions and overall probability\n        exploration_probabilities = (1 / (selection_counts + 1))  # Favor actions with fewer selections\n        exploration_probabilities /= exploration_probabilities.sum()  # Normalize\n        action_index = np.random.choice(action_count, p=exploration_probabilities)\n    else:\n        # Exploit: Choose the action with the highest average score\n        action_index = np.argmax(avg_scores)\n    \n    return action_index",
          "objective": 459115.5394800994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    \n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    \n    # Calculate average scores and selection counts\n    for action in range(n_actions):\n        if action in score_set and score_set[action]:\n            avg_scores[action] = np.mean(score_set[action])\n            selection_counts[action] = len(score_set[action])\n    \n    # Confidence level based exploration\n    confidence_intervals = np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1e-10))\n    combined_scores = avg_scores + confidence_intervals  # Upper confidence bound\n\n    # Epsilon-greedy selection with adaptive exploration rate\n    exploration_rate = max(0.1, 1 - (current_time_slot / max(total_time_slots, 1)))\n    exploration_rate = np.clip(exploration_rate, 0.05, 0.5)\n\n    if np.random.rand() < exploration_rate:\n        action_index = np.random.choice(np.arange(n_actions))  # Explore\n    else:\n        action_index = np.argmax(combined_scores)  # Exploit\n\n    return action_index",
          "objective": 504816.86393583467,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n\n    # Calculate mean scores for each action\n    mean_scores = np.zeros(num_actions)\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        if scores:\n            mean_scores[i] = np.mean(scores)\n        else:\n            mean_scores[i] = 0  # Assign zero if there are no scores yet\n\n    # Parameters for exploration\n    min_exploration_rate = 0.1\n    max_exploration_rate = 1.0\n    exploration_decay = (max_exploration_rate - min_exploration_rate) / total_time_slots\n\n    # Determine current exploration rate\n    current_exploration_rate = max(min_exploration_rate, \n                                    max_exploration_rate - exploration_decay * current_time_slot)\n\n    # Calculate exploration probabilities\n    exploration_probabilities = np.zeros(num_actions)\n    \n    for i in range(num_actions):\n        if len(score_set[action_indices[i]]) == 0:\n            exploration_probabilities[i] = 1.0  # Fully explore unselected actions\n        else:\n            exploration_probabilities[i] = current_exploration_rate / (len(score_set[action_indices[i]]) + 1)\n\n    # Calculate final probabilities combining exploitation and exploration\n    total_mean_score = np.sum(mean_scores) if np.sum(mean_scores) > 0 else 1\n    exploitation_probs = mean_scores / total_mean_score\n\n    # Combine exploration and exploitation\n    final_probs = (1 - exploration_probabilities) * exploitation_probs\n    final_probs /= np.sum(final_probs)  # Normalize to sum to 1\n\n    # Sample an action based on the final probabilities\n    action_index = np.random.choice(action_indices, p=final_probs)\n    \n    return action_index",
          "objective": 511639.46686508344,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Constants for exploration-exploitation trade-off\n    base_epsilon = 0.1\n    min_epsilon = 0.01\n    \n    action_count = len(score_set)\n    avg_scores = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n\n    # Calculate average scores and selection counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        avg_scores[action_index] = np.mean(scores) if scores else 0.0\n\n    # Adaptive epsilon calculation\n    epsilon = max(min_epsilon, base_epsilon * (1 - total_selection_count / (total_time_slots + 1)))\n\n    # Decision to explore or exploit\n    if np.random.rand() < epsilon:\n        # Explore: select action inversely proportional to selection counts with a softmax approach\n        softmax_scores = np.exp(-selection_counts)\n        probabilities = softmax_scores / np.sum(softmax_scores)\n        action_index = np.random.choice(action_count, p=probabilities)\n    else:\n        # Exploit: select action with highest average score\n        action_index = np.argmax(avg_scores)\n\n    return action_index",
          "objective": 517172.59838482493,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Define parameters for epsilon\n    min_epsilon = 0.05  # Minimum value for epsilon\n    max_epsilon = 1.0   # Initial exploration probability\n    decay_rate = 0.99   # Rate at which epsilon decays\n\n    # Calculate epsilon\n    epsilon = max(min_epsilon, max_epsilon * (decay_rate ** (total_selection_count / (total_time_slots * 0.1))))\n\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        mean_score = np.mean(scores) if scores else 0\n        selection_count = len(scores)\n\n        # Weighted score calculation\n        if total_selection_count > 0 and selection_count > 0:\n            exploration_weight = (1 - (selection_count / total_selection_count))\n        else:\n            exploration_weight = 1.0  # Ensure new actions are favored if no selections have been made\n        \n        weighted_score = mean_score + exploration_weight * (1 / (selection_count + 1)) if selection_count > 0 else 1.0\n\n        # Store calculated score\n        action_scores.append(weighted_score)\n\n    # Use epsilon to decide between exploration and exploitation\n    if np.random.rand() < epsilon:\n        # Exploration: randomly select an action\n        action_index = np.random.choice(range(8))\n    else:\n        # Exploitation: select the action with the highest score\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": 565396.91291991,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate dynamic epsilon based on current time and selection count\n    base_exploration_rate = 0.1\n    epsilon = base_exploration_rate + (1 - base_exploration_rate) * (1 - (total_selection_count / (total_time_slots * 10)))\n\n    # Exploration phase\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(range(8))\n    else:\n        # Exploitation phase\n        avg_scores = np.zeros(8)\n        for action in range(8):\n            if action in score_set and score_set[action]:\n                avg_scores[action] = np.mean(score_set[action])\n        \n        # Use a small value to ensure all actions have some chance\n        exploration_bonuses = (1 / (np.array([len(score_set[action]) if action in score_set else 0 for action in range(8)]) + 1e-10))\n        avg_scores += exploration_bonuses\n        \n        action_index = np.argmax(avg_scores)\n\n    return action_index",
          "objective": 593445.8072833052,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Dynamic exploration probability (epsilon) based on total selections and current time slot\n    epsilon = max(0.1, min(1.0, 1.0 - (current_time_slot / total_time_slots)))\n    \n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        mean_score = np.mean(scores) if scores else 0  # Default score for untried actions\n        \n        selection_count = len(scores) + 1e-6  # Add a small constant to avoid division by zero\n        \n        # Calculate exploration component\n        exploration_term = np.sqrt(np.log(total_selection_count + 1) / selection_count)\n        action_score = mean_score + exploration_term\n        \n        action_scores.append(action_score)\n\n    # Epsilon-greedy selection: explore with probability epsilon, exploit otherwise\n    if np.random.rand() < epsilon:\n        # Exploration: random selection\n        action_index = np.random.choice(len(action_scores))\n    else:\n        # Exploitation: select the action with the highest score\n        action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": 605889.9084292864,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    action_count = len(action_indices)\n    \n    # Initialize average scores and selection counts\n    average_scores = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n\n    for i, action_index in enumerate(action_indices):\n        if score_set[action_index]:  # Avoid division by zero\n            average_scores[i] = np.mean(score_set[action_index])\n            selection_counts[i] = len(score_set[action_index])\n    \n    # Normalize average scores by total selection count to control bias towards highly selected actions\n    normalized_scores = average_scores / (1 + selection_counts / (total_selection_count + 1))\n\n    # Dynamic exploration strategy based on current time slot\n    exploration_bonus = (1 - current_time_slot / total_time_slots) * (1 / (selection_counts + 1))\n\n    # Calculate final scores incorporating exploration\n    final_scores = normalized_scores + exploration_bonus\n\n    # Select action index with the highest final score\n    action_index = action_indices[np.argmax(final_scores)]\n\n    return action_index",
          "objective": 626213.8215583622,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize parameters\n    action_indices = list(score_set.keys())\n    action_count = len(action_indices)\n\n    # Calculate average scores and selection counts\n    scores = np.array([np.mean(score_set[i]) if score_set[i] else 0 for i in action_indices])\n    selection_counts = np.array([len(score_set[i]) for i in action_indices])\n\n    # To avoid division by zero, use a small constant for the selection counts\n    adjusted_selection_counts = selection_counts + 1\n    \n    # Calculate exploration factor using variance (UCB variant)\n    exploration_factor = np.sqrt((2 * np.log(total_selection_count + 1)) / adjusted_selection_counts)\n\n    # Combine exploitation (average scores) and exploration\n    total_scores = scores + exploration_factor\n\n    # Select action index with the highest adjusted score\n    action_index = action_indices[np.argmax(total_scores)]\n\n    return action_index",
          "objective": 642918.7646816052,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Define parameters for dynamic epsilon\n    base_epsilon = 0.1\n    decay_rate = 0.1  # Rate for epsilon decay\n    min_epsilon = 0.01  # Minimum exploration rate\n\n    action_count = len(score_set)  # Should be from 0 to 7\n    \n    # Calculate average scores and selection counts for each action\n    avg_scores = []\n    selection_counts = []\n    \n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        avg_score = np.mean(scores) if scores else 0.0  # Average score\n        selection_count = len(scores)  # Selection count for the action\n        \n        avg_scores.append(avg_score)\n        selection_counts.append(selection_count)\n\n    # Dynamic epsilon calculation\n    epsilon = max(min_epsilon, base_epsilon * (1 - current_time_slot / total_time_slots))\n\n    # Epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        # Explore: Select an action randomly but favor those with fewer selections\n        probabilities = np.array([1 / (count + 1) for count in selection_counts])  # Favor under-explored actions\n        probabilities /= probabilities.sum()  # Normalize the probabilities\n        action_index = np.random.choice(action_count, p=probabilities)\n    else:\n        # Exploit: Select the action with the highest average score\n        action_index = np.argmax(avg_scores)\n\n    return action_index",
          "objective": 666188.8477287047,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Constants for exploration-exploitation trade-off\n    base_epsilon = 0.1\n    min_epsilon = 0.01\n    \n    action_count = len(score_set)\n    avg_scores = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n\n    # Calculate average scores and selection counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        avg_scores[action_index] = np.mean(scores) if scores else 0.0\n    \n    # Adaptive epsilon calculation\n    epsilon = max(min_epsilon, base_epsilon * (1 - (total_selection_count / (total_time_slots + 1))))\n\n    # Decision to explore or exploit\n    if np.random.rand() < epsilon:\n        # Explore: favor less selected actions\n        probabilities = 1 / (selection_counts + 1)  # Ensure no division by zero\n        probabilities /= probabilities.sum()\n        action_index = np.random.choice(action_count, p=probabilities)\n    else:\n        # Exploit: select action with highest average score\n        action_index = np.argmax(avg_scores)\n    \n    return action_index",
          "objective": 666317.4104829059,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    \n    # Calculate average scores and handle empty score lists\n    avg_scores = np.array([\n        np.mean(score_set[action_index]) if score_set[action_index] else 0\n        for action_index in action_indices\n    ])\n    \n    # Define epsilon to balance exploration and exploitation\n    epsilon = max(0.1, 1 - (total_selection_count / (10 * total_time_slots)))  # Minimum of 0.1\n    \n    # Decide whether to explore or exploit\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        action_index = np.argmax(avg_scores)  # Exploit\n    \n    return action_index",
          "objective": 669775.6666178071,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Adaptive exploration factor that decreases over time\n    exploration_factor = max(0.1, 1 - current_time_slot / total_time_slots)\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        mean_score = np.mean(scores) if scores else 0  # Mean score for the action\n        selection_count = len(scores)\n        \n        # Calculate exploration term using a modified version for epsilon-greedy\n        if selection_count > 0:\n            exploration_term = np.sqrt(np.log(total_selection_count) / selection_count)\n        else:\n            exploration_term = 1  # Untried actions are given a positive exploration term\n        \n        # Calculate final score\n        action_score = mean_score + exploration_factor * exploration_term\n        action_scores.append(action_score)\n    \n    # Select the action with the highest calculated score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 672408.7416819318,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    \n    # Calculate average scores for each action\n    avg_scores = np.array([np.mean(score_set[action_index]) if score_set[action_index] else 0 for action_index in action_indices])\n    \n    # Minimum exploration probability\n    min_exploration = 0.1\n\n    # Exploration factor (epsilon)\n    max_exploration = 1.0\n    epsilon = max(min_exploration, \n                  max_exploration * (1 - (current_time_slot / total_time_slots)) * (1 - (total_selection_count / (10 * total_time_slots))))\n\n    # Select actions based on epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        # Exploration: select a random action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploitation: select the action with the highest average score\n        action_index = action_indices[np.argmax(avg_scores)]\n        \n    return action_index",
          "objective": 729040.0798467791,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    base_exploration_rate = 0.1\n    epsilon = base_exploration_rate + (1 - base_exploration_rate) * (1 - (total_selection_count / (total_time_slots * 10)))\n\n    # Initialize average scores and selection counts \n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for action in range(n_actions):\n        if action in score_set:\n            if score_set[action]:  # Check if there are any scores recorded\n                avg_scores[action] = np.mean(score_set[action])\n            selection_counts[action] = len(score_set[action])\n    \n    # Exploration bonus inversely proportional to selection counts\n    exploration_bonuses = 1 / (selection_counts + 1e-10)\n    avg_scores += exploration_bonuses\n\n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(range(n_actions))  # Explore\n    else:\n        action_index = np.argmax(avg_scores)  # Exploit\n\n    return action_index",
          "objective": 743915.9817578639,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Dynamic epsilon based on total_selection_count\n    min_epsilon = 0.1  # Minimum exploration probability\n    max_epsilon = 0.5  # Maximum exploration probability at the beginning\n    epsilon = max(min_epsilon, max_epsilon * (1 - (total_selection_count / (total_time_slots * 2))))\n    \n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        mean_score = np.mean(scores) if scores else 0\n        \n        action_scores.append(mean_score)\n    \n    # Choose action based on epsilon-greedy approach\n    if np.random.rand() < epsilon:\n        # Exploration: randomly select an action\n        action_index = np.random.randint(0, 8)\n    else:\n        # Exploitation: select the action with the maximum average score\n        action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": 817772.2918398085,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters\n    epsilon = 0.1  # Exploration factor\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n\n    # Calculate average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if scores:  # Action has historical scores\n            average_scores[i] = np.mean(scores)\n\n    # Total selection count can help adjust scores to prioritize less selected actions\n    adjusted_scores = average_scores + (1 - selection_counts / (total_selection_count + 1)) * (1 / (1 + total_selection_count))\n\n    # Exploration vs. exploitation decision using epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        # Explore: Randomly select an action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploit: Select action with the highest adjusted score\n        max_score = np.max(adjusted_scores)\n        candidates = [i for i in range(num_actions) if adjusted_scores[i] == max_score]\n        action_index = action_indices[np.random.choice(candidates)]\n    \n    return action_index",
          "objective": 863678.3562563137,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration probability\n    action_count = len(score_set)  # Should be 8 (0 to 7)\n\n    # Calculate the average scores for each action\n    avg_scores = []\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        if scores:\n            avg_score = np.mean(scores)\n        else:\n            avg_score = 0.0  # If no scores, average is zero\n        avg_scores.append(avg_score)\n\n    # Epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        # Explore: Randomly select an action\n        action_index = np.random.choice(action_count)\n    else:\n        # Exploit: Select the action with the highest average score\n        action_index = np.argmax(avg_scores)\n\n    return action_index",
          "objective": 986322.4866514719,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    \n    # Initialize lists to hold average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Compute average scores and selection counts\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)  # Count how many times the action has been selected\n        if scores:  # Avoid division by zero\n            average_scores[i] = np.mean(scores)\n\n    # Dynamic exploration parameter\n    min_epsilon = 0.1\n    max_epsilon = 1.0\n    decay_rate = 0.01\n    epsilon = max(min_epsilon, max_epsilon * (1 - current_time_slot / total_time_slots))\n\n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        # Exploration strategy\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploitation strategy using average scores\n        # Apply Upper Confidence Bound (UCB) to encourage exploration of less-selected actions\n        ucb_values = average_scores + np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n        action_index = action_indices[np.argmax(ucb_values)]\n\n    return action_index",
          "objective": 1011162.1912667892,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration factor\n    action_indices = list(score_set.keys())\n    \n    # Calculate average scores for each action\n    average_scores = []\n    for action_index in action_indices:\n        scores = score_set[action_index]\n        if scores:  # Action has historical scores\n            average_score = np.mean(scores)\n        else:  # No historical scores\n            average_score = 0\n        average_scores.append(average_score)\n    \n    # Exploration vs. exploitation decision using epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        # Explore: Randomly select an action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploit: Select action with the highest average score\n        max_score = max(average_scores)\n        candidates = [i for i, score in enumerate(average_scores) if score == max_score]\n        action_index = np.random.choice([action_indices[i] for i in candidates])\n    \n    return action_index",
          "objective": 1078381.6473167418,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for action in range(n_actions):\n        if action in score_set:\n            if score_set[action]:  # Check if there are historical scores recorded\n                avg_scores[action] = np.mean(score_set[action])\n            selection_counts[action] = len(score_set[action])\n    \n    # Compute exploration factor based on selection frequency\n    exploration_bonus = np.clip(1 / (selection_counts + 1e-10), 0, 1)  # Avoid division by zero\n    avg_scores += exploration_bonus\n\n    # Dynamic epsilon-greedy exploration strategy\n    base_exploration_rate = 0.1\n    epsilon = base_exploration_rate * (1 - (current_time_slot / max(total_time_slots, 1)))  # Adjust epsilon\n    epsilon = np.clip(epsilon, 0.05, 0.5)  # Ensure epsilon stays within reasonable bounds\n\n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(range(n_actions))  # Explore\n    else:\n        action_index = np.argmax(avg_scores)  # Exploit\n\n    return action_index",
          "objective": 1115224.4869276464,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize parameters\n    action_indices = list(score_set.keys())\n    action_count = len(action_indices)\n    \n    # Calculate average scores and selection counts\n    average_scores = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n    \n    for i, action_index in enumerate(action_indices):\n        if score_set[action_index]:\n            average_scores[i] = np.mean(score_set[action_index])\n            selection_counts[i] = len(score_set[action_index])\n    \n    # Explore less-selected options with a factor related to their selection count\n    exploration_term = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1))\n    \n    # Strategy combines average scores with exploration factor\n    # Weighting the exploration more heavily for less frequently selected actions\n    scores = average_scores + exploration_term * np.minimum(1, (total_time_slots - current_time_slot) / total_time_slots)\n\n    # Select action index with the highest score\n    action_index = action_indices[np.argmax(scores)]\n\n    return action_index",
          "objective": 1229482.3586872453,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate epsilon based on the current time slot\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01)  # Decreasing exploration over time\n    \n    action_indices = list(score_set.keys())\n    average_scores = []\n    \n    # Calculate average scores for each action\n    for action_index in action_indices:\n        scores = score_set[action_index]\n        average_score = np.mean(scores) if scores else 0\n        average_scores.append(average_score)\n\n    # Exploration vs. exploitation decision using epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        # Explore: Randomly select an action, favoring less frequently selected actions\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploit: Select action with the highest average score\n        max_score = max(average_scores)\n        candidates = [i for i, score in enumerate(average_scores) if score == max_score]\n        action_index = np.random.choice([action_indices[i] for i in candidates])\n    \n    return action_index",
          "objective": 1400821.264188782,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n\n    # Initialize average scores and selection counts \n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for action in range(n_actions):\n        scores = score_set.get(action, [])\n        if scores:  # Avoid empty lists\n            avg_scores[action] = np.mean(scores)\n            selection_counts[action] = len(scores)\n    \n    # Calculate exploration bonuses inversely proportional to selection counts\n    exploration_bonuses = 1 / (selection_counts + 1e-10)\n    avg_scores += exploration_bonuses\n\n    # Adaptive epsilon based on the current time slot and total selections\n    epsilon = min(1.0, max(0.1, 1.0 - (current_time_slot / total_time_slots))) * (1.0 / (1 + total_selection_count))\n\n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(range(n_actions))  # Explore\n    else:\n        action_index = np.argmax(avg_scores)  # Exploit\n\n    return action_index",
          "objective": 1420380.6083726303,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    \n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts, handling possible missing actions\n    for action in range(n_actions):\n        if action in score_set:\n            selection_counts[action] = len(score_set[action])\n            if selection_counts[action] > 0:\n                avg_scores[action] = np.mean(score_set[action])\n\n    # Calculate exploration factors\n    exploration_factor = (total_selection_count + 1) / (selection_counts + 1)  # Avoid zero division\n    exploration_factor[selection_counts == 0] = np.inf  # Inf for unselected actions\n\n    # Combine average scores with exploration factor\n    combined_scores = avg_scores * exploration_factor\n\n    # Define exploration rate\n    exploration_rate = np.clip(1 - (current_time_slot / total_time_slots), 0.05, 0.5)\n\n    # Select action based on exploration or exploitation\n    if np.random.rand() < exploration_rate:\n        action_index = np.random.choice(np.arange(n_actions))  # Explore\n    else:\n        action_index = np.argmax(combined_scores)  # Exploit\n\n    return action_index",
          "objective": 1529270.7799165542,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    \n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for action in range(n_actions):\n        if action in score_set and score_set[action]:\n            avg_scores[action] = np.mean(score_set[action])\n            selection_counts[action] = len(score_set[action])\n\n    # Handle unselected actions by setting their scores to a minimum\n    avg_scores[selection_counts == 0] = -1  # Assign a very low score for unselected actions\n\n    # Calculate exploration factors\n    exploration_factors = total_selection_count / (selection_counts + 1)  # Encourage exploration based on selections\n    exploration_factors[selection_counts == 0] = np.inf  # Infinite exploration for unselected actions\n\n    # Combine scores using a weighted sum of average scores and exploration factors\n    combined_scores = avg_scores + exploration_factors\n\n    # Adaptive exploration rate\n    exploration_rate = 1 - (current_time_slot / (total_time_slots + 1e-10))  # Avoid division by zero\n    exploration_rate = np.clip(exploration_rate, 0.1, 0.5)  # Range between a minimum and maximum\n\n    # Epsilon-greedy selection with adaptive exploration\n    if np.random.rand() < exploration_rate:\n        action_index = np.random.choice(np.arange(n_actions))  # Explore\n    else:\n        action_index = np.argmax(combined_scores)  # Exploit\n\n    return action_index",
          "objective": 1673297.0180734051,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Dynamic epsilon value based on selection count\n    epsilon = max(0.1, 1.0 - (total_selection_count / (total_time_slots * 0.5)))\n    \n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        mean_score = np.mean(scores) if scores else 0\n        \n        # Append the mean score to action scores for determining the best action\n        action_scores.append(mean_score)\n\n    # Randomly explore with probability epsilon\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(8)  # Choose a random action for exploration\n    else:\n        action_index = np.argmax(action_scores)  # Exploit the best-known action\n\n    return action_index",
          "objective": 1709645.6044413596,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    \n    # Calculate the average scores and counts of selections\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if scores:\n            average_scores[i] = np.mean(scores)\n        else:\n            average_scores[i] = 0  # To handle actions with no selections\n    \n    # Dynamic epsilon calculation\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    decay_factor = 0.001  # Decay factor for epsilon\n    epsilon = max(epsilon_min, epsilon_max * (1 - decay_factor * total_selection_count))\n\n    # Epsilon-greedy selection with a fallback for less selected actions\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)  # Exploration\n    else:\n        # Select action using upper confidence bounds (UCB)\n        ucb_values = np.where(selection_counts > 0,\n                               average_scores + np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts),\n                               float('inf'))  # Handle actions never selected with high value\n        action_index = action_indices[np.argmax(ucb_values)]\n    \n    return action_index",
          "objective": 1827020.8948762806,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    \n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for action in range(n_actions):\n        if action in score_set and score_set[action]:\n            avg_scores[action] = np.mean(score_set[action])\n            selection_counts[action] = len(score_set[action])\n\n    # Use exploration factor to encourage exploration\n    exploration_factor = (total_selection_count + 1) / (selection_counts + 1e-10)  # Avoid division by zero\n    exploration_factor = np.where(selection_counts > 0, exploration_factor, np.inf)  # Assign infinite exploration factor for unselected actions\n\n    # Combine average scores with exploration factor\n    combined_scores = avg_scores * exploration_factor\n\n    # Define adaptive exploration rate\n    exploration_rate = 1 - (current_time_slot / max(total_time_slots, 1))\n    exploration_rate = np.clip(exploration_rate, 0.05, 0.5)\n\n    # Epsilon-greedy selection\n    if np.random.rand() < exploration_rate:\n        action_index = np.random.choice(np.arange(n_actions))  # Explore\n    else:\n        action_index = np.argmax(combined_scores)  # Exploit\n\n    return action_index",
          "objective": 1885436.8708148878,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon_base = 0.1  # Base exploration probability\n    decay_factor = 0.01  # Factor for decaying exploration over time\n    action_count = 8  # Number of actions (0 to 7)\n\n    # Calculate the average scores for each action\n    avg_scores = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        avg_scores[action_index] = np.mean(scores) if scores else 0.0\n        selection_counts[action_index] = len(scores)\n\n    # Calculate the conditional epsilon for exploration\n    epsilon = max(0, epsilon_base - (decay_factor * current_time_slot))\n\n    # Epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        # Explore: Randomly select an action\n        action_index = np.random.choice(action_count)\n    else:\n        # Determine which actions to promote for exploitation\n        action_weights = avg_scores / (selection_counts + 1e-5)  # Adjust for selection counts\n        action_index = np.argmax(action_weights)\n\n    return action_index",
          "objective": 2052774.5247057418,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8  # There are a total of 8 actions (0-7)\n\n    # Calculate the average scores and counts for each action\n    avg_scores = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n    \n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        avg_scores[action_index] = np.mean(scores) if scores else 0.0\n\n    # Exploring probability based on current time slot and selection counts\n    beta = 0.5  # Exploration parameter\n    exploration_factor = beta * (1 - (current_time_slot / total_time_slots))\n    \n    # Calculate total selections made so far for scaling\n    total_actions_selected = max(1, total_selection_count)\n    \n    # Calculate exploration probabilities\n    exploration_probabilities = (1 - (selection_counts / total_actions_selected))\n    exploration_probabilities = np.clip(exploration_probabilities, 0, 1)  # Ensure probabilities are between 0 and 1\n\n    exploit_probabilities = avg_scores / np.sum(avg_scores)  # Normalized average scores for exploitation\n\n    # Combine exploration and exploitation strategies\n    final_probabilities = (1 - exploration_factor) * exploit_probabilities + exploration_factor * exploration_probabilities\n    final_probabilities /= np.sum(final_probabilities)  # Normalize to sum to 1\n\n    # Randomly choose an action based on the derived probabilities\n    action_index = np.random.choice(action_count, p=final_probabilities)\n    \n    return action_index",
          "objective": 2165418.558752099,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    \n    # Calculate average scores for each action\n    avg_scores = np.array([\n        np.mean(score_set[action_index]) if score_set[action_index] else 0 \n        for action_index in action_indices\n    ])\n    \n    # Calculate selection counts for each action\n    selection_counts = np.array([len(score_set[action_index]) for action_index in action_indices])\n    \n    # Dynamic exploration rate (epsilon)\n    min_epsilon = 0.05\n    max_epsilon = 0.9\n    epsilon = max_epsilon * (1 - (current_time_slot / total_time_slots))\n    epsilon = max(epsilon, min_epsilon)\n    \n    # Choose whether to explore or exploit\n    if np.random.rand() < epsilon:\n        # Exploration: Select a random action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploitation: Select the action with the highest average score\n        action_index = action_indices[np.argmax(avg_scores)]\n    \n    return action_index",
          "objective": 2221468.9730500025,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    \n    # Calculate average scores and handle cases where action has never been selected\n    for index in range(8):\n        scores = score_set.get(index, [])\n        if scores:\n            average_score = np.mean(scores)\n        else:\n            average_score = 0\n        \n        # Capture the number of selections for this action\n        action_counts = len(scores)\n        action_scores.append((average_score, action_counts))\n    \n    # Define exploration rate that decreases over time\n    exploration_rate = max(0.1, (total_time_slots - current_time_slot) / total_time_slots)\n\n    # Select action based on exploration vs exploitation\n    if np.random.rand() < exploration_rate:\n        # Exploration: randomly select an action favoring those less frequently chosen\n        action_index = np.random.choice(range(8), p=[0.125] * 8)  # uniform random choice\n    else:\n        # Exploitation: select action with the highest average score\n        max_score = -1\n        action_index = -1\n        \n        for index, (avg_score, count) in enumerate(action_scores):\n            if (count == 0 and avg_score == 0):  # completely unselected actions\n                action_index = index  # select untested actions if necessary\n                break\n            if avg_score > max_score:\n                max_score = avg_score\n                action_index = index\n\n    return action_index",
          "objective": 2273470.1767723993,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Dynamic epsilon based on the current time slot\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n    \n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(8)\n    selection_counts = np.zeros(8)\n    \n    # Calculate average scores and counts for each action\n    for action in range(8):\n        if action in score_set:\n            scores = score_set[action]\n            if scores:\n                avg_scores[action] = np.mean(scores)\n                selection_counts[action] = len(scores)\n    \n    # Probability adjustment for exploitation based on the selection counts\n    selection_probabilities = (avg_scores * (total_selection_count - selection_counts + 1) /\n                                total_selection_count) if total_selection_count > 0 else avg_scores\n    \n    # Normalize selection probabilities\n    selection_probabilities = selection_probabilities / np.sum(selection_probabilities)\n    \n    # Randomly select action based on the adjusted probabilities\n    action_index = np.random.choice(range(8), p=selection_probabilities if np.random.rand() > epsilon else None)\n\n    return action_index",
          "objective": 2308806.459053616,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate exploration parameter epsilon\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01)\n    action_indices = list(score_set.keys())\n    \n    # Calculate average scores and selection counts\n    average_scores = np.zeros(len(action_indices))\n    selection_counts = np.zeros(len(action_indices))\n    \n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        if scores:\n            average_scores[i] = np.mean(scores)\n            selection_counts[i] = len(scores)\n\n    # Weight the average score by selection count to promote lesser-selected actions\n    weighted_scores = average_scores + (1 - (selection_counts / total_selection_count)) * (1 / (selection_counts + 1))\n    \n    # Perform action selection\n    if np.random.rand() < epsilon:\n        # Exploration: Randomly select an action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploitation: Select action with the highest weighted score\n        max_score = max(weighted_scores)\n        candidates = [i for i, score in enumerate(weighted_scores) if score == max_score]\n        action_index = np.random.choice([action_indices[i] for i in candidates])\n\n    return action_index",
          "objective": 2334397.0591781996,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    avg_scores = []\n    \n    for action in action_indices:\n        scores = score_set[action]\n        if len(scores) == 0:  # No historical score\n            avg_scores.append(0)\n        else:\n            avg_scores.append(np.mean(scores))\n    \n    # Exploration factor: encourages selection of less frequently chosen actions\n    action_counts = [len(score_set[action]) for action in action_indices]\n    exploration_bonus = np.array([1 / (count + 1) for count in action_counts])  # Avoid division by zero\n\n    # Combine average scores with exploration bonuses\n    combined_scores = np.array(avg_scores) + exploration_bonus\n\n    # Normalize scores using the total number of selections and current time slot\n    normalization_factor = 1 + (current_time_slot / total_time_slots)\n    normalized_scores = combined_scores / normalization_factor\n    \n    # Select action with the highest normalized score\n    action_index = action_indices[np.argmax(normalized_scores)]\n    \n    return action_index",
          "objective": 2407575.116450368,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon_start = 1.0  # Initial exploration rate\n    epsilon_min = 0.1    # Minimum exploration rate\n    epsilon_decay = 0.9   # Decay factor for exploration rate\n\n    # Calculate epsilon based on current time slot\n    epsilon = max(epsilon_min, epsilon_start * (epsilon_decay ** (current_time_slot / total_time_slots)))\n\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        mean_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n        normalized_selection_count = selection_count + 1e-5  # Avoid division by zero\n\n        # Calculate adjusted score considering selection frequency\n        adjusted_score = (mean_score * normalized_selection_count) / (total_selection_count + 1e-5)\n\n        # Formula to evaluate action score\n        action_score = adjusted_score * (1 - epsilon)  # Exploitation part\n        \n        # Exploration component: Encourage selection of less tested actions\n        exploration_term = np.sqrt(np.log(total_selection_count + 1) / normalized_selection_count)\n        action_score += epsilon * exploration_term  # Exploration part\n\n        action_scores.append(action_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 2615274.515413832,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))  # Exploration probability decreasing over time\n    if np.random.rand() < epsilon:  # Exploration\n        action_index = np.random.choice(range(8))\n    else:  # Exploitation\n        avg_scores = np.zeros(8)\n        for action in range(8):\n            if action in score_set and len(score_set[action]) > 0:\n                avg_scores[action] = np.mean(score_set[action])\n            elif total_selection_count > 0:  # If no scores but other actions selected\n                avg_scores[action] = 0\n                \n        action_index = np.argmax(avg_scores)\n\n    return action_index",
          "objective": 2691131.091675326,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Normalize the exploration factor across time slots\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n    \n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        mean_score = np.mean(scores) if scores else 0  # Default to 0 for untried actions\n        selection_count = len(scores) + 1e-6  # Add a small number to avoid division by zero\n        \n        # Calculate a normalized mean for fair comparison\n        normalized_mean = mean_score / (mean_score + 1e-6)  # Prevent division by zero\n        \n        # Combine exploration and exploitation using a modified score\n        exploration_term = np.sqrt(np.log(total_selection_count) / selection_count)\n        action_score = normalized_mean + exploration_factor * exploration_term\n        \n        action_scores.append(action_score)\n    \n    # Select the action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 2784436.0517094755,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration factor\n    action_indices = list(score_set.keys())\n    scores = np.array([np.mean(score_set[i]) if score_set[i] else 0 for i in action_indices])\n    \n    if total_selection_count == 0 or np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        exploration_bonus = np.sqrt(np.log(total_selection_count) / (np.array([len(score_set[i]) for i in action_indices]) + 1e-5))\n        total_scores = scores + exploration_bonus\n        action_index = action_indices[np.argmax(total_scores)]\n    \n    return action_index",
          "objective": 2932026.4799093753,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for action in range(n_actions):\n        if action in score_set:\n            if score_set[action]:  # Ignore empty score lists\n                avg_scores[action] = np.mean(score_set[action])\n                selection_counts[action] = len(score_set[action])\n    \n    # Implement Upper Confidence Bound (UCB) for exploration\n    total_selections = total_selection_count + 1  # To avoid division by zero\n    ucb_scores = avg_scores + np.sqrt(2 * np.log(total_selections) / (selection_counts + 1e-10))\n    \n    # Dynamic decision making using a hybrid strategy\n    exploration_factor = 0.1 * (1 - (current_time_slot / max(total_time_slots, 1)))  # Dynamic exploration factor\n    exploration_factor = np.clip(exploration_factor, 0.05, 0.5)  # Clamp within reasonable bounds\n    \n    # Epsilon-greedy selection based on the hybrid UCB scores\n    if np.random.rand() < exploration_factor:\n        action_index = np.random.choice(range(n_actions))  # Exploration\n    else:\n        action_index = np.argmax(ucb_scores)  # Exploitation based on UCB scores\n    \n    return action_index",
          "objective": 3122378.886878376,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8  # There are a total of 8 actions (0-7)\n    \n    avg_scores = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n\n    # Calculate average scores and selection counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        avg_scores[action_index] = np.mean(scores) if scores else 0.0\n        \n    # Dynamic exploration factor based on time\n    exploration_factor = (current_time_slot / total_time_slots) ** 0.5\n    \n    # Calculate confidence intervals for each action\n    confidence_intervals = np.zeros(action_count)\n    for action_index in range(action_count):\n        if selection_counts[action_index] > 0:\n            confidence_intervals[action_index] = np.sqrt(2 * np.log(total_selection_count) / selection_counts[action_index])\n        else:\n            confidence_intervals[action_index] = 1.0  # High uncertainty for unselected actions\n\n    # Score with confidence interval adjustment\n    upper_confidence_bounds = avg_scores + confidence_intervals\n    \n    # Set epsilon dynamic\n    epsilon = max(0.1, 0.5 * (1 - (current_time_slot / total_time_slots)))\n\n    if np.random.rand() < epsilon:\n        # Explore: choose action based on upper confidence bounds\n        action_index = np.argmax(upper_confidence_bounds)\n    else:\n        # Exploit: select the best average score\n        action_index = np.argmax(avg_scores)\n\n    return action_index",
          "objective": 4152107.54007747,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Decay factor for exploration\n    epsilon_initial = 0.5  # Starting exploration rate\n    epsilon_final = 0.1     # Ending exploration rate\n    exploration_factor = max(epsilon_initial - (current_time_slot / total_time_slots) * (epsilon_initial - epsilon_final), epsilon_final)\n\n    action_indices = list(score_set.keys())\n    \n    # Calculate the average scores and number of selections for each action\n    average_scores = np.zeros(len(action_indices))\n    selection_counts = np.zeros(len(action_indices))\n    \n    for action_index in action_indices:\n        if score_set[action_index]:  # Action has historical scores\n            average_scores[action_index] = np.mean(score_set[action_index])\n            selection_counts[action_index] = len(score_set[action_index])\n        else:  # No historical scores\n            average_scores[action_index] = 0\n            selection_counts[action_index] = 0\n    \n    # Exploration vs. exploitation decision\n    if np.random.rand() < exploration_factor:\n        # Explore: Randomly select an action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploit: Use a weighted scoring mechanism based on average score and selection counts\n        weights = average_scores / (1 + selection_counts)  # Normalizing scores by selection counts\n        action_index = np.random.choice(action_indices, p=weights / weights.sum())  # Softmax sampling\n\n    return action_index",
          "objective": 4241704.825771074,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 1.0 / (current_time_slot + 1)  # Dynamic exploration factor decreases with time\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        mean_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n        \n        # Epsilon-greedy strategy for exploration\n        if np.random.rand() < epsilon:\n            # Encourage exploration by randomly selecting an action\n            action_score = np.random.rand()  # random score in [0, 1] for exploration\n        else:\n            # Utilize mean score for exploitation\n            action_score = mean_score\n        \n        # Integrate the selection count for reducing confidence in under-explored actions\n        if selection_count > 0:\n            action_score += (np.sqrt(np.log(total_selection_count) / selection_count)) / (total_time_slots - current_time_slot)\n        \n        action_scores.append(action_score)\n\n    # Select the action with the highest calculated score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 4458047.021181965,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    min_epsilon = 0.1  # Minimum exploration probability\n    initial_epsilon = 1.0  # Initial exploration probability\n    epsilon_decay = 0.99  # Rate at which exploration probability decreases\n\n    # Calculate dynamic epsilon based on total selection count\n    epsilon = max(min_epsilon, initial_epsilon * (epsilon_decay ** total_selection_count))\n\n    # Calculate average scores and selection counts for each action\n    action_scores = []\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        mean_score = np.mean(scores) if scores else 0  # Handle cases with no previous scores\n        selection_count = len(scores)\n\n        # Store tuple of (mean score, selection count) for further evaluation\n        action_scores.append((mean_score, selection_count))\n\n    # Select action based on epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        # Explore: select action randomly (weighted by selection counts)\n        weighted_indices = np.arange(8)\n        weights = np.array([1.0 / (count + 1) for _, count in action_scores])  # simple weighting to favor less explored actions\n        normalized_weights = weights / np.sum(weights)  # Normalize weights for probability distribution\n        action_index = np.random.choice(weighted_indices, p=normalized_weights)\n    else:\n        # Exploit: select the action with the highest mean score\n        action_index = np.argmax([score for score, _ in action_scores])\n\n    return action_index",
          "objective": 6238828.625421455,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))  # Decrease exploration over time\n    random_val = np.random.rand()\n    \n    if random_val < epsilon:\n        # Explore: randomly choose an action\n        action_index = np.random.choice(range(8))\n    else:\n        # Exploit: choose the action with the highest average score\n        avg_scores = []\n        for action in range(8):\n            scores = score_set.get(action, [])\n            if scores:\n                avg_score = np.mean(scores)\n            else:\n                avg_score = 0  # If the action has never been selected\n            avg_scores.append(avg_score)\n        \n        action_index = np.argmax(avg_scores)\n\n    return action_index",
          "objective": 6975646.820935655,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate the mean scores for each action or use 0 if not selected\n    action_means = [\n        np.mean(scores) if scores else 0 \n        for scores in (score_set.get(action_index, []) for action_index in range(8))\n    ]\n    \n    # Adaptive epsilon based on the current time slot and total selections\n    epsilon = max(1.0 - (current_time_slot / total_time_slots), 0.1)\n    \n    # Generate a random number to determine exploration or exploitation\n    random_value = np.random.rand()\n    \n    if random_value < epsilon:\n        # Exploration: select a random action\n        action_index = np.random.choice(range(8))\n    else:\n        # Exploitation: select the action with the highest mean score\n        action_index = np.argmax(action_means)\n\n    return action_index",
          "objective": 7675615.113500046,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\nimport random\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Average scores for each action (handling actions with no scores)\n    average_scores = {}\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        if scores:\n            average_scores[action_index] = np.mean(scores)\n        else:\n            average_scores[action_index] = 0.0\n            \n    # Convert average scores to a list for easier handling\n    avg_scores_list = [average_scores[i] for i in range(8)]\n    \n    # Parameters for epsilon-greedy strategy\n    epsilon = 0.1  # Exploration factor\n    exploration = random.random() < epsilon\n    \n    if exploration or total_selection_count < 8 or current_time_slot < total_time_slots / 4:\n        # Explore: randomly select an action\n        action_index = random.randint(0, 7)\n    else:\n        # Exploit: select the action with the maximum average score\n        action_index = np.argmax(avg_scores_list)\n        \n    return action_index",
          "objective": 7833403.110064721,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Set parameters for exploration vs exploitation\n    exploration_weight = 1.0  # Weight for exploration\n    exploitation_weight = 2.0  # Weight for exploitation\n    \n    # Initialize action scores\n    action_scores = []\n    \n    # Iterate through each action index\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate the average score for the action\n        mean_score = np.mean(scores) if scores else 0\n        \n        # Prevent division by zero and normalize the mean score\n        normalized_score = mean_score / (total_selection_count + 1e-5) if total_selection_count else mean_score\n        \n        # Calculate the exploration-exploitation score\n        score = (exploration_weight / (selection_count + 1e-5)) + (exploitation_weight * normalized_score)\n        action_scores.append(score)\n    \n    # Determine the action index for the best score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": 8110084.065471209,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8  # There are a total of 8 actions (0-7)\n    \n    avg_scores = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n\n    # Compute average scores and selection counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        avg_scores[action_index] = np.mean(scores) if scores else 0.0\n\n    # Calculate dynamic epsilon based on total selection count and current time slot\n    base_epsilon = 1.0\n    min_epsilon = 0.05\n    decay_factor = 0.95  # Decay factor for epsilon\n    # Adjust epsilon decay using the current time slot\n    epsilon = max(min_epsilon, base_epsilon * (decay_factor ** (current_time_slot / total_time_slots)))\n\n    # Calculate the exploration-exploitation trade-off\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (1 + selection_counts))\n    adjusted_scores = avg_scores + exploration_bonus\n\n    if np.random.rand() < epsilon:\n        # Exploration: select an action based on adjusted scores\n        probabilities = adjusted_scores / adjusted_scores.sum()\n        action_index = np.random.choice(action_count, p=probabilities)\n    else:\n        # Exploitation: select the best average score action\n        action_index = np.argmax(avg_scores)\n\n    return action_index",
          "objective": 8600191.90440698,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate epsilon, which controls exploration rate\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01)  # Decreasing exploration over time\n\n    action_indices = list(score_set.keys())\n    average_scores = []\n    exploration_counts = []\n\n    # Calculate average scores and selection counts for each action\n    for action_index in action_indices:\n        scores = score_set[action_index]\n        average_score = np.mean(scores) if scores else 0\n        average_scores.append(average_score)\n        exploration_counts.append(len(scores))\n\n    # UCB for exploration-exploitation balance (Upper Confidence Bound)\n    delta = np.sqrt((2 * np.log(total_selection_count + 1)) / (np.array(exploration_counts) + 1e-5))\n    ucb_scores = np.array(average_scores) + delta\n\n    # Exploration vs. exploitation decision\n    if np.random.rand() < epsilon:\n        # Explore: Randomly select an action among the less frequent ones\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploit: Select action with the highest UCB score\n        action_index = action_indices[np.argmax(ucb_scores)]\n\n    return action_index",
          "objective": 10583302.561305432,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    \n    # Calculate midpoint for time slots\n    midpoint = total_time_slots / 2\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        mean_score = np.mean(scores) if scores else 0\n        \n        selection_count = len(scores)\n        \n        # Normalizing the mean score based on total selections\n        normalized_mean_score = mean_score * (selection_count / total_selection_count) if total_selection_count > 0 else mean_score\n        \n        # Adjust weights dynamically based on the current time slot\n        if current_time_slot < midpoint:  # Exploration phase\n            exploration_bonus = (1 + (midpoint - current_time_slot) / midpoint)  # Higher bonus for less selected actions\n            exploration_term = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1) if selection_count > 0 else 1)\n            action_score = normalized_mean_score + exploration_bonus * exploration_term\n        else:  # Exploitation phase\n            action_score = normalized_mean_score  # Focus solely on mean score in this phase\n\n        action_scores.append(action_score)\n    \n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 14749943.040218707,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    scores = np.array([np.mean(score_set[i]) if len(score_set[i]) > 0 else 0 for i in action_indices])\n    selection_counts = np.array([len(score_set[i]) for i in action_indices])\n\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-6))  # Avoid division by zero\n    exploration_factor = 1 - (current_time_slot / total_time_slots)  # Diminishes over time\n\n    adjusted_scores = scores + exploration_factor * exploration_bonus\n\n    action_index = action_indices[np.argmax(adjusted_scores)]\n    return action_index",
          "objective": 25518874.039318662,
          "other_inf": null
     }
]