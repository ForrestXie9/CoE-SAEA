[
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize variables\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    \n    # Calculate average scores and selection counts\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Calculate the exploration parameter using a nonlinear decay function\n    alpha = 1.0  # control the speed of decay\n    min_exploration_rate = 0.05\n    exploration_rate = max(min_exploration_rate, 1.0 - (current_time_slot / (total_time_slots + 1)) ** alpha)\n\n    # Calculate UCB values, incorporating selection counts\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts[i] + 1e-5))\n            ucb_values[i] = avg_scores[i] + confidence_interval\n        else:\n            ucb_values[i] = np.inf  # Assign infinity to unselected actions to ensure they are explored\n\n    # Randomly explore or exploit based on exploration rate\n    if np.random.rand() < exploration_rate:\n        action_index = np.random.choice(action_indices)  # Explore: select a random action\n    else:\n        action_index = np.argmax(ucb_values)  # Exploit: select the best UCB action\n\n    # Ensure action_index is valid\n    action_index = int(np.clip(action_index, 0, n_actions - 1))\n\n    return action_index",
          "objective": 17362.76583766365,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if scores else 0\n\n    # Dynamic exploration to exploitation balance\n    initial_exploration_factor = 0.5\n    epsilon = initial_exploration_factor * (1 - (current_time_slot / total_time_slots)) if total_time_slots > 0 else 0\n\n    # Compute UCB values and handle zero selection counts\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_bound = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_bound\n        else:\n            ucb_values[i] = np.inf  # Prioritize unselected actions\n\n    # Epsilon-greedy decision-making\n    if np.random.random() < epsilon:\n        # Explore: select randomly from all actions\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploit: select the action with the highest UCB\n        action_index = action_indices[np.argmax(ucb_values)]\n\n    return action_index",
          "objective": 17801.644076201013,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize arrays for average scores, standard deviations, and selection counts\n    avg_scores = np.zeros(n_actions)\n    std_devs = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Compute average scores, standard deviations, and selection counts\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n            std_devs[i] = np.std(scores)\n        else:\n            avg_scores[i] = 0.0\n            std_devs[i] = 0.0\n    \n    # Dynamic epsilon based on the current time slot with a minimum threshold\n    epsilon = max(0.2 * (1 - (current_time_slot / total_time_slots)), 0.05)\n\n    # Calculate potential values\n    total_selections = total_selection_count + 1  # Avoid division by zero\n    ucb_values = np.zeros(n_actions)\n\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt((2 * np.log(total_selections)) / selection_counts[i])\n            # Combine average scores and standard deviation into the assessment\n            ucb_values[i] = avg_scores[i] + confidence_interval + std_devs[i]\n        else:\n            ucb_values[i] = np.inf  # Maximally uncertain actions should always be selected if untried\n\n    # Select action based on epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = np.argmax(ucb_values)\n\n    # Ensure action index is valid\n    action_index = max(0, min(action_index, n_actions - 1))\n\n    return action_index",
          "objective": 17834.141798498047,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Initialize arrays for average scores, selection counts, and variances\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    variances = np.zeros(n_actions)\n\n    # Compute average scores, counts, and variances\n    for i, action_index in enumerate(action_indices):\n        scores = np.array(score_set[action_index])\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n            variances[i] = np.var(scores)\n        else:\n            avg_scores[i] = 0.0\n            variances[i] = np.inf  # Ensure unexplored actions are favored\n\n    # Dynamic epsilon based on the current time slot\n    epsilon = max(0.5 * (1 - (current_time_slot / total_time_slots)), 0.05)\n\n    # Calculate the exploration-exploitation score\n    exploration_scores = np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n    scores = avg_scores + exploration_scores - np.sqrt(variances)  # balancing mean with variance\n\n    # Select action based on epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = np.argmax(scores)\n\n    # Ensure action index is valid\n    action_index = max(0, min(action_index, n_actions - 1))\n\n    return action_index",
          "objective": 17953.01264880478,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Initialize arrays for average scores, selection counts, and score standard deviations\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Compute average scores and selection counts\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if selection_counts[i] > 0 else 0.0\n    \n    # Compute standard deviations of scores\n    std_devs = np.zeros(n_actions)\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        std_devs[i] = np.std(scores) if selection_counts[i] > 0 else 0.0\n\n    # Dynamic epsilon calculation based on exploration strategy\n    epsilon = max(0.5 * (1 - (total_selection_count / max(total_time_slots, 1))), 0.1)\n    \n    # Calculating the scores adjusted by standard deviation (to account for risk)\n    adjusted_scores = avg_scores - std_devs  # Actions with higher variability are less favored\n    \n    # Select action using adaptive epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        # Exploration: Randomly select one of the actions to gather more data\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploitation: Select action based on adjusted average scores\n        action_index = np.argmax(adjusted_scores)\n\n    # Ensure the action index is within bounds\n    action_index = max(0, min(action_index, n_actions - 1))\n\n    return action_index",
          "objective": 18182.262957166466,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Initialize arrays for average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Compute average scores and selection counts\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if selection_counts[i] > 0 else 0.0\n    \n    # Dynamic epsilon based on the current time slot with minimum threshold\n    epsilon = max(0.2 * (1 - (current_time_slot / total_time_slots)), 0.05)\n    \n    # Calculate UCB values\n    ucb_values = np.zeros(n_actions)\n    total_selections = total_selection_count + 1  # To avoid division by zero\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt((2 * np.log(total_selections)) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_interval\n        else:\n            ucb_values[i] = np.inf  # Not selected yet, very high UCB\n\n    # Select action based on epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        # Exploration: prefer less frequently selected actions\n        exploration_weights = np.where(selection_counts > 0, 1 / (selection_counts + 1e-5), 1)\n        exploration_scores = avg_scores * exploration_weights\n        action_index = np.argmax(exploration_scores)\n    else:\n        # Exploitation: select action based on UCB values\n        action_index = np.argmax(ucb_values)\n\n    # Ensure action index is valid\n    action_index = max(0, min(action_index, n_actions - 1))\n\n    return action_index",
          "objective": 18187.18062032787,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Initialize arrays for average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Compute average scores and selection counts\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if selection_counts[i] > 0 else 0.0\n\n    # Dynamic exploration/exploitation balance\n    # Epsilon annealing: starting high and decay over time\n    max_epsilon = 0.5\n    min_epsilon = 0.05\n    epsilon = max(min_epsilon, max_epsilon * (1 - (total_selection_count / max(total_time_slots, 1))))\n\n    # Calculate score variance for each action\n    score_variance = np.zeros(n_actions)\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        score_variance[i] = np.var(scores) if selection_counts[i] > 0 else 0.0\n\n    # Potential value considered both average score and variability\n    potential_values = avg_scores + np.sqrt(score_variance + 1e-5)  # Small value to avoid division by zero\n\n    # Select action based on adaptive epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)  # Exploration\n    else:\n        action_index = np.argmax(potential_values)  # Exploitation\n\n    # Ensure action index is within bounds\n    action_index = max(0, min(action_index, n_actions - 1))\n\n    return action_index",
          "objective": 18492.832843227057,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize variables\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if selection_counts[i] > 0 else 0.0\n\n    # Dynamic epsilon value with a minimum threshold of 0.1\n    min_epsilon = 0.1\n    epsilon = max(min_epsilon, 1.0 - (total_selection_count / (total_time_slots + 1)))\n\n    # Calculate standard deviation for each action to assess variability\n    std_devs = np.zeros(n_actions)\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        std_devs[i] = np.std(scores) if selection_counts[i] > 0 else 0.0\n\n    # Comprehensive scoring based on average and standard deviation\n    comprehensive_scores = avg_scores - std_devs * 0.5  # Penalize high variability\n\n    # UCB values combining avg_scores and confidence intervals\n    ucb_values = avg_scores + np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n\n    # Explore more for less chosen actions\n    exploration_weights = np.where(selection_counts > 0, 1 / (selection_counts + 1e-5), 1)\n    exploration_scores = avg_scores * exploration_weights\n\n    # Select action based on exploration vs exploitation strategy\n    if np.random.rand() < epsilon:\n        action_index = np.argmax(exploration_scores)\n    else:\n        action_index = np.argmax(ucb_values)\n\n    # Ensure action_index is valid\n    action_index = int(np.clip(action_index, 0, n_actions - 1))\n\n    return action_index",
          "objective": 18933.93048894304,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize variables\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    \n    # Calculate average scores and selection counts\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if selection_counts[i] > 0 else 0.0\n\n    # Dynamic epsilon value with minimum threshold of 0.1\n    epsilon = max(0.1, 1.0 - (total_selection_count / (total_time_slots + 1)))\n    \n    # Calculate UCB values\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        confidence_interval = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts[i] + 1e-5))\n        ucb_values[i] = avg_scores[i] + confidence_interval if selection_counts[i] > 0 else np.inf\n\n    # Exploration based on unpopularity\n    exploration_weights = np.where(selection_counts > 0, 1 / (selection_counts + 1e-5), 1)\n    combined_scores = avg_scores * exploration_weights\n\n    # Decide action based on exploration vs exploitation\n    if np.random.rand() < epsilon:\n        action_index = np.argmax(combined_scores)\n    else:\n        action_index = np.argmax(ucb_values)\n\n    # Ensure action_index is valid\n    action_index = int(np.clip(action_index, 0, n_actions - 1))\n\n    return action_index",
          "objective": 18977.485582012378,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Initialize arrays for average scores, variances, and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    variances = np.zeros(n_actions)\n\n    # Compute average scores, variances, and selection counts\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if selection_counts[i] > 0 else 0.0\n        variances[i] = np.var(scores) if selection_counts[i] > 1 else 0.0  # Variance only if more than one score\n\n    # Dynamic epsilon based on the total selection count with a minimum threshold\n    epsilon = max(0.5 * (1 - (total_selection_count / (total_time_slots + 1))), 0.1)\n    \n    # Calculate UCB values\n    ucb_values = np.zeros(n_actions)\n    total_selections = total_selection_count + 1  # To avoid division by zero\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt((2 * np.log(total_selections)) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_interval + variances[i]\n        else:\n            ucb_values[i] = np.inf  # Not selected yet\n\n    # Select action based on epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        # Exploration: select a less frequently used action\n        unexplored = np.where(selection_counts == 0)[0]\n        if unexplored.size > 0:\n            action_index = np.random.choice(unexplored)\n        else:\n            exploration_weights = 1 / (selection_counts + 1e-5)\n            exploration_scores = avg_scores * exploration_weights\n            action_index = np.argmax(exploration_scores)\n    else:\n        # Exploitation: select action based on UCB values\n        action_index = np.argmax(ucb_values)\n\n    # Ensure action index is valid\n    action_index = max(0, min(action_index, n_actions - 1))\n\n    return action_index",
          "objective": 19675.879278846995,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize variables\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    variance_scores = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n            variance_scores[i] = np.var(scores)  # Variance for robustness\n           \n    # Dynamic epsilon value with a minimum threshold of 0.1\n    epsilon = max(0.1, 1.0 - (total_selection_count / (total_time_slots + 1)))\n\n    # Calculate Upper Confidence Bound (UCB) values\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts[i] + 1e-5))\n            ucb_values[i] = avg_scores[i] + confidence_interval + np.sqrt(variance_scores[i]) / (selection_counts[i] + 1e-5)\n        else:\n            ucb_values[i] = np.inf  # Assign infinity to unselected actions to ensure they are explored\n\n    # Calculate exploration weight based on selection counts\n    exploration_weights = np.where(selection_counts > 0, 1.0 / (selection_counts + 1e-5), 1.0)\n\n    # Combined scores for exploration and exploitation\n    combined_scores = avg_scores * exploration_weights\n\n    # Randomly explore or exploit based on epsilon policy\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)  # Random action selection\n    else:\n        action_index = np.argmax(ucb_values)  # Greedily select based on UCB\n\n    # Ensure action_index is valid\n    action_index = int(np.clip(action_index, 0, n_actions - 1))\n\n    return action_index",
          "objective": 20032.886997243924,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Initialize arrays for average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Compute average scores and selection counts\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if selection_counts[i] > 0 else 0.0\n\n    # Calculate an adaptive epsilon based on selection count to favor exploration in the beginning\n    epsilon = max(0.2 * (1 - (total_selection_count / max(total_time_slots, 1))), 0.05)\n\n    # Calculate score variance for each action\n    score_variance = np.zeros(n_actions)\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        score_variance[i] = np.var(scores) if selection_counts[i] > 0 else 0.0\n\n    # Calculate potential values using combined average scores and score variance\n    potential_values = avg_scores + np.sqrt(score_variance + 1e-5)  # Adding small value to avoid division by zero\n\n    # Select action based on adaptive epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        # Exploration: Randomly select one of the actions to gather more data\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploitation: Select action based on potential values\n        action_index = np.argmax(potential_values)\n\n    # Ensure action index is within bounds\n    action_index = max(0, min(action_index, n_actions - 1))\n\n    return action_index",
          "objective": 23218.9021861478,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize variables\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if scores else 0.0\n\n    # Dynamic exploration rate (epsilon) with nonlinear decay\n    if current_time_slot < total_time_slots / 2:\n        epsilon = 1.0 - (2 * current_time_slot / total_time_slots)\n    else:\n        epsilon = 0.1 + 0.9 * (1 - (current_time_slot - total_time_slots / 2) / (total_time_slots / 2))\n        \n    epsilon = max(min(epsilon, 0.9), 0.1)  # cap epsilon between 0.1 and 0.9\n\n    # Calculate UCB values\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts[i] + 1e-5))\n            ucb_values[i] = avg_scores[i] + confidence_interval\n        else:\n            ucb_values[i] = np.inf  # Encourage exploration of unselected actions\n\n    # Select action based on exploration-exploitation strategy\n    if np.random.rand() < epsilon:\n        # Exploration: Select with emphasis on lower selection counts\n        exploration_weights = 1 / (selection_counts + 1e-5)\n        combined_scores = avg_scores * exploration_weights\n        action_index = np.argmax(combined_scores)\n    else:\n        # Exploitation: Select based on UCB values\n        action_index = np.argmax(ucb_values)\n\n    # Ensure action_index is within valid bounds\n    action_index = np.clip(action_index, 0, n_actions - 1)\n\n    return action_index",
          "objective": 24242.967822906776,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize arrays for average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Compute average scores and selection counts\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if selection_counts[i] > 0 else 0.0\n\n    # Dynamic epsilon based on the current time slot\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01)\n\n    # Calculate UCB values\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_interval\n        else:\n            ucb_values[i] = np.inf  # Actions not selected yet\n\n    # Epsilon-Greedy with UCB consideration\n    if np.random.rand() < epsilon:\n        # Exploration: favor actions with fewer selections\n        exploration_weights = (1 + 1 / (selection_counts + 1e-5))  # Avoid division by zero\n        combined_scores = avg_scores * exploration_weights\n        action_index = np.argmax(combined_scores)\n    else:\n        # Exploitation: select action based on UCB values\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 24682.390163492466,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Track average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if selection_counts[i] > 0 else 0.0\n\n    # Dynamic exploration rate\n    min_epsilon = 0.1\n    epsilon = min_epsilon + (1.0 - min_epsilon) * (1.0 - total_selection_count / (total_time_slots + 1))\n\n    # Compute the confidence of the actions\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts[i] + 1e-5))\n            ucb_values[i] = avg_scores[i] + confidence_interval\n        else:\n            ucb_values[i] = np.inf  # Prioritize untried actions\n\n    # Variance calculation for reliability\n    variances = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 1:\n            variances[i] = np.var(score_set[action_indices[i]])\n\n    # Adjust UCB values based on variance to encourage stable performance\n    stability_factor = 1 / (variances + 1e-5)  # Inverse variance to weight stability\n    adjusted_ucb_values = ucb_values * stability_factor\n\n    # Select action based on modified exploration-exploitation strategy\n    if np.random.rand() < epsilon:\n        # Exploration: select the least selected actions weighted by their scores\n        explore_weights = np.where(selection_counts > 0, 1 / (selection_counts + 1e-5), 1)\n        explore_scores = avg_scores * explore_weights\n        action_index = np.argmax(explore_scores)\n    else:\n        # Exploitation: select action based on adjusted UCB values\n        action_index = np.argmax(adjusted_ucb_values)\n\n    # Ensure action_index is within valid bounds\n    action_index = int(np.clip(action_index, 0, n_actions - 1))\n\n    return action_index",
          "objective": 24791.713697132396,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize arrays for average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    \n    # Compute average scores and selection counts\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if selection_counts[i] > 0 else 0.0\n\n    # Dynamic epsilon to balance exploration and exploitation\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01) * (1 - total_selection_count / (total_selection_count + 1))\n    \n    # Calculate UCB values\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_interval\n        else:\n            ucb_values[i] = np.inf  # Actions not selected yet\n\n    # Selection probabilities based on UCB\n    probabilities = (1 - epsilon) * (ucb_values / np.sum(ucb_values)) + epsilon / n_actions\n    action_index = np.random.choice(n_actions, p=probabilities)\n\n    return action_index",
          "objective": 25463.158031238905,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize variables to track average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts for each action\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if selection_counts[i] > 0 else 0.0\n\n    # Dynamic exploration rate (epsilon)\n    epsilon = max(0.1, 1.0 - (total_selection_count / (total_time_slots + 1)))\n\n    # Calculate UCB values for exploitation\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        confidence_interval = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts[i] + 1e-5))\n        ucb_values[i] = avg_scores[i] + confidence_interval if selection_counts[i] > 0 else np.inf\n\n    # Select an action based on adaptive epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        # Exploration: select action based on its unpopularity\n        exploration_weights = np.where(selection_counts > 0, 1 / (selection_counts + 1e-5), 1)\n        combined_scores = avg_scores * exploration_weights\n        action_index = np.argmax(combined_scores)\n    else:\n        # Exploitation: select action based on UCB values\n        action_index = np.argmax(ucb_values)\n\n    # Ensure action_index is within valid bounds\n    action_index = int(np.clip(action_index, 0, n_actions - 1))\n\n    return action_index",
          "objective": 25701.822075068972,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Initialize arrays for average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    \n    # Compute average scores and selection counts\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if scores else 0.0\n\n    # Dynamic epsilon for exploration-exploitation balance\n    epsilon = max(0.2 * (1 - current_time_slot / total_time_slots) + 0.1 * (1 - total_selection_count / (total_selection_count + 1)), 0.1)\n\n    # Calculate UCB values\n    ucb_values = np.zeros(n_actions)\n    total_counts_plus_one = total_selection_count + 1  # To avoid division by zero\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt((2 * np.log(total_counts_plus_one)) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_interval\n        else:\n            ucb_values[i] = np.inf  # Prioritize unselected actions\n\n    # Selection probabilities based on UCB values\n    adjusted_ucb = (1 - epsilon) * (ucb_values / np.sum(ucb_values)) + epsilon / n_actions\n    action_index = np.random.choice(n_actions, p=adjusted_ucb / np.sum(adjusted_ucb))\n    \n    # Output validation\n    return action_index",
          "objective": 26308.315133472082,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    if n_actions == 0:\n        raise ValueError(\"No actions available for selection.\")\n    \n    # Initialize arrays for average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Compute average scores and selection counts\n    for i, action_index in enumerate(action_indices):\n        if score_set[action_index]:\n            scores = score_set[action_index]\n            selection_counts[i] = len(scores)\n            avg_scores[i] = np.mean(scores)\n        else:\n            selection_counts[i] = 0\n            avg_scores[i] = 0\n\n    # Calculate UCB values\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_interval\n        else:\n            ucb_values[i] = np.inf  # Encourage selection of untried actions\n\n    # Adaptive Epsilon-Greedy Mechanism\n    initial_epsilon = 1.0\n    min_epsilon = 0.1\n    decay_factor = (initial_epsilon - min_epsilon) / total_time_slots if total_time_slots > 0 else 0\n    epsilon = max(initial_epsilon - decay_factor * current_time_slot, min_epsilon)\n\n    # Exploration vs Exploitation\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)  # Exploration\n    else:\n        action_index = action_indices[np.argmax(ucb_values)]  # Exploitation based on UCB\n\n    # Validate action index is within range\n    if action_index < 0 or action_index >= n_actions:\n        raise ValueError(\"Selected action index is out of bounds.\")\n\n    # Logging for debugging (optional)\n    print(f\"Selected Action Index: {action_index}, Epsilon: {epsilon:.3f}, UCB Values: {ucb_values}, \"\n          f\"Avg Scores: {avg_scores}, Selection Counts: {selection_counts}\")\n\n    return action_index",
          "objective": 26384.21571849642,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize arrays for average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Compute average scores and selection counts\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if selection_counts[i] > 0 else 0.0\n\n    # Dynamic epsilon based on total selection count\n    epsilon = max(0.1 * (1 - total_selection_count / (20 + total_selection_count)), 0.01)\n\n    # Calculate UCB values\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_interval\n        else:\n            ucb_values[i] = np.inf  # Not selected yet, very high UCB\n\n    # Select action based on epsilon-greedy strategy with UCB consideration\n    if np.random.rand() < epsilon:\n        # Exploration: prefer actions with lower selection counts\n        exploration_weights = np.where(selection_counts > 0, 1 / (selection_counts + 1e-5), 1)\n        combined_scores = avg_scores * exploration_weights\n        action_index = np.argmax(combined_scores)\n    else:\n        # Exploitation: select action based on UCB values\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 26958.35191691104,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    \n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if scores else 0\n\n    # Dynamic exploration factor\n    if total_time_slots > 0:\n        epsilon = 1.0 / (1 + current_time_slot)\n    else:\n        epsilon = 0\n\n    # Compute UCB values and handle zero selection counts\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_bound = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_bound\n        else:\n            ucb_values[i] = np.inf  # Prioritize unselected actions\n\n    # Epsilon-greedy decision-making\n    if np.random.random() < epsilon:\n        # Explore: select randomly from all actions\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploit: select the action with the highest UCB\n        action_index = action_indices[np.argmax(ucb_values)]\n\n    # Validate action_index to ensure it is within range\n    action_index = int(action_index) % 8  # Ensure it's between 0 and 7\n    \n    return action_index",
          "objective": 27102.364302510956,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if scores else 0\n\n    # Dynamic exploration strategy\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots)) if total_time_slots > 0 else 0.1\n\n    # Calculate UCB values\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence\n        else:\n            ucb_values[i] = np.inf  # Prioritize unselected actions\n\n    # Combine UCB and epsilon-greedy approach for decision making\n    if np.random.random() < epsilon:\n        # Explore: select randomly from all actions\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploit: select based on UCB\n        action_index = action_indices[np.argmax(ucb_values)]\n\n    return action_index",
          "objective": 27205.850815525602,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if scores else 0\n\n    # Dynamic exploration (epsilon decay)\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots)) if total_time_slots > 0 else 0.1\n\n    # Calculate UCB values\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence\n        else:\n            ucb_values[i] = np.inf  # Prioritize unselected actions\n\n    # Combine UCB and epsilon-greedy approach for decision making\n    if np.random.random() < epsilon:\n        # Explore: select randomly from all actions\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploit: select based on UCB\n        action_index = action_indices[np.argmax(ucb_values)]\n\n    # Ensure the action index is within the specified bounds\n    action_index = max(0, min(action_index, 7))  # Keeps action_index in range [0, 7]\n\n    return action_index",
          "objective": 27246.048224829567,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if scores else 0\n\n    # Adaptive epsilon for exploration strategy\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots)) if total_time_slots > 0 else 0.1\n\n    # Enhanced UCB calculation\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_bound = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_bound\n        else:\n            ucb_values[i] = np.inf  # Prioritize unselected actions\n\n    # Epsilon-greedy decision-making\n    if np.random.random() < epsilon:\n        # Explore: select randomly from all actions\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploit: select the action with the highest UCB\n        action_index = action_indices[np.argmax(ucb_values)]\n\n    # Ensure the action index is within the valid range (0 to 7)\n    return action_index",
          "objective": 27382.155095821923,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Calculate average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if selection_counts[i] > 0 else 0\n\n    # Adaptive epsilon calculation\n    max_epsilon = 1.0\n    min_epsilon = 0.01\n    decay_factor = 0.99\n    epsilon = max(min_epsilon, max_epsilon * (decay_factor ** current_time_slot))\n\n    # Thompson Sampling mechanism\n    successes = avg_scores * selection_counts\n    failures = (np.ones(n_actions) * selection_counts) - successes\n\n    sampled_probs = np.random.beta(successes + 1, failures + 1)\n    \n    # Combined Selection Strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)  # Exploration\n    else:\n        # Exploitation: Combined UCB and Thompson Sampling\n        ucb_values = np.zeros(n_actions)\n        for i in range(n_actions):\n            if selection_counts[i] > 0:\n                confidence_interval = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[i])\n                ucb_values[i] = avg_scores[i] + confidence_interval\n            else:\n                ucb_values[i] = np.inf  # Not selected yet\n\n        # Select action from UCB and Thompson Sampling\n        combined_values = ucb_values + sampled_probs\n        action_index = np.argmax(combined_values)\n\n    # Ensure action index is valid\n    action_index = int(action_index) % n_actions\n\n    return action_index",
          "objective": 27394.53051143819,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if scores else 0\n\n    # Adaptive epsilon based on time\n    base_epsilon = 0.2\n    epsilon = base_epsilon * (1 - (current_time_slot / total_time_slots)) if total_time_slots > 0 else 0\n\n    # Compute UCB values\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_bound = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_bound\n        else:\n            ucb_values[i] = np.inf  # Prioritize unselected actions\n\n    # Epsilon-greedy decision-making\n    if np.random.random() < epsilon:\n        # Explore: select randomly from all actions\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploit: select the action with the highest UCB\n        action_index = action_indices[np.argmax(ucb_values)]\n\n    return action_index",
          "objective": 27523.106308217277,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize arrays for average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Compute average scores and selection counts\n    for i, action_index in enumerate(action_indices):\n        if score_set[action_index]:\n            scores = score_set[action_index]\n            selection_counts[i] = len(scores)\n            avg_scores[i] = np.mean(scores)\n        else:\n            selection_counts[i] = 0\n            avg_scores[i] = 0\n\n    # Calculate UCB values\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_interval\n        else:\n            ucb_values[i] = np.inf  # Encourage selection of untried actions\n\n    # Epsilon-Greedy Mechanism\n    initial_epsilon = 1.0\n    min_epsilon = 0.1\n    decay_rate = (initial_epsilon - min_epsilon) / total_time_slots\n    epsilon = max(initial_epsilon - decay_rate * current_time_slot, min_epsilon)\n\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)  # Exploration\n    else:\n        action_index = action_indices[np.argmax(ucb_values)]  # Exploitation based on UCB\n\n    # Logging mechanism\n    print(f\"Selected Action Index: {action_index}, Epsilon: {epsilon:.3f}, UCB Values: {ucb_values}\")\n\n    return action_index",
          "objective": 27525.895000744316,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize arrays for average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Compute selection counts and average scores\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if selection_counts[i] > 0 else 0.0\n\n    # Dynamic exploration factor based on selection count and time slot\n    exploration_factor = max(0.1, 1 - total_selection_count / (total_selection_count + 1))\n    \n    # Compute Upper Confidence Bounds (UCB)\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_interval\n        else:\n            ucb_values[i] = np.inf  # For actions not taken yet\n\n    # Softmax probabilities for selection\n    exp_ucb = np.exp(ucb_values)\n    softmax_probabilities = exp_ucb / np.sum(exp_ucb)\n\n    # Add exploration to softmax probabilities\n    adjusted_probabilities = (1 - exploration_factor) * softmax_probabilities + (exploration_factor / n_actions)\n\n    # Randomly select action based on adjusted probabilities\n    action_index = np.random.choice(n_actions, p=adjusted_probabilities)\n\n    return action_index",
          "objective": 27831.289554654417,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Initialize arrays for average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Compute average scores and selection counts\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if scores else 0\n\n    # Calculate UCB values\n    ucb_values = np.zeros(n_actions)\n    total_count_adjusted = total_selection_count + 1  # to avoid divide by zero\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt((2 * np.log(total_count_adjusted)) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_interval\n        else:\n            ucb_values[i] = np.inf  # Encourage selection of untried actions\n\n    # Adaptive Epsilon-Greedy Mechanism\n    initial_epsilon = 1.0\n    min_epsilon = 0.1\n    decay_rate = (initial_epsilon - min_epsilon) / total_time_slots\n    epsilon = max(initial_epsilon - decay_rate * current_time_slot, min_epsilon)\n\n    # Select Action based on Epsilon-Greedy Strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)  # Exploration\n    else:\n        action_index = action_indices[np.argmax(ucb_values)]  # Exploitation based on UCB\n\n    # Ensure action_index is within the range\n    action_index = max(0, min(action_index, 7))  # Range should be between 0 and 7\n    \n    # Logging mechanism\n    print(f\"Selected Action Index: {action_index}, Epsilon: {epsilon:.3f}, UCB Values: {ucb_values}, Avg Scores: {avg_scores}, Selection Counts: {selection_counts}\")\n\n    return action_index",
          "objective": 28424.701868645145,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Calculate selection counts and average scores\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if selection_counts[i] > 0 else 0\n\n    # Calculate epsilon for exploration\n    max_epsilon = 1.0\n    min_epsilon = 0.01\n    epsilon = max(min_epsilon, max_epsilon * (1 - current_time_slot / total_time_slots))\n\n    # Epsilon-greedy exploration\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)  # Exploration\n    else:\n        # Upper Confidence Bound (UCB) calculation for exploitation\n        ucb_values = np.zeros(n_actions)\n        for i in range(n_actions):\n            if selection_counts[i] > 0:\n                ucb_values[i] = avg_scores[i] + np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[i])\n            else:\n                ucb_values[i] = np.inf  # Assign infinity for unselected actions\n\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 28550.636647529896,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Initialize arrays for average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Compute average scores and selection counts\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if scores else 0.0\n\n    # Calculate UCB values\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_interval\n        else:\n            ucb_values[i] = np.inf  # Encourage selection of untried actions\n\n    # Flexible Epsilon-Greedy Mechanism\n    initial_epsilon = 1.0\n    min_epsilon = 0.1\n    decay_rate = (initial_epsilon - min_epsilon) / total_time_slots\n    epsilon = max(initial_epsilon - decay_rate * current_time_slot, min_epsilon)\n\n    # Action selection using Epsilon-Greedy and UCB\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)  # Exploration\n    else:\n        action_index = action_indices[np.argmax(ucb_values)]  # Exploitation based on UCB\n\n    # Logging mechanism for feedback and monitoring\n    print(f\"Selected Action Index: {action_index}, Epsilon: {epsilon:.3f}, \"\n          f\"UCB Values: {ucb_values}, Avg Scores: {avg_scores}, Selection Counts: {selection_counts}\")\n\n    return action_index",
          "objective": 28668.30133966287,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize arrays for average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    \n    # Compute average scores and selection counts\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if scores else 0.0\n\n    # Dynamic epsilon for exploration-exploitation balance\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01)\n    epsilon += (1 - total_selection_count / (total_selection_count + 1)) * 0.1  # adjust for selection count\n    \n    # Calculate UCB values\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_interval\n        else:\n            ucb_values[i] = np.inf  # Actions not selected yet\n\n    # Selection probabilities based on UCB\n    probabilities = (1 - epsilon) * (ucb_values / np.sum(ucb_values)) + epsilon / n_actions\n    action_index = np.random.choice(n_actions, p=probabilities)\n\n    return action_index",
          "objective": 28671.894350985676,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Initialize arrays for average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Compute average scores and selection counts\n    for i, action_index in enumerate(action_indices):\n        if score_set[action_index]:\n            scores = score_set[action_index]\n            selection_counts[i] = len(scores)\n            avg_scores[i] = np.mean(scores)\n        else:\n            selection_counts[i] = 0\n            avg_scores[i] = 0\n\n    # Calculate UCB values\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_interval\n        else:\n            ucb_values[i] = np.inf  # Encourage selection of untried actions\n\n    # Adaptive Epsilon-Greedy Mechanism\n    initial_epsilon = 1.0\n    min_epsilon = 0.1\n    decay_rate = (initial_epsilon - min_epsilon) / total_time_slots\n    epsilon = max(initial_epsilon - decay_rate * current_time_slot, min_epsilon)\n\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)  # Exploration\n    else:\n        action_index = action_indices[np.argmax(ucb_values)]  # Exploitation based on UCB\n\n    # Logging mechanism\n    print(f\"Selected Action Index: {action_index}, Epsilon: {epsilon:.3f}, UCB Values: {ucb_values}, Avg Scores: {avg_scores}, Selection Counts: {selection_counts}\")\n\n    return action_index",
          "objective": 28727.509965078825,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize arrays for average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Compute average scores and selection counts\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if selection_counts[i] > 0 else 0\n\n    # Calculate epsilon based on the current time slot\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01)\n\n    # Calculate UCB values\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_interval\n        else:\n            ucb_values[i] = np.inf  # Actions not selected yet\n\n    # Combine scores using epsilon-greedy\n    if np.random.rand() < epsilon:\n        # Exploration: randomly select one of the actions\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploitation: select action based on UCB values\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 28834.15344406811,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize arrays for average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Compute average scores and selection counts\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if scores else 0.0\n\n    # Dynamic epsilon based on current time slot and selection count\n    epsilon = max(0.1, 1.0 - (current_time_slot / total_time_slots))\n\n    # Calculate UCB values\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_interval\n        else:\n            ucb_values[i] = np.inf  # Actions not selected yet\n\n    # Combine exploration and exploitation using epsilon-greedy\n    probabilities = (1 - epsilon) * (ucb_values / np.sum(ucb_values)) + (epsilon / n_actions)\n    \n    # Ensure probabilities sum to 1\n    probabilities /= np.sum(probabilities)\n\n    # Select action based on calculated probabilities\n    action_index = np.random.choice(action_indices, p=probabilities)\n\n    return action_index",
          "objective": 29003.727357282773,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Compute average scores and selection counts\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if selection_counts[i] > 0 else 0.0\n\n    # Dynamic epsilon-greedy determination\n    initial_epsilon = 0.5  # Start with a high exploration probability\n    final_epsilon = 0.05   # Final minimum exploration probability\n    epsilon = max(final_epsilon, initial_epsilon * (1 - total_selection_count / (total_selection_count + 1)))\n    \n    # Variability assessment: standard deviation of scores\n    variability_scores = np.array([np.std(score_set[action_index]) if selection_counts[i] > 0 else 1 for i, action_index in enumerate(action_indices)])\n    \n    # Combine exploration (via epsilon) and exploitation using calculated scores\n    combined_scores = avg_scores + (1 / (1 + variability_scores))  # Adjust with variability    \n    ucb_values = np.zeros(n_actions)\n\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[i])\n            ucb_values[i] = combined_scores[i] + confidence_interval\n        else:\n            ucb_values[i] = np.inf  # Encourages selection for untried actions\n\n    # Select action using epsilon-greedy approach\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(n_actions)  # Explore\n    else:\n        action_index = np.random.choice(n_actions, p=ucb_values / np.sum(ucb_values))  # Exploit\n    \n    return action_index",
          "objective": 29022.71956698765,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialization\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and counts\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if scores else 0\n\n    # Dynamic exploration-exploitation balance\n    exploration_rate = 1.0 - (current_time_slot / total_time_slots)\n    exploration_rate = max(0.1, exploration_rate)  # Minimum exploration rate\n\n    # Compute UCB values\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_bound = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_bound\n        else:\n            ucb_values[i] = np.inf  # Prioritize unselected actions\n\n    # Epsilon-greedy decision-making\n    if np.random.random() < exploration_rate:\n        # Explore: select randomly from all actions\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploit: select the action with the highest UCB\n        action_index = action_indices[np.argmax(ucb_values)]\n\n    return action_index",
          "objective": 29293.451195644357,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize arrays for average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Compute average scores and selection counts\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if selection_counts[i] > 0 else 0.0\n\n    # Dynamic epsilon based on the current time slot\n    epsilon = max(0.5 * (1 - current_time_slot / total_time_slots), 0.05)\n\n    # Calculate UCB values\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_interval + epsilon / (selection_counts[i] + 1e-5)\n        else:\n            ucb_values[i] = np.inf  # Consider unselected actions as promising\n\n    # Epsilon-Greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.argmax(avg_scores)  # Favoring the action with the highest average score\n    else:\n        action_index = np.argmax(ucb_values)  # Exploit action with the highest UCB value\n\n    return action_index",
          "objective": 29598.05685981403,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Initialize arrays for average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts, avoid division by zero\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if scores else 0\n\n    # Dynamic epsilon parameter for Epsilon-Greedy strategy\n    epsilon = max(0.1 * (1 - total_selection_count / max(total_time_slots, 1)), 0.01)\n\n    # Calculate UCB values\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_interval\n        else:\n            ucb_values[i] = np.inf  # Action hasn't been selected yet\n\n    # Combine UCB and Epsilon-Greedy strategy\n    if np.random.rand() < epsilon or total_selection_count < 5:  # Favor exploration initially\n        action_index = np.random.choice(action_indices)\n    else:\n        # Weighted selection based on both UCB values and average scores\n        weighted_scores = avg_scores + ucb_values  # Combine criteria\n        action_index = np.argmax(weighted_scores)\n\n    return action_index",
          "objective": 30036.95520819124,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if selection_counts[i] > 0 else 0\n\n    # Dynamic epsilon calculation\n    max_epsilon = 1.0\n    min_epsilon = 0.01\n    decay_factor = 0.99\n    epsilon = max(min_epsilon, max_epsilon * (decay_factor ** current_time_slot))\n\n    # Calculate UCB values\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_interval\n        else:\n            ucb_values[i] = np.inf  # Not selected yet\n\n    # Select action based on epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        # Exploration: prioritize unexplored actions or random choice\n        unexplored_actions = np.where(selection_counts == 0)[0]\n        if len(unexplored_actions) > 0:\n            action_index = np.random.choice(unexplored_actions)  # Select unexplored actions\n        else:\n            action_index = np.random.choice(action_indices)  # Random choice among all actions\n    else:\n        # Exploitation: choose the best action based on UCB\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 30627.016355640404,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Calculate average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if scores else 0.0\n\n    # Adaptive epsilon based on selection count and time\n    epsilon = max(0.05, min(1.0, \n                             1.0 - (total_selection_count / (total_time_slots * 0.4))))\n\n    # Epsilon-Greedy selection with Upper Confidence Bound\n    if np.random.rand() < epsilon:\n        # Exploration phase\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploitation phase using UCB\n        ucb_values = np.zeros(n_actions)\n        for i in range(n_actions):\n            if selection_counts[i] > 0:\n                confidence_interval = np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts[i])\n                ucb_values[i] = avg_scores[i] + confidence_interval\n            else:\n                ucb_values[i] = np.inf  # action not yet taken maximizes UCB\n        \n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 30762.38233776095,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if selection_counts[i] > 0 else 0\n\n    # Dynamic epsilon parameter\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01)\n\n    # Calculate UCB values\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_interval\n        else:\n            ucb_values[i] = np.inf  # Not selected yet\n\n    # Select action based on epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)  # Exploration\n    else:\n        action_index = np.argmax(ucb_values)  # Exploitation\n\n    return action_index",
          "objective": 31080.05629218378,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if scores else 0\n\n    # Dynamic epsilon based on total selections\n    min_epsilon = 0.05\n    max_epsilon = 0.5\n    epsilon = min_epsilon + (max_epsilon - min_epsilon) * (total_selection_count / (total_time_slots + 1)) \\\n              if total_time_slots > 0 else max_epsilon\n    \n    # Compute UCB values\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_bound = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_bound\n        else:\n            ucb_values[i] = np.inf  # Prioritize unselected actions\n\n    # Epsilon-greedy decision-making\n    if np.random.random() < epsilon:\n        # Explore: select an action not previously selected with more probability\n        unselected_actions = [i for i, count in enumerate(selection_counts) if count == 0]\n        if unselected_actions:\n            action_index = np.random.choice(unselected_actions)\n        else:\n            action_index = np.random.choice(action_indices)  # Randomly select from all actions\n    else:\n        # Exploit: select the action with the highest UCB\n        action_index = action_indices[np.argmax(ucb_values)]\n\n    return action_index",
          "objective": 31190.699750847747,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\nimport logging\n\nlogging.basicConfig(level=logging.INFO)\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Compute average scores and selection counts\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if selection_counts[i] > 0 else 0\n\n    # Calculate epsilon: starts high and decreases over time\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n\n    # Calculate UCB values\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_interval\n        else:\n            ucb_values[i] = np.inf  # Initially consider unselected actions as promising\n\n    # Epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n        logging.info(f\"Exploration: Selected action {action_index} with epsilon {epsilon:.2f}\")\n    else:\n        action_index = np.argmax(ucb_values)\n        logging.info(f\"Exploitation: Selected action {action_index} with UCB values {ucb_values}\")\n\n    return action_index",
          "objective": 31661.594097669524,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Initialize arrays for average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Compute average scores and selection counts\n    for i, action_index in enumerate(action_indices):\n        if score_set[action_index]:\n            scores = score_set[action_index]\n            selection_counts[i] = len(scores)\n            avg_scores[i] = np.mean(scores)\n        else:\n            selection_counts[i] = 0\n            avg_scores[i] = 0\n\n    # Calculate UCB values\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_interval\n        else:\n            ucb_values[i] = np.inf  # Encourage selection of untried actions\n\n    # Epsilon decay strategy\n    initial_epsilon = 1.0\n    min_epsilon = 0.1\n    decay_rate = (initial_epsilon - min_epsilon) / total_time_slots\n    epsilon = max(initial_epsilon - decay_rate * current_time_slot, min_epsilon)\n\n    # Adaptive exploration-exploitation balance\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)  # Exploration\n    else:\n        action_index = action_indices[np.argmax(ucb_values)]  # Exploitation based on UCB\n\n    # Logging mechanism\n    log_data = {\n        \"selected_action\": action_index,\n        \"epsilon\": epsilon,\n        \"ucb_values\": ucb_values,\n        \"avg_scores\": avg_scores,\n        \"selection_counts\": selection_counts,\n    }\n    print(f\"Log: {log_data}\")\n\n    return action_index",
          "objective": 31952.077249499533,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize arrays for average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if selection_counts[i] > 0 else 0\n\n    # Dynamic epsilon calculation decreasing over time\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01)\n\n    # Calculate UCB values\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_interval\n        else:\n            ucb_values[i] = np.inf  # Unselected actions have infinite confidence\n\n    # Combine exploration and exploitation through weighted scores\n    exploration_weights = np.exp(-selection_counts / (total_selection_count + 1))  # Favor under-selected actions\n    combined_scores = (1 - epsilon) * ucb_values + epsilon * exploration_weights\n\n    # Epsilon-greedy choice\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = action_indices[np.argmax(combined_scores)]\n\n    return action_index",
          "objective": 32613.639042151422,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize arrays for average scores, selection counts, and exploration probabilities\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Compute average scores and selection counts\n    for i, action_index in enumerate(action_indices):\n        if score_set[action_index]:\n            scores = score_set[action_index]\n            selection_counts[i] = len(scores)\n            avg_scores[i] = np.mean(scores)\n        else:\n            selection_counts[i] = 0\n            avg_scores[i] = 0\n\n    # Define UCB values\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_interval\n        else:\n            ucb_values[i] = np.inf  # Encourage selection of untried actions\n\n    # Adaptive exploration rate based on current time slot\n    max_epsilon = 1.0\n    min_epsilon = 0.1\n    epsilon_decay = (max_epsilon - min_epsilon) / total_time_slots\n    epsilon = max(min_epsilon, max_epsilon - epsilon_decay * current_time_slot)\n\n    # Phased exploration/exploitation: Higher exploration at the start, lower later\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)  # Exploration\n    else:\n        action_index = action_indices[np.argmax(ucb_values)]  # Exploitation based on UCB\n\n    # Comprehensive logging\n    print(f\"Time Slot: {current_time_slot}, Selected Action Index: {action_index}, Epsilon: {epsilon:.3f}, Avg Scores: {avg_scores}, UCB Values: {ucb_values}\")\n\n    return action_index",
          "objective": 33339.601374105194,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize variables to track average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts for each action\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if selection_counts[i] > 0 else 0.0\n\n    # Dynamic exploration rate (epsilon), starting high and decaying\n    epsilon = 1.0 - (current_time_slot / (total_time_slots + 1))\n    epsilon = max(epsilon, 0.1)  # Minimum exploration\n\n    # Calculate UCB values\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        confidence_interval = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts[i] + 1e-5))\n        ucb_values[i] = avg_scores[i] + confidence_interval if selection_counts[i] > 0 else np.inf\n\n    # Select an action based on adaptive epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        # Exploration: select the action with the highest uncertainty (lowest selection counts)\n        exploration_weights = np.where(selection_counts > 0, 1 / (selection_counts + 1e-5), 1)\n        combined_scores = avg_scores * exploration_weights\n        action_index = np.argmax(combined_scores)\n    else:\n        # Exploitation: select action based on UCB values\n        action_index = np.argmax(ucb_values)\n\n    # Ensure action_index is within valid bounds\n    action_index = np.clip(action_index, 0, n_actions - 1)\n\n    return action_index",
          "objective": 33810.70624992373,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Calculate average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if len(scores) > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Epsilon decay strategy\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n\n    # Explore or exploit decision\n    if np.random.rand() < epsilon:\n        # Explore: Select action with the lowest selection count\n        action_index = action_indices[np.argmin(selection_counts)]\n    else:\n        # Exploit: Calculate UCB values for exploitation\n        ucb_values = np.zeros(n_actions)\n        for i in range(n_actions):\n            if selection_counts[i] > 0:\n                confidence_interval = np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts[i])\n                ucb_values[i] = avg_scores[i] + confidence_interval\n            else:\n                ucb_values[i] = np.inf  # Encourage exploration of untried actions\n\n        # Select the action with the highest UCB value\n        action_index = action_indices[np.argmax(ucb_values)]\n\n    return action_index",
          "objective": 33863.60689279209,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Initialize arrays for average scores, selection counts and UCB values\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    \n    # Compute average scores and selection counts\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if selection_counts[i] > 0 else 0\n        \n    # Calculate epsilon based on the current time slot\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01)\n\n    # Calculate UCB values\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_interval\n        else:\n            ucb_values[i] = np.inf  # Actions not selected yet\n    \n    # Combine scores using epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        # Exploration: randomly select one of the actions\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploitation: select action based on UCB values\n        action_index = np.argmax(ucb_values)\n    \n    return action_index",
          "objective": 34734.01161176042,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if len(scores) > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Adaptive epsilon calculation\n    epsilon = min(0.1, 1 - (current_time_slot / total_time_slots))\n\n    # Select action based on epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        # Explore: Select action with fewest historical selections\n        action_index = action_indices[np.argmin(selection_counts)]\n    else:\n        # Exploit: Calculate UCB values and select the best action\n        ucb_values = np.zeros(n_actions)\n        for i in range(n_actions):\n            if selection_counts[i] > 0:\n                confidence_interval = np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts[i])\n                ucb_values[i] = avg_scores[i] + confidence_interval\n            else:\n                ucb_values[i] = np.inf  # Encourage exploration of untried actions\n\n        # Select the action index with the highest UCB value\n        action_index = action_indices[np.argmax(ucb_values)]\n\n    return action_index",
          "objective": 35740.021926601155,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Calculate average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Dynamic epsilon for exploration\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n\n    # Hybrid selection mechanism: balancing exploration and exploitation\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_interval\n        else:\n            ucb_values[i] = np.inf  # Encourage exploration of untried actions\n\n    # Epsilon-Greedy Selection\n    if np.random.rand() < epsilon:\n        # Explore: Select lesser tried actions based on selection counts\n        probabilities = (1 / (selection_counts + 1))  # Adding 1 to avoid division by zero\n        action_index = np.random.choice(action_indices, p=probabilities / probabilities.sum())\n    else:\n        # Exploit: Select action with highest UCB value\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 36924.3830793349,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if selection_counts[i] > 0 else 0\n\n    # Calculate epsilon based on current time slot (p decreasing exploration)\n    exploration_ratio = 1.0 - (current_time_slot / total_time_slots)\n    epsilon = max(0.1 * exploration_ratio, 0.01)\n\n    # Calculate UCB values\n    ucb_values = avg_scores + np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n\n    # Epsilon-greedy decision\n    if np.random.rand() < epsilon:\n        # Exploration: select randomly from less explored actions\n        unexplored_actions = [i for i in range(n_actions) if selection_counts[i] == 0]\n        if unexplored_actions:\n            action_index = np.random.choice(unexplored_actions)\n        else:\n            action_index = np.random.choice(action_indices)\n    else:\n        # Exploitation: select the action with the maximum UCB value\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 38139.92550342649,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Calculate average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if selection_counts[i] > 0 else 0.0\n\n    # Decaying epsilon for exploration\n    initial_epsilon = 1.0\n    min_epsilon = 0.05\n    epsilon = max(min_epsilon, initial_epsilon * (1 - (current_time_slot / total_time_slots)))\n\n    # Epsilon-Greedy exploration vs exploitation decision\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)  # Exploration\n    else:\n        # Calculating UCB values for each action\n        ucb_values = np.zeros(n_actions)\n\n        for i in range(n_actions):\n            if selection_counts[i] > 0:\n                confidence_interval = np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts[i])\n                ucb_values[i] = avg_scores[i] + confidence_interval\n            else:\n                ucb_values[i] = float('inf')  # Prioritize unselected actions\n\n        action_index = np.argmax(ucb_values)  # Exploitation\n    \n    return action_index",
          "objective": 38296.92349346672,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Calculate average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if selection_counts[i] > 0 else 0.0\n\n    # Calculate epsilon (decay from initial to minimum)\n    initial_epsilon = 1.0\n    min_epsilon = 0.05\n    epsilon = max(min_epsilon, initial_epsilon * (1 - current_time_slot / total_time_slots))\n\n    # Epsilon-Greedy selection\n    if np.random.rand() < epsilon:\n        # Exploration phase: select a random action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploitation phase using UCB\n        ucb_values = np.zeros(n_actions)\n\n        for i in range(n_actions):\n            if selection_counts[i] > 0:\n                confidence_interval = np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts[i])\n                ucb_values[i] = avg_scores[i] + confidence_interval\n            else:\n                # Assign a very high initial value for unselected actions to encourage exploration\n                ucb_values[i] = float('inf')\n\n        # Select action with the highest UCB value\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 38413.25127199567,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Adaptive epsilon for exploration\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n\n    # Select action based on exploration or exploitation\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        # Calculate UCB values for exploitation\n        ucb_values = np.zeros(n_actions)\n        for i in range(n_actions):\n            if selection_counts[i] > 0:\n                confidence_interval = np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts[i])\n                ucb_values[i] = avg_scores[i] + confidence_interval\n            else:\n                ucb_values[i] = np.inf  # Encourage exploration of untried actions\n\n        # Select the action index with the highest UCB value\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 39639.07257681301,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize arrays for average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Compute average scores and counts\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if selection_counts[i] > 0 else 0\n\n    # Dynamic epsilon for exploration\n    epsilon_initial = 1.0\n    epsilon_final = 0.01\n    epsilon_decay_steps = total_time_slots\n    epsilon = epsilon_final + (epsilon_initial - epsilon_final) * (1 - current_time_slot / epsilon_decay_steps)\n\n    # Calculate UCB values\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_interval\n        else:\n            ucb_values[i] = np.inf  # Prioritize unselected actions\n\n    # Exploration bonus\n    exploration_bonus = np.zeros(n_actions)\n    if total_selection_count > 0:\n        exploration_bonus = (1 - epsilon) * (selection_counts / total_selection_count)\n    \n    # Combine UCB with exploration factor\n    combined_scores = ucb_values - exploration_bonus\n\n    # Select action based on the highest combined score\n    action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": 40449.336853736626,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Calculate average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Calculate epsilon based on current time slot\n    epsilon = max(0.05, min(1.0, 1.0 - (total_selection_count / (total_time_slots * 0.4))))\n\n    # Determine selection criteria with a blend of exploration and exploitation\n    exploration_factor = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))  # Avoid division by zero\n    ucb_values = avg_scores + exploration_factor\n\n    # Epsilon-Greedy decision making\n    if np.random.rand() < epsilon:\n        # Exploration - Select less frequently chosen action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploitation - Select action with the highest UCB value\n        action_index = action_indices[np.argmax(ucb_values)]\n\n    return action_index",
          "objective": 41124.16646431839,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Calculate average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if selection_counts[i] > 0 else 0.0\n\n    # Adaptive epsilon calculation\n    initial_epsilon = 1.0\n    min_epsilon = 0.05\n    decay_rate = (initial_epsilon - min_epsilon) / total_time_slots\n    epsilon = max(min_epsilon, initial_epsilon - decay_rate * current_time_slot)\n\n    # Epsilon-Greedy selection\n    if np.random.rand() < epsilon:\n        # Exploration phase\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploitation phase using a modified UCB\n        ucb_values = np.zeros(n_actions)\n        total_scores = np.sum(avg_scores)  # Total score for normalization\n        \n        for i in range(n_actions):\n            if selection_counts[i] > 0:\n                confidence_interval = np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts[i])\n                ucb_values[i] = avg_scores[i] + confidence_interval\n            else:\n                # Assign a very high initial value for unselected actions\n                ucb_values[i] = np.inf\n\n        # Normalize UCB values to prevent preference for extremes\n        ucb_values = (ucb_values - np.min(ucb_values)) / (np.max(ucb_values) - np.min(ucb_values) + 1e-10)\n        \n        # Select action with highest UCB value\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 41232.24159986447,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Dynamic epsilon for exploration\n    epsilon = 1 - (current_time_slot / total_time_slots)\n    epsilon = max(0.05, min(epsilon, 0.1))  # Range between 0.05 and 0.1\n\n    # Calculate UCB values\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_interval\n        else:\n            ucb_values[i] = np.inf  # Encourage exploration of untried actions\n\n    # Epsilon-Greedy selection mechanism\n    if np.random.rand() < epsilon:\n        # Explore: Select less-tried actions\n        exploration_probs = (1 / (selection_counts + 1))  # Add 1 to avoid division by zero\n        exploration_probs /= exploration_probs.sum()  # Normalize probabilities\n        action_index = np.random.choice(action_indices, p=exploration_probs)\n    else:\n        # Exploit: Select action with highest UCB value\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 41538.937323353864,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Calculate average scores and counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if selection_counts[i] > 0 else 0\n\n    # Adaptive epsilon for exploration\n    epsilon = 0.1 * (1 - (current_time_slot / total_time_slots))\n\n    # Compute UCB values\n    ucb_values = np.zeros(n_actions)\n    confidence_intervals = np.zeros(n_actions)\n\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_interval\n            confidence_intervals[i] = confidence_interval\n        else:\n            ucb_values[i] = np.inf  # Unselected actions\n\n    # Combine UCB with an exploration term\n    exploration_bonus = (1 - epsilon) * (1 - selection_counts / total_selection_count) if total_selection_count > 0 else np.ones(n_actions)\n    combined_scores = ucb_values + exploration_bonus\n\n    # Select action based on the highest combined score\n    action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": 41625.50138254597,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Dynamic Epsilon-Greedy strategy\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n\n    # Select a random action for exploration with probability epsilon\n    if np.random.rand() < epsilon:\n        return np.random.choice(action_indices)\n\n    # Calculate modified UCB values\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_interval\n        else:\n            # Encourage exploration for unselected actions\n            ucb_values[i] = np.inf\n\n    # Select the action index with the highest UCB value\n    action_index = np.argmax(ucb_values)\n    \n    return action_index",
          "objective": 41964.73507671871,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Calculate average scores and counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if scores else 0\n\n    # Dynamic epsilon based on time slot\n    epsilon = max(0.1, 0.5 * (1 - (current_time_slot / total_time_slots)))\n\n    # Compute UCB values\n    ucb_values = np.zeros(n_actions)\n    \n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_interval\n        else:\n            ucb_values[i] = np.inf  # Unexplored action\n\n    # Weighted combination of UCB and random exploration\n    combined_scores = (1 - epsilon) * ucb_values + epsilon * np.random.rand(n_actions)\n\n    # Select action based on the combined scores\n    action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": 42802.248427940285,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize arrays for average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Compute average scores and selection counts\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if selection_counts[i] > 0 else 0\n\n    # Dynamic epsilon for exploration\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n\n    # Calculate UCB values\n    ucb_values = np.zeros(n_actions)\n\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_interval\n        else:\n            ucb_values[i] = np.inf  # Actions not selected yet\n\n    # Adjust UCB values with exploration term\n    exploration_factor = (1 - epsilon) * (1 - selection_counts / total_selection_count) if total_selection_count > 0 else np.ones(n_actions)\n    combined_scores = ucb_values + exploration_factor\n\n    # Select action based on the highest combined score\n    action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": 43906.95913507173,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize arrays for average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Compute average scores and selection counts\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if selection_counts[i] > 0 else 0\n\n    # Exploration vs. Exploitation Strategy\n    initial_epsilon = 1.0\n    min_epsilon = 0.1\n    epsilon_decay_rate = (initial_epsilon - min_epsilon) / total_time_slots\n    epsilon = max(initial_epsilon - epsilon_decay_rate * current_time_slot, min_epsilon)\n\n    # Calculate UCB values\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_interval\n        else:\n            ucb_values[i] = np.inf  # Actions not selected yet\n\n    # Epsilon-Greedy Mechanism for exploration vs. exploitation\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)  # Exploration\n    else:\n        action_index = action_indices[np.argmax(ucb_values)]  # Exploitation\n\n    # Logging mechanism (simple print for demonstration, can be enhanced)\n    print(f\"Selected Action Index: {action_index}, Epsilon: {epsilon:.3f}, UCB Values: {ucb_values}\")\n\n    return action_index",
          "objective": 47698.66395247692,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Calculate average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if selection_counts[i] > 0 else 0.0\n\n    # Calculate epsilon\n    epsilon = max(0.05, min(1.0, 1.0 - (total_selection_count / (total_time_slots * 0.4))))\n\n    # Epsilon-Greedy selection with UCB\n    if np.random.rand() < epsilon:\n        # Exploration phase\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploitation phase using UCB\n        ucb_values = np.zeros(n_actions)\n        for i in range(n_actions):\n            if selection_counts[i] > 0:\n                confidence_interval = np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts[i])\n                ucb_values[i] = avg_scores[i] + confidence_interval\n            else:\n                # Assign the highest possible UCB value\n                ucb_values[i] = np.inf\n\n        # Select action with highest UCB value\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 48414.15872306189,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Dynamic epsilon for exploration\n    exploration_factor = np.clip(1 - (current_time_slot / total_time_slots), 0, 1) * 0.9 + 0.1\n\n    # Select action using a hybrid of Epsilon-Greedy and UCB\n    if np.random.rand() < exploration_factor:\n        # Explore: select a random action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploit: Calculate modified UCB values considering average scores\n        ucb_values = np.zeros(n_actions)\n        for i in range(n_actions):\n            if selection_counts[i] > 0:\n                confidence_interval = np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts[i])\n                ucb_values[i] = avg_scores[i] + confidence_interval\n            else:\n                # Encourage exploration if the action has never been selected\n                ucb_values[i] = np.inf\n\n        # Select the action with the highest UCB value\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 48504.78898461518,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores, selection counts, and initial exploration factor\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Epsilon for exploration, decaying over time\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n\n    # Use a random action if exploration is chosen\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        # Calculate UCB values for exploitation\n        ucb_values = np.zeros(n_actions)\n        for i in range(n_actions):\n            if selection_counts[i] > 0:\n                confidence_interval = np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts[i])\n                ucb_values[i] = avg_scores[i] + confidence_interval\n            else:\n                # Encourage exploration if action has never been selected\n                ucb_values[i] = np.inf\n        \n        # Select the action index with the highest UCB value\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 48531.89201451042,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Calculate exploration decay based on current time slot\n    exploration_decay = 1 - (current_time_slot / total_time_slots)\n    \n    # Calculate UCB values\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + exploration_decay * confidence_interval\n        else:\n            # If the action has never been selected, encourage exploration\n            ucb_values[i] = np.inf\n\n    # Select the action index with the highest UCB value\n    action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 50230.269928063564,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize action indices\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Calculate average scores and selection counts\n    avg_scores = np.array([np.mean(score_set[action_index]) if score_set[action_index] else 0 \n                           for action_index in action_indices])\n    selection_counts = np.array([len(score_set[action_index]) for action_index in action_indices])\n\n    # Dynamic epsilon for exploration\n    epsilon = max(0.01, 1 - (float(current_time_slot) / total_time_slots))\n\n    # Compute UCB values\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_interval\n        else:\n            ucb_values[i] = np.inf  # Favor unexplored actions\n\n    # Combine UCB with dynamic epsilon-greedy strategy\n    random_component = np.random.rand(n_actions)\n    combined_scores = (1 - epsilon) * ucb_values + epsilon * random_component\n\n    # Select the action based on the combined scores\n    action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": 51258.2530464101,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Calculate average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if selection_counts[i] > 0 else 0.0\n\n    # Epsilon adjustment: Decaying epsilon strategy\n    epsilon = max(0.1, min(1.0, 1.0 - (total_selection_count / (total_time_slots * 0.5))))\n    \n    # Calculate UCB values\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_interval\n        else:\n            ucb_values[i] = np.inf  # High initial value for unselected actions\n\n    # Epsilon-Greedy decision\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 51796.62789268088,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Calculate average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if scores else 0\n\n    # Dynamic epsilon based on the elapsed proportion of time\n    epsilon = max(0.1, 0.5 * (1 - (current_time_slot / total_time_slots)))\n\n    # Compute UCB values\n    ucb_values = np.zeros(n_actions)\n    \n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_interval\n        else:\n            ucb_values[i] = np.inf  # Mark unexplored action\n\n    # Combine UCB with random exploration based on epsilon\n    exploration_bonus = epsilon * np.random.rand(n_actions)\n    combined_scores = (1 - epsilon) * ucb_values + exploration_bonus\n\n    # Select action based on the maximum score\n    action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": 51940.56032605008,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialization\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Compute average scores and selection counts\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if selection_counts[i] > 0 else 0\n\n    # Dynamic epsilon for exploration\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n\n    # Calculate UCB with variance\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_interval\n        else:\n            ucb_values[i] = np.inf  # Favor actions that haven't been selected\n\n    # Incorporate both exploration and exploitation\n    exploration_scores = np.random.rand(n_actions) * epsilon\n    combined_scores = (1 - epsilon) * ucb_values + exploration_scores\n\n    # Select action based on combined scores\n    action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": 52809.4702340942,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if selection_counts[i] > 0 else 0\n\n    # Dynamic epsilon for exploration\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n\n    # Calculate UCB scores\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_interval\n        else:\n            ucb_values[i] = np.inf   # Favor unexplored actions\n\n    # Compute exploration vs exploitation scores\n    exploration_scores = np.random.rand(n_actions) * epsilon\n    combined_scores = (1 - epsilon) * ucb_values + exploration_scores\n\n    # Select action based on combined scores\n    action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": 53031.31425301185,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if selection_counts[i] > 0 else 0\n\n    # Dynamic epsilon for exploration\n    epsilon = np.clip(1 - (current_time_slot / total_time_slots), 0.1, 1.0)\n\n    # Calculate UCB scores\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_interval\n        else:\n            ucb_values[i] = np.inf  # Favor unexplored actions\n\n    # Exploration bonus based on average score\n    exploration_scores = np.random.rand(n_actions) * epsilon\n  \n    # Combined scores\n    combined_scores = (1 - epsilon) * ucb_values + exploration_scores\n\n    # Select action based on combined scores\n    action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": 53187.51524926233,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores, selection counts, and an array for UCB values\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Epsilon for exploration, decaying over time\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n\n    # Select an action based on the exploration-exploitation strategy\n    if np.random.rand() < epsilon:\n        # Exploration: choose randomly\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploitation: calculate UCB values\n        ucb_values = np.zeros(n_actions)\n        for i in range(n_actions):\n            if selection_counts[i] > 0:\n                confidence_interval = np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts[i])\n                ucb_values[i] = avg_scores[i] + confidence_interval\n            else:\n                # Encourage exploration of unselected actions\n                ucb_values[i] = np.inf\n        \n        # Choose the action with the highest UCB value\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 53654.29598039266,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Set epsilon that decays over time and total selections\n    epsilon = max(0.05, min(1.0, 1.0 - (total_selection_count / (total_time_slots * 0.4))))\n    \n    # Select action based on Epsilon-Greedy or UCB\n    if np.random.rand() < epsilon:\n        # Exploration: Randomly select an action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploitation: Calculate UCB values\n        ucb_values = np.zeros(n_actions)\n        for i in range(n_actions):\n            if selection_counts[i] > 0:\n                confidence_interval = np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts[i])\n                ucb_values[i] = avg_scores[i] + confidence_interval\n            else:\n                # Encourage exploration for unselected actions\n                ucb_values[i] = np.inf\n\n        # Select the action index with the highest UCB value\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 54680.640288004884,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize action indices\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Dynamic epsilon for exploration\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots) ** 2)\n\n    # Select action using Epsilon-Greedy strategy\n    if np.random.rand() < epsilon:\n        # Explore: Choose a random action from less selected actions\n        action_index = np.random.choice([i for i in action_indices if selection_counts[i] == 0] or action_indices)\n    else:\n        # Exploit: Calculate UCB values, emphasizing the uncertainty\n        ucb_values = np.zeros(n_actions)\n        for i in range(n_actions):\n            if selection_counts[i] > 0:\n                confidence_interval = np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts[i])\n                ucb_values[i] = avg_scores[i] + confidence_interval\n            else:\n                ucb_values[i] = np.inf  # Encourage exploration of never-selected actions\n\n        # Select the action with the highest UCB value\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 55310.874684979804,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Epsilon decay strategy\n    epsilon = max(0.1, 1 - (current_time_slot / (total_time_slots // 2)))\n\n    # Calculate UCB values\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_interval\n        else:\n            ucb_values[i] = np.inf  # Encourage exploration for untried actions\n\n    # Choose an action based on the hybrid strategy\n    if np.random.rand() < epsilon:\n        # Exploration: select a random action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploitation: select action based on UCB\n        action_index = action_indices[np.argmax(ucb_values)]\n\n    return action_index",
          "objective": 55456.453348720264,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize arrays for average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Adaptive epsilon setup\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n    \n    # Upper Confidence Bound (UCB) values calculation\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_interval\n        else:\n            ucb_values[i] = np.inf  # Encourage exploration of untried actions\n\n    # Epsilon-Greedy Selection combined with UCB\n    if np.random.rand() < epsilon:\n        # Explore by selecting action inversely proportional to selection counts\n        if np.any(selection_counts > 0):\n            probabilities = (1 / (selection_counts + 1))  # Avoid division by zero\n            action_index = np.random.choice(action_indices, p=probabilities / probabilities.sum())\n        else:\n            action_index = np.random.choice(action_indices)  # All actions untried\n    else:\n        # Exploit by selecting action with the highest UCB value\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 56272.84035856435,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize action indices\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Calculate average scores and selection counts\n    avg_scores = np.array([np.mean(score_set[i]) if len(score_set[i]) > 0 else 0 for i in action_indices])\n    selection_counts = np.array([len(score_set[i]) for i in action_indices])\n\n    # Dynamic epsilon for exploration\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n\n    # Initialize UCB values\n    ucb_values = np.zeros(n_actions)\n    \n    total_seconds = total_selection_count + 1  # To avoid division by zero in logarithm\n\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt((2 * np.log(total_seconds)) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_interval\n        else:\n            ucb_values[i] = np.inf  # Encourage exploration for untried actions\n\n    # Decision making: Epsilon-Greedy + UCB\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)  # Exploration\n    else:\n        action_index = action_indices[np.argmax(ucb_values)]  # Exploitation based on UCB\n\n    return action_index",
          "objective": 57192.3259211978,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if selection_counts[i] > 0 else 0\n\n    # Epsilon-Greedy parameters\n    epsilon_start = 1.0\n    epsilon_end = 0.1\n    epsilon_decay_time = total_time_slots\n    epsilon = max(epsilon_start * (1 - current_time_slot / epsilon_decay_time), epsilon_end)\n\n    # Randomly explore with probability epsilon\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n        return action_index\n\n    # Calculate UCB values\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_interval\n        else:\n            # Encourage exploration if the action has never been selected\n            ucb_values[i] = np.inf\n\n    # Select the action index with the highest UCB value\n    action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 57560.38750004233,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Adaptive epsilon-greedy exploration\n    epsilon = max(0.05, 1 - (current_time_slot / total_time_slots) ** 2)\n\n    # UCB exploration factor\n    total_count_adjusted = total_selection_count + 1  # Avoid division by zero\n    exploration_bonus = np.sqrt((2 * np.log(total_count_adjusted)) / (selection_counts + 1e-5))\n\n    # Calculate final scores\n    final_scores = avg_scores + exploration_bonus\n\n    # Epsilon-greedy strategy implementation\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(n_actions)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": 58494.81827278285,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Decay epsilon based on time and total selections\n    epsilon = max(0.1, 1.0 - (total_selection_count / (total_time_slots * 0.5)))\n    exploration = np.random.rand() < epsilon\n\n    if exploration:\n        # Exploration: Randomly select an action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploitation: Calculate UCB values\n        ucb_values = np.full(n_actions, np.inf)\n        for i in range(n_actions):\n            if selection_counts[i] > 0:\n                confidence_interval = np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts[i])\n                ucb_values[i] = avg_scores[i] + confidence_interval\n\n        # Select the action index with the highest UCB value\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 58513.89290863424,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Calculate average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if selection_counts[i] > 0 else 0\n\n    # Epsilon decay for exploration\n    epsilon = 1.0 - (current_time_slot / total_time_slots) * 0.9\n    epsilon = max(epsilon, 0.1)  # Ensuring it doesn\u2019t drop below 0.1\n\n    # Compute UCB values\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_interval\n        else:\n            ucb_values[i] = np.inf  # Unselected actions have infinite potential\n\n    # Combine UCB and exploration\n    combined_scores = (1 - epsilon) * ucb_values + epsilon * (1 - selection_counts / (total_selection_count + 1))\n\n    # Select action based on the highest combined score\n    action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": 58595.860218132504,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    \n    for action_index in action_indices:\n        scores = score_set[action_index]\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n    \n    # Explore vs. exploit factor\n    exploration_factor = max(0.1, (total_time_slots - current_time_slot) / total_time_slots)  # Decreases as time progresses\n    exploration_threshold = (1 / (selection_counts + 1e-5)) * exploration_factor  # Dynamic exploration factor\n\n    # Calculate scores with adjusted exploration bonus\n    adjusted_scores = avg_scores + exploration_threshold\n\n    # Select the action with the highest adjusted score\n    action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 59240.82254535033,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n    \n    # Epsilon-Greedy exploration probability\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n\n    # Calculate UCB values\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_interval\n        else:\n            ucb_values[i] = np.inf  # Encourage exploration for unselected actions\n\n    # Hybrid strategy: Epsilon-Greedy and UCB\n    if np.random.rand() < epsilon:\n        # Explore randomly\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploit by selecting the action with the highest UCB value\n        action_index = action_indices[np.argmax(ucb_values)]\n\n    return action_index",
          "objective": 59961.89684477948,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize action indices\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if selection_counts[i] > 0 else 0\n\n    # Dynamic epsilon for exploration\n    epsilon = max(0.1, 1 - (float(current_time_slot) / total_time_slots))\n\n    # Compute UCB values\n    ucb_values = np.zeros(n_actions)\n    \n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_interval\n        else:\n            ucb_values[i] = np.inf  # Favor unexplored actions\n\n    # Combine UCB with Epsilon-Greedy strategy for action selection\n    combined_scores = (1 - epsilon) * ucb_values + epsilon * np.random.rand(n_actions)\n\n    # Select action based on the combined scores\n    action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": 60191.39070282915,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Calculate exploration factor (Epsilon-Greedy approach)\n    epsilon = 0.1 * (1 - current_time_slot / total_time_slots)\n\n    # Choose a random action with probability epsilon for exploration\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        # Calculate UCB values\n        ucb_values = np.zeros(n_actions)\n        for i in range(n_actions):\n            if selection_counts[i] > 0:\n                confidence_interval = np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts[i])\n                ucb_values[i] = avg_scores[i] + confidence_interval\n            else:\n                # If the action has never been selected, encourage exploration\n                ucb_values[i] = np.inf\n\n        # Select the action index with the highest UCB value\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 61038.643187408496,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8  # action indices from 0 to 7\n    \n    # Initialize arrays to store average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Compute average scores and selection counts\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        avg_scores[action_index] = np.mean(scores) if selection_counts[action_index] > 0 else 0\n\n    # Dynamic exploration factor reducing over time\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n\n    # Calculate UCB values\n    total_selections = np.sum(selection_counts) + 1e-5  # Added small constant for stability\n    ucb_values = np.zeros(n_actions)\n\n    for action_index in range(n_actions):\n        if selection_counts[action_index] > 0:\n            ucb_values[action_index] = avg_scores[action_index] + \\\n                np.sqrt((2 * np.log(total_selections)) / selection_counts[action_index])\n        else:\n            ucb_values[action_index] = float('inf')  # Encourage exploration for actions never selected\n\n    # Combine Epsilon-Greedy and UCB approaches\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(n_actions)\n    else:\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 62007.08565851477,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8  # action indices from 0 to 7\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n\n    # Epsilon-Greedy exploration factor\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n\n    # UCB calculation\n    total_selection_counts = np.sum(selection_counts)\n    ucb_values = np.zeros(n_actions)\n    for action_index in range(n_actions):\n        if selection_counts[action_index] > 0:\n            ucb_values[action_index] = avg_scores[action_index] + \\\n                np.sqrt((2 * np.log(total_selection_counts + 1)) / (selection_counts[action_index] + 1e-5))\n        else:\n            ucb_values[action_index] = float('inf')  # Encourage exploration for unselected actions\n\n    # Combine Epsilon-Greedy and UCB\n    if np.random.rand() < epsilon:\n        # Select randomly among all actions\n        action_index = np.random.choice(n_actions)\n    else:\n        # Select based on adjusted UCB values\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 62552.78050274531,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Average scores and selection counts initialization\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Dynamic epsilon parameter\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n\n    # Explore with epsilon probability\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        # Calculate UCB values for exploitation\n        ucb_values = np.zeros(n_actions)\n        for i in range(n_actions):\n            if selection_counts[i] > 0:\n                confidence_interval = np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts[i])\n                ucb_values[i] = avg_scores[i] + confidence_interval\n            else:\n                ucb_values[i] = np.inf  # Encourage exploration for unselected actions\n        \n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 63257.88371110734,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize action indices\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if selection_counts[i] > 0 else 0\n\n    # Compute dynamic epsilon for exploration\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots)**2)\n\n    # Compute UCB values\n    ucb_values = np.zeros(n_actions)\n\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_interval\n        else:\n            ucb_values[i] = np.inf  # Favor unexplored actions\n\n    # Combine UCB with Epsilon-Greedy strategy for action selection\n    random_exploration = np.random.rand(n_actions) < epsilon\n    combined_scores = (1 - random_exploration) * ucb_values + random_exploration * np.random.rand(n_actions)\n\n    # Select action based on the combined scores\n    action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": 63291.409079257304,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize action indices\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Dynamic epsilon for exploration\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n\n    # Select action using Epsilon-Greedy strategy with UCB for exploitation\n    if np.random.rand() < epsilon:\n        # Explore: Choose a random action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploit: Calculate modified UCB values\n        ucb_values = np.zeros(n_actions)\n        for i in range(n_actions):\n            if selection_counts[i] > 0:\n                confidence_interval = np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts[i])\n                ucb_values[i] = avg_scores[i] + confidence_interval\n            else:\n                # Encourage exploration if the action has never been selected\n                ucb_values[i] = np.inf\n        \n        # Select the action with highest UCB value\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 63297.58985576887,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if scores else 0\n\n    # Dynamic epsilon calculation\n    eps_max = 0.9\n    eps_min = 0.1\n    decay_rate = (eps_max - eps_min) / total_time_slots if total_time_slots > 0 else 0\n    epsilon = max(eps_min, eps_max - decay_rate * current_time_slot)\n\n    # Calculate UCB values\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_bound = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_bound\n        else:\n            ucb_values[i] = np.inf  # Prioritize unselected actions\n\n    # Epsilon-greedy decision-making\n    if np.random.rand() < epsilon:\n        # Explore: select randomly from all actions\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploit: select the action with the highest UCB\n        action_index = action_indices[np.argmax(ucb_values)]\n\n    return action_index",
          "objective": 63640.343186378086,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if selection_counts[i] > 0 else 0\n\n    # Dynamic epsilon for exploration\n    epsilon = max(0.1, 1.0 - (current_time_slot / total_time_slots))\n\n    # Generate Thompson Sampling beta distributions for each action\n    beta_samples = np.random.beta(avg_scores + 1, (1 - avg_scores) + 1)\n\n    # Combine UCB and Thompson Sampling scores\n    combined_scores = (1 - epsilon) * beta_samples + epsilon * np.random.rand(n_actions)\n\n    # Favor unexplored actions\n    combined_scores[selection_counts == 0] = np.inf\n\n    # Select action based on combined scores\n    action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": 64635.054198599166,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Adaptive epsilon based on current time slot\n    epsilon = max(0.05, 1.0 - (current_time_slot / total_time_slots) * 0.95)\n\n    # Calculate UCB values\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_interval\n        else:\n            ucb_values[i] = np.inf  # Encourage exploration of unselected actions\n\n    # Epsilon-Greedy with UCB\n    if np.random.rand() < epsilon:\n        # Exploration: Select an action uniformly at random\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploitation: Select action with the highest UCB value\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 66202.20738724759,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Calculate epsilon for adaptive exploration\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n\n    # Determine whether to explore or exploit based on epsilon\n    if np.random.rand() < epsilon:\n        # Exploration: Select an action uniformly at random\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploitation: Calculate UCB values\n        ucb_values = np.zeros(n_actions)\n        for i in range(n_actions):\n            if selection_counts[i] > 0:\n                confidence_interval = np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts[i])\n                ucb_values[i] = avg_scores[i] + confidence_interval\n            else:\n                # If the action has never been selected, encourage exploration\n                ucb_values[i] = np.inf\n                \n        # Select the action with the highest UCB value\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 68675.3550950901,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    \n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if selection_counts[i] > 0 else 0.0\n\n    # Define epsilon with decaying strategy\n    epsilon = max(0.05, min(1.0, 1.0 - (total_selection_count / (total_time_slots * 0.4))))\n\n    # Early game exploration vs late game exploitation\n    if np.random.rand() < epsilon:\n        # Exploration phase\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploitation phase with UCB\n        ucb_values = np.zeros(n_actions)\n        for i in range(n_actions):\n            if selection_counts[i] > 0:\n                confidence_interval = np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts[i])\n                ucb_values[i] = avg_scores[i] + confidence_interval\n            else:\n                # Assign a high UCB value for actions that have never been selected\n                ucb_values[i] = np.inf\n\n        # Select the action with the highest UCB value\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 68775.86002491148,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Epsilon-Greedy Exploration Rate\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n    \n    # Calculate UCB values\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_interval\n        else:\n            ucb_values[i] = np.inf  # Untried actions get an infinite UCB value to encourage exploration\n\n    # Random component to encourage exploration\n    random_exploration = np.random.rand(n_actions) * epsilon\n\n    # Total scores combining UCB and random exploration\n    total_scores = (1 - epsilon) * ucb_values + random_exploration\n\n    # Select action based on computed scores\n    action_index = np.argmax(total_scores)\n\n    return action_index",
          "objective": 71658.7365393201,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Calculate epsilon with linear decay\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n    random_value = np.random.rand()\n\n    # Epsilon-greedy selection logic\n    if random_value < epsilon:\n        action_index = np.random.choice(action_indices)  # Exploration\n    else:\n        # UCB calculation for exploitation\n        ucb_scores = np.zeros(n_actions)\n        for i in range(n_actions):\n            if selection_counts[i] > 0:\n                ucb_scores[i] = avg_scores[i] + np.sqrt(2 * np.log(total_selection_count) / selection_counts[i])\n            else:\n                ucb_scores[i] = float('inf')  # Encourage exploration of unselected actions\n\n        action_index = np.argmax(ucb_scores)  # Exploitation\n\n    return action_index",
          "objective": 74420.31618773077,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8  # action indices from 0 to 7\n    \n    # Initialize arrays to store average scores, selection counts and variances\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Compute average scores and selection counts\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        if selection_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n\n    # Dynamic exploration factor (Epsilon)\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n\n    # Calculate variance for each score set to encourage exploration of diverse actions\n    variances = np.zeros(n_actions)\n    for action_index in range(n_actions):\n        if selection_counts[action_index] > 1:  # Need at least 2 samples to compute variance\n            variances[action_index] = np.var(score_set[action_index])\n    \n    # Calculate UCB values with variance considered\n    total_selection_counts = np.sum(selection_counts)\n    ucb_values = np.zeros(n_actions)\n    for action_index in range(n_actions):\n        if selection_counts[action_index] > 0:\n            ucb_values[action_index] = avg_scores[action_index] + \\\n                np.sqrt((2 * np.log(total_selection_counts + 1)) / (selection_counts[action_index] + 1e-5)) + \\\n                0.1 * np.sqrt(variances[action_index])  # Variance bonus for exploration\n        else:\n            ucb_values[action_index] = float('inf')  # Encourage exploration for actions never selected\n\n    # Decision making: Epsilon-Greedy + UCB\n    if np.random.rand() < epsilon:\n        # Select randomly among all actions\n        action_index = np.random.choice(n_actions)\n    else:\n        # Select based on adjusted UCB values\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 75407.08733438188,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    \n    # Calculate average scores and selection counts\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n    \n    # Epsilon for exploration, decaying over time\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n    \n    # Decide between exploration and exploitation\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        # Calculate UCB values for all actions to exploit\n        ucb_values = np.zeros(n_actions)\n        for i in range(n_actions):\n            if selection_counts[i] > 0:\n                confidence_interval = np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts[i])\n                ucb_values[i] = avg_scores[i] + confidence_interval\n            else:\n                ucb_values[i] = np.inf  # Prioritize unselected actions\n        \n        # Select the action with the highest UCB value\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 75667.71965345641,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Adaptive exploration rate (epsilon)\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n\n    # Calculate UCB values\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts[i])\n            # Combine UCB with avg scores and exploration factor\n            ucb_values[i] = avg_scores[i] + confidence_interval\n        else:\n            # If the action has never been selected, encourage exploration\n            ucb_values[i] = np.inf\n\n    # Epsilon-Greedy selection mechanism\n    if np.random.rand() < epsilon:\n        # Explore: Select a random action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploit: Select the action with the highest UCB value\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 77226.6307856788,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Adaptive epsilon\n    epsilon = max(0.1, 1.0 - (current_time_slot / total_time_slots))\n\n    # Explore with probability epsilon\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        # Calculate UCB values for exploitation\n        ucb_values = np.zeros(n_actions)\n        for i in range(n_actions):\n            if selection_counts[i] > 0:\n                confidence_interval = np.sqrt(2 * np.log(total_selection_count) / selection_counts[i])\n                ucb_values[i] = avg_scores[i] + confidence_interval\n            else:\n                ucb_values[i] = np.inf  # Encourage exploration of untried actions\n\n        # Select the action index with the highest UCB value\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 78188.6045701978,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Dynamic epsilon for exploration\n    epsilon = max(0.1, 1.0 - (current_time_slot / total_time_slots) * 0.9)\n\n    # Use a random action if exploration is chosen\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        # Calculate UCB values for exploitation with a slight adjustment for exploration\n        ucb_values = np.zeros(n_actions)\n        for i in range(n_actions):\n            if selection_counts[i] > 0:\n                confidence_interval = np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts[i])\n                ucb_values[i] = avg_scores[i] + confidence_interval\n            else:\n                # Encourage exploration for actions not yet selected\n                ucb_values[i] = np.inf\n\n        # Select the action index with the highest UCB value\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 78530.726513236,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Adaptive epsilon calculation\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n\n    # Calculate UCB values\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_interval\n        else:\n            ucb_values[i] = np.inf  # Encourage exploration for unselected actions\n\n    # Combine exploration and exploitation using epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        # Explore: select a random action from all actions\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploit: select the action with the highest UCB value\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 78873.33815561037,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for action_index in action_indices:\n        scores = score_set[action_index]\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n\n    # Epsilon-greedy exploration\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n    \n    # UCB exploration factor\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n\n    # Calculate final scores\n    final_scores = avg_scores + exploration_bonus\n\n    # Select action with epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        # Explore: randomly choose one of the actions\n        action_index = np.random.choice(n_actions)\n    else:\n        # Exploit: choose the action with the highest adjusted score\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": 80100.1268307851,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    \n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if len(scores) > 0:\n            avg_scores[i] = np.mean(scores)\n    \n    # Epsilon decay strategy\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n\n    # Exploration vs Exploitation\n    if np.random.rand() < epsilon:\n        # Explore: Select the action with the lowest selection count\n        action_index = action_indices[np.argmin(selection_counts)]\n    else:\n        # Exploit: Calculate UCB values\n        ucb_values = np.zeros(n_actions)\n        for i in range(n_actions):\n            if selection_counts[i] > 0:\n                confidence_interval = np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts[i])\n                ucb_values[i] = avg_scores[i] + confidence_interval\n            else:\n                ucb_values[i] = np.inf  # Encourage exploration of untried actions\n\n        # Select the action with the highest UCB value\n        action_index = action_indices[np.argmax(ucb_values)]\n\n    return action_index",
          "objective": 81377.54389755461,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Calculate average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Calculate exploration factor based on current time slot\n    exploration_factor = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n    \n    # Adaptive epsilon that decreases over time\n    epsilon = max(0.1, 1.0 - (current_time_slot / total_time_slots))\n\n    # Calculate the score for each action\n    combined_scores = avg_scores + epsilon * exploration_factor\n\n    # Select action with the highest combined score\n    action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": 84454.4367765848,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Calculate average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Epsilon-Greedy exploration factor\n    epsilon = max(0.1, 0.5 * (1 - current_time_slot / total_time_slots))  # more exploration initially\n    random_value = np.random.rand()\n\n    # Epsilon-greedy selection\n    if random_value < epsilon:\n        # Explore: Choose a random action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploit: UCB selection\n        ucb_scores = np.zeros(n_actions)\n        for i, action_index in enumerate(action_indices):\n            if selection_counts[i] > 0:\n                # Use adjusted UCB to balance exploration and exploitation\n                ucb_scores[i] = avg_scores[i] + np.sqrt(2 * np.log(total_selection_count) / selection_counts[i])\n            else:\n                ucb_scores[i] = float('inf')  # Encourage exploration of unselected actions\n\n        # Select action with highest UCB score\n        action_index = np.argmax(ucb_scores)\n\n    return action_index",
          "objective": 86953.22296208104,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # UCB calculation\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            ucb_values[i] = avg_scores[i] + np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts[i])\n        else:\n            # Assign a high score to unselected actions to encourage exploration\n            ucb_values[i] = float('inf')\n    \n    # Select the action with the highest UCB value\n    action_index = action_indices[np.argmax(ucb_values)]\n\n    return action_index",
          "objective": 87556.85145287668,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Calculate average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Epsilon decay strategy\n    epsilon = max(0.05, 0.5 * (1 - current_time_slot / total_time_slots))\n    random_value = np.random.rand()\n\n    if random_value < epsilon:\n        # Explore: Choose a random action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploit: UCB selection\n        ucb_scores = np.zeros(n_actions)\n        for i, action_index in enumerate(action_indices):\n            if selection_counts[i] > 0:\n                # Use UCB to balance exploration and exploitation\n                ucb_scores[i] = avg_scores[i] + np.sqrt(2 * np.log(total_selection_count) / selection_counts[i])\n            else:\n                ucb_scores[i] = float('inf')  # Encourage unselected actions\n\n        # Select action with highest UCB score\n        action_index = np.argmax(ucb_scores)\n    \n    return action_index",
          "objective": 90747.82984246082,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if selection_counts[i] > 0 else 0.0\n\n    # Calculate a temperature parameter for softmax based on the time slot\n    temperature = max(0.1, 1 - (current_time_slot / total_time_slots))\n\n    # Calculate UCB values\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_interval\n        else:\n            ucb_values[i] = np.inf  # Encourage exploration for unselected actions\n\n    # Combine UCB values with a softmax approach to balance exploration and exploitation\n    combined_scores = ucb_values / temperature\n    probabilities = np.exp(combined_scores - np.max(combined_scores))  # Softmax stabilization\n    probabilities /= np.sum(probabilities)  # Normalize to sums to 1\n\n    # Select an action based on calculated probabilities\n    action_index = np.random.choice(action_indices, p=probabilities)\n\n    return action_index",
          "objective": 91262.55576631405,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Calculate average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    \n    for action_index in action_indices:\n        scores = score_set[action_index]\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n\n    # Epsilon-greedy exploration\n    epsilon = max(0.1, 1 - current_time_slot / total_time_slots)\n    \n    # UCB exploration factor\n    ucb_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n\n    # Calculate final scores\n    final_scores = avg_scores + ucb_bonus\n\n    # Decide action based on exploration and exploitation\n    if np.random.rand() < epsilon:\n        # Explore: Select a random action\n        action_index = np.random.choice(n_actions)\n    else:\n        # Exploit: Select the action with the highest adjusted score\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": 91345.13199144298,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if selection_counts[i] > 0 else 0\n\n    # Epsilon strategy dynamically based on total_selection_count\n    epsilon = max(0.05, min(1.0, 1.0 - (total_selection_count / (total_time_slots * 0.5))))\n\n    # Random selection for exploration\n    if np.random.rand() < epsilon:\n        return np.random.choice(action_indices)\n\n    # Calculate UCB values for exploitation\n    ucb_values = np.zeros(n_actions)\n    \n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_interval\n        else:\n            # Encourage exploration for unselected actions\n            ucb_values[i] = np.inf\n\n    # Select action with highest UCB value\n    action_index = np.argmax(ucb_values)\n    \n    return action_index",
          "objective": 95305.18972142713,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts for each action\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Define exploration rate (Epsilon)\n    epsilon = 1 / np.sqrt(total_selection_count + 1)\n    \n    # Calculate UCB values\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        confidence_interval = np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts[i] + 1e-6))  # Adding a small value to avoid division by zero\n        ucb_values[i] = avg_scores[i] + confidence_interval\n\n    # Determine total selection potential that emphasizes exploration\n    exploratory_scores = np.zeros(n_actions)\n    for i in range(n_actions):\n        exploratory_scores[i] = -selection_counts[i] / (current_time_slot + 1)  # Encourage exploration of less-selected actions\n\n    # Combine UCB and exploration scores\n    combined_scores = ucb_values + epsilon * exploratory_scores\n    \n    # Select the action index with the highest combined score\n    action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": 95815.87276319493,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Calculate average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if len(scores) > 0 else 0\n\n    # Dynamic exploration factor using Epsilon decay\n    max_epsilon = 0.5\n    min_epsilon = 0.1\n    decay_rate = (max_epsilon - min_epsilon) / total_time_slots\n    epsilon = max(min_epsilon, max_epsilon - decay_rate * current_time_slot)\n\n    # Randomly decide to explore or exploit\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        # Calculate Upper Confidence Bound scores for exploitation\n        ucb_scores = np.zeros(n_actions)\n        for i in range(n_actions):\n            if selection_counts[i] > 0:\n                ucb_scores[i] = avg_scores[i] + np.sqrt(2 * np.log(total_selection_count) / selection_counts[i])\n            else:\n                ucb_scores[i] = float('inf')  # Encourage unselected actions\n\n        # Select action with highest UCB score\n        action_index = np.argmax(ucb_scores)\n\n    return action_index",
          "objective": 95871.82478442046,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for action_index in action_indices:\n        scores = score_set[action_index]\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n\n    # Calculate the exploitation term based on average scores\n    max_avg_score = np.max(avg_scores) if len(avg_scores) > 0 else 0\n    exploitation_scores = avg_scores / (max_avg_score + 1e-5)\n\n    # Calculate exploration term using UCB\n    exploration_scores = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n    \n    # Combine exploration and exploitation scores\n    combined_scores = exploitation_scores + exploration_scores\n\n    # Select the action with the highest combined score\n    action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": 96144.81938517094,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Calculate average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if len(scores) > 0 else 0\n\n    # Calculate dynamic exploration factor\n    exploration_factor = max(0, (total_time_slots - current_time_slot) / total_time_slots)  # Linear decay\n    epsilon = 0.1 + 0.4 * exploration_factor  # Range from 0.1 to 0.5\n\n    # Choose action: explore or exploit\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)  # Exploration\n    else:\n        # Calculate UCB scores for exploitation\n        ucb_scores = np.zeros(n_actions)\n        for i in range(n_actions):\n            if selection_counts[i] > 0:\n                ucb_scores[i] = avg_scores[i] + np.sqrt((2 * np.log(total_selection_count)) / selection_counts[i])\n            else:\n                ucb_scores[i] = float('inf')  # Encourage unselected actions\n\n        action_index = action_indices[np.argmax(ucb_scores)]  # Exploitation\n\n    return action_index",
          "objective": 97804.28218151927,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    \n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if selection_counts[i] > 0 else 0\n\n    # Dynamic epsilon for exploration\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n\n    # Calculate UCB scores\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_interval\n        else:\n            ucb_values[i] = np.inf  # Prioritize unexplored actions\n\n    # Exploration vs exploitation\n    exploration_scores = np.random.rand(n_actions) * epsilon\n    combined_scores = (1 - epsilon) * ucb_values + exploration_scores\n\n    # Choose action with the highest combined score\n    action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": 98352.15782232654,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if selection_counts[i] > 0 else 0.0\n\n    # Dynamic epsilon decay based on the time slot\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))  # Minimum epsilon is 0.1\n\n    # Determine exploration or exploitation\n    if np.random.rand() < epsilon:\n        # Exploration: randomly select an action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploitation: calculate UCB values\n        ucb_values = np.zeros(n_actions)\n        for i in range(n_actions):\n            if selection_counts[i] > 0:\n                confidence_interval = np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts[i])\n                ucb_values[i] = avg_scores[i] + confidence_interval\n            else:\n                # Encourage exploration for unselected actions\n                ucb_values[i] = np.inf\n\n        # Select the action with the highest UCB value\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 99966.06944298232,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8  # action indices from 0 to 7\n    \n    # Initialize arrays to store average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Compute average scores and selection counts\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n\n    # Dynamic exploration factor (Epsilon) reducing over time\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n\n    # Calculate UCB values\n    total_selection_counts = np.sum(selection_counts)\n    ucb_values = np.zeros(n_actions)\n    for action_index in range(n_actions):\n        if selection_counts[action_index] > 0:\n            ucb_values[action_index] = avg_scores[action_index] + \\\n                np.sqrt((2 * np.log(total_selection_counts + 1)) / (selection_counts[action_index] + 1e-5))\n        else:\n            ucb_values[action_index] = float('inf')  # Encourage exploration for actions never selected\n\n    # Combine Epsilon-Greedy and UCB approaches\n    if np.random.rand() < epsilon:\n        # Select randomly among all actions\n        action_index = np.random.choice(n_actions)\n    else:\n        # Select based on UCB values\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 104671.71073826688,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Calculate average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Choose actions to explore\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n    \n    # Calculate the UCB score for each action\n    ucb_scores = avg_scores + exploration_bonus\n\n    # Select action with highest UCB score\n    action_index = action_indices[np.argmax(ucb_scores)]\n    \n    return action_index",
          "objective": 110898.92008848963,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize arrays for average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Compute average scores and selection counts\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if selection_counts[i] > 0 else 0\n\n    # Calculate epsilon, starting high for exploration\n    epsilon_start = 0.9\n    epsilon_end = 0.1\n    decay = (epsilon_start - epsilon_end) / total_time_slots\n    epsilon = max(epsilon_end, epsilon_start - decay * current_time_slot)\n\n    # Calculate UCB values\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_interval\n        else:\n            ucb_values[i] = np.inf  # Actions not selected yet\n\n    # Choose action based on epsilon-greedy approach\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 111262.23635386337,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Calculate average scores and selection counts\n    avg_scores = np.array([np.mean(score_set[action_index]) if score_set[action_index] else 0 for action_index in action_indices])\n    selection_counts = np.array([len(score_set[action_index]) for action_index in action_indices])\n\n    # Dynamic epsilon for exploration\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n\n    # Calculate Upper Confidence Bound (UCB)\n    ucb_values = np.zeros(n_actions)\n    total_selections_plus_one = total_selection_count + 1\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt((2 * np.log(total_selections_plus_one)) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_interval\n        else:\n            ucb_values[i] = np.inf  # Prefer actions that haven't been selected\n\n    # Combine UCB and exploration factor\n    exploration_scores = np.random.rand(n_actions) * epsilon\n    combined_scores = (1 - epsilon) * ucb_values + exploration_scores\n\n    # Select the action with the highest combined score\n    action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": 112043.13851592632,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Select action using UCB\n    action_index = -1\n    max_upper_bound = -np.inf\n    \n    # Loop through each action to compute its UCB\n    for i in range(n_actions):\n        if selection_counts[i] == 0:\n            # If an action has not been selected, assign high preference to explore it\n            upper_bound = np.inf  # Assign a very high upper bound to untried actions\n        else:\n            upper_bound = avg_scores[i] + np.sqrt((2 * np.log(total_selection_count)) / selection_counts[i])\n        \n        # Select the action with the highest upper confidence bound\n        if upper_bound > max_upper_bound:\n            max_upper_bound = upper_bound\n            action_index = action_indices[i]\n\n    return action_index",
          "objective": 112700.37410418151,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Epsilon-Greedy parameters\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))  # Decaying epsilon\n    \n    # Randomly select an action based on epsilon\n    if np.random.rand() < epsilon:\n        # Exploration: select a random action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploitation: select the action with the highest average score\n        action_index = action_indices[np.argmax(avg_scores)]\n\n    return action_index",
          "objective": 116134.99098611329,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Compute average scores and selection counts\n    for action_index in action_indices:\n        scores = score_set[action_index]\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n\n    # Epsilon for exploration, decreases over time\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n    \n    # Upper confidence bound (UCB) factor for exploration\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n\n    # Calculate final scores\n    final_scores = avg_scores + exploration_bonus\n\n    # Epsilon-greedy decision\n    if np.random.rand() < epsilon:\n        # Explore: randomly choose an action\n        action_index = np.random.choice(n_actions)\n    else:\n        # Exploit: choose the action with the highest score\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": 116154.06748753999,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Epsilon-Greedy strategy\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))  # Decay epsilon over time\n    if np.random.rand() < epsilon:\n        # Explore: randomly select an action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploit: select action with the highest average score\n        action_index = action_indices[np.argmax(avg_scores)]\n\n    return action_index",
          "objective": 117979.73449125818,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions) + 1e-5  # avoid division by zero by initializing with a small value\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Dynamic Epsilon-Greedy strategy\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))  # Decreases over time\n    if np.random.rand() < epsilon:\n        # Explore: uniformly select an action at random\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploit: Advanced UCB strategy\n        ucb_values = avg_scores + np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts)\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 119565.19099912429,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Calculate the exploration bonus using UCB\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_interval\n        else:\n            # If the action has never been selected, give it a high initial score to encourage exploration\n            ucb_values[i] = np.inf\n\n    # Select the action index with the highest UCB value\n    action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 120934.84211628283,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Calculate average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if scores else 0.0\n\n    # Variable epsilon based on total selection count\n    epsilon = max(0.1, 1.0 - 1.0 * total_selection_count / (total_time_slots * 2))\n\n    # Compute UCB values\n    ucb_values = np.zeros(n_actions)\n    \n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_interval\n        else:\n            ucb_values[i] = np.inf  # Unexplored action\n\n    # Define a strategy combining UCB and Epsilon-Greedy\n    combined_scores = (1 - epsilon) * ucb_values + epsilon * np.random.rand(n_actions)\n\n    # Select action based on the combined scores\n    action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": 120944.54522829514,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if len(scores) > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Epsilon-decayed exploration factor\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n    \n    # Calculate UCB scores\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_interval\n        else:\n            ucb_values[i] = np.inf  # Encourage exploration for untried actions\n\n    # Randomly select an action with probability epsilon (exploration)\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        # Select the action with the highest UCB value\n        action_index = action_indices[np.argmax(ucb_values)]\n    \n    return action_index",
          "objective": 124730.56852901318,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if selection_counts[i] > 0 else 0\n\n    # Adjust exploration factor based on the time decay\n    exploration_factor = np.clip(1.0 - (current_time_slot / total_time_slots), 0.01, 1.0)\n    \n    # Calculate UCB scores\n    ucb_scores = np.zeros(n_actions)\n    confidence_interval = np.log(total_selection_count + 1)  # To avoid log(0)\n\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            ucb_scores[i] = avg_scores[i] + exploration_factor * np.sqrt(confidence_interval / selection_counts[i])\n        else:\n            ucb_scores[i] = float('inf')  # Prioritize untried actions\n\n    # Select action based on the highest UCB score\n    action_index = np.argmax(ucb_scores)\n\n    return action_index",
          "objective": 127323.55343705538,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = len(score_set)\n    \n    # Initialize arrays for average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Compute average scores and selection counts\n    for action_index, scores in score_set.items():\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n\n    # Define exploration rates\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n\n    # Calculate overall scores considering both exploitation and exploration\n    final_scores = avg_scores + exploration_bonus\n\n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(n_actions)  # Explore\n    else:\n        action_index = np.argmax(final_scores)  # Exploit\n\n    return action_index",
          "objective": 128752.85796228108,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Enhanced exploration factor using exponential decay\n    beta = 0.1  # Controls the rate of decay\n    epsilon = max(0.1, np.exp(-beta * current_time_slot))  # Decay epsilon over time\n    random_value = np.random.rand()\n\n    # Epsilon-greedy selection\n    if random_value < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        # Calculate UCB scores\n        ucb_scores = np.zeros(n_actions)\n        for i, action_index in enumerate(action_indices):\n            # UCB with a safety net for unselected actions\n            if selection_counts[i] > 0:\n                ucb_scores[i] = avg_scores[i] + np.sqrt((2 * np.log(total_selection_count)) / selection_counts[i])\n            else:\n                ucb_scores[i] = float('inf')  # Force exploration of unselected actions\n\n        # Select action with highest UCB score\n        action_index = np.argmax(ucb_scores)\n\n    return action_index",
          "objective": 129571.53138007665,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and update selection counts\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Set epsilon that decays over time, increasing exploration at the beginning\n    exploration_weight = max(0.1, 1.0 - (total_selection_count / (total_time_slots * 0.5)))\n    epsilon = min(0.5, exploration_weight + (0.5 * (1.0 - current_time_slot / total_time_slots)))\n\n    # Select action based on Epsilon-Greedy with UCB\n    if np.random.rand() < epsilon:\n        # Exploration\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploitation: Calculate UCB values\n        ucb_values = np.zeros(n_actions)\n        for i in range(n_actions):\n            if selection_counts[i] > 0:\n                confidence_interval = np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts[i])\n                ucb_values[i] = avg_scores[i] + confidence_interval\n            else:\n                # Encourage exploration for unselected actions\n                ucb_values[i] = np.inf\n\n        # Select the action index with the highest UCB value\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 131575.28704059072,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if scores:\n            avg_scores[i] = np.mean(scores)\n\n    # Adaptive epsilon calculation\n    epsilon_start = 0.9\n    epsilon_end = 0.1\n    epsilon_decay = (epsilon_start - epsilon_end) / total_time_slots\n    epsilon = max(epsilon_end, epsilon_start - epsilon_decay * current_time_slot)\n\n    # Select action based on epsilon-greedy strategy with a focus on UCB\n    if np.random.rand() < epsilon:\n        # Explore: Select action with fewest selections, penalizing those with no selections\n        action_index = action_indices[np.argmin(selection_counts)]\n    else:\n        # Exploit: Calculate UCB values and select the action with the highest UCB value\n        ucb_values = np.zeros(n_actions)\n        for i in range(n_actions):\n            if selection_counts[i] > 0:\n                confidence_interval = np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts[i])\n                ucb_values[i] = avg_scores[i] + confidence_interval\n            else:\n                ucb_values[i] = np.inf  # Encourage exploration of actions that have not been tried\n\n        action_index = action_indices[np.argmax(ucb_values)]\n\n    return action_index",
          "objective": 132200.3763389831,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Calculate average scores and counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if selection_counts[i] > 0 else 0\n\n    # Adaptive epsilon calculation\n    initial_epsilon = 1.0\n    final_epsilon = 0.1\n    epsilon = initial_epsilon * (1 - (current_time_slot / total_time_slots)) + final_epsilon\n\n    # Compute UCB values\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_interval\n        else:\n            ucb_values[i] = np.inf  # Unselected actions\n\n    # Explore less frequently selected actions\n    exploration_factor = (1 - selection_counts / (total_selection_count + 1)) if total_selection_count > 0 else np.ones(n_actions)\n    combined_scores = ucb_values * (1 - epsilon) + exploration_factor * epsilon\n\n    # Select action based on the highest combined score\n    action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": 133715.97814196613,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for idx in action_indices:\n        scores = score_set[idx]\n        selection_counts[idx] = len(scores)\n        if selection_counts[idx] > 0:\n            avg_scores[idx] = np.mean(scores)\n\n    # Calculate exploration term using Epsilon-Greedy approach\n    epsilon = max(0.1, 0.5 - (current_time_slot / total_time_slots) * 0.4)  # Decrease epsilon over time\n    random_val = np.random.rand()\n    \n    if random_val < epsilon:  # Explore\n        action_index = np.random.choice(action_indices)\n    else:  # Exploit\n        # Ensure we don't divide by zero for actions that have not been selected\n        exploration_scores = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n        combined_scores = avg_scores + exploration_scores\n        action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": 136699.43699693037,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    \n    for action_index in action_indices:\n        scores = score_set[action_index]\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n\n    # Calculate UCB values\n    ucb_values = np.zeros(n_actions)\n    for action_index in action_indices:\n        if selection_counts[action_index] > 0:\n            ucb_values[action_index] = avg_scores[action_index] + np.sqrt(2 * np.log(total_selection_count) / selection_counts[action_index])\n        else:\n            ucb_values[action_index] = float('inf')  # Prioritize exploration for unselected actions\n\n    # Implement a time-dependent exploration factor\n    exploration_factor = max(0.1, 1 - (current_time_slot / total_time_slots))  # Decrease exploration over time\n    \n    # Select action based on UCB with the exploration factor\n    weighted_ucb = ucb_values * (1 + exploration_factor)\n\n    action_index = np.argmax(weighted_ucb)\n    \n    return action_index",
          "objective": 139382.01247300813,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    \n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Epsilon decay mechanism\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n    \n    # Exploration: randomly select an action with probability epsilon\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploitation: select the action with the highest average score\n        action_index = np.argmax(avg_scores)\n\n    return action_index",
          "objective": 142191.6407782373,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    \n    # Compute average scores and selection counts\n    for action_index in action_indices:\n        scores = score_set[action_index]\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n    \n    # Epsilon-Greedy parameters\n    epsilon = max(0.1, 1.0 - (current_time_slot / total_time_slots))\n    \n    # UCB scores\n    ucb_scores = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            ucb_scores[i] = avg_scores[i] + np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts[i] + 1e-5))\n        else:\n            ucb_scores[i] = float('inf')  # Encourage exploration of unselected actions\n\n    # Combine Epsilon-Greedy and UCB selection\n    if np.random.rand() < epsilon:\n        # Explore\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploit using UCB scores\n        action_index = action_indices[np.argmax(ucb_scores)]\n\n    return action_index",
          "objective": 142382.7919074757,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Parameters for exploration vs exploitation\n    epsilon_decay = 0.1  # Decay rate for epsilon\n    min_epsilon = 0.05   # Minimum epsilon value\n    exploration_probability = max(min_epsilon, 1 - (epsilon_decay * current_time_slot / total_time_slots))\n\n    # Decide whether to explore or exploit\n    if np.random.rand() < exploration_probability:\n        # Exploration: Randomly select an action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploitation: Calculate UCB values\n        ucb_values = np.zeros(n_actions)\n        for i in range(n_actions):\n            if selection_counts[i] > 0:\n                confidence_interval = np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts[i])\n                ucb_values[i] = avg_scores[i] + confidence_interval\n            else:\n                # If the action has never been selected, encourage exploration\n                ucb_values[i] = np.inf\n        \n        # Select the action index with the highest UCB value\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 143407.11731885214,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Adaptive epsilon based on total selections\n    epsilon = max(0.1, min(1.0, 1.0 - (total_selection_count / (total_time_slots * n_actions)) * 0.9))\n    \n    # Calculate UCB values\n    confidence_intervals = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_intervals[i] = np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts[i])\n        else:\n            confidence_intervals[i] = np.inf  # Encouragement for unselected actions\n\n    ucb_values = avg_scores + confidence_intervals\n\n    # Epsilon-Greedy with UCB-based exploitation\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 144564.7704167058,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and counts\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Handle the case where no action has been selected yet\n    if total_selection_count == 0:\n        return np.random.choice(action_indices)\n\n    # Calculate epsilon based on current time slot\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n\n    # Epsilon-Greedy Selection\n    if np.random.rand() < epsilon:\n        # Exploration: Choose a random action\n        return np.random.choice(action_indices)\n    else:\n        # Exploitation + UCB selection\n        ucb_values = avg_scores + np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n        action_index = np.argmax(ucb_values)\n        return action_index",
          "objective": 146778.17306529946,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Define the exploration factor using UCB\n    exploration_bonus = np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n\n    # Adaptive epsilon based on current time slot\n    decay_rate = 1.0 - (current_time_slot / total_time_slots)\n    epsilon = max(0.1, decay_rate)\n\n    # Integrate exploration and exploitation\n    combined_scores = avg_scores + (epsilon * exploration_bonus)\n\n    # Handle cases where no selection has been made yet to avoid division by zero\n    combined_scores += (selection_counts == 0).astype(float) * np.inf \n\n    # Select action with the highest adjusted score\n    action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": 150412.28845315613,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if selection_counts[i] > 0 else 0\n\n    # Dynamic epsilon for exploration, decaying over time\n    epsilon = max(0.1, 1.0 - (current_time_slot / (total_time_slots + 1)))\n\n    ucb_values = np.zeros(n_actions)\n\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_interval\n        else:\n            # Assign a high value for unexplored actions to encourage exploration\n            ucb_values[i] = np.inf\n\n    # Epsilon-Greedy Decision\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)  # Exploration\n    else:\n        action_index = np.argmax(ucb_values)  # Exploitation based on UCB\n\n    return action_index",
          "objective": 152838.02307760992,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for action_index in action_indices:\n        scores = score_set[action_index]\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n\n    # Epsilon-greedy exploration parameter\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))  # Decreases over time\n    exploration_choices = (np.random.rand() < epsilon)  # Randomly decide to explore or exploit\n\n    if exploration_choices:\n        # Explore: uniformly select an action at random\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploit: UCB strategy\n        ucb_values = np.zeros(n_actions)\n        total_counts = total_selection_count + 1e-5  # Avoid division by zero\n\n        for i in range(n_actions):\n            if selection_counts[i] > 0:\n                confidence_interval = np.sqrt((2 * np.log(total_counts)) / selection_counts[i])\n                ucb_values[i] = avg_scores[i] + confidence_interval\n            else:\n                ucb_values[i] = 1  # Explore unselected actions fully\n\n        # Select the action with the highest UCB value\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 155272.62049730137,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    \n    for action_index in action_indices:\n        scores = score_set[action_index]\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n    \n    # Calculate UCB values\n    ucb_values = np.zeros(n_actions)\n    for action_index in action_indices:\n        if selection_counts[action_index] > 0:\n            ucb_values[action_index] = avg_scores[action_index] + np.sqrt(2 * np.log(total_selection_count) / selection_counts[action_index])\n        else:\n            ucb_values[action_index] = float('inf')  # Encourage exploration of unselected actions\n\n    # Select action based on the highest UCB\n    action_index = np.argmax(ucb_values)\n    \n    return action_index",
          "objective": 156483.0381172704,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    \n    for action_index in action_indices:\n        scores = score_set[action_index]\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n    \n    # Time-based exploration factor\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n    \n    # Calculate adjusted scores with exploration\n    adjusted_scores = np.zeros(n_actions)\n    for action_index in action_indices:\n        if selection_counts[action_index] > 0:\n            adjusted_scores[action_index] = (\n                avg_scores[action_index] + \n                exploration_factor * np.sqrt(np.log(total_selection_count) / selection_counts[action_index])\n            )\n        else:\n            adjusted_scores[action_index] = float('inf')  # Encourage exploration of unselected actions\n\n    # Select action based on the highest adjusted score\n    action_index = np.argmax(adjusted_scores)\n    \n    return action_index",
          "objective": 157477.64612472718,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if selection_counts[i] > 0 else 0.0\n\n    # Adaptive epsilon based on total selections\n    epsilon = max(0.1, 1.0 - (total_selection_count / (total_time_slots * 10)))\n\n    # Calculate UCB values\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_interval\n        else:\n            ucb_values[i] = np.inf  # Encouragement for unselected actions\n\n    # Epsilon-Greedy with UCB-based exploitation\n    if np.random.rand() < epsilon:\n        # Exploration: Randomly select an action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploitation: Use UCB to select action\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 160533.6204060836,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Set epsilon that decays over time and total selections\n    epsilon = max(0.1, 1.0 - (total_selection_count / (total_time_slots * 0.5)))\n\n    # Select action based on Epsilon-Greedy with UCB\n    if np.random.rand() < epsilon:\n        # Exploration: Randomly select an action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploitation: Calculate UCB values\n        ucb_values = np.zeros(n_actions)\n        for i in range(n_actions):\n            if selection_counts[i] > 0:\n                confidence_interval = np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts[i])\n                ucb_values[i] = avg_scores[i] + confidence_interval\n            else:\n                # Encourage exploration for unselected actions\n                ucb_values[i] = np.inf\n\n        # Select the action index with the highest UCB value\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 160936.39528461167,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    \n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n    \n    # Calculate exploration factor based on time\n    exploration_factor = max(1.0 - (current_time_slot / total_time_slots), 0.01)\n    \n    # Calculate UCB scores\n    ucb_scores = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            ucb_scores[i] = avg_scores[i] + exploration_factor * np.sqrt(np.log(total_selection_count) / selection_counts[i])\n        else:\n            ucb_scores[i] = float('inf')  # Encourage exploration of unselected actions\n\n    # Select action based on the highest UCB score\n    action_index = np.argmax(ucb_scores)\n    \n    return action_index",
          "objective": 166064.87906552682,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize action indices\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if selection_counts[i] > 0 else 0\n\n    # Dynamic epsilon for exploration\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n\n    # Initialize UCB values\n    ucb_values = np.zeros(n_actions)\n    \n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_interval\n        else:\n            # Encourage exploration for untried actions\n            ucb_values[i] = np.inf\n\n    # Select action using Epsilon-Greedy strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)  # Exploration\n    else:\n        action_index = np.argmax(ucb_values)  # Exploitation based on UCB\n\n    return action_index",
          "objective": 166219.42617343296,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores, selection counts and exploration bonuses\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for action_index in action_indices:\n        scores = score_set[action_index]\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n    \n    # UCB calculation\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))  # add small constant to avoid division by zero\n    ucb_scores = avg_scores + exploration_bonus\n\n    # Select the action with the highest UCB score\n    action_index = np.argmax(ucb_scores)\n\n    return action_index",
          "objective": 169497.01106051315,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    \n    for action_index in action_indices:\n        scores = score_set[action_index]\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n    \n    # Dynamic epsilon based on current_time_slot\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n    \n    # Generate random number to decide exploration vs exploitation\n    if np.random.rand() < epsilon:\n        # Exploration: randomly select among actions\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploitation: select action with highest average score\n        action_index = action_indices[np.argmax(avg_scores)]\n    \n    return action_index",
          "objective": 170103.21699731617,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Calculate exploration factor using dynamic epsilon\n    exploration_rate = max(0.1, 1 - (current_time_slot / total_time_slots))\n    \n    # Epsilon-Greedy approach with additional exploration using UCB\n    random_value = np.random.rand()\n\n    if random_value < exploration_rate:\n        # Explore by selecting a random action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Calculate UCB scores for exploitation\n        ucb_scores = np.zeros(n_actions)\n        for i, action_index in enumerate(action_indices):\n            if selection_counts[i] > 0:\n                ucb_scores[i] = avg_scores[i] + np.sqrt(2 * np.log(total_selection_count) / selection_counts[i])\n            else:\n                ucb_scores[i] = float('inf')  # Prioritize unselected actions\n\n        # Select the action with the highest UCB score\n        action_index = action_indices[np.argmax(ucb_scores)]\n\n    return action_index",
          "objective": 184016.0349257158,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Calculate exploration factor (epsilon)\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n\n    # Select action with epsilon-greedy strategy or based on UCB\n    if np.random.rand() < epsilon:\n        # Exploration: select a random action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploitation: calculate UCB values\n        ucb_values = np.zeros(n_actions)\n        for i in range(n_actions):\n            if selection_counts[i] > 0:\n                confidence_interval = np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts[i])\n                ucb_values[i] = avg_scores[i] + confidence_interval\n            else:\n                # Encourage exploration for unseen actions\n                ucb_values[i] = np.inf\n        \n        # Select the action with the highest UCB value\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 186199.018452675,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Calculate exploration factor using epsilon decay\n    epsilon = max(0.1, (total_time_slots - current_time_slot) / total_time_slots)  # decay epsilon\n    random_value = np.random.rand()\n\n    # Epsilon-greedy selection\n    if random_value < epsilon:\n        # Explore: Choose a random action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploit: Calculate UCB scores\n        ucb_scores = np.zeros(n_actions)\n        for i, action_index in enumerate(action_indices):\n            if selection_counts[i] > 0:\n                ucb_scores[i] = avg_scores[i] + np.sqrt(2 * np.log(total_selection_count) / selection_counts[i])\n            else:\n                ucb_scores[i] = float('inf')  # Encourage exploration of unselected actions\n\n        # Select action with highest UCB score\n        action_index = np.argmax(ucb_scores)\n\n    return action_index",
          "objective": 192557.91696990133,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Calculate epsilon based on current time slot\n    # Start with high exploration and decay over time\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n\n    # Generate a random number to choose between exploration and exploitation\n    if np.random.rand() < epsilon:\n        # Exploration: randomly choose an action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploitation: choose action with the highest average score\n        action_index = np.argmax(avg_scores)\n\n    return action_index",
          "objective": 199361.29600956858,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Calculate average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if len(scores) > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Dynamic epsilon based on total selections\n    epsilon = max(0.1, 1.0 - (total_selection_count / max(1, total_time_slots * 5)) * 0.9)\n    \n    # Calculate UCB values\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_interval\n        else:\n            ucb_values[i] = np.inf  # Encourage selection of unselected actions\n\n    # Variance of scores to adjust exploration\n    score_variances = np.zeros(n_actions)\n    for i, action_index in enumerate(action_indices):\n        if selection_counts[i] > 0:\n            score_variances[i] = np.var(score_set[action_index])\n    \n    # Adjust exploration weight based on variance\n    variance_weight = np.exp(-score_variances / np.max(score_variances + 1e-10))\n\n    # Combine exploration strategy\n    exploration_scores = epsilon * variance_weight\n    combined_values = (1 - epsilon) * ucb_values + exploration_scores\n\n    # Select action based on combined values\n    action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": 199805.82403310196,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if selection_counts[i] > 0 else 0.0\n\n    # Define epsilon with a decreasing strategy\n    epsilon = 1.0 - (current_time_slot / total_time_slots) * 0.95\n    epsilon = max(0.1, min(1.0, epsilon))  # Keep epsilon within [0.1, 1.0]\n\n    if np.random.rand() < epsilon:\n        # Exploration phase\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploitation phase with UCB\n        ucb_values = np.zeros(n_actions)\n        for i in range(n_actions):\n            if selection_counts[i] > 0:\n                confidence_interval = np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts[i])\n                ucb_values[i] = avg_scores[i] + confidence_interval\n            else:\n                ucb_values[i] = np.inf  # Encourage selection of untried actions\n\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 209131.58281391396,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Dynamic epsilon decreasing over time to balance exploration and exploitation\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n\n    # Select action using Epsilon-Greedy strategy\n    if np.random.rand() < epsilon:\n        # Explore: Select action with minimum selection count \n        action_index = np.argmin(selection_counts)\n    else:\n        # Exploit: Calculate modified UCB values\n        ucb_values = np.zeros(n_actions)\n        for i in range(n_actions):\n            if selection_counts[i] > 0:\n                # Adjusted confidence interval considering variance\n                variance = np.var(score_set[action_indices[i]]) if selection_counts[i] > 1 else 0\n                confidence_interval = np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts[i]) + variance\n                ucb_values[i] = avg_scores[i] + confidence_interval\n            else:\n                ucb_values[i] = np.inf  # Encourage exploration for unselected actions\n\n        # Select the action index with the highest UCB value\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 215985.8741733847,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Calculate average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.array([len(score_set[action_index]) for action_index in action_indices])\n\n    for i, action_index in enumerate(action_indices):\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(score_set[action_index])\n        else:\n            avg_scores[i] = 0  # Action not tried yet\n\n    # Epsilon-greedy exploration strategy\n    epsilon = 1.0 - (current_time_slot / total_time_slots)  # Decay epsilon over time\n    epsilon = np.clip(epsilon, 0.01, 0.2)  # Maintain a minimum exploration rate\n    \n    if np.random.rand() < epsilon:  # Explore\n        action_index = np.random.choice(action_indices)\n    else:  # Exploit\n        # Adjust average scores with exploration bonus\n        exploration_bonus = np.log(total_selection_count + 1) / (selection_counts + 1e-5)  # Avoid division by zero\n        ucb_scores = avg_scores + exploration_bonus\n        action_index = np.argmax(ucb_scores)\n\n    return action_index",
          "objective": 222330.74345256688,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize action indices\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Calculate average scores and selection counts\n    avg_scores = np.array([np.mean(score_set[action]) if len(score_set[action]) > 0 else 0 for action in action_indices])\n    selection_counts = np.array([len(score_set[action]) for action in action_indices])\n\n    # Exploration parameter based on time slot\n    exploration_rate = max(0.1, 1 - (current_time_slot / total_time_slots))\n\n    # Calculate UCB values\n    ucb_values = np.zeros(n_actions)\n    total_count_adjusted = total_selection_count + 1  # To avoid division by zero\n\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt((2 * np.log(total_count_adjusted)) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_interval\n        else:\n            ucb_values[i] = np.inf  # Encourage exploration\n\n    # Select action using Epsilon-Greedy strategy\n    if np.random.rand() < exploration_rate:\n        action_index = np.random.choice(action_indices)  # Exploration\n    else:\n        action_index = action_indices[np.argmax(ucb_values)]  # Exploitation based on UCB\n\n    return action_index",
          "objective": 224407.28634607582,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Epsilon that decays and explores less selected actions initially\n    decay_factor = 0.5\n    epsilon = max(0.1, np.exp(-decay_factor * (current_time_slot / total_time_slots)))\n\n    # Select action based on Epsilon-Greedy with modified UCB\n    if np.random.rand() < epsilon:\n        # Exploration: Prefer actions with fewer selections\n        action_index = np.argmin(selection_counts)\n    else:\n        # Exploitation: Calculate UCB values\n        ucb_values = np.zeros(n_actions)\n        for i in range(n_actions):\n            if selection_counts[i] > 0:\n                confidence_interval = np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts[i])\n                ucb_values[i] = avg_scores[i] + confidence_interval\n            else:\n                # Encourage exploration for unselected actions\n                ucb_values[i] = np.inf\n\n        # Select the action index with the highest UCB value\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 237680.96815940164,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate scores and counts\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Handle the case where no action has been selected yet\n    if total_selection_count == 0:\n        return np.random.choice(action_indices)\n\n    # Calculate UCB values\n    ucb_values = avg_scores + np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n\n    # Select the action with the highest UCB value\n    action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 263235.72405519703,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = len(score_set)  # Should be 8\n    average_scores = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n\n    # Calculate average scores and selection counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Dynamic exploration rate based on time\n    exploration_rate = 1 - (current_time_slot / total_time_slots)\n    exploration_rate = max(0.1, exploration_rate)  # Ensure minimum exploration\n\n    # Explore or exploit based on exploration rate\n    if np.random.rand() < exploration_rate:\n        # Exploration: choose an action randomly\n        action_index = np.random.choice(range(action_count))\n    else:\n        # Exploitation: use UCB approach with exploration bonus\n        exploration_bonus = np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1e-5))  # Small constant to avoid division by zero\n        total_scores = average_scores + exploration_bonus\n        action_index = np.argmax(total_scores)\n\n    return action_index",
          "objective": 278365.732630347,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8  # Fixed number of actions\n\n    # Initialize lists to hold average scores and selection counts\n    average_scores = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Calculate the exploration probability (epsilon)\n    epsilon = max(0.1, 1.0 - (current_time_slot / total_time_slots))\n\n    # Calculate UCB (Upper Confidence Bound) for each action\n    ucb_values = average_scores + np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n\n    # Select action based on epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        # Explore: Choose action randomly\n        action_index = np.random.choice(np.arange(action_count))\n    else:\n        # Exploit: Choose action based on UCB\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 283490.2973296129,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = np.array(list(score_set.keys()))\n    n_actions = len(action_indices)\n    \n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    \n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if scores else 0  # Handle case of no scores\n    \n    # Calculate confidence intervals (pseudocounts) for less frequently chosen actions\n    confidence = 1 + selection_counts\n    scores_with_confidence = avg_scores + np.log(confidence) / np.sqrt(confidence)\n    \n    # Softmax temperature controls exploration\n    temperature = max(0.1, 1 - (current_time_slot / total_time_slots))\n    exp_scores = np.exp(scores_with_confidence / temperature)\n    \n    # Probabilities for each action based on softmax\n    probabilities = exp_scores / np.sum(exp_scores)\n    \n    # Action selection based on computed probabilities\n    action_index = np.random.choice(action_indices, p=probabilities)\n    \n    return action_index",
          "objective": 284664.20116050425,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8  # Fixed number of actions\n    average_scores = []\n    \n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        if len(scores) > 0:\n            average_score = np.mean(scores)\n        else:\n            average_score = 0  # Handle actions that have never been selected\n        \n        average_scores.append(average_score)\n    \n    # Normalize average scores to prevent bias due to selection count\n    normalized_scores = np.array(average_scores) / (total_selection_count + 1e-5)  # Smooth with small value to avoid division by zero\n    \n    # Calculate the exploration rate; more exploration in earlier time slots\n    epsilon = 1.0 - (current_time_slot / total_time_slots)\n    \n    # Softmax probabilities for actions: assigning higher probability to better scores\n    exp_scores = np.exp(normalized_scores)\n    softmax_probs = exp_scores / np.sum(exp_scores)\n    \n    # Exploration: add stochastic component based on epsilon\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(range(action_count))\n    else:\n        action_index = np.random.choice(range(action_count), p=softmax_probs)\n    \n    return action_index",
          "objective": 288391.83573109633,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Calculate UCB for each action\n    ucb_scores = np.zeros(n_actions)\n    for i, action_index in enumerate(action_indices):\n        if selection_counts[i] > 0:\n            # UCB formula\n            ucb_scores[i] = avg_scores[i] + np.sqrt(2 * np.log(total_selection_count) / selection_counts[i])\n        else:\n            # Encourage exploration of actions that haven't been selected\n            ucb_scores[i] = float('inf')\n\n    # Select action based on the highest UCB score\n    action_index = np.argmax(ucb_scores)\n\n    return action_index",
          "objective": 290813.75906552904,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = len(score_set)  # Should be 8\n    average_scores = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n\n    # Calculate average scores and selection counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Calculate exploration bonus based on selection counts and current time\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1))\n    exploration_factor = 1 / (1 + current_time_slot / total_time_slots)\n\n    # Combine exploitation and exploration\n    total_scores = average_scores + exploration_factor * exploration_bonus\n\n    # Normalize scores to use in softmax\n    exp_scores = np.exp(total_scores - np.max(total_scores))\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Select action index based on calculated probabilities\n    action_index = np.random.choice(range(action_count), p=probabilities)\n    \n    return action_index",
          "objective": 291687.83564559,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Calculate average scores and counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if selection_counts[i] > 0 else 0\n\n    # Dynamic epsilon for exploration\n    epsilon = max(0.05, 1 - (current_time_slot / total_time_slots))\n\n    # Compute UCB values\n    ucb_values = np.zeros(n_actions)\n    \n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_interval\n        else:\n            ucb_values[i] = np.inf  # Indicate unselected actions\n\n    # Enhanced combined scores using a balance of UCB and Exploration\n    combined_scores = (1 - epsilon) * ucb_values + epsilon * np.random.rand(n_actions)\n\n    # Select action based on the combined scores\n    action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": 293408.95037295064,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8  # There are 8 actions, indexed from 0 to 7\n    average_scores = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n\n    # Calculate average scores and selection counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        average_scores[action_index] = np.mean(scores) if selection_counts[action_index] > 0 else 0\n\n    # Calculate exploration term (upper confidence bound)\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))  # 1e-5 to avoid division by zero\n\n    # Adjust exploration factor for diminishing exploration\n    exploration_factor = 1.0 - (current_time_slot / total_time_slots)\n\n    # Combine average scores with exploration\n    adjusted_scores = average_scores + exploration_factor * exploration_bonus\n\n    # Use softmax to determine probabilities\n    exp_scores = np.exp(adjusted_scores - np.max(adjusted_scores))\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Select action index based on calculated probabilities\n    action_index = np.random.choice(np.arange(action_count), p=probabilities)\n    \n    return action_index",
          "objective": 295296.7579923067,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = len(score_set)  # Should be 8\n    average_scores = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n\n    # Calculate average scores and selection counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Calculate the exploration bonus using UCB\n    ucb_values = np.zeros(action_count)\n    for action_index in range(action_count):\n        if selection_counts[action_index] > 0:\n            ucb_values[action_index] = average_scores[action_index] + \\\n                                       np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[action_index])\n        else:\n            # For actions that haven't been selected yet, assign a high UCB value\n            ucb_values[action_index] = float('inf')  # Encourage exploration of unselected actions\n\n    # Select the action with the highest UCB value\n    action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 295351.99674243195,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n    \n    # To ensure we don't encounter division by zero\n    total_selection_count = total_selection_count if total_selection_count > 0 else 1\n\n    # Calculate the UCB values\n    ucb_values = avg_scores + np.sqrt(2 * np.log(total_selection_count) / (selection_counts + 1e-6))\n\n    # Select action: choose the one with the highest UCB value\n    action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 302477.87186406215,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8  # Fixed number of actions\n    average_scores = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n\n    # Calculate average scores and selection counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Calculate an exploration bonus based on Upper Confidence Bound (UCB)\n    exploration_bonus = np.zeros(action_count)\n    for action_index in range(action_count):\n        if selection_counts[action_index] > 0:\n            exploration_bonus[action_index] = np.sqrt(np.log(total_selection_count) / selection_counts[action_index])\n        else:\n            exploration_bonus[action_index] = float('inf')  # Prioritize unselected actions\n\n    # Combine average scores and exploration bonuses\n    total_scores = average_scores + exploration_bonus\n    \n    # Select the action with the highest total score\n    action_index = np.argmax(total_scores)\n\n    return action_index",
          "objective": 305062.9264077917,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores, selection counts, and other necessary variables\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Compute an adaptive epsilon based on current time slot and selection count\n    epsilon = max(0.05, 1.0 - (current_time_slot / total_time_slots))\n\n    # Initialize the exploration strategy\n    explore = np.random.rand() < epsilon\n\n    if explore:\n        # Exploration: Select a random action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploitation: Calculate UCB values\n        ucb_values = np.zeros(n_actions)\n        for i in range(n_actions):\n            if selection_counts[i] > 0:\n                confidence_interval = np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts[i])\n                ucb_values[i] = avg_scores[i] + confidence_interval\n            else:\n                # Favor unexplored actions\n                ucb_values[i] = np.inf\n\n        # Select the action index with the highest UCB value\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 315355.86348287645,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8  # Fixed number of actions\n    average_scores = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n    \n    # Calculate average scores and selection counts\n    for action_index, scores in score_set.items():\n        selection_counts[action_index] = len(scores)\n        if scores:\n            average_scores[action_index] = np.mean(scores)\n    \n    # Set epsilon based on the current time slot\n    epsilon = max(0.1, 0.5 * (1 - (current_time_slot / total_time_slots)))\n    \n    # Calculate UCB values\n    ucb_values = np.zeros(action_count)\n    for i in range(action_count):\n        if selection_counts[i] == 0:\n            ucb_values[i] = np.inf  # Assign high value for unselected actions\n        else:\n            ucb_values[i] = average_scores[i] + np.sqrt(2 * np.log(total_selection_count) / selection_counts[i])\n    \n    # Combine UCB with epsilon and convert to probabilities\n    weighted_ucb = (1 - epsilon) * (ucb_values / np.sum(ucb_values)) \n    exploration = epsilon / action_count\n    action_probabilities = weighted_ucb + exploration\n    \n    # Select an action according to the computed probabilities\n    action_index = np.random.choice(action_count, p=action_probabilities)\n    \n    return action_index",
          "objective": 322260.5249564263,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    average_scores = np.zeros(action_count)\n    \n    # Calculate average scores for each action\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        if scores:\n            average_scores[action_index] = np.mean(scores)\n        else:\n            average_scores[action_index] = 0  # No scores available\n\n    # Compute selection probabilities using more balanced approach\n    selection_count = np.array([len(score_set.get(i, [])) for i in range(action_count)]) + 1  # Add 1 for smoothing\n    normalized_average_scores = average_scores / (selection_count / total_selection_count + 1e-5)  # Smooth division\n\n    # Adjust exploration factor dynamically\n    epsilon = (1.0 - (current_time_slot / total_time_slots)) ** 2  # Increased focus on exploitation as time progresses\n\n    # Compute softmax probabilities for actions\n    exp_scores = np.exp(normalized_average_scores - np.max(normalized_average_scores))  # Stability improvement\n    softmax_probs = exp_scores / np.sum(exp_scores)\n\n    # Exploration vs. exploitation\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(range(action_count))  # Exploration\n    else:\n        action_index = np.random.choice(range(action_count), p=softmax_probs)  # Exploitation based on softmax probabilities\n\n    return action_index",
          "objective": 331188.70626450074,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Calculate Epsilon based on the current time slot\n    epsilon = max(0.05, 1.0 - (current_time_slot / total_time_slots))\n\n    # If total_selection_count is zero, select randomly\n    if total_selection_count == 0:\n        action_index = np.random.choice(action_indices)\n        return action_index\n\n    # Hybrid selection strategy: Exploration vs Exploitation\n    if np.random.rand() < epsilon:\n        # Exploration: Select an underselected action with a modified threshold\n        threshold = np.percentile(selection_counts, 25)  # lower quartile as threshold\n        exploration_choices = [i for i in range(n_actions) if selection_counts[i] < threshold]\n        if exploration_choices:\n            action_index = np.random.choice(exploration_choices)\n        else:\n            # fallback to random selection if all are selected enough\n            action_index = np.random.choice(action_indices)\n    else:\n        # Exploitation: Calculate modified UCB values\n        ucb_values = np.zeros(n_actions)\n        for i in range(n_actions):\n            if selection_counts[i] > 0:\n                confidence_interval = np.sqrt(2 * np.log(total_selection_count) / selection_counts[i])\n                ucb_values[i] = avg_scores[i] + confidence_interval\n            else:\n                # Encourage exploration for unselected actions\n                ucb_values[i] = np.inf\n        \n        # Select the action index with the highest UCB value\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 332989.0518570728,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8  # Number of actions is fixed at 8\n    average_scores = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n\n    # Calculate average scores and selection counts\n    for action_index, scores in score_set.items():\n        selection_counts[action_index] = len(scores)\n        if len(scores) > 0:\n            average_scores[action_index] = np.mean(scores)\n    \n    # Normalize average scores to [0, 1]\n    normalized_scores = average_scores / np.max(average_scores) if np.max(average_scores) > 0 else average_scores\n    \n    # Exploration factor using a decaying epsilon-greedy strategy\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n    \n    # Softmax calculation for action probabilities\n    exp_scores = np.exp(normalized_scores - np.max(normalized_scores))  # Shift for numerical stability\n    softmax_probs = exp_scores / np.sum(exp_scores)\n\n    # Combine softmax probabilities with exploration\n    action_probabilities = (1 - epsilon) * softmax_probs + (epsilon / action_count)\n\n    # Select an action based on calculated probabilities\n    action_index = np.random.choice(action_count, p=action_probabilities)\n\n    return action_index",
          "objective": 333068.8508324244,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Calculate epsilon which decays over time\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n\n    # Create an exploration/exploitation decision\n    if np.random.rand() < epsilon:\n        # Explore: select action randomly, weighted by selection counts\n        weights = np.maximum(1, 1 / (selection_counts + 1e-5))  # avoid division by zero\n        action_index = np.random.choice(action_indices, p=weights / weights.sum())\n    else:\n        # Exploit: select based on modified UCB\n        ucb_values = np.zeros(n_actions)\n        for i in range(n_actions):\n            if selection_counts[i] > 0:\n                confidence_interval = np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts[i])\n                ucb_values[i] = avg_scores[i] + confidence_interval\n            else:\n                ucb_values[i] = np.inf  # Encourage exploration of unselected actions\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 343424.2125204296,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    \n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Calculate UCB values\n    ucb_values = np.zeros(n_actions)\n    for i, action_index in enumerate(action_indices):\n        if selection_counts[i] > 0:\n            ucb_values[i] = avg_scores[i] + np.sqrt(2 * np.log(total_selection_count) / selection_counts[i])\n        else:\n            ucb_values[i] = float('inf')  # Prioritize exploration for unselected actions\n\n    # Epsilon-Greedy Exploration Factor\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))  # Decrease exploration over time\n    exploration_decision = np.random.rand() < epsilon\n\n    if exploration_decision:\n        action_index = np.random.choice(action_indices)  # Select a random action for exploration\n    else:\n        action_index = np.argmax(ucb_values)  # Select action with the highest UCB value for exploitation\n    \n    return action_index",
          "objective": 345481.61223894724,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for action_index in action_indices:\n        scores = score_set[action_index]\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n    \n    # Handle cases where actions have never been selected\n    exploration_weight = total_selection_count / (selection_counts + 1e-5)\n    \n    # Dynamic exploration factor based on the current time slot\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots + 0.1\n    \n    # Calculate selection scores\n    selection_scores = (avg_scores * exploration_weight) + exploration_factor * (1 / (selection_counts + 1e-5))\n    \n    # Select the action with the highest selection score\n    action_index = np.argmax(selection_scores)\n\n    return action_index",
          "objective": 349562.96982514305,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Dynamic epsilon for exploration\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n\n    # Calculate exploration weights based on selection frequency\n    exploration_weights = 1 / (selection_counts + 1)  # Avoid division by zero\n\n    # Calculate modified UCB values\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_interval * exploration_weights[i]\n        else:\n            ucb_values[i] = np.inf  # Encourage exploration if not selected yet\n\n    # Decide action\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        action_index = np.argmax(ucb_values)  # Exploit based on UCB values\n\n    return action_index",
          "objective": 355238.95497156586,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for action_index in action_indices:\n        scores = score_set[action_index]\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n    \n    # Softmax-based exploration\n    exploration_factor = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1))\n    \n    # Calculate utility for each action based on average score and exploration factor\n    utilities = avg_scores + exploration_factor\n\n    # Incorporate a decay factor on exploration based on time\n    decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n    utilities *= decay_factor\n\n    # Handle actions that haven\u2019t been selected yet (assume high enough utility)\n    utilities[selection_counts == 0] = float('inf')\n\n    # Select action based on the highest utility\n    action_index = np.argmax(utilities)\n\n    return action_index",
          "objective": 361447.55234638747,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores, selection counts, and UCB values\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    \n    for action_index in action_indices:\n        scores = score_set[action_index]\n        selection_count = len(scores)\n        selection_counts[action_index] = selection_count\n        \n        if selection_count > 0:\n            avg_scores[action_index] = np.mean(scores)\n\n    # Calculate a baseline for exploration\n    exploration_factor = (np.log(total_selection_count + 1) / (selection_counts + 1e-6))\n    \n    # Calculate UCB for each action\n    ucb_values = avg_scores + exploration_factor\n\n    # Normalize UCB values\n    normalized_ucb = ucb_values - np.max(ucb_values)  # For numerical stability\n    exp_ucb = np.exp(normalized_ucb)\n    probabilities = exp_ucb / np.sum(exp_ucb)\n\n    # Sample an action based on the calculated probabilities\n    action_index = np.random.choice(action_indices, p=probabilities)\n\n    return action_index",
          "objective": 363227.87447753217,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Calculate average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if len(scores) > 0 else 0\n\n    # Epsilon-decay strategy for exploration\n    epsilon = max(0.1, 0.5 * (1 - (current_time_slot / total_time_slots)))\n\n    # Determine action to explore or exploit\n    if np.random.rand() < epsilon:\n        # Exploration: select an unselected action or randomly select a less frequently chosen action\n        unselected_actions = [i for i in range(n_actions) if selection_counts[i] == 0]\n        \n        if unselected_actions:\n            action_index = np.random.choice(unselected_actions)  # Choose any unselected action\n        else:\n            action_index = np.random.choice(action_indices)  # Choose any action\n    else:\n        # Exploitation: Compute UCB scores\n        ucb_scores = np.zeros(n_actions)\n        for i in range(n_actions):\n            if selection_counts[i] > 0:\n                ucb_scores[i] = avg_scores[i] + np.sqrt((2 * np.log(total_selection_count)) / selection_counts[i])\n            else:\n                ucb_scores[i] = float('inf')  # Encourage unselected actions\n\n        action_index = action_indices[np.argmax(ucb_scores)]  # Select the action with the highest UCB score\n\n    return action_index",
          "objective": 382586.03640836355,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    \n    # Calculate average scores and selection counts\n    for action_index in action_indices:\n        scores = score_set[action_index]\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n    \n    # Calculate an exploration factor based on the current time slot\n    exploration_factor = max(0.5, 1 - (current_time_slot / total_time_slots))\n    \n    # Select an action based on exploration and exploitation\n    ucb_values = np.zeros(n_actions)\n    for action_index in action_indices:\n        if selection_counts[action_index] > 0:\n            exploration_component = np.sqrt(2 * np.log(total_selection_count) / selection_counts[action_index])\n            ucb_values[action_index] = avg_scores[action_index] + exploration_component * exploration_factor\n        else:\n            ucb_values[action_index] = float('inf')  # Encourage exploration of unselected actions\n\n    # Select action based on the highest modified UCB\n    action_index = np.argmax(ucb_values)\n    \n    return action_index",
          "objective": 399086.265171735,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    \n    for action_index in action_indices:\n        scores = score_set[action_index]\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n    \n    # Calculate the exploration factor which decays over time\n    exploration_factor = 1.0 - (current_time_slot / total_time_slots)\n    \n    # Calculate UCB values with decaying exploration factor\n    ucb_values = np.zeros(n_actions)\n    for action_index in action_indices:\n        if selection_counts[action_index] > 0:\n            ucb_values[action_index] = (\n                avg_scores[action_index] + \n                np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts[action_index] + 1))\n            ) * exploration_factor\n        else:\n            ucb_values[action_index] = float('inf')  # Unexplored actions\n    \n    # Select action based on the highest UCB\n    action_index = np.argmax(ucb_values)\n    \n    return action_index",
          "objective": 399234.6536412761,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for action_index in action_indices:\n        scores = score_set[action_index]\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n    \n    # Determine the exploration factor\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n    \n    # Randomly explore with probability epsilon\n    if np.random.rand() < epsilon:\n        # Select a random action\n        action_index = np.random.choice(action_indices)\n    else:\n        # UCB calculation\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))  # Small constant to avoid division by zero\n        ucb_scores = avg_scores + exploration_bonus\n        \n        # Select the action with the highest UCB score\n        action_index = np.argmax(ucb_scores)\n\n    return action_index",
          "objective": 405880.1734193866,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = len(score_set)  # Should be 8\n    average_scores = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n\n    # Calculate average scores and selection counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Dynamic exploration strategy using softmax\n    temperature = max(0.1, 1 - (current_time_slot / total_time_slots))\n    \n    # Calculate action probabilities using softmax\n    exp_scores = np.exp(average_scores / temperature)\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Choose action based on calculated probabilities\n    action_index = np.random.choice(range(action_count), p=probabilities)\n\n    return action_index",
          "objective": 409982.4504974622,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    \n    # Compute average scores and selection counts\n    for action_index in action_indices:\n        scores = score_set[action_index]\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n    \n    # Dynamic epsilon based on the time slot\n    epsilon = 1 - (current_time_slot / total_time_slots)\n    epsilon = max(0.1, epsilon)  # Ensure epsilon doesn't go too low but allows for exploration adjustments\n    \n    # Generate a random number to decide whether to explore or exploit\n    if np.random.rand() < epsilon:\n        # Exploration: select an action uniformly at random\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploitation: select the action with the highest average score\n        action_index = np.argmax(avg_scores)\n    \n    return action_index",
          "objective": 413732.9473090255,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        \n        # Handle the case where no scores are available\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Calculate exploration rate based on current time slot\n    exploration_factor = max(1.0 - (current_time_slot / total_time_slots), 0.1)\n    \n    # Use an epsilon decay for exploration strategy\n    epsilon = exploration_factor\n    adjusted_scores = avg_scores + (1.0 / (selection_counts + 1e-5))  # Small constant to avoid division by zero\n    \n    # Normalize adjusted scores into a probability distribution using softmax\n    exp_adjusted_scores = np.exp(adjusted_scores - np.max(adjusted_scores))\n    probabilities = exp_adjusted_scores / np.sum(exp_adjusted_scores)\n\n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        # Exploration: select a random action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploitation: select based on calculated probabilities\n        action_index = np.random.choice(action_indices, p=probabilities)\n\n    return action_index",
          "objective": 421300.4411574226,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Calculate average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n        else:\n            avg_scores[i] = 0  # Action not tried yet\n\n    # Epsilon-greedy with a decay for exploration\n    epsilon = max(0.01, 1.0 - (current_time_slot / total_time_slots) * 0.9)  # Decay epsilon over time but keep above 0.01\n    random_choice = np.random.rand() < epsilon\n\n    if random_choice:  # Explore\n        action_index = np.random.choice(action_indices)\n    else:  # Exploit using UCB\n        exploration_factor = np.log(total_selection_count + 1) / (selection_counts + 1e-5)  # Adding a small constant to avoid division by zero\n        ucb_scores = avg_scores + np.sqrt(2 * exploration_factor)\n        action_index = action_indices[np.argmax(ucb_scores)]\n\n    return action_index",
          "objective": 425631.4908732943,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = len(score_set)  # Should be 8\n    average_scores = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n\n    # Calculate average scores and selection counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Define epsilon value that decreases as time progresses\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n\n    # Randomly select an action based on epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        # Exploration: choose an action randomly\n        action_index = np.random.choice(range(action_count))\n    else:\n        # Exploitation: choose action with the highest average score\n        exploration_bonus = np.where(selection_counts > 0,\n                                      np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1)),\n                                      0)  # Set to 0 for actions not selected yet\n        total_scores = average_scores + exploration_bonus\n        action_index = np.argmax(total_scores)\n\n    return action_index",
          "objective": 426556.527770402,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts for each action\n    for action_index in action_indices:\n        scores = score_set[action_index]\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n        else:\n            # Assign a small initial score to unselected actions to encourage exploration\n            avg_scores[action_index] = 0.5\n\n    # Dynamic epsilon for exploration/exploitation balance\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n    exploration = np.random.rand() < epsilon\n\n    if exploration:\n        # Explore: Select a random action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploit: Use UCB strategy\n        ucb_values = np.copy(avg_scores)\n        total_counts = total_selection_count + 1e-5  # Avoid division by zero\n\n        for i in range(n_actions):\n            if selection_counts[i] > 0:\n                confidence_interval = np.sqrt((2 * np.log(total_counts)) / selection_counts[i])\n                ucb_values[i] += confidence_interval\n\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 435688.10198254924,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Compute average scores and selection counts\n    for action_index in action_indices:\n        scores = score_set[action_index]\n        selection_counts[action_index] = len(scores)\n        \n        if selection_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n\n    # Calculate epsilon for exploration-exploitation trade-off\n    epsilon = max(0.1, 1.0 - current_time_slot / total_time_slots)\n\n    # UCB-based approach to compute bonus scores\n    def calculate_bonus(counts):\n        return np.sqrt(np.log(total_selection_count + 1) / (counts + 1e-5))\n\n    bonus_scores = calculate_bonus(selection_counts)\n    adjusted_scores = avg_scores + bonus_scores\n\n    # Normalize adjusted scores into a probability distribution\n    exp_adjusted_scores = np.exp(adjusted_scores - np.max(adjusted_scores))\n    probabilities = exp_adjusted_scores / np.sum(exp_adjusted_scores)\n\n    # Epsilon-greedy selection based on exploration-exploitation\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = np.random.choice(action_indices, p=probabilities)\n\n    return action_index",
          "objective": 436348.25104568683,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for action_index in action_indices:\n        scores = score_set[action_index]\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n    \n    # Calculate exploration factor (Epsilon)\n    epsilon = 1 / (current_time_slot + 1)  # Decay exploration over time\n\n    # Select action using Epsilon-Greedy strategy\n    if np.random.rand() < epsilon:\n        # Explore: select a random action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploit: select the action with the highest average score\n        action_index = np.argmax(avg_scores)\n\n    return action_index",
          "objective": 439523.9488012071,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = len(score_set)  # Should be 8\n    average_scores = np.zeros(action_count)\n    \n    # Calculate average scores\n    for action_index in range(action_count):\n        scores = score_set[action_index]\n        if len(scores) > 0:\n            average_scores[action_index] = np.mean(scores)\n        else:\n            average_scores[action_index] = 0  # Action has never been selected\n\n    # Normalize average scores to avoid potential zero division using a small constant\n    epsilon = 1e-6\n    normalized_scores = average_scores + epsilon\n    \n    # Calculate selection probabilities using epsilon-greedy strategy\n    exploration_probability = max(0.1, 1 - (current_time_slot / total_time_slots))  # Decaying exploration\n    exploitation_probability = 1 - exploration_probability\n\n    # Compute probabilities\n    probabilities = np.full(action_count, exploration_probability / action_count)  # Uniform exploration\n    best_action_index = np.argmax(normalized_scores)\n    probabilities[best_action_index] += exploitation_probability\n\n    # Stochastic action selection based on computed probabilities\n    action_index = np.random.choice(action_count, p=probabilities)\n    \n    return action_index",
          "objective": 442637.49124043743,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = len(score_set)  # Should be 8\n    average_scores = []\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n\n    # Calculate average scores for each action\n    for action_index in range(action_count):\n        scores = score_set[action_index]\n        average_score = np.mean(scores) if scores else 0\n        average_scores.append(average_score)\n\n    average_scores = np.array(average_scores)\n\n    # Softmax for probability distribution of actions\n    exp_scores = np.exp(average_scores - np.max(average_scores))  # Stability improvement\n    softmax_probabilities = exp_scores / np.sum(exp_scores)\n\n    # Dynamic exploration factor\n    exploration_rate = 1.0 - (current_time_slot / total_time_slots) ** 2  # Smoother decay\n\n    # Epsilon-greedy adjustment for exploration\n    epsilon = 0.1 * exploration_rate\n    greedy_choice = np.argmax(average_scores)  # Exploit the best-known action\n\n    # Choice balancing exploration and exploitation\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_count)  # Explore\n    else:\n        action_index = greedy_choice  # Exploit\n\n    return action_index",
          "objective": 462097.1577400231,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    \n    for action_index in action_indices:\n        scores = score_set[action_index]\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n    \n    # Calculate UCB for each action\n    ucb_values = np.zeros(n_actions)\n    for action_index in action_indices:\n        if selection_counts[action_index] == 0:\n            # Encourage exploration of unselected actions\n            ucb_values[action_index] = float('inf')\n        else:\n            ucb_values[action_index] = avg_scores[action_index] + \\\n                                       np.sqrt((2 * np.log(total_selection_count)) / selection_counts[action_index])\n\n    # Introduce a time-based exploration factor\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n    weighted_ucb = exploration_factor * ucb_values\n    \n    # Select action based on the highest weighted UCB value\n    action_index = np.argmax(weighted_ucb)\n    \n    return action_index",
          "objective": 467055.9423513686,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = len(score_set)  # Should be 8\n    average_scores = []\n    selection_counts = []\n\n    # Calculate average scores and selection counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        avg_score = np.mean(scores) if scores else 0.0\n        average_scores.append(avg_score)\n        selection_counts.append(len(scores))\n\n    # Calculate selection bonus based on total selection counts\n    selection_bonus = np.array([1 / (count + 1) for count in selection_counts])\n\n    # Combine average scores with selection bonus\n    adjusted_scores = np.array(average_scores) + selection_bonus\n    \n    # Softmax for action selection\n    exp_scores = np.exp(adjusted_scores - np.max(adjusted_scores))  # Avoid overflow\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Sample action based on calculated probabilities\n    action_index = np.random.choice(range(action_count), p=probabilities)\n    \n    return action_index",
          "objective": 479615.22879101755,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Initialize average scores, selection counts and explored actions\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for action_index in action_indices:\n        scores = score_set[action_index]\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n\n    # Epsilon for exploration vs exploitation\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n    \n    # Upper Confidence Bound calculation\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            ucb_values[i] = avg_scores[i] + np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[i])\n        else:\n            ucb_values[i] = float('inf')  # Favors actions not yet selected\n\n    # Adjust exploration based on epsilon\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        action_index = np.argmax(ucb_values)  # Exploit\n\n    return action_index",
          "objective": 485097.62741590507,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for action_index in action_indices:\n        scores = score_set[action_index]\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n    \n    # Avoid division by zero: Use a small constant in case of zero selections\n    selection_counts[selection_counts == 0] = 1e-10\n    \n    # Compute a softmax score to balance exploration and exploitation\n    softmax_scores = avg_scores / selection_counts\n    exp_scores = np.exp(softmax_scores - np.max(softmax_scores))  # Stability with max normalization\n    action_probabilities = exp_scores / np.sum(exp_scores)\n\n    # Dynamic exploration bonus based on the current time slot\n    exploration_bonus = (1 - (current_time_slot / total_time_slots)) * (1 / (1 + selection_counts))\n    adjusted_probabilities = action_probabilities + exploration_bonus\n    adjusted_probabilities = adjusted_probabilities / np.sum(adjusted_probabilities)  # Normalize probabilities\n\n    # Select action based on adjusted probabilities\n    action_index = np.random.choice(action_indices, p=adjusted_probabilities)\n\n    return action_index",
          "objective": 489880.25085575896,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    \n    for action_index in action_indices:\n        scores = score_set[action_index]\n        selection_count = len(scores)\n        selection_counts[action_index] = selection_count\n        \n        if selection_count > 0:\n            avg_scores[action_index] = np.mean(scores)\n\n    # Calculate a baseline for exploration\n    exploration_factor = 1.0 - (current_time_slot / total_time_slots)\n\n    # Calculate adjusted scores incorporating the exploration factor\n    adjusted_scores = avg_scores + exploration_factor * (1.0 / (selection_counts + 1))\n\n    # Apply softmax to the adjusted scores to get probability distribution over actions\n    exp_scores = np.exp(adjusted_scores - np.max(adjusted_scores))  # Stability for large exponents\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Sample an action based on the calculated probabilities\n    action_index = np.random.choice(action_indices, p=probabilities)\n\n    return action_index",
          "objective": 492872.163796585,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8  # Fixed number of actions\n    average_scores = []\n    score_variances = []\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        if len(scores) > 0:\n            average_score = np.mean(scores)\n            variance_score = np.var(scores)\n        else:\n            average_score = 0\n            variance_score = 0  # Handle actions that have never been selected\n        \n        average_scores.append(average_score)\n        score_variances.append(variance_score)\n    \n    average_scores = np.array(average_scores)\n    score_variances = np.array(score_variances)\n    \n    # Normalize average scores and variances\n    normalized_scores = average_scores / (total_selection_count + 1e-5)\n    normalized_variances = score_variances / (total_selection_count + 1e-5)\n\n    # Combine average scores and variances into a single metric for better exploration-exploitation balance\n    combined_scores = normalized_scores - normalized_variances  # Prefer actions with high scores and low variance\n\n    # Calculate exploration rate; more exploration in earlier time slots\n    epsilon = 1.0 - (current_time_slot / total_time_slots)\n    epsilon = max(0.1, epsilon)  # Ensure a minimum exploration rate\n\n    # Softmax probabilities based on combined scores\n    exp_scores = np.exp(combined_scores - np.max(combined_scores))  # Subtract max for numerical stability\n    softmax_probs = exp_scores / np.sum(exp_scores)\n\n    # Exploration: add stochastic component based on epsilon\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(range(action_count))\n    else:\n        action_index = np.random.choice(range(action_count), p=softmax_probs)\n    \n    return action_index",
          "objective": 498557.7558569247,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if selection_counts[i] > 0 else 0.0\n\n    # Calculate exploration bonus using the UCB formula\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n    \n    # Calculate adjusted scores for selection\n    adjusted_scores = avg_scores + exploration_bonus\n\n    # Select action based on the highest adjusted score\n    action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 505024.61497847375,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    \n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Calculate the exploration factor using an Epsilon-Greedy strategy\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n    \n    # Exploration: Randomly select based on epsilon\n    if np.random.rand() < epsilon:\n        # Choose a random action for exploration\n        return np.random.choice(action_indices)\n\n    # Exploitation: Use modified UCB for balanced selection\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            # Calculate modified UCB value\n            confidence_interval = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_interval\n        else:\n            # If the action has never been selected, assign a very high UCB to encourage its selection\n            ucb_values[i] = np.inf\n\n    # Select the action index with the highest UCB value\n    action_index = np.argmax(ucb_values)\n    \n    return action_index",
          "objective": 507125.1231446135,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Adaptive epsilon based on total selections and time slots\n    epsilon = max(0.1, 1.0 - (current_time_slot / total_time_slots) * 0.9)\n    \n    # Calculate UCB values\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_interval\n        else:\n            ucb_values[i] = np.inf  # Encouragement for unselected actions\n\n    # Epsilon-Greedy with UCB-based exploitation\n    if np.random.rand() < epsilon:\n        # Exploration: Randomly select an action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploitation: Use UCB to select action\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 540155.2859248703,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8  # Fixed number of actions\n    average_scores = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n    \n    # Compute the average scores and the count of selections for each action\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Dynamic exploration factor decreasing over time\n    exploration_factor = np.clip(1 - (current_time_slot / total_time_slots), 0.1, 1)\n\n    # Softmax distribution for selecting actions based on average scores\n    temperature = 1 / exploration_factor  # Higher temperature leads to more exploration\n    exp_scores = np.exp(average_scores / temperature)\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Use the probabilities to select an action\n    action_index = np.random.choice(range(action_count), p=probabilities)\n\n    return action_index",
          "objective": 541535.5136849333,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Calculate average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for action_index in action_indices:\n        scores = score_set[action_index]\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n\n    # Dynamically adjust exploration rate\n    epsilon = (total_time_slots - current_time_slot) / total_time_slots\n    \n    # Randomly decide whether to explore or exploit\n    if np.random.rand() < epsilon:\n        # Exploration: select a random action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploitation: select the action with the highest average score\n        action_index = np.argmax(avg_scores)\n\n    return action_index",
          "objective": 555918.3060001155,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8  # Fixed number of actions\n    average_scores = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n\n    # Calculate average scores and selection counts\n    for action_index, scores in score_set.items():\n        selection_counts[action_index] = len(scores)\n        if scores:  # Avoid division by zero\n            average_scores[action_index] = np.mean(scores)\n\n    # Calculate Upper Confidence Bound (UCB) values\n    ucb_values = np.zeros(action_count)\n    for i in range(action_count):\n        if selection_counts[i] > 0:\n            ucb_values[i] = average_scores[i] + np.sqrt(2 * np.log(total_selection_count) / selection_counts[i])\n        else:\n            ucb_values[i] = np.inf  # Prioritize unselected actions\n\n    # Exploration factor that decays over time\n    epsilon = max(0.1, 1.0 - (current_time_slot / total_time_slots))  # Decay epsilon\n\n    # Combine UCB values with the exploration factor\n    combined_scores = (1 - epsilon) * ucb_values + epsilon * (np.ones(action_count) / action_count)\n    action_probabilities = combined_scores / np.sum(combined_scores)  # Normalize probabilities\n\n    # Select an action based on the computed probabilities\n    action_index = np.random.choice(action_count, p=action_probabilities)\n    \n    return action_index",
          "objective": 559151.0531443472,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Adaptive exploration rate (epsilon)\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n\n    # Calculate UCB values\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_interval\n        else:\n            ucb_values[i] = np.inf  # Encourage exploration for untried actions\n\n    # Combine UCB and average scores to create a hybrid strategy\n    hybrid_scores = (1 - epsilon) * ucb_values + epsilon * np.random.rand(n_actions)\n\n    # Select action based on scores calculated\n    action_index = np.argmax(hybrid_scores)\n\n    return action_index",
          "objective": 574010.7317230392,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8  # Fixed number of actions\n    average_scores = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n\n    # Calculate average scores and selection counts\n    for action_index, scores in score_set.items():\n        selection_counts[action_index] = len(scores)\n        if scores:\n            average_scores[action_index] = np.mean(scores)\n\n    # Calculate the exploration parameter using a decaying epsilon strategy\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))  # Epsilon decreases over time\n\n    # Calculate Upper Confidence Bound (UCB) values\n    ucb_values = np.zeros(action_count)\n    for i in range(action_count):\n        if selection_counts[i] > 0:\n            ucb_values[i] = average_scores[i] + np.sqrt(2 * np.log(total_selection_count) / selection_counts[i])\n        else:\n            ucb_values[i] = np.inf  # Unselected actions get the highest UCB\n\n    # Combine UCB and explore-exploit decision\n    action_probabilities = (1 - epsilon) * (ucb_values / np.sum(ucb_values)) + (epsilon / action_count)\n\n    # Select an action using the computed probabilities\n    action_index = np.random.choice(action_count, p=action_probabilities)\n\n    return action_index",
          "objective": 575694.3422598032,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        # Calculate average score only if there are scores\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Dynamic epsilon based on the current time slot\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n    \n    # Select action based on Epsilon-Greedy strategy\n    if np.random.rand() < epsilon:\n        # Exploration: choose a random action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploitation: calculate adjusted UCB for each action\n        ucb_values = np.zeros(n_actions)\n        for i in range(n_actions):\n            if selection_counts[i] > 0:\n                confidence_interval = np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts[i])\n                ucb_values[i] = avg_scores[i] + confidence_interval\n            else:\n                # If the action has never been selected, ensure it is considered\n                ucb_values[i] = np.inf\n\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 581646.4929974766,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = len(score_set)  # Should be 8\n    average_scores = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n\n    # Calculate average scores and selection counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Calculate exploration bonus\n    exploration_bonus = np.where(selection_counts > 0, \n                                  np.sqrt(np.log(total_selection_count + 1) / selection_counts), \n                                  np.inf)  # Use inf for unselected actions\n\n    # Dynamic exploration factor based on current time slot\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n\n    # Combine exploitation and exploration\n    total_scores = average_scores + exploration_factor * exploration_bonus\n\n    # Normalize scores to use in softmax\n    exp_scores = np.exp(total_scores - np.max(total_scores))\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Select action index based on calculated probabilities\n    action_index = np.random.choice(range(action_count), p=probabilities)\n    \n    return action_index",
          "objective": 609068.679590165,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8  # Fixed number of actions\n    average_scores = []\n    selection_counts = [len(scores) for scores in score_set.values()]\n    \n    # Calculate average scores for each action\n    for action_index in range(action_count):\n        scores = score_set[action_index]\n        if len(scores) > 0:  # If the action has been selected before\n            average_score = np.mean(scores)\n        else:\n            average_score = 0  # Actions not selected have a score of 0\n        average_scores.append(average_score)\n\n    # Convert to numpy array for easier manipulation\n    average_scores = np.array(average_scores)\n    selection_counts = np.array(selection_counts)\n    \n    # Calculate a score to encourage exploration\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1))  # Avoid division by zero\n    exploration_scores = average_scores + exploration_bonus\n    \n    # Softmax probabilities calculation based on exploration scores\n    exp_scores = np.exp(exploration_scores - np.max(exploration_scores))  # For numerical stability\n    action_probabilities = exp_scores / np.sum(exp_scores)\n\n    # Temperature parameter for adaptive exploration\n    temperature = max(0.1, 1 - (current_time_slot / total_time_slots))\n\n    # Adjust probabilities based on temperature\n    adjusted_probabilities = np.power(action_probabilities, 1 / temperature)\n    adjusted_probabilities /= np.sum(adjusted_probabilities)  # Normalize to sum to 1\n\n    # Select action based on computed probabilities\n    action_index = np.random.choice(range(action_count), p=adjusted_probabilities)\n    \n    return action_index",
          "objective": 625029.7518588105,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8  # Fixed number of actions\n    average_scores = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n    \n    # Calculate average scores and selection counts\n    for action_index, scores in score_set.items():\n        selection_counts[action_index] = len(scores)\n        if len(scores) > 0:\n            average_scores[action_index] = np.mean(scores)\n    \n    # Calculate exploration probability using a decaying epsilon-greedy strategy\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))  # Epsilon decreases over time\n\n    # Calculate Upper Confidence Bound (UCB)\n    ucb_values = np.zeros(action_count)\n    for i in range(action_count):\n        if selection_counts[i] == 0:\n            ucb_values[i] = np.inf  # High value for unselected actions to ensure exploration\n        else:\n            ucb_values[i] = average_scores[i] + np.sqrt(2 * np.log(total_selection_count) / selection_counts[i])\n    \n    # Combine UCB with exploration decisions for action probabilities\n    total_ucb = np.sum(ucb_values)\n    action_probabilities = (1 - epsilon) * (ucb_values / total_ucb) + (epsilon / action_count)\n    \n    # Select an action based on computed probabilities\n    action_index = np.random.choice(action_count, p=action_probabilities)\n    \n    return action_index",
          "objective": 626606.3277933426,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8  # Number of actions is fixed at 8\n    average_scores = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n    \n    # Calculate average scores and selection counts\n    for action_index, scores in score_set.items():\n        selection_counts[action_index] = len(scores)\n        if scores:  # If there are scores available\n            average_scores[action_index] = np.mean(scores)\n\n    # Calculate exploration probability using a decaying epsilon strategy\n    exploration_strength = max(0.1, 1 - (current_time_slot / total_time_slots))\n\n    # Calculate Upper Confidence Bound (UCB)\n    ucb_values = np.zeros(action_count)\n    for i in range(action_count):\n        if selection_counts[i] == 0:\n            ucb_values[i] = np.inf  # Action has never been selected\n        else:\n            ucb_values[i] = average_scores[i] + np.sqrt(2 * np.log(total_selection_count) / selection_counts[i])\n    \n    # Normalize UCB values for probability calculation\n    ucb_total = np.sum(np.exp(ucb_values))\n    ucb_probabilities = np.exp(ucb_values) / ucb_total\n    \n    # Combine UCB with exploration strategy\n    action_probabilities = (1 - exploration_strength) * ucb_probabilities + (\n        exploration_strength / action_count\n    )\n    \n    # Select an action based on the computed probabilities\n    action_index = np.random.choice(action_count, p=action_probabilities)\n    \n    return action_index",
          "objective": 652549.3097634075,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Calculate the exploration factor (epsilon)\n    epsilon = 1 - (current_time_slot / total_time_slots)\n\n    # Explore lesser-selected actions with some probability\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        # Calculate UCB values for exploitation\n        ucb_values = np.zeros(n_actions)\n        for i in range(n_actions):\n            if selection_counts[i] > 0:\n                confidence_interval = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[i])\n                ucb_values[i] = avg_scores[i] + confidence_interval\n            else:\n                ucb_values[i] = np.inf  # Encourage exploration of never-selected actions\n\n        # Select the action with the highest UCB value\n        action_index = np.argmax(ucb_values)\n    \n    return action_index",
          "objective": 653949.181934264,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    \n    for action_index in action_indices:\n        scores = score_set[action_index]\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n    \n    # Handle exploration via softmax\n    # Adding a small constant to avoid zero scores affecting exponentiation\n    adjusted_scores = avg_scores + 1e-5\n    exp_scores = np.exp(adjusted_scores)\n    \n    # Calculate softmax probabilities\n    probabilities = exp_scores / np.sum(exp_scores)\n    \n    # Select action based on computed probabilities\n    action_index = np.random.choice(action_indices, p=probabilities)\n\n    return action_index",
          "objective": 700520.5826507602,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for action_index in action_indices:\n        scores = score_set[action_index]\n        selection_counts[action_index] = len(scores)\n        avg_scores[action_index] = np.mean(scores) if selection_counts[action_index] > 0 else 0\n\n    # Calculate exploration factor (Epsilon) - decaying exploration\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))  # Min exploration chance\n\n    # Calculate softmax probabilities\n    softmax_denominator = np.sum(np.exp(avg_scores))\n    softmax_probs = np.exp(avg_scores) / softmax_denominator if softmax_denominator > 0 else np.zeros(n_actions)\n\n    # Explore: random selection based on exploration probability\n    if np.random.rand() < epsilon:\n        # Generate a random action based on selection counts for more balanced exploration\n        selection_probabilities = (1 / (selection_counts + 1e-5))  # Adding a small constant to avoid division by zero\n        selection_probabilities /= np.sum(selection_probabilities)\n        action_index = np.random.choice(action_indices, p=selection_probabilities)\n    else:\n        # Exploit: select action based on softmax probabilities\n        action_index = np.random.choice(action_indices, p=softmax_probs)\n\n    return action_index",
          "objective": 729662.2272702847,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8  # Fixed number of actions\n    average_scores = []\n    selection_counts = [len(scores) for scores in score_set.values()]\n\n    # Calculate average scores for each action\n    for action_index in range(action_count):\n        scores = score_set[action_index]\n        if len(scores) > 0:\n            average_score = np.mean(scores)\n        else:\n            average_score = 0  # Actions not selected have a score of 0\n        average_scores.append(average_score)\n\n    # Softmax probabilities calculation\n    exp_scores = np.exp(average_scores - np.max(average_scores))  # For numerical stability\n    action_probabilities = exp_scores / np.sum(exp_scores)\n\n    # Temperature parameter for exploration\n    temperature = max(0.1, 1 - (current_time_slot / total_time_slots))\n\n    # Adjust probabilities based on temperature\n    adjusted_probabilities = np.power(action_probabilities, 1 / temperature)\n    adjusted_probabilities /= np.sum(adjusted_probabilities)  # Normalize to sum to 1\n\n    # Select action based on computed probabilities\n    action_index = np.random.choice(range(action_count), p=adjusted_probabilities)\n    \n    return action_index",
          "objective": 756295.1779866582,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8  # Fixed number of actions\n    average_scores = []\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n\n    # Calculate average scores for each action\n    for action_index in range(action_count):\n        scores = score_set[action_index]\n        average_score = np.mean(scores) if len(scores) > 0 else 0\n        average_scores.append(average_score)\n\n    # Convert average scores to an array for better manipulation\n    average_scores = np.array(average_scores)\n\n    # Calculate action exploration risks\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1))\n    \n    # Combine average scores with exploration bonus\n    adjusted_scores = average_scores + exploration_bonus\n\n    # Normalize scores to get probabilities\n    exp_scores = np.exp(adjusted_scores - np.max(adjusted_scores))  # Numerical stability\n    action_probabilities = exp_scores / np.sum(exp_scores)\n\n    # Epsilon-greedy strategy for exploration\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n    if np.random.rand() < epsilon:\n        # Randomly explore an action\n        action_index = np.random.choice(range(action_count))\n    else:\n        # Select action based on computed probabilities\n        action_index = np.random.choice(range(action_count), p=action_probabilities)\n\n    return action_index",
          "objective": 773064.3707952393,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = len(score_set)  # Should be 8\n    average_scores = []\n    \n    for action_index in range(action_count):\n        scores = score_set[action_index]\n        if len(scores) > 0:\n            average_score = np.mean(scores)\n        else:\n            average_score = 0  # Handle actions that have never been selected\n        \n        average_scores.append(average_score)\n\n    # Exploration factor (epsilon-greedy)\n    epsilon = 0.1 * (total_time_slots - current_time_slot) / total_time_slots  # Decrease as time slots progress\n    \n    if np.random.rand() < epsilon:  # Explore\n        action_index = np.random.choice(range(action_count))\n    else:  # Exploit\n        action_index = np.argmax(average_scores)\n    \n    return action_index",
          "objective": 782964.9852796731,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Initialize arrays to store average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in action_indices:\n        scores = score_set[action_index]\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n    \n    # Define epsilon for exploration\n    epsilon = max(0.1, 1 - current_time_slot / total_time_slots)  # Decrease exploration over time\n    \n    # Decide whether to explore or exploit\n    if np.random.rand() < epsilon:\n        # Exploration: select a random action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploitation: select the action with the highest average score\n        action_index = np.argmax(avg_scores)\n    \n    return action_index",
          "objective": 813877.6017989211,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8  # Fixed number of actions\n    average_scores = []\n    action_selection_counts = []\n\n    # Calculate average scores and selection counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_selection_count = len(scores)\n        action_selection_counts.append(action_selection_count)\n        \n        average_score = np.mean(scores) if action_selection_count > 0 else 0\n        average_scores.append(average_score)\n\n    # Dynamic epsilon calculation based on current time slot\n    epsilon = max(0.1, min(1.0, 1 - (current_time_slot / total_time_slots))) \n\n    # Exploration vs. exploitation\n    if np.random.rand() < epsilon:\n        # Exploration: Randomly select an action\n        action_index = np.random.randint(action_count)\n    else:\n        # Exploitation: Select the action with the highest average score\n        action_index = np.argmax(average_scores)\n\n    return action_index",
          "objective": 824629.1072125563,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = len(score_set)  # Should be 8\n    average_scores = []\n    \n    # Calculate average scores and handle edge cases\n    for action_index in range(action_count):\n        scores = score_set[action_index]\n        if len(scores) > 0:\n            average_score = np.mean(scores)\n        else:\n            average_score = 0  # Handle actions that have never been selected\n        \n        average_scores.append(average_score)\n\n    # Convert average scores to a numpy array for further calculations\n    average_scores = np.array(average_scores)\n    # Apply softmax to the average scores to get selection probabilities\n    exp_scores = np.exp(average_scores - np.max(average_scores))  # Stability improvement\n    selection_probabilities = exp_scores / np.sum(exp_scores)\n\n    # Exploration factor\n    exploration_factor = 1 - (current_time_slot / total_time_slots)\n    \n    # Adjust probabilities based on exploration factor\n    probabilities_with_exploration = selection_probabilities * exploration_factor + (1 / action_count) * (1 - exploration_factor)\n    \n    # Select an action based on the adjusted probabilities\n    action_index = np.random.choice(action_count, p=probabilities_with_exploration)\n    \n    return action_index",
          "objective": 825798.6207934541,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Exploration factor decays over time\n    exploration_factor = np.sqrt(total_time_slots / (current_time_slot + 1))  # Decay factor\n\n    # UCB calculation\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            ucb_values[i] = avg_scores[i] + exploration_factor * np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts[i])\n        else:\n            # Assign a high score to unselected actions to encourage exploration\n            ucb_values[i] = float('inf')\n    \n    # Select the action with the highest UCB value\n    action_index = action_indices[np.argmax(ucb_values)]\n\n    return action_index",
          "objective": 889728.3187042305,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n    \n    # Dynamic epsilon based on the current time slot\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n    \n    # Random selection based on epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        # Exploration: select a random action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploitation: select the action with the highest average score\n        action_index = action_indices[np.argmax(avg_scores)]\n    \n    return action_index",
          "objective": 913893.2342265173,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8  # Fixed number of actions\n    average_scores = np.zeros(action_count)\n    score_variances = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Calculate average scores, variances, and action counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        \n        if action_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n            score_variances[action_index] = np.var(scores)\n    \n    # Handling division by total_selection_count\n    normalized_scores = average_scores / (total_selection_count + 1)\n    normalized_variances = score_variances / (total_selection_count + 1)\n\n    # Combine scores (to prefer actions with high scores and low variance)\n    combined_scores = normalized_scores - normalized_variances\n\n    # Determine exploration rate based on the current time slot\n    exploration_rate = 1.0 - (current_time_slot / total_time_slots)\n    exploration_rate = max(0.1, exploration_rate)  # Ensure minimum exploration rate\n\n    # Softmax probabilities based on combined scores\n    exp_scores = np.exp(combined_scores - np.max(combined_scores))  # For stability\n    softmax_probs = exp_scores / np.sum(exp_scores)\n\n    # Implement epsilon-greedy exploration\n    if np.random.rand() < exploration_rate:\n        action_index = np.random.choice(range(action_count))\n    else:\n        action_index = np.random.choice(range(action_count), p=softmax_probs)\n\n    return action_index",
          "objective": 914201.1166894592,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize variables\n    actions_count = len(score_set)  # Number of actions (should be 8)\n\n    # Calculate average scores for each action\n    average_scores = np.zeros(actions_count)\n    for action_index in range(actions_count):\n        scores = score_set.get(action_index, [])\n        if scores:\n            average_scores[action_index] = np.mean(scores)\n\n    # Calculate a temperature parameter to control exploration\n    temperature = total_time_slots / (total_time_slots - current_time_slot + 1)\n\n    # Compute the softmax probabilities for each action\n    exp_scores = np.exp(average_scores / temperature)\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Select an action based on the computed probabilities\n    action_index = np.random.choice(actions_count, p=probabilities)\n\n    return action_index",
          "objective": 931739.1041123219,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(8)\n    selection_counts = np.zeros(8)\n\n    # Calculate average scores and selection counts for each action\n    for action_index in range(8):\n        if action_index in score_set and score_set[action_index]:\n            average_scores[action_index] = np.mean(score_set[action_index])\n            selection_counts[action_index] = len(score_set[action_index])\n    \n    # Handle cases where selection count is zero to avoid division by zero\n    selection_counts = np.where(selection_counts == 0, 0.1, selection_counts)\n\n    # Exploration factor adjusts exploration based on time\n    epsilon = max(0.1 * (total_time_slots - current_time_slot) / total_time_slots, 0.01)\n\n    # Epsilon-greedy exploration strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(range(8))\n    else:\n        # Calculate exploration bonus\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / selection_counts)\n        # Compute scores with the exploration bonus\n        scores_with_bonus = average_scores + exploration_bonus\n        action_index = np.argmax(scores_with_bonus)\n\n    return action_index",
          "objective": 968305.312667721,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    \n    for action_index in action_indices:\n        scores = score_set[action_index]\n        selection_count = len(scores)\n        selection_counts[action_index] = selection_count\n        \n        if selection_count > 0:\n            avg_scores[action_index] = np.mean(scores)\n\n    # Calculate exploration rate\n    epsilon = np.clip(1.0 - (current_time_slot / total_time_slots), 0.1, 1.0)\n\n    # Compute the scores adjusted by counts (UCB-like)\n    adjusted_scores = avg_scores + (1.0 / (selection_counts + 1))\n\n    # Normalize adjusted scores into a probability distribution\n    exp_adjusted_scores = np.exp(adjusted_scores - np.max(adjusted_scores))\n    probabilities = exp_adjusted_scores / np.sum(exp_adjusted_scores)\n\n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        # Exploration: choose a random action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploitation: choose based on the calculated probabilities\n        action_index = np.random.choice(action_indices, p=probabilities)\n\n    return action_index",
          "objective": 976594.4538885138,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if scores:  # Ensure the scores list is not empty\n            avg_scores[i] = np.mean(scores)\n\n    # Calculate adaptive epsilon value\n    epsilon = max(0.1, min(1, (total_time_slots - current_time_slot) / (total_time_slots * 0.5)))\n\n    if np.random.rand() < epsilon:\n        # Explore: Choose a random action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploit: Calculate adjusted UCB scores\n        ucb_scores = np.zeros(n_actions)\n        for i, action_index in enumerate(action_indices):\n            if selection_counts[i] > 0:  # Avoid division by zero\n                ucb_scores[i] = avg_scores[i] + np.sqrt(2 * np.log(total_selection_count) / selection_counts[i])\n            else:\n                ucb_scores[i] = float('inf')  # Encourage exploration of unselected actions\n\n        # Select action with highest UCB score\n        action_index = np.argmax(ucb_scores)\n\n    return action_index",
          "objective": 990793.315012459,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8  # Fixed number of actions\n    average_scores = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n\n    # Calculate average scores and selection counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Handle total selection count for exploration bonus\n    selection_bonus = 1 + (1 / (selection_counts + 1e-6))  # Small constant to prevent division by zero\n    adjusted_scores = average_scores * selection_bonus\n\n    # Dynamic exploration strategy\n    exploration_probability = max(0.1, 1 - (current_time_slot / total_time_slots))  # Decaying exploration\n    exploitation_probability = 1 - exploration_probability\n\n    # Compute probabilities using Softmax for exploration and exploitation\n    exp_scores = np.exp(adjusted_scores - np.max(adjusted_scores))  # For numerical stability\n    softmax = exp_scores / np.sum(exp_scores)\n    \n    # Blend the probabilities\n    probabilities = exploration_probability / action_count + exploitation_probability * softmax\n\n    # Stochastic action selection based on computed probabilities\n    action_index = np.random.choice(action_count, p=probabilities)\n\n    return action_index",
          "objective": 1014387.9818782168,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Calculate epsilon based on the current time slot\n    epsilon = max(0.1, 1 - current_time_slot / total_time_slots)\n    \n    # Calculate UCB for each action\n    ucb_scores = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            ucb_scores[i] = avg_scores[i] + np.sqrt(2 * np.log(total_selection_count) / selection_counts[i])\n        else:\n            ucb_scores[i] = float('inf')\n\n    # Randomly explore with epsilon probability\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = np.argmax(ucb_scores)\n\n    return action_index",
          "objective": 1016612.3236342646,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8  # Fixed number of actions\n    average_scores = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n\n    # Calculate average scores and selection counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Epsilon-greedy strategy with decaying exploration\n    epsilon = max(0.1, 1 - current_time_slot / total_time_slots)\n    \n    # Randomly select an action based on epsilon\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(range(action_count))\n    else:\n        # Exploit high average scores\n        action_index = np.argmax(average_scores)\n\n    return action_index",
          "objective": 1050397.9828984444,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = len(score_set)  # Should be 8\n    average_scores = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n\n    # Calculate average scores and selection counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Calculate the exploration rate based on the current time slot\n    exploration_rate = np.clip(1 - (current_time_slot / total_time_slots), 0.1, 1)\n\n    # Softmax calculation for exploration\n    exp_scores = np.exp(average_scores - np.max(average_scores))  # For numerical stability\n    softmax_probs = exp_scores / np.sum(exp_scores)\n\n    # Dynamic UCB bonus calculation\n    exploration_bonus = np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n    total_scores = average_scores + exploration_bonus\n\n    # Choose based on exploration (softmax) and exploitation (UCB)\n    if np.random.rand() < exploration_rate:\n        # Exploration using softmax\n        action_index = np.random.choice(range(action_count), p=softmax_probs)\n    else:\n        # Exploitation using UCB\n        action_index = np.argmax(total_scores)\n\n    return action_index",
          "objective": 1237116.95997467,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize a list to store the average scores and selection counts\n    average_scores = np.zeros(8)\n    selection_counts = np.zeros(8)\n    \n    # Calculate average scores and selection counts for each action\n    for action_index in range(8):\n        if action_index in score_set and score_set[action_index]:\n            average_scores[action_index] = np.mean(score_set[action_index])\n            selection_counts[action_index] = len(score_set[action_index])\n    \n    # Exploration factor: decay exploration over time\n    epsilon = max(0.1 * (total_time_slots - current_time_slot) / total_time_slots, 0.01)\n    \n    # Exploration strategy (epsilon-greedy approach)\n    if np.random.rand() < epsilon:\n        # Explore: select an action randomly\n        action_index = np.random.choice(range(8))\n    else:\n        # Calculate a bonus based on the number of times an action has been selected\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1))\n        # Select action by combining average scores with exploration bonus\n        scores_with_bonus = average_scores + exploration_bonus\n        action_index = np.argmax(scores_with_bonus)\n    \n    return action_index",
          "objective": 1304363.4059105332,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = len(score_set)  # Should be 8\n    average_scores = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Calculate a dynamic exploration factor using softmax\n    exploration_factor = 1 / (1 + current_time_slot / total_time_slots)\n    scaled_scores = average_scores * (1 - exploration_factor) + exploration_factor * np.random.rand(action_count)\n\n    action_index = np.random.choice(range(action_count), p=np.exp(scaled_scores - np.max(scaled_scores)) / np.sum(np.exp(scaled_scores - np.max(scaled_scores))))\n    \n    return action_index",
          "objective": 1310330.8472414142,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8  # Fixed number of actions\n    average_scores = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n\n    # Calculate average scores and selection counts for each action\n    for action_index in range(action_count):\n        scores = score_set[action_index]\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Avoid division by zero in case of zero selections for an action\n    total_counts = total_selection_count if total_selection_count > 0 else 1\n    adjusted_scores = average_scores + (selection_counts / total_counts)\n    \n    # Softmax probabilities calculation\n    exp_scores = np.exp(adjusted_scores - np.max(adjusted_scores))  # For numerical stability\n    action_probabilities = exp_scores / np.sum(exp_scores)\n\n    # Epsilon-greedy exploration strategy\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))  # Linear decay of exploration probability\n    \n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(range(action_count))  # Explore\n    else:\n        action_index = np.random.choice(range(action_count), p=action_probabilities)  # Exploit\n\n    return action_index",
          "objective": 1328336.3961442427,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize variables\n    exploration_weight = 0.1  # Exploration weight\n    actions_count = len(score_set)  # Number of actions (should be 8)\n    \n    # Calculate average scores for each action\n    average_scores = np.zeros(actions_count)\n    for action_index in range(actions_count):\n        scores = score_set.get(action_index, [])\n        if len(scores) > 0:\n            average_scores[action_index] = np.mean(scores)\n        else:\n            average_scores[action_index] = 0  # No historical score\n    \n    # Calculate epsilon based on current time slot (decay exploration probability over time)\n    epsilon = exploration_weight * (1 - current_time_slot / total_time_slots)\n    \n    # Decide whether to explore or exploit\n    if np.random.rand() < epsilon:\n        # Exploration: Select a random action\n        action_index = np.random.choice(actions_count)\n    else:\n        # Exploitation: Select the action with the highest average score\n        action_index = np.argmax(average_scores)\n    \n    return action_index",
          "objective": 1338215.6598329032,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate the average score and fill selection counts\n    for action_index in action_indices:\n        scores = score_set[action_index]\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n\n    # Calculate exploration factor using softmax\n    temperature = 0.5 + 0.5 * (current_time_slot / total_time_slots)  # Decaying temperature\n    softmax_values = np.exp(avg_scores / temperature) / np.sum(np.exp(avg_scores / temperature))\n\n    # Select action based on softmax probabilities\n    action_index = np.random.choice(action_indices, p=softmax_values)\n\n    return action_index",
          "objective": 1353104.3150550046,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    \n    for action_index in action_indices:\n        scores = score_set[action_index]\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n\n    # Calculate UCB values\n    ucb_values = np.zeros(n_actions)\n    for i, action_index in enumerate(action_indices):\n        if selection_counts[action_index] > 0:\n            ucb_values[i] = avg_scores[i] + np.sqrt(2 * np.log(total_selection_count) / selection_counts[action_index])\n        else:\n            ucb_values[i] = float('inf')  # Prioritize exploration for unselected actions\n\n    # Epsilon-greedy strategy to enhance exploration\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))  # Decay exploration over time\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        action_index = action_indices[np.argmax(ucb_values)]  # Exploit\n\n    return action_index",
          "objective": 1432283.7857563766,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8  # Number of actions is fixed at 8\n    average_scores = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n\n    # Calculate average scores and selection counts\n    for action_index, scores in score_set.items():\n        selection_counts[action_index] = len(scores)\n        if len(scores) > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Exploration factor using increasing epsilon-greedy strategy\n    epsilon = min(0.1 + (0.9 * (current_time_slot / total_time_slots)), 1.0)\n\n    # Calculate Upper Confidence Bound (UCB)\n    ucb_values = np.zeros(action_count)\n    for i in range(action_count):\n        if selection_counts[i] > 0:\n            ucb_values[i] = average_scores[i] + np.sqrt(2 * np.log(total_selection_count) / selection_counts[i])\n        else:\n            ucb_values[i] = np.inf  # If an action has never been selected, give it a high value\n\n    # Create a combined probability distribution for selection\n    combined_scores = (1 - epsilon) * ucb_values + epsilon * (np.ones(action_count) / action_count)\n    action_probabilities = combined_scores / np.sum(combined_scores)  # Normalize probabilities\n\n    # Select an action based on the computed probabilities\n    action_index = np.random.choice(action_count, p=action_probabilities)\n    \n    return action_index",
          "objective": 1495708.0746450378,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8  # Fixed number of actions\n    average_scores = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)  # Count of selections for the action\n        \n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Avoid division by zero by using a small epsilon\n    normalized_scores = average_scores / (total_selection_count + 1e-5)\n    \n    # Incorporate exploration factor based on selection counts\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1e-5) / (selection_counts + 1e-5))\n    combined_scores = normalized_scores + exploration_bonus\n    \n    # Exploration rate changes with time\n    epsilon = max(0.1, 1.0 - (current_time_slot / total_time_slots))\n    \n    # Softmax probabilities based on combined scores\n    exp_scores = np.exp(combined_scores - np.max(combined_scores))  # Numerical stability\n    softmax_probs = exp_scores / np.sum(exp_scores)\n\n    # Stochastic exploration based on epsilon\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(range(action_count))\n    else:\n        action_index = np.random.choice(range(action_count), p=softmax_probs)\n\n    return action_index",
          "objective": 1520239.8114172448,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for action_index in action_indices:\n        scores = score_set[action_index]\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n\n    # Epsilon-greedy exploration factor\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))  # Decay epsilon\n\n    # Random exploration condition\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        # Select action based on the highest average score\n        action_index = np.argmax(avg_scores)\n\n    return action_index",
          "objective": 1533016.2715217355,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = len(score_set)  # Should be 8\n    average_scores = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n\n    # Calculate average scores and selection counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Define epsilon value that decreases as time progresses\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n\n    # Exploration term based on total selection counts and action selection counts\n    exploration_bonus = np.where(selection_counts > 0,\n                                  np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1)),\n                                  np.inf)  # Use infinity for unselected actions to ensure exploration\n\n    # Calculate total scores combining average scores and exploration bonuses\n    total_scores = average_scores + exploration_bonus\n\n    # Randomly select an action based on epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        # Exploration: choose an action randomly\n        action_index = np.random.choice(range(action_count))\n    else:\n        # Exploitation: choose action with the highest adjusted score\n        action_index = np.argmax(total_scores)\n\n    return action_index",
          "objective": 1563243.6944493786,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    \n    for action_index in action_indices:\n        scores = score_set[action_index]\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n    \n    # Epsilon-greedy strategy: define epsilon\n    epsilon = max(1.0 - (current_time_slot / total_time_slots), 0.1)  # Decrease exploration gradually\n    \n    if np.random.rand() < epsilon:  # Explore\n        unselected_actions = [i for i in range(n_actions) if selection_counts[i] == 0]\n        if unselected_actions:  # If there are unselected actions, choose randomly from them\n            action_index = np.random.choice(unselected_actions)\n        else:  # If all actions are selected, choose randomly from all actions\n            action_index = np.random.choice(action_indices)\n    else:  # Exploit\n        action_index = np.argmax(avg_scores)\n\n    return action_index",
          "objective": 1571303.2291761788,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    \n    for action_index in action_indices:\n        scores = score_set[action_index]\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n    \n    # Calculate the epsilon value for exploration\n    epsilon = max(0.1, 1 - current_time_slot / total_time_slots)\n    \n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        # Explore: select a random action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploit: select the action with the highest average score\n        action_index = np.argmax(avg_scores)\n        \n        # In case of a tie, select randomly among the actions with the highest score\n        max_score = avg_scores[action_index]\n        actions_with_max_score = [i for i in action_indices if avg_scores[i] == max_score]\n        action_index = np.random.choice(actions_with_max_score)\n    \n    return action_index",
          "objective": 1595989.116851772,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i in action_indices:\n        scores = score_set[i]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Epsilon as a function of the time slot\n    epsilon = (total_time_slots - current_time_slot) / total_time_slots\n\n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        # Explore: randomly select an action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploit: select based on scores\n        ucb_values = np.zeros(n_actions)\n        for i in action_indices:\n            if selection_counts[i] > 0:\n                ucb_values[i] = avg_scores[i] + np.sqrt(2 * np.log(total_selection_count) / selection_counts[i])\n            else:\n                ucb_values[i] = float('inf')  # Encourage exploration of unselected actions\n\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 1629283.896800275,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores for each action\n    average_scores = []\n    for action_index in range(8):\n        if action_index in score_set and score_set[action_index]:\n            avg_score = np.mean(score_set[action_index])\n        else:\n            avg_score = 0.0  # Default to 0.0 if no scores are available\n        average_scores.append(avg_score)\n    \n    # Exploration strategy (epsilon-greedy approach)\n    epsilon = 0.1  # Exploration factor\n    if np.random.rand() < epsilon:\n        # Explore: randomly select an action\n        action_index = np.random.choice(range(8))\n    else:\n        # Exploit: select the action with the highest average score\n        action_index = np.argmax(average_scores)\n    \n    return action_index",
          "objective": 1676931.1082490154,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8  # Number of actions is fixed at 8\n    average_scores = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n    \n    # Calculate average scores and selection counts\n    for action_index, scores in score_set.items():\n        selection_counts[action_index] = len(scores)\n        if len(scores) > 0:\n            average_scores[action_index] = np.mean(scores)\n    \n    # Calculate exploration probability using a decaying epsilon-greedy strategy\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))  # Epsilon decreases over time\n\n    # Calculate Upper Confidence Bound (UCB)\n    ucb_values = np.zeros(action_count)\n    for i in range(action_count):\n        if selection_counts[i] == 0:\n            ucb_values[i] = np.inf  # If an action has never been selected, give it a high value\n        else:\n            ucb_values[i] = average_scores[i] + np.sqrt(2 * np.log(total_selection_count) / selection_counts[i])\n    \n    # Combine UCB with exploration decisions\n    action_probabilities = (1 - epsilon) * (ucb_values / np.sum(ucb_values)) + (epsilon / action_count)\n    \n    # Select an action based on the computed probabilities\n    action_index = np.random.choice(action_count, p=action_probabilities)\n    \n    return action_index",
          "objective": 1710050.468499907,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8  # Fixed number of actions\n    average_scores = []\n    action_selection_counts = []\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_selection_count = len(scores)\n        action_selection_counts.append(action_selection_count)\n        \n        if action_selection_count > 0:\n            average_score = np.mean(scores)\n        else:\n            average_score = 0\n        \n        average_scores.append(average_score)\n\n    # Calculate UCB for each action\n    ucb_values = []\n    for i in range(action_count):\n        if action_selection_counts[i] == 0:\n            ucb_value = float('inf')  # Infinite UCB for unselected actions\n        else:\n            confidence_interval = np.sqrt(np.log(total_selection_count + 1) / action_selection_counts[i])\n            ucb_value = average_scores[i] + confidence_interval\n        ucb_values.append(ucb_value)\n\n    # Select the action with the highest UCB value\n    action_index = np.argmax(ucb_values)\n    \n    return action_index",
          "objective": 1796314.9836488832,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Epsilon-Greedy parameters\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))  # Epsilon decreases over time\n    if np.random.rand() < epsilon:\n        # Exploration: Select a random action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploitation: Select the action with the highest average score\n        action_index = action_indices[np.argmax(avg_scores)]\n    \n    return action_index",
          "objective": 1849028.5057162999,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8  # Fixed number of actions\n    average_scores = []\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n\n    # Calculate average scores for each action\n    for action_index in range(action_count):\n        scores = score_set[action_index]\n        if len(scores) > 0:\n            average_score = np.mean(scores)\n        else:\n            average_score = 0  # Actions not selected have a score of 0\n        average_scores.append(average_score)\n\n    average_scores = np.array(average_scores)\n\n    # Epsilon-greedy approach\n    epsilon = max(0.1, 1 - (total_selection_count / (total_time_slots * action_count)))  # Decay epsilon over time\n    if np.random.rand() < epsilon:\n        # Exploration: Select a random action\n        action_index = np.random.choice(range(action_count))\n    else:\n        # Exploitation: Select action with highest average score\n        action_index = np.argmax(average_scores)\n    \n    return action_index",
          "objective": 1868370.088369925,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts for each action\n    for action_index in action_indices:\n        scores = score_set[action_index]\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n\n    # Define exploration probability that decays over time\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))  # Epsilon decays from 1 to 0.1 over time\n\n    # Determine whether to explore or exploit\n    if np.random.rand() < epsilon:\n        # Exploration: Select a random action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploitation: Select the action with the highest average score\n        action_index = np.argmax(avg_scores)\n\n    return action_index",
          "objective": 1888752.705490177,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8  # Number of actions remains fixed at 8\n    average_scores = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n    \n    # Calculate average scores and selection counts\n    for action_index, scores in score_set.items():\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n    \n    # Implement Softmax for action selection\n    # A softmax temperature that decreases over time to favor exploitation\n    temperature = max(0.1, 1.0 - (current_time_slot / total_time_slots))\n    \n    # Calculate softmax probabilities\n    exp_scores = np.exp(average_scores / temperature)\n    probabilities = exp_scores / np.sum(exp_scores)\n    \n    # Select an action based on the computed probabilities\n    action_index = np.random.choice(action_count, p=probabilities)\n    \n    return action_index",
          "objective": 1893843.7750763916,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores and handle actions with zero selections\n    action_scores = np.zeros(8)  # For actions 0 to 7\n    for action_index in range(8):\n        if score_set[action_index]:\n            action_scores[action_index] = np.mean(score_set[action_index])\n        else:\n            action_scores[action_index] = 0\n\n    # Determine exploration factor based on current time slot\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n\n    # Softmax temperature for exploration-exploitation balance\n    temperature = np.exp(exploration_factor)\n    adjusted_scores = action_scores / temperature\n    \n    # Calculate probabilities for each action\n    probabilities = np.exp(adjusted_scores - np.max(adjusted_scores))  # For numerical stability\n    probabilities /= np.sum(probabilities)\n\n    # Select action based on a probability distribution\n    action_index = np.random.choice(range(8), p=probabilities)\n\n    return action_index",
          "objective": 1978706.7434978483,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8  # Fixed number of actions\n    average_scores = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n\n    # Calculate average scores and selection counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Exploration factor (epsilon)\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n\n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        # Explore: select an action uniformly at random\n        action_index = np.random.choice(action_count)\n    else:\n        # Exploit: select based on average scores\n        # Normalize average scores to probabilities\n        exp_scores = np.exp(average_scores - np.max(average_scores))  # For numerical stability\n        action_probabilities = exp_scores / np.sum(exp_scores) if np.sum(exp_scores) > 0 else np.ones(action_count) / action_count\n        \n        # Select action based on computed probabilities\n        action_index = np.random.choice(range(action_count), p=action_probabilities)\n\n    return action_index",
          "objective": 2001244.6518638362,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for action_index in action_indices:\n        scores = score_set[action_index]\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n\n    # Prevent division by zero and handle exploration\n    exploration_bonus = np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n    \n    # Calculate UCB values\n    ucb_values = avg_scores + exploration_bonus\n    \n    # Choose action based on UCB values\n    action_index = action_indices[np.argmax(ucb_values)]\n    \n    return action_index",
          "objective": 2049868.906537091,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores for each action, handling edge cases\n    average_scores = np.zeros(8)  # Initialize an array for average scores\n    for action_index in range(8):\n        if action_index in score_set and score_set[action_index]:\n            average_scores[action_index] = np.mean(score_set[action_index])\n\n    # Determine the exploration rate (epsilon) dynamically\n    exploration_factor = max(0.01, 1 - (current_time_slot / total_time_slots))  # Decay from 1 to 0.01\n\n    # Exploration strategy (epsilon-greedy approach)\n    if np.random.rand() < exploration_factor:\n        # Explore: randomly select an action\n        action_index = np.random.choice(range(8))\n    else:\n        # Exploit: select the action with the highest average score\n        action_index = np.argmax(average_scores)\n\n    return action_index",
          "objective": 2057158.8650479796,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8  # Fixed number of actions\n    average_scores = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n    \n    # Calculate average scores and selection counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n    \n    # Calculate UCB values\n    ucb_values = average_scores + np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n    \n    # Dynamic exploration rate based on current time slot\n    exploration_bonus = (1 - current_time_slot / total_time_slots) * np.max(ucb_values)\n    action_values = ucb_values + exploration_bonus\n    \n    # Select action based on action values\n    action_index = np.argmax(action_values)\n\n    return action_index",
          "objective": 2071505.7614786308,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for action_index in action_indices:\n        scores = score_set[action_index]\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n        \n    # Calculate epsilon for exploration\n    epsilon = 1.0 - (current_time_slot / total_time_slots)\n    epsilon = max(0.1, epsilon)  # Minimum exploration probability\n\n    # Generate random value to decide exploration vs exploitation\n    if np.random.rand() < epsilon:\n        # Explore: select action uniformly at random\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploit: select action with the highest average score\n        action_index = np.argmax(avg_scores)\n    \n    return action_index",
          "objective": 2190139.764848652,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if selection_counts[i] > 0 else 0\n\n    # Adaptive exploration factor\n    exploration_factor = max(0.01, 1.0 - (current_time_slot / total_time_slots))\n    epsilon = max(0.1, exploration_factor)  # Decrease epsilon over time\n\n    # Select action using epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        # Explore: Select an action uniformly\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploit: Calculate UCB scores\n        ucb_scores = np.zeros(n_actions)\n        confidence_interval = np.log(total_selection_count + 1)  # To avoid log(0)\n\n        for i in range(n_actions):\n            if selection_counts[i] > 0:\n                ucb_scores[i] = avg_scores[i] + np.sqrt(confidence_interval / selection_counts[i])\n            else:\n                ucb_scores[i] = float('inf')  # Encourage exploration of untried actions\n\n        action_index = np.argmax(ucb_scores)\n\n    return action_index",
          "objective": 2355349.6399263465,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration probability\n    action_count = 8  # Number of actions (0-7)\n    \n    # Calculate average scores for each action\n    average_scores = np.zeros(action_count)\n    \n    for action_index in range(action_count):\n        if score_set[action_index]:  # Check if there are historical scores\n            average_scores[action_index] = np.mean(score_set[action_index])\n        else:\n            average_scores[action_index] = 0  # No scores yet\n\n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon or total_selection_count < action_count:\n        action_index = np.random.choice(range(action_count))  # Explore\n    else:\n        action_index = np.argmax(average_scores)  # Exploit best-known action\n        \n    return action_index",
          "objective": 3071625.826660398,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8  # Fixed number of actions (0 to 7)\n    average_scores = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n\n    # Calculate average scores and selection counts\n    for action_index, scores in score_set.items():\n        selection_counts[action_index] = len(scores)\n        average_scores[action_index] = np.mean(scores) if scores else 0.0\n\n    # Prevent division by zero by calculating exploration bonus\n    epsilon = 0.1 * (total_time_slots - current_time_slot) / total_time_slots\n    exploration_bonus = 1 / (1 + selection_counts)  # Scale exploration by selection counts\n\n    # Calculate total scores combining exploitation (average scores) and exploration\n    total_scores = average_scores + epsilon * exploration_bonus\n\n    # Select action index by finding the action with the highest total score\n    action_index = np.argmax(total_scores)\n\n    return action_index",
          "objective": 3074092.644763294,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Initialize arrays for average scores, selection counts, and UCB values\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Compute average scores and selection counts\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        avg_scores[i] = np.mean(scores) if scores else 0.0\n\n    # Calculate UCB values, incorporating exploration bonus\n    ucb_values = np.zeros(n_actions)\n    for i in range(n_actions):\n        if selection_counts[i] > 0:\n            confidence_interval = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_interval\n        else:\n            ucb_values[i] = np.inf  # Prioritize untried actions\n\n    # Dynamic Epsilon-Greedy Mechanism\n    initial_epsilon = 1.0\n    min_epsilon = 0.1\n    decay_rate = (initial_epsilon - min_epsilon) / total_time_slots\n    epsilon = max(initial_epsilon - decay_rate * current_time_slot, min_epsilon)\n\n    # Select action based on exploration vs exploitation\n    if np.random.rand() < epsilon:\n        # Exploration: favored selection of low-selection actions\n        action_index = np.random.choice(action_indices, p=(1/selection_counts if selection_counts.sum() else np.ones(n_actions)/n_actions))\n    else:\n        action_index = action_indices[np.argmax(ucb_values)]  # Exploitation based on UCB\n\n    # Ensure the action index is within valid range\n    action_index = max(0, min(action_index, 7))\n\n    # Logging mechanism\n    print(f\"Selected Action Index: {action_index}, Epsilon: {epsilon:.3f}, UCB Values: {ucb_values}, Avg Scores: {avg_scores}, Selection Counts: {selection_counts}\")\n\n    return action_index",
          "objective": 3099698.2360646683,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))  # Decreasing exploration factor\n    action_avg_scores = []\n    \n    for action_index in range(8):\n        if score_set[action_index]:\n            avg_score = np.mean(score_set[action_index])\n        else:\n            avg_score = 0  # If no scores, set average score to zero\n        action_avg_scores.append(avg_score)\n\n    if np.random.rand() < epsilon:\n        # Explore: choose a random action\n        action_index = np.random.randint(0, 8)\n    else:\n        # Exploit: choose the best action based on average scores\n        action_index = np.argmax(action_avg_scores)\n\n    return action_index",
          "objective": 3130920.068287652,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    \n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n    \n    # Dynamic epsilon calculation\n    epsilon = max(0.1, 1.0 - (current_time_slot / total_time_slots))\n\n    # Exploration vs exploitation decision\n    if np.random.rand() < epsilon:\n        # Explore: randomly select an action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploit: select action with the highest average score\n        action_index = action_indices[np.argmax(avg_scores)]\n    \n    return action_index",
          "objective": 3342651.3183506955,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8  # Fixed number of actions\n    average_scores = []\n    selection_counts = [len(scores) for scores in score_set.values()]\n\n    # Calculate average scores for each action\n    for action_index in range(action_count):\n        scores = score_set[action_index]\n        if selection_counts[action_index] > 0:\n            average_score = np.mean(scores)\n        else:\n            average_score = 0  # If no scores, score is 0\n        average_scores.append(average_score)\n\n    # Epsilon-greedy exploration parameter\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n\n    # Randomly select with probability epsilon, else select the best action\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(range(action_count))  # Randomly explore\n    else:\n        # Select action with the highest average score\n        action_index = np.argmax(average_scores)\n\n    return action_index",
          "objective": 3532034.662752111,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores and handle actions that have never been selected\n    average_scores = np.zeros(8)  # Array for storing average scores\n    for action_index in range(8):\n        if action_index in score_set and score_set[action_index]:\n            average_scores[action_index] = np.mean(score_set[action_index])\n        else:\n            average_scores[action_index] = 0  # Actions with no scores retain zero average\n\n    # Temperature parameter for softmax to control exploration\n    temperature = max(0.1, 1 - (current_time_slot / total_time_slots))  # Decays from 1 to 0.1\n\n    # Apply softmax function to average scores for action selection\n    exp_scores = np.exp(average_scores / temperature)\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Select an action based on the calculated probabilities\n    action_index = np.random.choice(range(8), p=probabilities)\n\n    return action_index",
          "objective": 3728569.2889319477,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = len(score_set)  # Should be 8\n    average_scores = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n\n    # Calculate average scores and selection counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Epsilon value depends on current time_slot and total time slots (decay over time)\n    epsilon = 1 - (current_time_slot / total_time_slots)\n    epsilon = max(epsilon, 0.1)  # Don't let epsilon go below 0.1 for exploration\n\n    # Select an action using epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        # Exploration: choose an action uniformly at random\n        action_index = np.random.randint(action_count)\n    else:\n        # Exploitation: select the action with the highest average score,\n        # adjusting for selection counts (to encourage exploration of less selected actions)\n        adjusted_scores = average_scores + (1.0 / (selection_counts + 1))  # Smooth for less selected actions\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 3997127.575813478,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for i, action_index in enumerate(action_indices):\n        scores = score_set[action_index]\n        selection_counts[i] = len(scores)\n        if selection_counts[i] > 0:\n            avg_scores[i] = np.mean(scores)\n\n    # Epsilon-greedy exploration parameter\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n    explore = np.random.rand() < epsilon\n\n    if explore:\n        action_index = np.random.choice(action_indices)\n    else:\n        # UCB approach for exploitation\n        total_counts = total_selection_count + 1e-5  # Avoid division by zero\n        ucb_values = avg_scores + np.sqrt((2 * np.log(total_counts)) / (selection_counts + 1e-5))\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 4030688.294290864,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_factor = (1 - (current_time_slot / total_time_slots)) * 0.5\n    action_scores = []\n    \n    for action_index in range(8):\n        if action_index in score_set and len(score_set[action_index]) > 0:\n            avg_score = np.mean(score_set[action_index])\n            selection_count = len(score_set[action_index])\n        else:\n            avg_score = 0\n            selection_count = 0\n            \n        if total_selection_count > 0:\n            selection_weight = selection_count / total_selection_count\n        else:\n            selection_weight = 0\n        \n        exploration_bonus = exploration_factor * (1 - selection_weight)\n        \n        combined_score = avg_score + exploration_bonus\n        action_scores.append(combined_score)\n    \n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 5107866.253979237,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    exploration_rate = 0.1  # Can be adjusted to control exploration factor\n\n    # Calculate average scores\n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:  # If there are historical scores\n            avg_scores[action_index] = np.mean(scores)\n            selection_counts[action_index] = len(scores)\n\n    # Handle cases where no action has been selected\n    if total_selection_count == 0:\n        return np.random.randint(0, num_actions)\n\n    # Compute exploration bonuses\n    exploration_bonuses = np.sqrt(np.log(total_selection_count) / (selection_counts + 1e-5))\n\n    # Selection values combine average scores with exploration bonuses\n    selection_values = avg_scores + exploration_rate * exploration_bonuses\n\n    # Select the action with the highest value\n    action_index = np.argmax(selection_values)\n    \n    return action_index",
          "objective": 5237153.6699841535,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    average_scores = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n    \n    # Calculate average scores and selection counts\n    for action_index, scores in score_set.items():\n        if scores:\n            average_scores[action_index] = np.mean(scores)\n            selection_counts[action_index] = len(scores)\n\n    # Avoid division by zero for selection counts\n    exploration_values = np.floor(np.sqrt(total_selection_count) / (selection_counts + 1e-5))\n\n    # Combine average score and exploration values\n    adjusted_scores = average_scores + exploration_values\n    \n    # Dynamic exploration factor (epsilon)\n    epsilon = 0.1 * (total_time_slots - current_time_slot) / total_time_slots\n\n    # Select action using epsilon-greedy\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(range(action_count))\n    else:\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 7634660.840469315,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8  # Fixed number of actions\n    \n    # Initialize average scores and selection counts\n    average_scores = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n    \n    # Calculate average scores and selection counts for each action\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n        else:\n            average_scores[action_index] = 0  # Default for unselected actions\n    \n    # Softmax probabilities calculation\n    exp_scores = np.exp(average_scores - np.max(average_scores))  # For numerical stability\n    action_probabilities = exp_scores / np.sum(exp_scores) if np.sum(exp_scores) > 0 else np.zeros(action_count)\n\n    # Temperature parameter for exploration (decreasing over time)\n    temperature = max(0.1, 1 - (current_time_slot / total_time_slots))\n    \n    # Adjust probabilities based on temperature\n    adjusted_probabilities = np.power(action_probabilities, 1 / temperature)\n    adjusted_probabilities /= np.sum(adjusted_probabilities) if np.sum(adjusted_probabilities) > 0 else 1  # Prevent division by zero\n    \n    # Select action based on computed probabilities\n    action_index = np.random.choice(range(action_count), p=adjusted_probabilities)\n    \n    return action_index",
          "objective": 10504817.491604738,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8  # Fixed number of actions\n    average_scores = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n\n    # Calculate average scores and selection counts for each action\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n        else:\n            average_scores[action_index] = 0  # Action has not been selected\n\n    # Epsilon-greedy strategy parameters\n    epsilon = max(0.1, 1.0 - (current_time_slot / total_time_slots))\n    \n    if np.random.rand() < epsilon:\n        # Exploration: Select a random action\n        action_index = np.random.choice(range(action_count))\n    else:\n        # Exploitation: Select the action with the highest average score\n        action_index = np.argmax(average_scores)\n    \n    return action_index",
          "objective": 11181585.09437612,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    scores = np.array([np.mean(score_set[i]) if score_set[i] else 0 for i in action_indices])\n    selection_counts = np.array([len(score_set[i]) for i in action_indices])\n    \n    # Calculate exploration factor\n    exploration_factor = 1.0 - (current_time_slot / total_time_slots)\n    \n    # Calculate an exploration bonus based on selection counts\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))  # Adding a small value to avoid division by zero\n    \n    # Calculate the final score combining average scores and exploration bonus\n    final_scores = scores + exploration_factor * exploration_bonus\n    \n    # Select the action based on the maximum score\n    action_index = action_indices[np.argmax(final_scores)]\n    \n    return action_index",
          "objective": 11573867.55224902,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores for each action\n    avg_scores = {}\n    for action_index, scores in score_set.items():\n        if len(scores) > 0:\n            avg_scores[action_index] = np.mean(scores)\n        else:\n            avg_scores[action_index] = 0  # Default to 0 if no selections\n\n    # Exploration factor (epsilon): Decays over time\n    epsilon = max(0.1, (total_time_slots - current_time_slot) / total_time_slots)\n    \n    # Choose an action based on epsilon-greedy strategy\n    if np.random.rand() < epsilon:  # Explore\n        action_index = np.random.choice(list(score_set.keys()))\n    else:  # Exploit\n        action_index = max(avg_scores, key=avg_scores.get)\n\n    return action_index",
          "objective": 29607435.51574654,
          "other_inf": null
     }
]