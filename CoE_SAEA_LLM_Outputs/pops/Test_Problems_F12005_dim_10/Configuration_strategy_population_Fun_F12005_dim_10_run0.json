[
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Handle zero selections with a small epsilon\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Calculate UCB components with adaptive exploration factor\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    combined_values = expected_values + (exploration_bonus * exploration_weight)\n\n    # Select action based on maximum combined value\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    \n    # Calculate the mean scores and selection counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Handle cases with no selections\n    counts_with_min = counts + 1e-5\n\n    # Calculate average scores\n    average_scores = scores\n\n    # Explore-exploit factor: \u03b5-greedy method\n    exploration_prob = 0.1  # 10% of the time we explore\n    if np.random.rand() < exploration_prob:\n        # Explore: Randomly select an action\n        action_index = np.random.choice(actions)\n    else:\n        # Exploit: Calculate UCB\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n        \n        # Adjust the exploration bonus with a decay over time\n        exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_scores = average_scores + exploration_bonus * exploration_scaling\n\n        # Select the action with the highest adjusted score\n        action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Assign a minimum count to avoid division by zero\n    counts_with_min = counts + 1e-5\n    average_scores = scores\n\n    # Calculate exploration factor using UCB\n    exploration_factor = 1.5\n    exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n\n    # Scale the exploration bonus\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = average_scores + exploration_bonus * exploration_scaling\n\n    # Select the action with the highest adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            \n    # Handling zero counts and using a small constant\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Preferred exploration-exploitation balance\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1) + 1) / total_action_counts)\n    \n    # Time-dependent normalization factor\n    time_factor = ((current_time_slot + 1) / total_time_slots) ** 0.5\n    \n    # Combining expected values with exploration bonus\n    combined_values = expected_values + exploration_bonus * time_factor\n\n    # Select the action with the highest combined value\n    action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Avoid division by zero by adding a small epsilon\n    epsilon = 1e-5\n\n    # Upper Confidence Bound calculation\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1) + 1) / (action_counts + epsilon))\n\n    # Combine expected values with exploration bonus\n    combined_values = expected_values + exploration_bonus\n\n    # Adaptive exploration using a time factor\n    time_factor = (current_time_slot + 1) / total_time_slots\n    adjusted_values = combined_values * (1 - time_factor) + expected_values * time_factor\n\n    # Select the action with the highest adjusted value\n    action_index = np.argmax(adjusted_values)\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Handle division safely with np.where for zero selections\n    counts_with_min = np.where(counts > 0, counts, 1e-5)\n\n    # Calculate average scores\n    average_scores = scores\n\n    # Calculate exploration bonus (UCB)\n    exploration_factor = 1.5\n    exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n\n    # Scale exploration bonus according to the remaining time slots\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = average_scores + exploration_bonus * exploration_scaling\n\n    # Select action with the highest adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    num_actions = len(actions)\n    \n    # Initialize scores and counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Avoid division by zero and implement a small constant for exploration\n    counts_with_min = counts + 1e-5\n    \n    # Calculate average scores\n    average_scores = scores\n    \n    # Calculate exploration bonuses using UCB\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / counts_with_min)\n    \n    # Adjust exploration factor as time progresses\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = average_scores + exploration_bonus * exploration_scaling\n\n    # Select the action with the highest adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Avoid division by zero\n    counts_with_min = counts + 1e-5\n\n    # Calculate average scores\n    average_scores = scores\n\n    # Current exploration factor, dynamically adjusting to total_selection_count\n    exploration_factor = 1.5\n    exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n\n    # Linear decay of exploration bonus over time\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = average_scores + exploration_bonus * exploration_scaling\n\n    # Select the action with the highest adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    epsilon = 1e-5  # Small constant to prevent division by zero\n    exploration_factor = np.sqrt(np.log(total_selection_count + 1) / (action_counts + epsilon))\n\n    # Adjust the exploration factor over time to prioritize historical performance\n    time_adjustment = (total_time_slots - current_time_slot) / total_time_slots\n\n    # Calculate combined values using both expected values and exploration factors\n    combined_values = expected_values + exploration_factor * time_adjustment\n\n    # Apply decay to favor recent performance proportional to time\n    decay_weight = np.exp(-current_time_slot / (total_time_slots + epsilon))\n    adjusted_combined_values = (1 - decay_weight) * combined_values + decay_weight * expected_values\n\n    # Select the action with the maximum adjusted combined value\n    action_index = np.argmax(adjusted_combined_values)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Handle division by zero with small epsilon\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Calculate exploration bonus using UCB\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / total_action_counts)\n    \n    # Normalize time factor for exploration weight\n    time_factor = (current_time_slot + 1) / total_time_slots\n    \n    # Calculate combined values for exploitation and exploration\n    combined_values = expected_values + exploration_bonus * time_factor\n\n    # Select the action with the maximum combined value\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    total_action_counts = action_counts + 1e-5  # Small epsilon to avoid division by zero\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / total_action_counts)\n    \n    # Decay exploration based on time\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    combined_values = expected_values + exploration_bonus * exploration_weight\n\n    # Select action based on maximum combined value\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        if action_counts[action_index] > 0:\n            expected_values[action_index] = np.mean(scores)\n    \n    # Handle cases with no selections\n    exploration_bonus = np.sqrt(2 * np.log(total_selection_count + 1) / (action_counts + 1e-5))\n    \n    # Adjust exploration based on the current time slot\n    time_factor = (current_time_slot + 1) / total_time_slots\n    exploration_component = exploration_bonus * time_factor\n    \n    # Combine expected values with exploration component\n    combined_values = expected_values + exploration_component\n    \n    # Adjust for recency with exponential decay\n    decay_factor = np.exp(-current_time_slot / total_time_slots)\n    recent_adjusted_values = (1 - decay_factor) * combined_values + decay_factor * expected_values\n    \n    # Select the action with the highest adjusted value\n    action_index = np.argmax(recent_adjusted_values)\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n    \n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    action_counts += 1e-5  # Smoothing to prevent division by zero\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / action_counts)\n\n    # Dynamic exploration weight\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n\n    combined_values = expected_values + exploration_bonus * exploration_weight\n\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n        action_counts[action_index] = len(scores)\n\n    # Avoid division by zero and other potential issues\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Calculate UCB components\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    \n    # Combining exploitation and exploration factors\n    combined_values = expected_values + (exploration_bonus * exploration_weight)\n\n    # Select action based on maximum combined value\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    n_actions = len(actions)\n\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Prevent division by zero\n    counts_with_min = counts + 1e-5\n\n    # Calculate the average score\n    average_scores = scores\n\n    # Exploration factor settings\n    exploration_factor = 2.0\n    epsilon = 0.1  # Epsilon greedy approach \n\n    # Calculate exploration bonus\n    exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n\n    # Softmax weights and selection for exploration and exploitation balance\n    adjusted_scores = average_scores + exploration_bonus\n    softmax_exponentials = np.exp(adjusted_scores - np.max(adjusted_scores))\n    probabilities = softmax_exponentials / np.sum(softmax_exponentials)\n\n    # Epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(actions)  # Explore\n    else:\n        action_index = np.random.choice(actions, p=probabilities)  # Exploit\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate expected values and action counts\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        if action_counts[action_index] > 0:\n            expected_values[action_index] = np.mean(scores)\n\n    # Handle zero selections by adding a small epsilon\n    epsilon = 1e-5\n    adjusted_counts = action_counts + epsilon\n\n    # Calculate UCB values\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / adjusted_counts)\n    # Dynamic exploration based on time\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    combined_values = expected_values + exploration_bonus * exploration_weight\n\n    # Select action based on maximum combined value\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Avoid division by zero by adding a small constant to counts\n    adjusted_counts = counts + 1e-5\n    \n    # Calculate the exploration factor using UCB\n    exploration_term = np.sqrt(np.log(total_selection_count + 1) / adjusted_counts) \n\n    # Incorporating a time decay factor to prioritize recent selections\n    time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n    exploration_adjusted = exploration_term * time_decay_factor\n\n    # Calculate final scores combining exploitation and exploration\n    final_scores = scores + exploration_adjusted\n\n    # Select the action with the highest adjusted score\n    action_index = actions[np.argmax(final_scores)]\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Handling counts to avoid division by zero\n    counts_with_min = counts + 1e-5\n    \n    # Calculate average scores\n    average_scores = scores\n\n    # Explore-exploit: Epsilon-Greedy Strategy\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))  # Epsilon decreases over time\n    if np.random.rand() < epsilon:\n        # Exploration: select a random action\n        action_index = np.random.choice(actions)\n    else:\n        # Exploitation: select the best action based on adjusted scores\n        exploration_factor = 1.5\n        exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n\n        # Scale the exploration bonus down as time progresses\n        exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_scores = average_scores + exploration_bonus * exploration_scaling\n        \n        action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Handle zero selections with a small epsilon\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Calculate UCB components\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n    \n    # Adaptive exploration weight based on remaining time slots\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    combined_values = expected_values + (exploration_bonus * exploration_weight)\n\n    # Select action based on maximum combined value\n    action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    n_actions = len(actions)\n    \n    # Calculate average scores and counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Calculate exploration term (\u03b5-greedy approach)\n    epsilon = 1 / (1 + total_selection_count)  # Decaying exploration rate\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (counts + 1e-5))\n    \n    # Scale exploration by current time slot\n    time_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    exploration_adjusted = (1 - epsilon) * scores + epsilon * exploration_bonus * time_scaling\n\n    # Select action based on the highest adjusted score\n    action_index = actions[np.argmax(exploration_adjusted)]\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Calculate the exploration factor using UCB\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (counts + 1e-5))\n\n    # Modify exploration term based on time dynamics\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_exploration = exploration_bonus * exploration_scaling\n\n    # Calculate total scores combining exploration and exploitation\n    total_scores = scores + adjusted_exploration\n\n    # Select action based on the highest score\n    action_index = actions[np.argmax(total_scores)]\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n\n    # Small epsilon to avoid division by zero\n    epsilon = 1e-5\n\n    # Upper Confidence Bound with dynamic exploration\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / (action_counts + epsilon))\n    \n    # Dynamic exploration/exploitation balance\n    time_factor = (total_time_slots - current_time_slot) / total_time_slots\n    ucb_values = expected_values + exploration_bonus * time_factor\n    \n    # Softmax for action probabilities\n    exp_values = np.exp(ucb_values - np.max(ucb_values))  # Stability in softmax\n    action_probabilities = exp_values / np.sum(exp_values)\n\n    # Epsilon-greedy choice\n    epsilon_greedy = 0.1 * time_factor\n    if np.random.rand() < epsilon_greedy:\n        action_index = np.random.choice(num_actions)\n    else:\n        action_index = np.random.choice(num_actions, p=action_probabilities)\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Handle the case where no actions have been selected\n    counts_safe = counts + 1e-5\n\n    # Calculate average scores\n    average_scores = scores\n\n    # Epsilon-greedy exploration factor\n    epsilon = 1.0 / np.sqrt(current_time_slot + 1)\n\n    # Calculate probabilities for exploration and exploitation\n    exploration_prob = epsilon / len(actions)\n    exploitation_prob = (1 - epsilon) * (average_scores / np.sum(average_scores))\n\n    # Combine probabilities to form a selection score\n    selection_scores = exploitation_prob + exploration_prob\n\n    # Select the action with the highest score\n    action_index = actions[np.argmax(selection_scores)]\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Avoid division by zero for unexplored actions\n    with np.errstate(divide='ignore', invalid='ignore'):\n        # Calculate the exploration bonus using UCB\n        exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / (counts + 1e-5))\n\n    # Introduce a decay factor for exploration based on the current time slot\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_exploration = exploration_bonus * exploration_scaling\n\n    # Combine scores with the exploration term\n    combined_scores = scores + adjusted_exploration\n\n    # Select the action with the highest combined score\n    action_index = actions[np.argmax(combined_scores)]\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    num_actions = len(actions)\n    \n    # Calculate average scores and counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Handle counts for exploration calculation\n    counts_with_min = counts + 1e-5\n    \n    # Calculate average scores\n    average_scores = scores\n    \n    # Exploration factor (using UCB method)\n    exploration_factor = 1.5\n    exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n    \n    # Scale the exploration bonus\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = average_scores + exploration_bonus * exploration_scaling\n    \n    # Select the action with the highest adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Avoid division by zero when calculating exploration term\n    with np.errstate(divide='ignore', invalid='ignore'):\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (counts + 1e-5))\n    \n    # Scale exploration based on time slot proximity\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    exploration_adjusted = exploration_bonus * exploration_scaling\n\n    # Combine scores with the exploration term\n    adjusted_scores = scores + exploration_adjusted\n\n    # Select action based on the highest adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        if action_counts[action_index] > 0:\n            expected_values[action_index] = np.mean(scores)\n\n    # Handle zero selections with a small epsilon\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Use a modified Upper Confidence Bound (UCB)\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / total_action_counts)\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    combined_values = expected_values + exploration_bonus * exploration_weight * exploration_weight\n\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n    exploration_rate = max(0, (total_time_slots - current_time_slot) / total_time_slots)  # Decay exploration\n    combined_values = expected_values + (exploration_bonus * exploration_rate)\n\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Handle potential division by zero\n    epsilon = 1e-5\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1) + 1) / (action_counts + epsilon))\n\n    # Time decay factor\n    decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n\n    # Adaptive exploration: balance exploration and exploitation\n    combined_values = expected_values + exploration_bonus + decay_factor * expected_values\n    action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Avoid division by zero for actions with zero counts\n    counts_adjusted = counts + 1e-5\n    \n    # Calculate the UCB score for each action\n    ucb_scores = scores + np.sqrt((2 * np.log(total_selection_count + 1)) / counts_adjusted)\n    \n    # Scale UCB scores by the remaining time slots\n    time_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = ucb_scores * time_scaling\n\n    # Select the action with the highest adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n\n    # Smooth out counts to avoid division by zero\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Calculate the beta parameter for Thompson Sampling\n    alpha = expected_values * total_action_counts\n    beta = (1 - expected_values) * total_action_counts\n\n    # Sample from the beta distribution for each action\n    sampled_values = np.random.beta(alpha, beta)\n\n    # Time-dependent normalization factor for exploration\n    time_factor = (1 - (current_time_slot / total_time_slots)) ** 2\n\n    # Combine sampled values with time factor to bias towards exploration\n    combined_values = sampled_values + time_factor * (1 - sampled_values)\n\n    # Select the action with the highest combined value\n    action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Assign a minimum count to avoid division by zero\n    counts_with_min = counts + 1e-5\n    average_scores = scores\n    \n    # Calculate exploration factor using UCB\n    exploration_factor = 2  # Increased exploration factor for more exploration\n    exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n    \n    # Adaptive scaling based on remaining time\n    remaining_time_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = average_scores + exploration_bonus * remaining_time_scaling\n    \n    # Select the action with the highest adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Handle zero selections with a small epsilon\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Calculate UCB components\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n    time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n    combined_values = expected_values + (exploration_bonus * time_decay_factor)\n\n    # Select action based on maximum combined value\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Avoid division by zero by adding a small epsilon\n    epsilon = 1e-5\n    counts_with_epsilon = counts + epsilon\n\n    # Calculate average scores\n    average_scores = scores\n\n    # Implement Upper Confidence Bound (UCB) for exploration\n    exploration_factor = 2  # Adjust exploration factor as necessary\n    exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / counts_with_epsilon)\n\n    # Dynamic scaling of exploration bonus based on remaining time slots\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = average_scores + (exploration_bonus * exploration_scaling)\n\n    # Select the action with the highest adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Assign a minimum count to avoid division by zero\n    counts_with_min = counts + 1e-5\n    average_scores = scores\n\n    # Calculate exploration factor using epsilon-greedy\n    epsilon_base = 0.1\n    exploration_probability = epsilon_base * (total_time_slots - current_time_slot) / total_time_slots\n\n    # Using UCB for exploration\n    exploration_factor = 2\n    exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n\n    # Adjust the scores by combining exploration and exploitation\n    adjusted_scores = average_scores + (1 - exploration_probability) * exploration_bonus\n\n    # Randomly explore with a probability of epsilon\n    if np.random.rand() < exploration_probability:\n        action_index = np.random.choice(actions)\n    else:\n        action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Handle zero selections\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n    \n    # Calculate the exploration factor using UCB\n    ucb_exploration = np.sqrt((2 * np.log(total_selection_count + 1)) / total_action_counts)\n    \n    # Adjust exploration based on the time slot\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    combined_values = expected_values + ucb_exploration * exploration_weight\n\n    # Select action based on maximum value\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    epsilon = 1e-5  # Small value to prevent division by zero\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1) + 1) / (action_counts + epsilon))\n\n    # Adjust exploration based on time and cumulative score\n    time_factor = 1 - (current_time_slot / total_time_slots)\n    combined_values = expected_values + (exploration_bonus * time_factor)\n\n    # Score adjustment to favor actions with fewer trials\n    adjusted_values = combined_values * (1 + (1 / (action_counts + 1)))\n\n    # Select action with the highest value\n    action_index = np.argmax(adjusted_values)\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    scores = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, action_scores in score_set.items():\n        action_counts[action_index] = len(action_scores)\n        if action_scores:\n            scores[action_index] = np.mean(action_scores)\n\n    # Handling zero counts\n    epsilon = 1e-5\n    adjusted_counts = action_counts + epsilon\n    \n    # UCB exploration factor\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1) / adjusted_counts))\n    \n    # Dynamic scaling based on time slot\n    time_factor = (total_time_slots + 1) / (current_time_slot + 1)\n\n    # Combining expected scores with exploration bonus\n    combined_values = scores + exploration_bonus * time_factor\n\n    # Select the action with the highest combined value\n    action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    num_actions = len(actions)\n    \n    # Initialize scores and counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Avoid division by zero and implement a small constant for exploration\n    counts_with_min = counts + 1e-5\n    \n    # Calculate adjusted scores with a focus on exploration and exploitation\n    average_scores = scores\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / counts_with_min)\n\n    # Dynamic scaling for exploration based on the remaining time slots\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = average_scores + (exploration_bonus * exploration_scaling * 0.5)  # Scale factor for exploration\n\n    # Select the action with the highest adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Handle zero selections with a small epsilon for stability\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Calculate UCB components with an exploration factor\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    combined_values = expected_values + exploration_bonus * exploration_weight\n\n    # Use a threshold for exploration and exploitation\n    if np.random.rand() < (total_time_slots - current_time_slot) / total_time_slots:\n        action_index = np.random.choice(np.arange(num_actions)[action_counts == 0]) if np.any(action_counts == 0) else np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n    \n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n    \n    # Calculate weighted exploration using UCB\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    \n    # Combine exploitation and exploration\n    combined_values = expected_values + (exploration_bonus * exploration_weight)\n    \n    # Select action based on max combined value\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate expected values and counts\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            selection_counts[action_index] = len(scores)\n\n    # Handle zero selections and apply small epsilon\n    epsilon = 1e-5\n    selection_counts += epsilon\n\n    # Compute exploration bonus using Upper Confidence Bound (UCB)\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / selection_counts)\n\n    # Temporal decay factor for exploration\n    time_weight = (total_time_slots - current_time_slot) / total_time_slots\n\n    # Combine exploration and exploitation\n    combined_values = expected_values + (exploration_bonus * time_weight)\n\n    # Select action based on maximum combined value\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    action_counts = np.maximum(action_counts, 1)  # Avoid division by zero\n    total_action_counts = action_counts + 1e-5\n\n    # Calculate UCB exploration bonus\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / total_action_counts)\n\n    # Normalize time factor for exploration weight\n    time_factor = (current_time_slot + 1) / total_time_slots\n    \n    # Combine expected values and exploration bonuses\n    combined_values = expected_values + exploration_bonus * time_factor\n\n    # Select the action with maximum combined value\n    action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n    \n    # Avoid division by zero by using a small epsilon\n    epsilon = 1e-5\n\n    # Calculate UCB with exploration bonus\n    ucb_bonus = np.sqrt((np.log(total_selection_count + 1) + 1) / (action_counts + epsilon))\n    \n    # Combine expected values with UCB exploration bonus\n    combined_ucb_values = expected_values + ucb_bonus\n\n    # Use an \u03b5-greedy approach for exploration-exploitation\n    epsilon_value = max(0.1, 1 - current_time_slot / total_time_slots)  # Decay epsilon over time\n    if np.random.rand() < epsilon_value:\n        action_index = np.random.choice(num_actions)  # Randomly explore\n    else:\n        action_index = np.argmax(combined_ucb_values)  # Exploit best action\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n    \n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n    \n    # Avoid division by zero by adding a small epsilon\n    epsilon = 1e-5\n\n    # Upper Confidence Bound calculation\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1) + 1) / (action_counts + epsilon))\n    \n    # Combine expected values with exploration bonuses\n    ucb_values = expected_values + exploration_bonus \n\n    # Adaptive exploration strategy using a smooth decay factor\n    decay_factor = min(current_time_slot / total_time_slots, 1)\n    exploration_weight = 0.5 * (1 - decay_factor)\n\n    # Adjust final values for selection\n    adjusted_values = (1 - exploration_weight) * ucb_values + exploration_weight * expected_values\n\n    # Select the action with the highest adjusted value\n    action_index = np.argmax(adjusted_values)\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate expected values and counts\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Handle zero selections by adding a small epsilon\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Calculate exploration bonus\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n    \n    # Adjust exploration weight based on time slots remaining\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    combined_values = expected_values + (exploration_bonus * exploration_weight)\n\n    # Select the action with the highest combined value\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Avoid division by zero by adding a small value\n    counts_with_min = counts + 1e-5\n    \n    # Calculate average scores\n    average_scores = scores\n\n    # Calculate exploration factor using UCB method\n    exploration_factor = 2  # Higher factor to increase exploration\n    exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n\n    # Adjust scores based on exploration bonus and the remaining time slots\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = average_scores + exploration_bonus * exploration_scaling\n\n    # Implement Softmax for action selection\n    exp_scores = np.exp(adjusted_scores - np.max(adjusted_scores))  # For numerical stability\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Choose action based on the computed probabilities\n    action_index = np.random.choice(actions, p=probabilities)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Avoid division by zero\n    epsilon = 1e-5\n\n    # Upper Confidence Bound calculation\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1) + 1) / (action_counts + epsilon))\n\n    # Combine expected values with exploration bonus\n    combined_values = expected_values + exploration_bonus\n\n    # Adjustable exploration factor based on time\n    time_weight = current_time_slot / total_time_slots\n    adjusted_values = combined_values * (1 - time_weight) + expected_values * time_weight\n\n    # Select the action with the highest adjusted value\n    action_index = np.argmax(adjusted_values)\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    \n    # Calculate the mean scores and selection counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Handle cases with no selections\n    counts_with_min = counts + 1e-5\n\n    # Calculate average scores\n    average_scores = scores\n\n    # Exploration rate adjustment based on time\n    exploration_rate = 1 / (current_time_slot + 1)\n\n    # Epsilon for exploration\n    epsilon = 0.1\n\n    if np.random.rand() < epsilon:\n        # Explore: Randomly select an action\n        action_index = np.random.choice(actions)\n    else:\n        # Exploit: Calculate UCB\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n        \n        # Adjust exploration bonus with a scaling based on remaining time slots\n        exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_scores = average_scores + exploration_bonus * exploration_scaling\n        \n        # Select the action with the highest adjusted score\n        action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Handle division by zero securely\n    total_action_counts = action_counts + 1e-5\n\n    # Calculate exploration bonus using UCB with a carefully tuned exploration factor\n    exploration_bonus = np.sqrt(2 * np.log(total_selection_count + 1) / total_action_counts)\n    \n    # Dynamic scaling factor for exploration based on the time slot\n    time_factor = (current_time_slot + 1) / total_time_slots\n\n    # Calculate combined values for exploitation and exploration\n    combined_values = expected_values + time_factor * exploration_bonus\n\n    # Select the action with the maximum combined value\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n    \n    # Prevent division by zero\n    total_action_counts = action_counts + 1  # Add 1 to avoid zero counts\n\n    # Use UCB for exploration\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / total_action_counts)\n    \n    # Time-dependent normalization factor\n    time_factor = ((current_time_slot + 1) / total_time_slots) ** 0.5\n    \n    # Combining expected values with exploration bonus\n    combined_values = expected_values + exploration_bonus * time_factor\n\n    # Select the action with the highest combined value\n    action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Handle zero selections with a small epsilon\n    epsilon = 1e-5\n    adjusted_action_counts = action_counts + epsilon\n\n    # Calculate exploration-exploitation balance\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / adjusted_action_counts)\n    time_ratio = (total_time_slots - current_time_slot) / total_time_slots\n\n    combined_values = expected_values + exploration_bonus * time_ratio\n\n    # Select action based on maximum combined value\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        if action_counts[action_index] > 0:\n            expected_values[action_index] = np.mean(scores)\n    \n    # Handle actions with no selections\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Calculate exploration factor with an adaptive exploration strategy\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    combined_values = expected_values + exploration_bonus * exploration_weight\n\n    # Select action based on maximum combined value\n    action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Handle zero selections with a small epsilon\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # UCB calculation\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n\n    # Combine expected values with exploration\n    combined_values = expected_values + (exploration_bonus * exploration_weight)\n\n    # Apply a decay factor for the remaining time slots\n    decay_factor = 1 - (current_time_slot / total_time_slots)\n    combined_values *= decay_factor\n\n    # Select the action that maximizes the combined value\n    action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Handle zero selections and calculate exploration factor\n    epsilon = 1e-5\n    action_counts = action_counts + epsilon\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / action_counts)\n\n    # Dynamic exploration weight based on remaining time slots\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    combined_values = expected_values + (exploration_bonus * exploration_weight)\n\n    # Select action based on maximum combined value\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        if action_counts[action_index] > 0:\n            expected_values[action_index] = np.mean(scores)\n    \n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1) + 1) / total_action_counts)\n\n    # Use a higher power for time factor to emphasize earlier exploration\n    time_factor = ((current_time_slot + 1) / total_time_slots) ** 1.5\n\n    combined_values = expected_values + exploration_bonus * time_factor\n\n    action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate expected values and action counts\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n\n    # Use a small constant to prevent division by zero\n    epsilon = 1e-5\n\n    # Calculate exploration term (UCB based)\n    exploration_term = np.sqrt(np.log(total_selection_count + 1) / (action_counts + epsilon))\n\n    # Weight exploration by time remaining\n    time_weight = (total_time_slots - current_time_slot) / total_time_slots\n\n    # Calculate total values for each action\n    total_values = expected_values + exploration_term * time_weight\n\n    # Incorporate a decay factor based on the time slot to prioritize recent performance\n    decay_factor = np.exp(-current_time_slot / (total_time_slots + epsilon))\n    adjusted_values = (1 - decay_factor) * total_values + decay_factor * expected_values\n\n    # Select the action with the maximum adjusted value\n    action_index = np.argmax(adjusted_values)\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    \n    # Calculate mean scores and counts with handling for empty score lists\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Avoid division by zero by adding a small epsilon\n    counts_plus_epsilon = counts + 1e-5\n\n    # Calculate average scores\n    average_scores = scores\n\n    # Explore-exploit factor: \u03b5-greedy method\n    exploration_prob = 0.1  # 10% of the time we explore\n    if np.random.rand() < exploration_prob:\n        # Explore: Randomly select an action\n        action_index = np.random.choice(actions)\n    else:\n        # Upper Confidence Bound (UCB) strategy\n        exploration_bonus = np.sqrt(np.log(total_selection_count) / counts_plus_epsilon)\n\n        # Adjust the exploration_bonus accounting for the time decay\n        exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_scores = average_scores + exploration_bonus * exploration_scaling\n\n        # Select the action with the highest adjusted score\n        action_index = actions[np.argmax(adjusted_scores)]\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    num_actions = len(actions)\n    \n    # Initialize scores and counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Calculate total selections with a minimum value to prevent division errors\n    counts_with_min = counts + 1e-5\n    \n    # Calculate average scores\n    average_scores = scores / counts_with_min\n\n    # Calculate exploration bonuses using modified UCB\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / counts_with_min)\n    \n    # Adjust exploration factor based on time progression\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = average_scores + exploration_bonus * exploration_scaling\n\n    # Select the action with the highest adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate expected values and action counts\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n\n    # Avoid division by zero by adding a small epsilon\n    epsilon = 1e-5\n    \n    # Upper Confidence Bound calculation\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1) + 1) / (action_counts + epsilon))\n    \n    # Combine expected values with exploration bonus\n    combined_values = expected_values + exploration_bonus\n\n    # Adaptive exploration using a time factor and \u03b5-greedy concept\n    exploration_rate = max(0, 1 - (current_time_slot / total_time_slots))\n    adjusted_values = (1 - exploration_rate) * combined_values + exploration_rate * np.random.rand(num_actions)\n\n    # Normalize to retain exploration-exploitation trade-off\n    adjusted_values = adjusted_values / np.sum(adjusted_values)\n\n    # Select the action with the highest adjusted value\n    action_index = np.random.choice(range(num_actions), p=adjusted_values)\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    epsilon = 1e-5  # Small constant to avoid division by zero\n    adjusted_exploration = np.sqrt(np.log(total_selection_count + 1) / (action_counts + epsilon))\n    \n    # Calculate decay based on the current time slot\n    time_decay = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_values = expected_values + adjusted_exploration * time_decay\n    \n    # Use softmax to select actions based on adjusted values for better exploration\n    exp_values = np.exp(adjusted_values - np.max(adjusted_values))  # Stability for softmax\n    action_probabilities = exp_values / np.sum(exp_values)\n\n    # Sample an action based on the computed probabilities\n    action_index = np.random.choice(num_actions, p=action_probabilities)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n    \n    # Calculate expected values and action counts\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n    \n    # Epsilon for avoidance of division by zero\n    epsilon = 1e-5\n    exploration_factor = np.sqrt(np.log(total_selection_count + 1) / (action_counts + epsilon))\n\n    # Logarithmic time adjustment to encourage early exploration\n    time_weight = np.log(total_time_slots - current_time_slot + 1) / np.log(total_time_slots + 1)\n    \n    # Calculate combined values\n    combined_values = expected_values + exploration_factor * time_weight\n    \n    # Select the action with the maximum combined value\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Compute expected values and action counts\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Avoid division by zero\n    epsilon = 1e-6\n    action_counts = np.clip(action_counts, 1, None)  # Ensure no zero counts\n\n    # Upper Confidence Bound (UCB) calculation\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1) + 1) / action_counts)\n\n    # Combine expected values with exploration bonus\n    combined_values = expected_values + exploration_bonus\n\n    # Dynamic exploration-exploitation adjustment\n    time_factor = (current_time_slot + 1) / total_time_slots\n    adjusted_values = (1 - time_factor) * combined_values + time_factor * expected_values\n\n    # Select the action with the highest adjusted value\n    action_index = np.argmax(adjusted_values)\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Handle division safely with np.where for zero selections\n    counts_with_min = np.where(counts > 0, counts, 1e-5)\n\n    # Calculate average scores\n    average_scores = scores\n\n    # Calculate exploration bonus (UCB)\n    exploration_factor = 1.5\n    exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n\n    # Dynamically scale exploration based on progression through time slots\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = average_scores + exploration_bonus * exploration_scaling\n\n    # Select action with the highest adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    \n    # Initialize arrays for scores and counts\n    scores = np.zeros(len(actions))\n    counts = np.zeros(len(actions))\n    \n    # Calculate scores and counts\n    for index, action in enumerate(actions):\n        if score_set[action]:\n            scores[index] = np.mean(score_set[action])\n            counts[index] = len(score_set[action])\n    \n    # Avoid division by zero by adding a small constant to counts\n    counts_with_min = counts + 1e-5\n    \n    # Calculate average scores\n    average_scores = scores\n    \n    # Exploration factor\n    exploration_prob = 0.2  # Increased exploration percentage\n    if np.random.rand() < exploration_prob:\n        # Randomly select an action (exploration)\n        action_index = np.random.choice(actions)\n    else:\n        # UCB calculation (exploitation)\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n        \n        # Adjust exploration bonus considering time left\n        exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_scores = average_scores + exploration_bonus * exploration_scaling\n        \n        # Select the action with the highest adjusted score\n        action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Handle division safely with a small constant for counts\n    counts_with_min = np.where(counts > 0, counts, 1e-5)\n\n    # Calculate average scores\n    average_scores = scores\n\n    # Upper Confidence Bound (UCB) calculation\n    exploration_factor = 2.0\n    exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n\n    # Scale exploration bonus according to the remaining time slots\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = average_scores + exploration_bonus * exploration_scaling\n\n    # Select action with the highest adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Handle division by zero with small epsilon\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Calculate the exploration factor using Thompson Sampling\n    beta_parameters = np.zeros((num_actions, 2))\n    for action_index in range(num_actions):\n        beta_parameters[action_index] = [1 + action_counts[action_index], 1 + (total_selection_count - action_counts[action_index])]\n\n    # Sample from beta distributions\n    sampled_values = np.random.beta(beta_parameters[:, 0], beta_parameters[:, 1])\n    \n    # Normalize time factor for exploration weight\n    time_weight = 1 - (current_time_slot / total_time_slots)\n    \n    # Combine expected values and sampled exploration values\n    combined_scores = expected_values + time_weight * sampled_values\n\n    # Select the action with the maximum score\n    action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n\n    # Avoid division by zero by adding a small constant\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n    \n    # UCB Exploration Term\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / total_action_counts)\n    \n    # Time-dependent normalization factor\n    time_factor = ((current_time_slot + 1) / total_time_slots) ** 0.5\n    \n    # Combine expected values with exploration bonus\n    combined_values = expected_values + exploration_bonus * time_factor\n\n    # Select the action with the highest combined value\n    action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Avoid division by zero\n    counts_with_min = counts + 1e-5\n    average_scores = scores\n\n    # Calculate exploration probability\n    exploration_factor = 2\n    exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n\n    # Dynamic scaling based on remaining time slots\n    scaling_factor = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = average_scores + exploration_bonus * scaling_factor\n\n    # Select action with the highest adjusted score\n    action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n\n    # Handling zero counts by adding a small constant\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n    \n    # UCB strategy with a twist\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1) + 1) / total_action_counts)\n    \n    # Time-dependent normalization factor\n    time_factor = ((current_time_slot + 1) / total_time_slots) ** 0.5\n\n    # Combining expected values with exploration bonus\n    combined_values = expected_values + exploration_bonus * time_factor\n\n    # Select the action with the highest combined value\n    action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    action_counts += 1e-5  # Avoid zero counts for UCB calculation\n\n    # Calculate Upper Confidence Bound (UCB)\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / action_counts)\n    combined_values = expected_values + exploration_bonus\n\n    # Adaptive exploration incorporating a decay based on the time slot\n    decay_factor = (current_time_slot + 1) / total_time_slots\n    adjusted_values = (1 - decay_factor) * combined_values + decay_factor * expected_values\n\n    # Select the action with the highest adjusted value\n    action_index = np.argmax(adjusted_values)\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    \n    # Calculate mean scores and selection counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Handle cases with no selections\n    counts_with_min = counts + 1e-5\n\n    # Calculate average scores\n    average_scores = scores\n    \n    # Adaptive exploration-exploitation balance\n    exploration_prob = 0.1  # 10% exploration\n    if np.random.rand() < exploration_prob:\n        action_index = np.random.choice(actions)\n    else:\n        # UCB calculation\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n        \n        # Decay exploration over time\n        exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_scores = average_scores + exploration_bonus * exploration_scaling\n        \n        # Select the action with the highest adjusted score\n        action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Calculate total action counts, ensuring to avoid division by zero\n    total_action_counts = action_counts + 1e-5\n\n    # Calculate the exploration bonus\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1) + 1) / total_action_counts)\n    remaining_time_fraction = (total_time_slots - current_time_slot) / total_time_slots\n\n    # Combine expected values and exploration bonuses\n    combined_values = expected_values + (exploration_bonus * remaining_time_fraction)\n\n    # Select the action that maximizes the combined value\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n\n    # Handling zero counts with a small constant for stability\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Upper Confidence Bound (UCB) implementation\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / total_action_counts)\n\n    # Time-dependent normalization factor for exploration\n    time_factor = ((current_time_slot + 1) / total_time_slots) ** 0.5\n\n    # Combining expected values and exploration bonuses\n    combined_values = expected_values + exploration_bonus * time_factor\n\n    # Select the action with the highest combined value\n    action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        if action_counts[action_index] > 0:\n            expected_values[action_index] = np.mean(scores)\n\n    # Handle zero selections with a small epsilon for robust division\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Calculate exploration bonus\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n\n    # Dynamic exploration weight based on time\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    combined_values = expected_values + exploration_bonus * exploration_weight\n\n    # Select action based on maximum combined value\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0.0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Avoid division by zero\n    counts_with_min = counts + 1e-5\n\n    # Calculate average scores\n    average_scores = scores\n\n    # UCB exploration factor\n    exploration_bonus = np.sqrt(2 * np.log(total_selection_count + 1) / counts_with_min)\n\n    # Time-decayed exploration scaling\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = average_scores + (exploration_bonus * exploration_scaling)\n\n    # Select the action with the highest adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Handle zero selections with a small value\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Calculate UCB components\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / total_action_counts)\n    \n    # Adjust exploration factor over time\n    time_decay = (total_time_slots - current_time_slot + 1) / total_time_slots\n    combined_values = expected_values + (exploration_bonus * time_decay)\n\n    # Select action based on the maximum combined value\n    action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n    \n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n    \n    # Avoid division by zero by adding a small epsilon\n    epsilon = 1e-5\n   \n    # Calculate Upper Confidence Bound (UCB)\n    ucb = expected_values + np.sqrt((2 * np.log(total_selection_count + 1)) / (action_counts + epsilon))\n    \n    # Adapt the exploration strategy with a dynamic time factor\n    time_factor = (current_time_slot + 1) / total_time_slots\n    adjusted_scores = (1 - time_factor) * ucb + time_factor * expected_values\n    \n    # Selection of action with highest adjusted score\n    action_index = np.argmax(adjusted_scores)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        if action_counts[action_index] > 0:\n            expected_values[action_index] = np.mean(scores)\n\n    # Handle zero selections by assigning a small score\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Calculate the UCB component\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n\n    # Adaptive exploration factor\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots if total_time_slots > 1 else 0\n    combined_values = expected_values + (exploration_bonus * exploration_weight)\n\n    # Select action based on maximum combined value\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Avoid division by zero\n    counts_with_min = counts + 1e-5\n    average_scores = scores\n\n    # \u03b5-greedy exploration strategy\n    exploration_rate = max(0.1, (total_time_slots - current_time_slot) / (total_time_slots + 1))\n    \n    # Determine if we exploit or explore\n    if np.random.rand() < exploration_rate:\n        action_index = np.random.choice(actions)\n    else:\n        # Upper Confidence Bound calculation\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n        adjusted_scores = average_scores + exploration_bonus\n\n        # Select the action with the highest adjusted score\n        action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate mean scores and counts\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Handle zero selections with a small epsilon\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Calculate UCB components\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n    \n    # Normalize exploration weight based on time\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    \n    # Combine the components for final scores\n    combined_scores = expected_values + (exploration_bonus * exploration_weight)\n\n    # Select action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Avoid division by zero by adding a small constant\n    counts_with_min = counts + 1e-5\n    average_scores = scores\n\n    # Balancing exploration and exploitation using Softmax\n    temperature = 1.0  # Control the randomness of action selection\n    exp_scores = np.exp(average_scores / temperature)\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Implementing \u03b5-greedy strategy for exploration\n    epsilon = 0.1  # Probability for exploration\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(actions)\n    else:\n        action_index = actions[np.argmax(probabilities)]\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Handle zero selections gracefully\n    epsilon = 1e-5\n    action_counts += epsilon\n\n    # Calculate the combined expected value with UCB\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / action_counts)\n    \n    # Dynamic exploration factor based on remaining time slots\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    combined_values = expected_values + (exploration_bonus * exploration_weight)\n\n    # Select action with maximum combined value\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Define a small epsilon for zero division avoidance\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Calculate a dynamic exploration factor\n    exploration_factor = np.sqrt(np.log(total_selection_count + 1) / total_action_counts)\n    remaining_time_weight = (total_time_slots - current_time_slot) / total_time_slots\n    combined_values = expected_values + (exploration_factor * remaining_time_weight)\n\n    # Select action based on the highest combined value\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n    \n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n    \n    # Handle zero selections with a low epsilon\n    epsilon = 1e-5\n    adjusted_counts = action_counts + epsilon\n    \n    # Calculate UCB components\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / adjusted_counts)\n    exploitation_weight = expected_values\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    \n    # Combine UCB and expected values\n    combined_values = exploitation_weight + (exploration_bonus * exploration_weight)\n    \n    # Select action based on maximum combined value\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    \n    # Calculate mean scores and selection counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Small constant to avoid division by zero\n    counts_with_min = counts + 1e-5\n    \n    # Calculate average scores\n    average_scores = scores / counts_with_min\n\n    # Exploration factor (\u03b5-greedy method with modified \u03b5)\n    exploration_prob = max(0.1, 1 - (current_time_slot / total_time_slots))\n    if np.random.rand() < exploration_prob:\n        action_index = np.random.choice(actions)\n    else:\n        # Upper Confidence Bound (UCB)\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n        \n        # Adjust scores with exploration bonus\n        adjusted_scores = average_scores + exploration_bonus\n        \n        # Select the action with the highest adjusted score\n        action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Set a small epsilon to avoid division by zero\n    epsilon = 1e-5\n    adjusted_action_counts = action_counts + epsilon\n    \n    # Calculate UCB\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / adjusted_action_counts)\n    \n    # Proportional exploration based on time left\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    \n    # Combine exploration and exploitation\n    combined_values = expected_values + (exploration_bonus * exploration_weight)\n    \n    # Ensure at least one action is selected even if all scores are empty\n    if total_selection_count == 0:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    \n    # Initialize scores and counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Calculate an adjusted count to avoid division by zero effectively\n    adjusted_counts = counts + 1e-5\n\n    # Calculate average scores\n    avg_scores = scores / adjusted_counts\n\n    # Dynamic exploration factor (\u03b5-greedy style)\n    exploration_rate = max(0.1, (total_time_slots - current_time_slot) / total_time_slots)\n    \n    # Calculate exploration bonus\n    exploration_bonus = (1.5 * np.sqrt(np.log(total_selection_count + 1) / adjusted_counts)) * exploration_rate\n\n    # Adjusted scores combining average scores and exploration bonuses\n    adjusted_scores = avg_scores + exploration_bonus\n\n    # Select the action with the highest adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n    \n    # Handling zero counts with a small constant\n    epsilon = 1e-5\n    adjusted_action_counts = action_counts + epsilon\n    \n    # Exploration parameter\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1) + 1) / adjusted_action_counts)\n    \n    # Time-awareness factor that encourages early exploration\n    time_factor = np.sqrt((current_time_slot + 1) / total_time_slots)\n    \n    # Updated combined values for selection decision\n    combined_values = expected_values + exploration_bonus * time_factor\n\n    # Select action with the highest combined value\n    action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    total_action_counts = action_counts + 1e-5  # Avoid division by zero\n\n    # Calculate exploration factor (UCB-like)\n    exploration = np.sqrt(np.log(total_selection_count + 1) / total_action_counts)\n\n    # Adjust exploration for remaining time\n    remaining_time_factor = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_exploration = exploration * remaining_time_factor\n\n    # Combine exploration and exploitation\n    combined_values = expected_values + adjusted_exploration\n\n    # Select action with the highest combined value\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Prevent division by zero\n    action_counts[action_counts == 0] = 1  # Ensures no action has a zero count\n\n    # Calculate UCB with coefficient adjustment\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / action_counts)\n    combined_values = expected_values + exploration_bonus\n\n    # Implement a time-based decay for exploration\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    combined_values *= exploration_weight\n\n    # Select action based on maximum value\n    action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n    \n    # Calculate expected values and counts for each action\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n    \n    # Handle cases where an action has never been selected\n    epsilon = 1e-5\n    action_counts = np.where(action_counts == 0, 1, action_counts)  # Avoid division by zero\n    exploration_factor = np.sqrt(np.log(total_selection_count + 1) / action_counts)\n    \n    # Enhanced exploration scaling\n    scaling_factor = (total_time_slots - current_time_slot) / total_time_slots\n    exploration_values = exploration_factor * scaling_factor\n    \n    # Combine expected values and exploration values\n    combined_values = expected_values + exploration_values\n    \n    # Normalize combined values for better stability\n    max_combined_value = np.max(combined_values)\n    exp_values = np.exp(combined_values - max_combined_value)\n    softmax_values = exp_values / np.sum(exp_values)\n    \n    # Select action based on softmax probabilities\n    action_index = np.random.choice(num_actions, p=softmax_values)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Handle zero selections by adding a small count to avoid division by zero\n    total_action_counts = action_counts + 1e-5\n\n    # Calculate the exploration factor\n    exploration_factor = np.sqrt(np.log(total_selection_count + 1) / total_action_counts)\n\n    # Consider the remaining number of time slots for adaptive exploration\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    combined_values = expected_values + exploration_factor * exploration_weight\n\n    # Select the action with the maximum combined value\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Assign a minimum count to avoid division by zero\n    min_count = 1e-5\n    counts_with_min = counts + min_count\n\n    # Calculate the average scores and exploration term\n    average_scores = scores\n    exploration_factor = 1.5\n    exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n\n    # Adaptations to explore less frequently selected actions\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = average_scores + exploration_bonus * exploration_scaling\n\n    # Use Softmax for selection to better handle exploration-exploitation balance\n    exp_scores = np.exp(adjusted_scores - np.max(adjusted_scores))  # for numerical stability\n    selection_probs = exp_scores / np.sum(exp_scores)\n\n    # Randomly select an action based on the computed probabilities\n    action_index = np.random.choice(actions, p=selection_probs)\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Handle actions never selected before\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / (action_counts + 1e-5))\n\n    # Compute UCB adjustment\n    ucb_values = expected_values + exploration_bonus\n    \n    # Time decay factor to make recent actions more impactful\n    time_decay = 1 - (current_time_slot / total_time_slots)\n    \n    # Compute scored values balancing exploration and current expected performance\n    adjusted_values = ucb_values * time_decay + expected_values * (1 - time_decay)\n\n    # Select action index with the maximum adjusted value\n    action_index = np.argmax(adjusted_values)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Use epsilon for exploration\n    epsilon = 0.1\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (counts + 1e-5))\n    \n    # Calculate adjusted scores using epsilon-greedy strategy\n    adjusted_scores = scores + exploration_bonus\n\n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(actions)  # Explore\n    else:\n        action_index = actions[np.argmax(adjusted_scores)]  # Exploit\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    \n    # Calculate mean scores and selection counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Adjust counts to prevent division by zero\n    adjusted_counts = counts + 1e-5\n\n    # Calculate average scores\n    average_scores = scores / adjusted_counts\n\n    # Exploring versus Exploiting\n    exploration_prob = 0.1\n    if np.random.rand() < exploration_prob:\n        action_index = np.random.choice(actions)\n    else:\n        # UCB calculation\n        confidence_bounds = np.sqrt((2 * np.log(total_selection_count + 1)) / adjusted_counts)\n        action_values = average_scores + confidence_bounds\n        \n        # Decay factor to prioritize immediate rewards over uncertain future rewards\n        decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_action_values = action_values * decay_factor\n        \n        action_index = actions[np.argmax(adjusted_action_values)]\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate expected values and counts for each action\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Smooth count to avoid division by zero\n    action_counts += 1e-5\n    \n    # Calculate Upper Confidence Bound components\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / action_counts)\n    \n    # Adjust exploration factor dynamically\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    combined_values = expected_values + exploration_bonus * exploration_weight\n\n    # Select action with maximum value\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Calculate exploration bonus with a decay factor based on the remaining time slots\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n    decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n    combined_values = expected_values + exploration_bonus * decay_factor\n\n    # Implementing a softmax selection mechanism for better exploration\n    exp_values = np.exp(combined_values - np.max(combined_values))\n    probabilities = exp_values / np.sum(exp_values)\n\n    action_index = np.random.choice(num_actions, p=probabilities)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Avoid division by zero by adding a small constant\n    counts_with_min = counts + 1e-5\n    \n    # Calculate average scores\n    average_scores = scores\n    \n    # Calculate exploration bonus using a Bayesian approach (Beta distribution)\n    exploration_bonus = np.sqrt(2 * np.log(total_selection_count + 1) / counts_with_min)\n    \n    # Calculate adjusted scores for exploration versus exploitation\n    adjusted_scores = average_scores + exploration_bonus\n    \n    # Incorporate time decay to balance actions based on time slot\n    time_decay = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores *= time_decay\n    \n    # Select the action with the highest adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Assign a minimum count to avoid division by zero\n    counts_with_min = counts + 1e-5\n    \n    # Calculate the exploration factor (\u03b5-greedy approach)\n    exploration_rate = max(0.1, (total_time_slots - current_time_slot) / total_time_slots)\n    \n    # Calculate the exploitation score\n    adjusted_scores = scores * (1 - exploration_rate)\n    \n    # Calculate exploration bonus\n    exploration_bonus = exploration_rate * (np.sqrt(np.log(total_selection_count + 1) / counts_with_min))\n    \n    # Final adjusted scores\n    final_scores = adjusted_scores + exploration_bonus\n\n    # Select the action with the highest final score\n    action_index = actions[np.argmax(final_scores)]\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Avoid division by zero\n    counts_with_min = counts + 1e-5\n    \n    # Calculate the average scores\n    average_scores = scores / counts_with_min\n    \n    # Exploration-exploitation parameter\n    exploration_prob = 0.1\n    \n    if np.random.rand() < exploration_prob:\n        # Explore: Randomly select an action\n        action_index = np.random.choice(actions)\n    else:\n        # Exploit: Use UCB with time adjustment\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n        exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_scores = average_scores + exploration_bonus * exploration_scaling\n        \n        # Select action with the highest adjusted score\n        action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Handle cases where counts are zero to avoid division by zero\n    counts_with_min = counts + 1e-5\n    average_scores = scores\n\n    # Epsilon-greedy exploration factor depending on the current time slot\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n    \n    # Calculate exploration bonus with UCB\n    exploration_factor = np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n    \n    # Combined score with exploration tuning\n    adjusted_scores = average_scores + epsilon * exploration_factor\n\n    # Select the action with the highest adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n    \n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    epsilon = 1e-5  # Small value to prevent division by zero\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1) + 1) / (action_counts + epsilon))\n\n    # Adjust exploration bonus based on time and exploration factor\n    time_factor = (current_time_slot + 1) / total_time_slots\n    combined_values = expected_values + exploration_bonus * time_factor\n    \n    # Incorporate linear decay to prefer recent results\n    decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n    weighted_combined_values = decay_factor * combined_values + (1 - decay_factor) * expected_values\n\n    # Select the action with the highest value\n    action_index = np.argmax(weighted_combined_values)\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Extract actions and their corresponding scores\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Calculate the exploration factor\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (counts + 1e-5))\n\n    # Scale exploration by the current time slot\u2019s proximity to the total time slots\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    exploration_adjusted = exploration_bonus * exploration_scaling\n\n    # Combine scores with the exploration term\n    adjusted_scores = scores + exploration_adjusted\n\n    # Select action based on the highest adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Handle the case where no actions have been selected (to avoid division by zero)\n    counts_with_min = counts + 1e-5\n\n    # Calculate the average score\n    average_scores = scores\n\n    # Calculate the exploration factor (UCB)\n    exploration_factor = 1.5\n    exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n\n    # Scale the exploration bonus down as time progresses\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = average_scores + exploration_bonus * exploration_scaling\n\n    # Select the action with the highest adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n\n    # Small epsilon to avoid division by zero\n    epsilon = 1e-5\n\n    # Upper Confidence Bound with dynamic exploration\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / (action_counts + epsilon))\n    \n    # Dynamic exploration/exploitation balance\n    time_factor = (total_time_slots - current_time_slot) / total_time_slots\n    combined_values = expected_values + exploration_bonus * time_factor\n\n    # \u03b5-greedy parameter\n    epsilon_greedy = 0.1 * time_factor\n    if np.random.rand() < epsilon_greedy:\n        action_index = np.random.choice(num_actions)\n    else:\n        action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Avoid division by zero with small epsilon\n    epsilon = 1e-5\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1) + 1) / (action_counts + epsilon))\n\n    # Normalized time factor for exploration\n    time_factor = (current_time_slot + 1) / total_time_slots\n\n    # Calculate combined score using expected values and exploration\n    combined_values = expected_values + exploration_bonus * time_factor\n\n    # Apply a decay factor based on time to prioritize recent performance\n    decay_factor = np.exp(-current_time_slot / total_time_slots)\n    weighted_combined_values = combined_values * (1 - decay_factor) + (expected_values * decay_factor)\n\n    # Select the action with the maximum weighted combined value\n    action_index = np.argmax(weighted_combined_values)\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Calculate the exploration-adjusted average scores\n    exploration_factor = 1.5\n    exploration_bonus = exploration_factor * (np.sqrt(np.log(total_selection_count + 1) / (counts + 1e-5)))\n\n    # Encourage exploration based on the current time slot\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = scores + exploration_bonus * exploration_scaling\n\n    # Select the action with the highest adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Handle zero actions by using a small epsilon\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n    \n    # UCB calculations\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n    \n    # Weighted exploration based on time factor\n    time_factor = (current_time_slot + 1) / total_time_slots\n    \n    # Calculate combined values with exploration bonus and expected values\n    combined_values = expected_values + exploration_bonus * time_factor\n\n    # Select action based on maximum combined value\n    action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n\n    # Small epsilon to avoid division by zero\n    epsilon = 1e-5\n    \n    # Calculate exploration factor using a more refined UCB method\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / (action_counts + epsilon))\n    \n    # Dynamic time factor for exploration\n    time_factor = 1 - (current_time_slot / total_time_slots)\n    \n    # Combine expected values with exploration bonus\n    combined_values = expected_values + exploration_bonus * time_factor\n    \n    # Add a small random noise for exploration among actions with equal values\n    random_factor = np.random.uniform(0, 1, num_actions) * (exploration_bonus > 0)\n    \n    # Select the action with the maximum adjusted value\n    action_index = np.argmax(combined_values + random_factor)\n\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Avoid division by zero with small epsilon\n    epsilon = 1e-5\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1) + 1) / (action_counts + epsilon))\n    \n    # Normalized time factor for exploration\n    time_factor = (current_time_slot + 1) / total_time_slots\n\n    # Calculate combined score using a weighted sum of expected values and exploration\n    combined_values = expected_values + exploration_bonus * time_factor\n\n    # Select the action with the maximum combined value\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Handle zero actions using small epsilon\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Calculate UCB components\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1) + 1) / total_action_counts)\n    time_factor = (current_time_slot + 1) / total_time_slots\n    combined_values = expected_values + exploration_bonus * time_factor\n\n    # Select action based on maximum combined value\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n\n    # Small epsilon to avoid division by zero\n    epsilon = 1e-5\n    # Calculate exploration bonus using the UCB method\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / (action_counts + epsilon))\n    \n    # Normalize time factor for exploration\n    time_factor = (current_time_slot + 1) / total_time_slots\n    \n    # Combine expected values with exploration using weighted sum\n    combined_values = expected_values + exploration_bonus * time_factor\n    \n    # Select the action with the maximum combined value\n    action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # To handle division by zero, use small constant when action has not been selected\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1) + 1) / (action_counts + 1e-5))\n    \n    # Normalize scores across time slots to encourage exploration based on time\n    time_factor = (current_time_slot + 1) / total_time_slots\n\n    # Combined score balancing exploration and exploitation\n    combined_values = expected_values + exploration_bonus * time_factor\n\n    # Select action with the maximum combined value\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Handle division by zero effectively\n    with np.errstate(divide='ignore', invalid='ignore'):\n        exploration_bonus = np.sqrt((np.log(total_selection_count + 1) + 1) / (action_counts + 1e-5))\n    \n    # Adaptive exploration factor based on time\n    exploration_factor = np.clip((total_time_slots - current_time_slot) / total_time_slots, 0, 1)\n\n    # Combine exploitation and exploration into one score\n    combined_values = expected_values + exploration_factor * exploration_bonus\n\n    # Select action with the maximum combined value\n    action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Compute the exploration term using UCB\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1) + 1) / (counts + 1e-5))\n    \n    # Calculate UCB scores\n    ucb_scores = scores + exploration_bonus\n    \n    # Dynamic exploration adjustment based on time remaining\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = ucb_scores * (1 + exploration_scaling)\n    \n    # Handle potential negative results and NaNs\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    selections = np.array([len(score_set[action]) for action in actions])\n\n    # Avoid division by zero by adding a small constant (1)\n    adjusted_selection_counts = selections + 1\n    exploration_factor = np.sqrt(np.log(total_selection_count + 1) / adjusted_selection_counts)\n\n    # Compute Upper Confidence Bound (UCB) values\n    ucb_values = scores + exploration_factor\n\n    # Implement an \u03b5-greedy strategy, with \u03b5 decreasing over time\n    epsilon = max(0.01, 0.1 * (1 - current_time_slot / total_time_slots))  # Gradual decay of exploration\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(actions)\n    else:\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n    \n    # Adding a small epsilon to avoid division by zero\n    epsilon = 1e-5\n    exploration_term = np.sqrt(np.log(total_selection_count + 1) / (action_counts + epsilon))\n\n    # Normalize exploration term with respect to time left\n    remaining_time_factor = (total_time_slots - current_time_slot) / total_time_slots\n\n    # Calculate the utility of each action\n    utility_values = expected_values + exploration_term * remaining_time_factor\n    \n    # Perform softmax for action selection to balance exploration and exploitation\n    exp_utilities = np.exp(utility_values - np.max(utility_values))  # for numerical stability\n    probabilities = exp_utilities / np.sum(exp_utilities)\n\n    action_index = np.random.choice(num_actions, p=probabilities)\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Avoid division by zero by adding a small constant\n    counts_with_min = counts + 1e-5\n\n    # Calculate average scores\n    average_scores = scores\n    \n    # Calculate exploration factor\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n\n    # Dynamic exploration scaling based on remaining time\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    \n    # Combine exploitation and exploration\n    adjusted_scores = average_scores + exploration_bonus * exploration_scaling\n\n    # Select the action with the highest adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n\n    # Handling zero counts\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n    \n    # Using UCB for exploration\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / total_action_counts)\n    \n    # Normalizing the exploration bonus based on time\n    time_factor = (current_time_slot + 1) / total_time_slots\n    \n    # Combining expected values with exploration bonus\n    combined_values = expected_values + exploration_bonus * time_factor\n\n    # Select the action with the highest combined value\n    action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    \n    # Calculate mean scores and counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Adjust counts to avoid division by zero\n    adjusted_counts = counts + 1e-5\n\n    # Calculate average scores\n    average_scores = scores / adjusted_counts\n\n    # Exploration-exploitation parameter\n    exploration_prob = 0.1  # 10% shift towards exploration\n    softmax_temperature = 0.5  # Controls exploration through softmax\n    ucb_c = 2.0  # Confidence level for UCB\n\n    if np.random.rand() < exploration_prob:\n        # Explore: Random selection\n        action_index = np.random.choice(actions)\n    else:\n        # Exploit: Calculate UCB and Softmax probabilities\n        exploration_bonus = ucb_c * np.sqrt(np.log(total_selection_count + 1) / adjusted_counts)\n        ucb_scores = average_scores + exploration_bonus\n        \n        # Softmax probabilities\n        exp_scores = np.exp(ucb_scores / softmax_temperature)\n        probabilities = exp_scores / np.sum(exp_scores)\n\n        # Select action based on softmax probabilities\n        action_index = np.random.choice(actions, p=probabilities)\n\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    \n    # Initialize arrays for scores and counts\n    scores = np.zeros(len(actions))\n    counts = np.zeros(len(actions))\n    \n    # Extract scores and counts\n    for idx, action in enumerate(actions):\n        if score_set[action]:\n            scores[idx] = np.mean(score_set[action])\n            counts[idx] = len(score_set[action])\n    \n    # Handle the case when count is zero using the exploration term\n    ucb_values = np.zeros(len(actions))\n    for idx in range(len(actions)):\n        if counts[idx] > 0:\n            ucb_values[idx] = scores[idx] + np.sqrt(2 * np.log(total_selection_count) / counts[idx])\n        else:\n            ucb_values[idx] = np.inf  # Prefer actions that haven't been selected yet\n\n    # Scale exploration based on the current time slot\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    scaled_ucb_values = ucb_values * exploration_scaling\n\n    # Select action based on highest scaled UCB value\n    action_index = actions[np.argmax(scaled_ucb_values)]\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n    \n    epsilon = 1e-5  # Small constant to prevent division by zero\n    total_selections = total_selection_count + 1\n\n    # Calculate Upper Confidence Bound (UCB)\n    exploration_factors = np.sqrt(2 * np.log(total_selections) / (action_counts + epsilon))\n    ucb_values = expected_values + exploration_factors\n\n    # Time-based decay factor to prioritize recent performance\n    time_factor = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_ucb_values = ucb_values * time_factor + expected_values * (1 - time_factor)\n\n    # Select action based on the maximum adjusted UCB value\n    action_index = np.argmax(adjusted_ucb_values)\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Avoid division by zero\n    counts_with_min = counts + 1e-5\n    average_scores = scores\n\n    # Dynamic exploration factor based on current time slot\n    exploration_factor = 1.5\n    epsilon = 0.1 * (total_time_slots - current_time_slot) / total_time_slots\n    exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n\n    # Combine exploration and exploitation\n    adjusted_scores = average_scores + exploration_bonus * (1 - epsilon)\n    \n    # Apply epsilon-greedy strategy for exploration\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(actions)\n    else:\n        action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate expected values and action counts\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Handle zero selections with a small epsilon\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Calculate the exploration bonus using UCB\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n    \n    # Dynamic adjustment to exploration based on remaining time slots\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    combined_values = expected_values + (exploration_bonus * exploration_weight)\n\n    # Select the action with the maximum combined value\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    \n    # Calculate the mean scores and selection counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Handle cases with no selections\n    adjusted_counts = counts + 1e-5\n    \n    # Calculate average scores\n    average_scores = scores\n\n    # Exploration factor: \u03b5-greedy method\n    exploration_prob = max(0.1, 0.9 * (total_time_slots - current_time_slot) / total_time_slots)\n    if np.random.rand() < exploration_prob:\n        # Explore: Randomly select an action\n        action_index = np.random.choice(actions)\n    else:\n        # Exploit: Calculate UCB\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / adjusted_counts)\n        adjusted_scores = average_scores + exploration_bonus\n\n        # Select the action with the highest adjusted score\n        action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    \n    # Calculate mean scores and selection counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Avoid division by zero by adding a small value to counts\n    counts_with_min = counts + 1e-5\n\n    # Calculate average scores for actions\n    average_scores = scores\n\n    # Set exploration probability: decaying based on time\n    exploration_prob = max(0.01, (total_time_slots - current_time_slot) / total_time_slots * 0.2)  # Decrease exploration over time\n\n    if np.random.rand() < exploration_prob:\n        # Explore: Randomly select an action\n        action_index = np.random.choice(actions)\n    else:\n        # Exploit: Calculate UCB with decaying exploration bonus\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n        adjusted_scores = average_scores + exploration_bonus\n        \n        # Select the action with the highest adjusted score\n        action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    \n    # Calculate means and counts for each action\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Prevent division by zero\n    counts_with_min = counts + 1e-5\n\n    # Calculate the exploration factor (UCB approach)\n    exploration_bonus = np.sqrt(2 * np.log(total_selection_count + 1) / counts_with_min)\n    \n    # Calculate combined scores\n    adjusted_scores = scores + exploration_bonus\n    \n    # Linear decay based on the remaining time slots\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores *= exploration_scaling\n    \n    # Select action with the highest adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Calculate average scores\n    average_scores = scores\n\n    # Assign a small constant to avoid division by zero\n    counts_with_min = counts + 1e-5\n\n    # \u03b5-greedy strategy parameters\n    epsilon = 0.1\n    exploration_bonus = epsilon * np.random.rand(len(actions))\n\n    # Upper Confidence Bound (UCB) bonus\n    ucb_factor = np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n    ucb_bonus = 1.5 * ucb_factor\n\n    # Adjusted scores\n    adjusted_scores = average_scores + exploration_bonus + ucb_bonus\n\n    # Select the action with the highest adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    \n    # Calculate average scores and counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Handle cases with no selections elegantly\n    counts_safe = np.where(counts == 0, 1e-5, counts)\n\n    # Calculate average scores\n    average_scores = scores / counts_safe\n\n    # Calculate exploration bonus using UCB\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / counts_safe)\n    \n    # Dynamic scaling for exploration based on current time slot\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = average_scores + exploration_bonus * exploration_scaling\n\n    # Select action: Use the highest adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    \n    # Initialize scores and counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Avoid division by zero\n    counts_with_min = counts + 1e-5\n\n    # Calculate average scores\n    average_scores = scores / counts_with_min\n\n    # Exploration-Exploitation strategy\n    exploration_prob = 0.1  # 10% exploration chance\n    if np.random.rand() < exploration_prob:\n        action_index = np.random.choice(actions)\n    else:\n        # Upper Confidence Bound (UCB)\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n        \n        # Decay based on the remaining time slots\n        exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_scores = average_scores + exploration_bonus * exploration_scaling\n        \n        action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    num_actions = len(actions)\n    \n    # Calculate average scores and selection counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Adjust counts to avoid division by zero\n    counts_with_min = counts + 1e-5\n\n    # Calculate UCB for exploration\n    exploration_factor = 1.5\n    exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n\n    # Adaptive scaling of exploration bonus based on time\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = scores + exploration_bonus * exploration_scaling\n\n    # Implement \u03b5-greedy strategy for balancing exploration and exploitation\n    epsilon = 0.1\n    if np.random.random() < epsilon:\n        action_index = np.random.choice(actions)\n    else:\n        action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    \n    # Calculate mean scores and selection counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Handle counts to avoid division by zero\n    counts_with_min = counts + 1e-5\n    \n    # Calculate average scores\n    average_scores = scores\n    \n    # Define exploration-exploitation parameters\n    exploration_prob = 0.1 * (total_time_slots - current_time_slot) / total_time_slots\n    \n    if np.random.rand() < exploration_prob:\n        # Explore: Select action randomly\n        action_index = np.random.choice(actions)\n    else:\n        # Exploit: Calculate UCB\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n        adjusted_scores = average_scores + exploration_bonus\n        \n        # Select action with the highest adjusted score\n        action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Handle zero actions with a default value\n    epsilon = 1e-5\n    action_counts = action_counts + epsilon  # Prevent division by zero\n\n    # Calculate UCB with exploration factor\n    ucb_values = expected_values + np.sqrt((2 * np.log(total_selection_count + 1)) / action_counts)\n\n    # Introduce an epsilon to encourage exploration\n    exploration_factor = 0.1 * (total_time_slots - current_time_slot) / total_time_slots\n    randomized_values = (1 - exploration_factor) * ucb_values + exploration_factor * np.random.rand(num_actions)\n\n    # Select the best action\n    action_index = np.argmax(randomized_values)\n\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    num_actions = len(actions)\n    \n    # Calculate mean scores and counts for each action\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Avoid division by zero\n    counts_with_bonus = counts + 1e-5\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1) / counts_with_bonus))\n    \n    # Dynamic scaling of exploration based on time slot\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    exploration_adjusted_scores = scores + exploration_bonus * exploration_scaling\n    \n    # Adaptive epsilon for exploration versus exploitation\n    epsilon = 1.0 / (1.0 + total_selection_count / (num_actions * 5))  # adaptively decrease exploration\n    \n    # Stochastic decision based on epsilon\n    if np.random.rand() < epsilon:  # Explore\n        untried_actions = np.where(counts < 1)[0]\n        if untried_actions.size > 0:\n            action_index = np.random.choice(untried_actions)\n        else:\n            action_index = np.random.choice(num_actions)  # Random choice among all actions\n    else:  # Exploit\n        action_index = actions[np.argmax(exploration_adjusted_scores)]\n    \n    return action_index",
          "objective": -449.9999999999999,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Smooth the action counts to avoid division by zero\n    smoothed_counts = action_counts + 1e-5\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1) + 1) / smoothed_counts)\n    \n    # Decrease the exploration as we approach the last time slots\n    time_factor = (total_time_slots - current_time_slot) / total_time_slots\n\n    # Calculate combined values considering exploration and exploitation\n    combined_values = expected_values + exploration_bonus * time_factor\n\n    # Select action with the maximum combined value\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": -449.9999999999999,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores for each action, handling cases with no scores\n    avg_scores = np.array([np.mean(scores) if scores else 0 for scores in score_set.values()])\n    counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Handle divide by zero in UCB calculation by using a small constant\n    counts = np.where(counts == 0, 1, counts)\n    \n    # Calculate exploration weight using UCB\n    exploration_weights = np.sqrt(2 * np.log(total_selection_count + 1) / counts)\n    \n    # Compute upper confidence bound scores\n    ucb_scores = avg_scores + exploration_weights\n    \n    # Epsilon decay for exploration strategy\n    epsilon = max(0.1, 1 - current_time_slot / total_time_slots)  # Ensure a minimum exploration rate\n    \n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)  # Explore\n    else:\n        action_index = np.argmax(ucb_scores)  # Exploit\n\n    return action_index",
          "objective": -449.9999999999999,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate expected values and action counts\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        expected_values[action_index] = np.mean(scores) if scores else 0\n\n    # Handling zero counts by adding a small constant for stability\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n    \n    # UCB formula\n    confidence_bounds = np.sqrt((2 * np.log(total_selection_count + 1)) / total_action_counts)\n    \n    # Combine expected values with UCB confidence bounds\n    combined_values = expected_values + confidence_bounds\n\n    # Time-dependent decay factor to enhance exploration in earlier slots\n    time_exploration_factor = (1 - (current_time_slot / total_time_slots))\n    \n    # Final selection values considering exploration\n    final_values = combined_values * time_exploration_factor\n    \n    # Select the action with the highest value\n    action_index = np.argmax(final_values)\n\n    return action_index",
          "objective": -449.9999999999999,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Use Laplace smoothing to avoid division by zero\n    laps_smoothing = 1\n    adjusted_scores = scores + laps_smoothing\n    adjusted_counts = counts + laps_smoothing\n    \n    # Calculate UCB for exploration\n    ucb_scores = adjusted_scores + np.sqrt((2 * np.log(total_selection_count + 1)) / adjusted_counts)\n    \n    # Scale exploration based on current time slot\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_ucb_scores = ucb_scores * exploration_scaling\n    \n    # Select the action with the highest adjusted UCB score\n    action_index = actions[np.argmax(adjusted_ucb_scores)]\n    \n    return action_index",
          "objective": -449.9999999999999,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Handle the case where counts are zero to avoid division by zero\n    counts = np.where(counts == 0, 1, counts)\n\n    # Calculate the UCB for each action\n    ucb_values = scores + np.sqrt(2 * np.log(total_selection_count + 1) / counts)\n\n    # Scale UCB based on time slot proximity\n    time_penalty = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_ucb = ucb_values * time_penalty\n\n    # Select the action with the highest UCB value\n    action_index = actions[np.argmax(adjusted_ucb)]\n    \n    return action_index",
          "objective": -449.9999999999998,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    \n    # Calculate mean scores and counts safely\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Avoid division by zero by adding a small constant\n    counts_safe = counts + 1e-5\n    \n    # Calculate exploration bonus using UCB\n    exploration_factor = np.sqrt(np.log(total_selection_count + 1) / counts_safe)\n    \n    # Calculate time-adjusted exploration scaling\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    \n    # Combined score with exploration\n    adjusted_scores = scores + exploration_scaling * exploration_factor\n    \n    # Select the action with the highest adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -449.9999999999997,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores\n    avg_scores = np.array([np.mean(scores) if len(scores) > 0 else 0 for scores in score_set.values()])\n    counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Adjust counts to handle zero selections\n    counts = np.where(counts == 0, 1, counts)  # Ensure no division by zero\n\n    # Calculate exploration factor using a dynamic \u03b5-greedy approach\n    epsilon = 1 / (current_time_slot + 1)  # Decrease exploration over time\n    exploration_terms = np.random.rand(num_actions) < epsilon\n    \n    # Select with probability \u03b5 for exploration, otherwise exploit\n    final_scores = avg_scores + exploration_terms * (np.max(avg_scores) - avg_scores)\n\n    # Select action with the highest score\n    action_index = np.argmax(final_scores)\n    \n    return action_index",
          "objective": -449.9999999999997,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Prevent division by zero by using a small constant\n    counts_with_min = counts + 1e-5\n\n    # Calculate average scores\n    average_scores = scores\n\n    # Use UCB exploration strategy\n    exploration_factor = 1.5\n    exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n\n    # Scale exploration bonus based on remaining time slots\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = average_scores + exploration_bonus * exploration_scaling\n\n    # Use a temperature parameter to control exploration\n    temperature = max(1, total_time_slots - current_time_slot + 1)\n    softmax_scores = np.exp(adjusted_scores / temperature) / np.sum(np.exp(adjusted_scores / temperature))\n\n    # Use weighted probabilities to select action\n    action_index = np.random.choice(actions, p=softmax_scores)\n\n    return action_index",
          "objective": -449.9999999999995,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Calculate average scores\n    avg_scores = scores / (counts + 1e-5)\n    \n    # Implement Upper Confidence Bound (UCB) strategy\n    ucb_bonus = np.sqrt(2 * np.log(total_selection_count + 1) / (counts + 1e-5))\n    adjusted_scores = avg_scores + ucb_bonus\n    \n    # Adjust exploration based on remaining time slots\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores *= exploration_scaling\n    \n    # Select action with the highest adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -449.9999999999995,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores for actions\n    avg_scores = np.array([np.mean(scores) if scores else 0 for scores in score_set.values()])\n    counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Handle zero counts to avoid division by zero\n    counts_safe = np.where(counts == 0, 1, counts)\n\n    # Calculate exploration weight based on counts\n    exploration_weights = np.sqrt((2 * np.log(total_selection_count + 1)) / counts_safe)\n\n    # Compute UCB scores\n    ucb_scores = avg_scores + exploration_weights\n    \n    # Epsilon-greedy exploration strategy\n    epsilon = max(0.1, 1.0 - (current_time_slot / total_time_slots))  # Minimum epsilon\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)  # Explore\n    else:\n        action_index = np.argmax(ucb_scores)  # Exploit\n\n    return action_index",
          "objective": -449.99999999999943,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Calculate average scores\n    avg_scores = scores / (counts + 1e-5)  # To avoid division by zero\n\n    # Calculate uncertainty for each action\n    exploration_factor = 2\n    confidence_bounds = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / (counts + 1e-5))\n\n    # Dynamic exploration based on time remaining\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = avg_scores + confidence_bounds * exploration_scaling\n\n    # Select action based on adjusted scores\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -449.9999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Calculate baseline average scores\n    avg_scores = scores / (counts + 1e-5)\n    \n    # Implement an \u03b5-greedy strategy\n    epsilon = 1.0 / (total_time_slots - current_time_slot + 1)  # Decay epsilon over time\n    exploration = np.random.rand() < epsilon\n    \n    if exploration:\n        action_index = np.random.choice(actions)\n    else:\n        # Use adjusted scores for exploitation\n        exploration_factor = 2\n        confidence_bounds = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / (counts + 1e-5))\n        \n        # Adjust for remaining time slots for better exploration-exploitation balance\n        exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_scores = avg_scores + confidence_bounds * exploration_scaling\n        \n        action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -449.9999999999993,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Calculate the average scores\n    avg_scores = scores / (counts + 1e-5)\n    \n    # Use a UCB-like approach to balance exploration and exploitation\n    exploration_factor = 2\n    confidence_bounds = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / (counts + 1e-5))\n\n    # Encourage exploration based on the current time slot\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = avg_scores + confidence_bounds * exploration_scaling\n\n    # Select the action with the highest adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -449.99999999999926,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Handle case for new actions (i.e., unselected)\n    total_actions = len(actions)\n    with np.errstate(divide='ignore', invalid='ignore'):\n        ucb_values = scores + np.sqrt((2 * np.log(total_selection_count + 1)) / (counts + 1e-5))\n    \n    # Scale UCB values based on time left for exploration\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_ucb = ucb_values * exploration_scaling\n    \n    # Select the action with the highest adjusted UCB value\n    action_index = actions[np.argmax(adjusted_ucb)]\n    \n    return action_index",
          "objective": -449.9999999999992,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Avoid division by zero with small epsilon\n    epsilon = 1e-5\n    action_selection_probabilities = np.zeros(num_actions)\n\n    for i in range(num_actions):\n        if action_counts[i] > 0:\n            exploration = np.sqrt((np.log(total_selection_count + 1) + 1) / (action_counts[i] + epsilon))\n            action_selection_probabilities[i] = expected_values[i] + exploration\n        else:\n            # If an action hasn't been selected, give it a high exploration priority\n            action_selection_probabilities[i] = np.inf\n\n    # Normalize time factor for exploration such that exploration decreases over time.\n    time_factor = (total_time_slots - current_time_slot) / total_time_slots\n    action_selection_probabilities *= time_factor\n\n    # Select the action with the maximum value\n    action_index = np.argmax(action_selection_probabilities)\n    \n    return action_index",
          "objective": -449.999999999999,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    selections = np.array([len(score_set[action]) for action in actions])\n\n    # Handle cases where selections are zero to avoid division by zero\n    adjusted_selection_counts = selections + 1  # Adding 1 to avoid division by zero\n    exploration_factor = np.sqrt(np.log(total_selection_count + 1) / adjusted_selection_counts)\n\n    # Combine exploration and exploitation using a weighted approach\n    weighted_ucb_values = scores + exploration_factor\n\n    # Implementing an \u03b5-greedy strategy to incorporate exploration with a small probability\n    epsilon = 0.1  # Probability of exploration\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(actions)\n    else:\n        action_index = np.argmax(weighted_ucb_values)\n\n    return action_index",
          "objective": -449.9999999999989,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Prevent division by zero and calculate exploration bonuses\n    counts = np.maximum(counts, 1)  # Ensure counts are at least 1 for log and division calculations\n    exploration_bonus = np.sqrt(np.log(total_selection_count) / counts)\n\n    # Adjust exploration based on the current time slot\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = scores + exploration_bonus * exploration_scaling\n    \n    # Select the action with the highest adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -449.9999999999988,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores for actions\n    avg_scores = np.array([np.mean(scores) if scores else 0 for scores in score_set.values()])\n    counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Handle divide by zero in UCB calculation\n    counts = np.where(counts == 0, 1, counts)  # Avoid zero counts\n\n    # Calculate exploration weight using UCB\n    exploration_weights = np.sqrt(np.log(total_selection_count) / counts)\n    \n    # Compute upper confidence bound scores\n    ucb_scores = avg_scores + exploration_weights\n    \n    # Epsilon-greedy exploration strategy\n    epsilon = 1 / (current_time_slot + 1)  # Decrease exploration over time\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)  # Explore\n    else:\n        action_index = np.argmax(ucb_scores)  # Exploit\n\n    return action_index",
          "objective": -449.9999999999982,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    num_actions = len(actions)\n    \n    # Calculate mean scores and counts for each action, handling empty lists\n    mean_scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Calculate the exploration bonus using Upper Confidence Bound method\n    bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / (counts + 1e-5))\n    \n    # Adjust scores by adding exploration bonus\n    adjusted_scores = mean_scores + bonus\n    \n    # Calculate dynamic epsilon for exploration\n    epsilon = max(0.1, 1.0 - (current_time_slot / total_time_slots))  # Ensure some exploration remains\n    \n    # Stochastic decision based on epsilon\n    if np.random.rand() < epsilon:  # Explore\n        untried_actions = np.where(counts < 1)[0]\n        if untried_actions.size > 0:\n            action_index = np.random.choice(untried_actions)\n        else:\n            action_index = np.random.choice(num_actions)  # Random choice among all actions\n    else:  # Exploit\n        action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -449.9999999999978,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    num_actions = len(actions)\n    \n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Avoid division by zero by adding a small constant\n    counts_with_bonus = counts + 1e-5\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / counts_with_bonus)\n    \n    # Dynamic scaling of exploration based on time slot\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    exploration_adjusted_scores = scores + exploration_bonus * exploration_scaling\n    \n    # Select action with the highest score or explore new actions if some haven't been tried\n    if total_selection_count < num_actions:\n        action_index = np.argmax(counts < 1)  # Choose the first untried action if any\n    else:\n        action_index = actions[np.argmax(exploration_adjusted_scores)]\n    \n    return action_index",
          "objective": -449.9999999999975,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    n_actions = len(actions)\n    \n    # Calculate mean scores and counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Avoid division by zero\n    counts_with_epsilon = counts + 1e-5\n\n    # UCB implementation with dynamic exploration factor\n    exploration_factor = 1.5\n    ucb_values = scores + exploration_factor * np.sqrt(np.log(total_selection_count + 1) / counts_with_epsilon)\n    \n    # Adjust exploration based on current time slot\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = ucb_values * (1 + exploration_scaling)\n\n    # Handle epsilon-greedy exploration\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(actions)\n    else:\n        action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -449.999999999997,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    \n    # Calculate average scores\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Apply epsilon-greedy policy parameters\n    epsilon = 1 / (current_time_slot + 1)  # Decrease exploration over time\n    random_value = np.random.rand()\n    \n    if random_value < epsilon:\n        # Exploration: select a random action\n        action_index = np.random.choice(actions)\n    else:\n        # Exploitation: UCB-based selection\n        counts = np.maximum(counts, 1)  # Prevent division by zero\n        exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / counts)\n        adjusted_scores = scores + exploration_bonus\n        \n        # Select the action with the highest adjusted score\n        action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -449.99999999999693,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores with robustness against empty score lists\n    avg_scores = np.array([np.mean(scores) if len(scores) > 0 else 0 for scores in score_set.values()])\n    counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Adjust counts to handle zero selections, ensuring they are not zero for normalization\n    adjusted_counts = np.where(counts == 0, 1, counts)\n    normalized_scores = avg_scores * adjusted_counts / total_selection_count\n    \n    # Implement a modified \u03b5-greedy strategy for exploration vs exploitation\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))  # Decay exploration rate\n    exploration_terms = np.random.rand(num_actions) < epsilon\n    \n    # Incorporate exploration into the score calculation\n    final_scores = normalized_scores + exploration_terms * (np.max(normalized_scores) - normalized_scores)\n    \n    # Select the action with the highest score\n    action_index = np.argmax(final_scores)\n    \n    return action_index",
          "objective": -449.99999999999693,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Avoid division by zero in case of no selections\n    counts_with_epsilon = counts + 1e-5\n\n    # UCB implementation for action selection\n    exploration_factor = 1.5\n    ucb_values = scores + exploration_factor * np.sqrt(np.log(total_selection_count + 1) / counts_with_epsilon)\n\n    # Adjust exploration based on the current time slot\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = ucb_values * (1 + exploration_scaling)\n\n    # Select the action with the highest adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -449.9999999999946,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    n_actions = len(actions)\n    \n    # Calculate scores and ensure handling of empty lists\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0.0 for action in actions])\n    selections = np.array([len(score_set[action]) for action in actions])\n\n    # Avoid division by zero by adjusting selection counts\n    adjusted_selections = selections + 1  # Adding 1 to avoid divide by zero\n\n    # UCB calculation\n    exploration_bonus = np.sqrt(2 * np.log(total_selection_count + 1) / adjusted_selections)\n    ucb_values = scores + exploration_bonus\n\n    # Dynamic exploration-exploitation balance based on the current time slot\n    exploration_rate = max(0.1, 0.5 * (1 - (current_time_slot / total_time_slots)))\n    if np.random.rand() < exploration_rate:\n        action_index = np.random.choice(actions)\n    else:\n        action_index = actions[np.argmax(ucb_values)]\n\n    return action_index",
          "objective": -449.9999999999943,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores and selection counts\n    avg_scores = np.array([np.mean(scores) if scores else 0 for scores in score_set.values()])\n    counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Adjust counts to avoid division by zero and to factor in exploration\n    adjusted_counts = counts + 1e-5  # Small constant to avoid division by zero\n    total_adjusted_counts = total_selection_count + num_actions  # Adding num_actions for exploration\n    \n    # Calculate weighted average scores\n    weights = total_adjusted_counts / adjusted_counts\n    weighted_scores = avg_scores * weights\n    \n    # Exploration factor using Upper Confidence Bound (UCB)\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (adjusted_counts + 1e-5))\n    \n    # Final selection scores combining exploitation and exploration\n    final_scores = weighted_scores + exploration_bonus\n    \n    # Select action with the highest score\n    action_index = np.argmax(final_scores)\n    \n    return action_index",
          "objective": -449.9999999999926,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n\n    # Calculate average scores and counts\n    avg_scores = np.array([np.mean(scores) if len(scores) > 0 else 0 for scores in score_set.values()])\n    counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Calculate exploration bonus using Upper Confidence Bound (UCB)\n    confidence_bounds = np.sqrt((2 * np.log(total_selection_count + 1)) / (counts + 1e-5))\n    \n    # Combine exploitation and exploration\n    ucb_scores = avg_scores + confidence_bounds\n    \n    # Select action with the highest UCB score\n    action_index = np.argmax(ucb_scores)\n    \n    return action_index",
          "objective": -449.9999999999917,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores for actions\n    avg_scores = np.array([np.mean(scores) if scores else 0 for scores in score_set.values()])\n    counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Adjust counts to avoid divide by zero in UCB\n    adjusted_counts = counts + 1  # Add 1 to avoid zero count\n\n    # Calculate exploration weight using UCB\n    exploration_weights = np.sqrt((2 * np.log(total_selection_count + 1)) / adjusted_counts)\n    \n    # Compute upper confidence bound scores\n    ucb_scores = avg_scores + exploration_weights\n    \n    # Epsilon-greedy exploration strategy\n    epsilon = max(0.1, 1 / (current_time_slot + 1))  # Set a lower bound for epsilon\n    if np.random.rand() < epsilon:  # Explore \n        action_index = np.random.randint(num_actions)\n    else:  # Exploit\n        action_index = np.argmax(ucb_scores)\n\n    return action_index",
          "objective": -449.99999999999164,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8  # Fixed number of actions: 0 to 7\n    avg_scores = np.zeros(num_actions)\n    counts = np.zeros(num_actions)\n\n    # Calculate average scores and counts \n    for action_index, scores in score_set.items():\n        counts[action_index] = len(scores)\n        avg_scores[action_index] = np.mean(scores) if counts[action_index] > 0 else 0\n\n    # To avoid division by zero and encourage exploration consider a small epsilon\n    epsilon = 1e-5\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / (counts + epsilon))\n    \n    # Balance between exploration and exploitation with a scaling factor for exploration\n    exploration_scale = 1.5\n    ucb_scores = avg_scores + exploration_scale * exploration_bonus\n    \n    # Implement \u03b5-greedy strategy: with a small probability explore randomly\n    exploration_prob = 0.1\n    if np.random.rand() < exploration_prob:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(ucb_scores)\n    \n    return action_index",
          "objective": -449.99999999999136,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    \n    # Calculate averages and counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Handle division by zero by adding a small constant\n    total_counts = counts + 1e-5\n    \n    # Calculate the exploration bonus using the Upper Confidence Bound method\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / total_counts)\n    \n    # Adjust scores based on exploration factor\n    adjusted_scores = scores + exploration_bonus\n    \n    # Scale exploration based on the time left\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores *= exploration_scaling\n    \n    # Select the action with the highest adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -449.99999999999017,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Calculate average scores, avoiding division by zero by adding a small constant\n    counts = np.maximum(counts, 1)\n    \n    # Explore-Exploit Balancing using Upper Confidence Bound (UCB)\n    ucb_values = scores + np.sqrt((2 * np.log(total_selection_count + 1)) / counts)\n    \n    # Adjust for diminishing exploration opportunity over time\n    time_factor = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = ucb_values * time_factor\n    \n    # Select the action with the highest adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -449.9999999999901,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    n_actions = len(actions)\n    \n    # Calculate mean scores and selection counts\n    mean_scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    selection_counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Handle zero selections by adding a small constant to selection counts\n    adjusted_selection_counts = selection_counts + 1e-5  # Small constant to avoid zero\n    \n    # Compute exploration bonuses using a variant of UCB\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / adjusted_selection_counts)\n    \n    # Combine mean scores with exploration terms\n    ucb_values = mean_scores + exploration_bonus\n\n    # Implementing a softmax selection strategy for more balanced exploration\n    exp_ucb = np.exp(ucb_values - np.max(ucb_values))  # For numerical stability\n    probabilities = exp_ucb / np.sum(exp_ucb)\n\n    # Sample an action based on the computed probabilities\n    action_index = np.random.choice(actions, p=probabilities)\n\n    return action_index",
          "objective": -449.9999999999874,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores and selection counts\n    avg_scores = np.array([\n        np.mean(scores) if len(scores) > 0 else 0 for scores in score_set.values()\n    ])\n    counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Handle division by zero for the exploration term\n    counts[counts == 0] = 1\n    \n    # Calculate exploration parameter using Upper Confidence Bound (UCB)\n    ucb_values = np.sqrt((2 * np.log(total_selection_count + 1)) / counts)\n    \n    # Final scores balance exploitation and exploration\n    final_scores = avg_scores + ucb_values\n    \n    # Select action with the highest final score\n    action_index = np.argmax(final_scores)\n    \n    return action_index",
          "objective": -449.99999999997203,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    num_actions = len(actions)\n    \n    # Calculate average scores and selection counts\n    average_scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    selection_counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Handle cases where selections are zero to avoid division by zero\n    adjusted_selection_counts = selection_counts + 1\n    exploration_factor = np.sqrt(np.log(total_selection_count + 1) / adjusted_selection_counts)\n\n    # Combine exploration and exploitation using UCB\n    ucb_values = average_scores + exploration_factor\n    \n    # Leverage \u03b5-greedy approach to balance exploration and exploitation\n    epsilon = max(0.1, (total_time_slots - current_time_slot) / total_time_slots)  # Decaying epsilon\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(actions)\n    else:\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -449.99999999994856,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Use epsilon-greedy strategy\n    epsilon = 0.1\n    if np.random.rand() < epsilon:\n        # Explore: Select a random action\n        action_index = np.random.choice(actions)\n    else:\n        # Exploit: Calculate UCB scores\n        exploration_bonus = np.sqrt(2 * np.log(total_selection_count + 1) / (counts + 1e-5))\n        ucb_scores = scores + exploration_bonus\n        \n        # Encourage exploration based on time left\n        exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_ucb_scores = ucb_scores + exploration_scaling\n        \n        action_index = actions[np.argmax(adjusted_ucb_scores)]\n        \n    return action_index",
          "objective": -449.9999999999403,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0.0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Avoid division by zero by adding a small constant\n    counts_with_epsilon = counts + 1e-5\n    \n    # UCB implementation for action selection\n    exploration_factor = 1.5\n    ucb_values = scores + exploration_factor * np.sqrt(np.log(total_selection_count + 1) / counts_with_epsilon)\n    \n    # Adjust exploration based on the current time slot\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = ucb_values * (1 + exploration_scaling)\n\n    # Implementing \u03b5-greedy strategy\n    epsilon = 0.1  # Exploration probability\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(actions)\n    else:\n        action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -449.9999999999297,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    selections = np.array([len(score_set[action]) for action in actions])\n    \n    # Avoid division by zero by adding a small constant\n    adjusted_selection_counts = selections + 1  \n    exploration_factor = np.sqrt((total_selection_count + 1) * np.log(current_time_slot + 1) / adjusted_selection_counts)\n\n    # Combine exploration with a weighted UCB approach\n    weighted_ucb_values = scores + exploration_factor\n\n    # Implement an \u03b5-greedy strategy\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01)  # Decaying epsilon\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(actions)\n    else:\n        action_index = np.argmax(weighted_ucb_values)\n\n    return action_index",
          "objective": -449.9999999998926,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate mean scores for each action\n    expected_values = np.zeros(8)\n    action_counts = np.zeros(8)\n    \n    for action_index, scores in score_set.items():\n        if len(scores) > 0:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n    \n    # Calculate the exploration factor (UCB style)\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / (action_counts + 1e-5))\n    \n    # Calculate combined value for each action\n    combined_values = expected_values + exploration_bonus\n\n    # Select action with the maximum combined value\n    action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": -449.9999999998448,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    selections = np.array([len(score_set[action]) for action in actions])\n    \n    # Upper Confidence Bound (UCB) strategy for exploration and exploitation\n    exploration_factor = np.sqrt(2 * np.log(total_selection_count + 1) / (selections + 1e-5))\n    ucb_values = scores + exploration_factor\n    \n    action_index = np.argmax(ucb_values)\n    \n    return action_index",
          "objective": -449.9999999998125,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_weight = 1.5  # Exploration weight for UCB\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        average_score = np.mean(scores) if scores else 0\n        \n        # Upper Confidence Bound calculation\n        if selection_count > 0:\n            ucb = average_score + exploration_weight * np.sqrt(np.log(total_selection_count) / selection_count)\n        else:\n            ucb = float('inf')  # For actions never selected\n        \n        action_scores.append((ucb, action_index))\n    \n    # Sorting by UCB values \n    action_scores.sort(reverse=True, key=lambda x: x[0])\n    \n    # Decaying exploration rate\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.05)  # Minimum exploration chance\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice([i for _, i in action_scores])\n    else:\n        action_index = action_scores[0][1]\n    \n    return action_index",
          "objective": -449.9999999996468,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    n_actions = len(actions)\n    \n    # Avoid division by zero and calculate scores and selection counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    selections = np.array([len(score_set[action]) for action in actions])\n    \n    # Handle selection counts that may be zero\n    selections_with_zero = selections + 1  # Avoid division by zero for the UCB calculation\n    ucb_values = scores + np.sqrt(2 * np.log(total_selection_count + 1) / selections_with_zero)\n\n    # Adaptive exploration vs exploitation based on time progression\n    exploration_parameter = max(0.1, 1 - (current_time_slot / total_time_slots))  # Decrease exploration over time\n    if np.random.rand() < exploration_parameter:\n        # Explore: select a random action\n        action_index = np.random.choice(actions)\n    else:\n        # Exploit: select the action with the highest UCB value\n        action_index = actions[np.argmax(ucb_values)]\n\n    return action_index",
          "objective": -449.9999999995209,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    selections = np.array([len(score_set[action]) for action in actions])\n    \n    # Epsilon-greedy strategy to balance exploration and exploitation\n    epsilon = 0.1\n    if np.random.rand() < epsilon:\n        # Explore: select a random action\n        action_index = np.random.choice(actions)\n    else:\n        # Exploit: select the action with the highest average score\n        confidence_bounds = scores + np.sqrt(2 * np.log(total_selection_count + 1) / (selections + 1))\n        action_index = actions[np.argmax(confidence_bounds)]\n    \n    return action_index",
          "objective": -449.99999999894646,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    selections = np.array([len(score_set[action]) for action in actions])\n    \n    # Avoid division by zero and handle unexplored actions gracefully\n    with np.errstate(divide='ignore', invalid='ignore'):\n        exploration_factor = np.sqrt(2 * np.log(total_selection_count + 1) / (selections + 1e-5))\n    \n    # Introduce a decay factor for older time slots to consider more recent performance\n    decay_factor = 1 - (current_time_slot / total_time_slots)\n    adjusted_scores = scores * decay_factor\n\n    # UCB calculation\n    ucb_values = adjusted_scores + exploration_factor\n    action_index = np.argmax(ucb_values)\n    \n    return action_index",
          "objective": -449.9999999984965,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize average scores and counts for exploration\n    averages = np.zeros(8)\n    counts = np.zeros(8)\n    \n    # Calculate averages and counts for each action\n    for action_index, scores in score_set.items():\n        counts[action_index] = len(scores)\n        if counts[action_index] > 0:\n            averages[action_index] = np.mean(scores)\n    \n    # Upper Confidence Bound (UCB) calculation\n    exploration_weight = np.sqrt(np.log(total_selection_count + 1) / (counts + 1e-10))\n    \n    # Calculate final scores combining exploitation (averages) and exploration (UCB)\n    ucb_scores = averages + exploration_weight\n    final_scores = np.where(counts > 0, ucb_scores, float('inf'))  # Handle unselected actions\n    \n    # Select action with the highest score\n    action_index = np.argmax(final_scores)\n    \n    return action_index",
          "objective": -449.99999999835785,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    n_actions = len(actions)\n    \n    # Calculate scores, handling potential empty lists\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0.0 for action in actions])\n    selections = np.array([len(score_set[action]) for action in actions])\n\n    # Adjust selection counts to avoid division by zero\n    adjusted_selections = selections + 1  # Adding 1 to avoid divide by zero\n\n    # Calculate UCB values\n    exploration_bonus = np.sqrt(2 * np.log(total_selection_count + 1) / adjusted_selections)\n    ucb_values = scores + exploration_bonus\n\n    # Adaptive \u03b5-greedy strategy\n    exploration_rate = max(0.1, 0.5 * (1 - (current_time_slot / total_time_slots)))\n    if np.random.rand() < exploration_rate:\n        action_index = np.random.choice(actions)\n    else:\n        action_index = actions[np.argmax(ucb_values)]\n    \n    return action_index",
          "objective": -449.99999999817163,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Avoid division by zero for actions that haven't been selected\n    counts_with_bonus = counts + 1e-5\n\n    # Calculate the Upper Confidence Bound\n    ucb = scores + np.sqrt((2 * np.log(total_selection_count + 1)) / counts_with_bonus)\n\n    # Scale exploration by time remaining\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = ucb + exploration_scaling\n\n    # Select the action with the highest adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -449.99999999332476,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = 1  # Adjust exploration factor dynamically\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score, handling case for no scores\n        average_score = np.mean(scores) if scores else 0\n        \n        # Upper Confidence Bound calculation\n        if selection_count > 0:\n            ucb = average_score + exploration_factor * np.sqrt(np.log(total_selection_count) / selection_count)\n        else:\n            ucb = float('inf')  # For actions never selected\n        \n        action_scores.append((ucb, action_index))\n    \n    # Sorting by UCB values \n    action_scores.sort(reverse=True, key=lambda x: x[0])\n    \n    # Epsilon-greedy decision making with decay based on time\n    epsilon = 0.1 * (1 - (current_time_slot / total_time_slots))  # Decaying exploration chance\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice([i for _, i in action_scores])\n    else:\n        action_index = action_scores[0][1]\n    \n    return action_index",
          "objective": -449.9999999474396,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = 0.1  # Epsilon for exploration; adjustable\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        if scores:\n            average_score = np.mean(scores)\n        else:\n            average_score = 0  # Default score for unselected actions\n        \n        # Calculate Upper Confidence Bound\n        selection_count = len(scores)\n        if selection_count > 0:\n            ucb = average_score + exploration_factor * np.sqrt(np.log(total_selection_count) / selection_count)\n        else:\n            ucb = float('inf')  # Select actions that have never been selected\n        \n        action_scores.append((ucb, action_index))\n    \n    # Select action based on UCB scores\n    action_scores.sort(reverse=True, key=lambda x: x[0])\n    \n    # For the current time slot, we introduce some random selection based on time\n    if np.random.rand() < (current_time_slot / total_time_slots):\n        action_index = np.random.choice([i for _, i in action_scores])\n    else:\n        action_index = action_scores[0][1]\n    \n    return action_index",
          "objective": -449.99999991479115,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize average scores and counts for exploration\n    averages = np.zeros(8)\n    counts = np.zeros(8)\n    \n    # Calculate averages and counts for each action\n    for action_index, scores in score_set.items():\n        counts[action_index] = len(scores)\n        if counts[action_index] > 0:\n            averages[action_index] = np.mean(scores)\n    \n    # Calculate exploration factor using a dynamic epsilon value\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n    \n    # Random value for exploration threshold\n    exploration_threshold = np.random.rand()\n    \n    if exploration_threshold < epsilon:\n        # Explore: Select a random action\n        action_index = np.random.choice(np.arange(8))\n    else:\n        # Exploit: Use weighted average scores\n        exploitation_scores = averages + (np.sqrt(np.log(total_selection_count + 1) / (counts + 1e-10)) * 0.5)\n        action_index = np.argmax(exploitation_scores)\n    \n    return action_index",
          "objective": -449.99999983916155,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores for each action\n    avg_scores = np.array([\n        np.mean(scores) if len(scores) > 0 else 0 for scores in score_set.values()\n    ])\n    \n    # Define exploration parameter\n    exploration_factor = np.sqrt((np.log(total_selection_count + 1) / (np.array([len(scores) for scores in score_set.values()]) + 1e-5)))\n    \n    # Calculate epsilon for exploration\n    epsilon = 1.0 / (current_time_slot + 1)\n    \n    # Combine exploitation (avg_scores) with exploration (exploration_factor)\n    final_scores = avg_scores + (epsilon * exploration_factor)\n    \n    # Select action with the highest score\n    action_index = np.argmax(final_scores)\n    \n    return action_index",
          "objective": -449.99999935372983,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize parameters\n    num_actions = 8\n    averages = np.zeros(num_actions)\n    counts = np.zeros(num_actions)\n    \n    # Calculate averages and counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        counts[action_index] = len(scores)\n        if counts[action_index] > 0:\n            averages[action_index] = np.mean(scores)\n    \n    # Epsilon-Greedy Parameters\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))  # Decay exploration over time\n    \n    # Select action based on Epsilon-Greedy strategy\n    if np.random.rand() < epsilon:\n        # Explore: select randomly among all actions\n        action_index = np.random.choice(num_actions)\n    else:\n        # Exploit: select action with the highest average score\n        action_index = np.argmax(averages)\n    \n    return action_index",
          "objective": -449.99999913648657,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    \n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Epsilon for exploration\n    epsilon = 0.1\n    exploration_prob = np.random.rand()\n    \n    if exploration_prob < epsilon:\n        # Explore: randomly select an action\n        action_index = np.random.choice(actions)\n    else:\n        # Exploit: calculate Upper Confidence Bound (UCB)\n        ucb = scores + np.sqrt((2 * np.log(total_selection_count + 1)) / (counts + 1e-5))\n        \n        # Encourage additional exploration based on current time slot\n        exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n        ucb += exploration_scaling * 0.1\n        \n        action_index = actions[np.argmax(ucb)]\n    \n    return action_index",
          "objective": -449.99999781096665,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores for each action\n    avg_scores = np.array([\n        np.mean(scores) if len(scores) > 0 else 0\n        for scores in score_set.values()\n    ])\n    \n    # Epsilon for exploration vs exploitation\n    epsilon = 1.0 / (current_time_slot + 1)\n    \n    # Count how many times each action has been selected\n    action_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # UCB calculation: bonus based on unexplored actions\n    confidence_bounds = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n    \n    # Final scores combining exploitation and exploration\n    final_scores = avg_scores + (epsilon * confidence_bounds)\n    \n    # Select action with the highest final score\n    action_index = np.argmax(final_scores)\n    \n    return action_index",
          "objective": -449.9999962783034,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n    \n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        if action_counts[action_index] > 0:\n            expected_values[action_index] = np.mean(scores)\n        else:\n            expected_values[action_index] = 0  # Assign zero if no scores are available\n    \n    # Adjust exploration via epsilon decay based on the time slot\n    epsilon = max(0.1, ((total_time_slots - current_time_slot) / total_time_slots))\n    \n    # Generate a random number to decide between exploration and exploitation\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)  # Exploration\n    else:\n        # Exploitation: Select action with maximal expected value\n        action_index = np.argmax(expected_values)\n    \n    return action_index",
          "objective": -449.99998838867805,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    averages = np.zeros(num_actions)\n    counts = np.zeros(num_actions)\n    \n    # Calculate averages and counts for each action\n    for action_index, scores in score_set.items():\n        counts[action_index] = len(scores)\n        if counts[action_index] > 0:\n            averages[action_index] = np.mean(scores)\n    \n    # Dynamic epsilon for exploration\n    epsilon = max(0.1, 1 - (total_selection_count / (total_time_slots * 0.5)))\n    \n    if np.random.rand() < epsilon:\n        # Explore: randomly select an action\n        action_index = np.random.choice(num_actions)\n    else:\n        # Exploit: calculate UCB scores\n        exploration_weight = np.sqrt(np.log(total_selection_count + 1) / (counts + 1e-10))\n        final_scores = averages + exploration_weight\n        \n        # Handle unselected actions\n        final_scores[counts == 0] = float('inf')\n        \n        # Select action with the highest score\n        action_index = np.argmax(final_scores)\n    \n    return action_index",
          "objective": -449.99997771059594,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = 0.5  # Adjust exploration factor to balance exploration and exploitation\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        average_score = np.mean(scores) if scores else 0\n        selection_count = len(scores)\n\n        # Calculate UCB score\n        if selection_count > 0:\n            ucb_score = average_score + exploration_factor * np.sqrt(np.log(total_selection_count) / selection_count)\n        else:\n            ucb_score = float('inf')  # Explore untried actions\n            \n        action_scores.append((ucb_score, action_index))\n\n    # Time-based exploration adjustment\n    exploration_probability = current_time_slot / total_time_slots\n    if np.random.rand() < exploration_probability:\n        # Select action randomly weighted by selection counts (less frequent actions favored)\n        weights = np.array([1/(1 + len(score_set.get(i, []))) for i in range(8)])\n        weights /= weights.sum()  # Normalize weights\n        action_index = np.random.choice(range(8), p=weights)\n    else:\n        # Greedily select the highest UCB score\n        action_index = max(action_scores, key=lambda x: x[0])[1]\n    \n    return action_index",
          "objective": -449.9999697861217,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize action indices and their corresponding scores and selections\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    selections = np.array([len(score_set[action]) for action in actions])\n    \n    # Adjust the exploration rate based on time\n    exploration_rate = max(1 - (current_time_slot / total_time_slots), 0.1)\n    \n    # Epsilon-greedy strategy for exploration and exploitation\n    epsilon = exploration_rate\n    if np.random.rand() < epsilon:\n        # Explore: sample uniformly from the actions\n        action_index = np.random.choice(actions)\n    else:\n        # Exploit: using the Upper Confidence Bound (UCB) method\n        confidence_bounds = scores + np.sqrt(2 * np.log(total_selection_count + 1) / (selections + 1e-5))\n        action_index = actions[np.argmax(confidence_bounds)]\n    \n    return action_index",
          "objective": -449.9999593065416,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    selections = np.array([len(score_set[action]) for action in actions])\n    \n    # Normalize the scores to handle actions that have never been selected\n    normalized_scores = np.nan_to_num(scores, nan=0)\n\n    # Dynamic epsilon based on exploration need\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n    epsilon = 0.1 * exploration_factor\n\n    if np.random.rand() < epsilon:\n        # Explore: select an action that has been selected less frequently\n        action_index = np.argmin(selections)\n    else:\n        # Exploit: calculate confidence bounds for exploitation\n        confidence_bounds = normalized_scores + np.sqrt(2 * np.log(total_selection_count + 1) / (selections + 1))\n        action_index = actions[np.argmax(confidence_bounds)]\n    \n    return action_index",
          "objective": -449.99995058695373,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    \n    # Calculate average scores and selection counts\n    average_scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    selection_counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Dynamic exploration rate based on time\n    exploration_rate = max(1 - (current_time_slot / total_time_slots), 0.1)\n    \n    # Epsilon-greedy exploration/exploitation choice\n    if np.random.rand() < exploration_rate:\n        action_index = np.random.choice(actions)\n    else:\n        # Upper Confidence Bound (UCB)\n        ucb_values = average_scores + np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n        action_index = actions[np.argmax(ucb_values)]\n    \n    return action_index",
          "objective": -449.99988378448364,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0.0 for action in actions])\n    selections = np.array([len(score_set[action]) for action in actions])\n    \n    # Avoid division by zero; replace zero selections with a small value\n    adjusted_selections = selections + 1e-6\n    \n    # Calculate exploration factor\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n    epsilon = 0.1 * exploration_factor\n    \n    # Upper Confidence Bound (UCB) for exploitation\n    confidence_bounds = scores + np.sqrt(2 * np.log(total_selection_count + 1) / adjusted_selections)\n    \n    if np.random.rand() < epsilon:\n        # Explore: choose action with the least selections\n        action_index = np.argmin(selections)\n    else:\n        # Exploit: choose the action with the highest confidence bound\n        action_index = actions[np.argmax(confidence_bounds)]\n    \n    return action_index",
          "objective": -449.999668128017,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores for each action\n    avg_scores = np.array([\n        np.mean(scores) if len(scores) > 0 else 0\n        for scores in score_set.values()\n    ])\n    \n    # Count how many times each action has been selected\n    action_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Epsilon decay to balance exploration vs exploitation\n    epsilon = max(0.1, 1.0 - (current_time_slot / total_time_slots))\n    \n    # Select a random action with probability epsilon\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(num_actions)\n    else:\n        # UCB calculation to incorporate exploration\n        confidence_bounds = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n        final_scores = avg_scores + confidence_bounds\n        \n        # Select action with the highest final score\n        action_index = np.argmax(final_scores)\n    \n    return action_index",
          "objective": -449.9995878163316,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores for each action and ensure robustness against division by zero\n    averages = np.array([np.mean(scores) if scores else 0 for scores in score_set.values()])\n    \n    # Use a small constant to avoid division by zero when calculating selection counts\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Calculate exploration bonus based on UCB approach\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-10))\n    \n    # Combine exploitation (average scores) and exploration (exploration bonus)\n    final_scores = averages + exploration_bonus\n\n    # Select the action with the highest score\n    action_index = np.argmax(final_scores)\n    \n    return action_index",
          "objective": -449.99930527823835,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = 0.1  # Exploration rate; adjustable\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        average_score = np.mean(scores) if scores else 0\n        selection_count = len(scores)\n        \n        if selection_count > 0:\n            # Adding exploration bonus\n            exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count) / selection_count)\n            total_score = average_score + exploration_bonus\n        else:\n            total_score = float('inf')  # Explore untried actions\n            \n        action_scores.append((total_score, action_index))\n    \n    # Time-based adjustment for additional exploration\n    exploration_probability = current_time_slot / total_time_slots\n    if np.random.rand() < exploration_probability:\n        # Random selection among less frequently chosen actions\n        action_index = np.random.choice([i for _, i in action_scores])\n    else:\n        # Select the action with the highest score\n        action_index = max(action_scores, key=lambda x: x[0])[1]\n    \n    return action_index",
          "objective": -449.99446356398585,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores for each action\n    averages = {}\n    for action_index, scores in score_set.items():\n        if len(scores) > 0:\n            averages[action_index] = np.mean(scores)\n        else:\n            averages[action_index] = 0  # Default to 0 if no scores\n\n    # Convert averages to a numpy array for easier manipulation\n    avg_scores = np.array([averages[i] for i in range(8)])\n    \n    # Exploration factor (can be adjusted)\n    exploration_factor = 1.0  # Explore less selected actions, can tune this\n    \n    # Calculate exploratory scores\n    exploration_scores = np.array([exploration_factor / (np.sqrt(len(score_set[i])) + 1) if len(score_set[i]) > 0 else 1 for i in range(8)])\n    \n    # Calculate final scores combining exploitation and exploration\n    final_scores = avg_scores + exploration_scores\n    \n    # Select action with the highest score\n    action_index = np.argmax(final_scores)\n    \n    return action_index",
          "objective": -449.99405414566377,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = 1.0  # Increased exploration factor for better exploration\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        average_score = np.mean(scores) if scores else 0\n        selection_count = len(scores)\n\n        # Calculate UCB score with added safety for zero selection count\n        ucb_score = average_score + exploration_factor * np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1))\n\n        action_scores.append((ucb_score, action_index))\n\n    # Time-based exploration adjustment\n    exploration_probability = current_time_slot / total_time_slots\n    if np.random.rand() < exploration_probability:\n        # Select action randomly weighted by selection counts (less frequent actions favored)\n        counts = np.array([len(score_set.get(i, [])) for i in range(8)])\n        weights = 1 / (1 + counts)  # Less selected actions get more weight\n        weights /= weights.sum()  # Normalize weights\n        action_index = np.random.choice(range(8), p=weights)\n    else:\n        # Greedily select the action with the highest UCB score\n        action_index = max(action_scores, key=lambda x: x[0])[1]\n    \n    return action_index",
          "objective": -449.9783096495225,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize exploration-exploitation parameters\n    epsilon = max(0.05, 1 - (current_time_slot / total_time_slots))\n    explore_threshold = np.random.rand()\n    \n    # Extract actions and compute average scores and selection counts\n    actions = list(score_set.keys())\n    num_actions = len(actions)\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    for action_index in actions:\n        scores = score_set[action_index]\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Default to zero if a selection count is zero to avoid div/zero issues\n    selection_counts = np.where(selection_counts == 0, 1e-10, selection_counts)\n\n    if explore_threshold < epsilon:\n        # Exploration\n        action_index = np.random.choice(actions)\n    else:\n        # Exploitation using UCB\n        ucb_values = average_scores + np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts)\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -449.9775722692859,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Dynamic epsilon value for exploration based on the current time slot\n    epsilon = max(0.05, 1 - (current_time_slot / total_time_slots))\n    explore_threshold = np.random.rand()\n    \n    # Initialize actions, average scores, and selection counts\n    actions = list(score_set.keys())\n    num_actions = len(actions)\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in actions:\n        scores = score_set[action_index]\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # To prevent division by zero, replace zero selection counts\n    selection_counts = np.where(selection_counts == 0, 1e-10, selection_counts)\n\n    # Epsilon-Greedy Strategy for Exploration / Exploitation\n    if explore_threshold < epsilon:\n        action_index = np.random.choice(actions)\n    else:\n        # Use Upper Confidence Bound (UCB) for exploitation\n        ucb_values = average_scores + np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts)\n        action_index = np.argmax(ucb_values)\n    \n    return action_index",
          "objective": -449.8475235755666,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    selections = np.array([len(score_set[action]) for action in actions])\n    \n    # Avoid division by zero by adding a small constant\n    selections_with_noise = selections + 1e-6\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n    epsilon = 0.1 * exploration_factor\n\n    # Explore: Select an action based on counts\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(np.flatnonzero(selections == selections.min()))\n    else:\n        # Calculate upper confidence bounds\n        confidence_bounds = scores + np.sqrt(2 * np.log(total_selection_count + 1) / selections_with_noise)\n        action_index = actions[np.argmax(confidence_bounds)]\n    \n    return action_index",
          "objective": -449.79235231539906,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize parameters\n    num_actions = len(score_set)\n    exploration_rate = max(0, 1 - (total_selection_count / (total_time_slots * num_actions)))\n    \n    # Calculate average scores and selection counts for each action\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n    \n    # Implement Upper Confidence Bound (UCB) for exploration-exploitation balance\n    ucb_values = average_scores + np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n    \n    # Select an action based on exploration-exploitation strategy\n    if np.random.rand() < exploration_rate:\n        action_index = np.random.randint(0, num_actions)  # Random selection\n    else:\n        action_index = np.argmax(ucb_values)  # Select action with highest UCB\n\n    return action_index",
          "objective": -448.52543625692755,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = max(0, 1 - (current_time_slot / total_time_slots))  # Epsilon decreases with more time slots\n    explore_threshold = np.random.rand()\n    \n    if explore_threshold < epsilon:\n        # Exploration: select an action randomly\n        action_index = np.random.randint(0, 8)  # Random selection from 0 to 7\n    else:\n        # Exploitation: compute average scores\n        average_scores = []\n        \n        for action_index in range(8):\n            scores = score_set[action_index]\n            selection_count = len(scores)\n            if selection_count == 0:\n                average_scores.append(0)  # If not selected, average score is 0\n            else:\n                average_scores.append(np.mean(scores))  # Calculate mean score\n        \n        # Select action with the highest average score\n        action_index = np.argmax(average_scores)  # Index of the action with maximum average score\n\n    return action_index",
          "objective": -321.1916499205955,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    selections = np.array([len(score_set[action]) for action in actions])\n    \n    # Calculate confidence intervals and avoid division by zero\n    with np.errstate(divide='ignore', invalid='ignore'):\n        confidence_intervals = scores + np.sqrt(2 * np.log(total_selection_count + 1) / (selections + 1e-5))\n    \n    # Define exploration parameter for balancing exploration-exploitation trade-off, more intense as time progresses\n    exploration_factor = 1.0 / (current_time_slot + 1)  # Decreasing exploration over time\n    exploration_values = np.random.random(len(actions)) * exploration_factor\n    \n    total_scores = confidence_intervals + exploration_values\n    \n    # Select the action with the highest total score\n    action_index = actions[np.argmax(total_scores)]\n    \n    return action_index",
          "objective": 114.44801902682696,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores for each action\n    averages = np.zeros(8)\n    for action_index in range(8):\n        if len(score_set[action_index]) > 0:\n            averages[action_index] = np.mean(score_set[action_index])\n    \n    # Calculate selection counts and a bonus for less selected actions\n    selection_counts = np.array([len(score_set[i]) for i in range(8)])\n    selection_bonus = np.sqrt(total_selection_count) / (selection_counts + 1)  # Avoid division by zero\n    \n    # Dynamic exploration tuning based on time slot\n    exploration_rate = np.clip(1 - (current_time_slot / total_time_slots), 0.1, 1)\n    \n    # Combine average scores and exploration bonus\n    final_scores = averages + exploration_rate * selection_bonus\n    \n    # Select action with the highest score\n    action_index = np.argmax(final_scores)\n    \n    return action_index",
          "objective": 378.3197670972211,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate exploration probability based on remaining time slots\n    epsilon = max(0, 1 - (current_time_slot / total_time_slots))\n    \n    # Calculate average scores and selection counts\n    action_count = len(score_set)\n    average_scores = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n    \n    for action_index in range(action_count):\n        scores = score_set[action_index]\n        selection_count = len(scores)\n        selection_counts[action_index] = selection_count\n        \n        if selection_count > 0:\n            average_scores[action_index] = np.mean(scores)  # Compute mean score\n        else:\n            average_scores[action_index] = 0  # Default score if not selected\n\n    # Normalize the average scores by selection counts to encourage exploration of lesser-selected actions\n    if total_selection_count > 0:\n        normalized_scores = average_scores / (selection_counts + 1e-5)\n    else:\n        normalized_scores = average_scores  # No actions selected yet, avoid division by zero\n\n    # Exploration: Select a random action based on epsilon\n    explore_threshold = np.random.rand()\n    \n    if explore_threshold < epsilon:\n        # Encourage exploration of lesser-selected actions\n        selection_probabilities = (1 / (selection_counts + 1e-5))  # Inverse of selection counts\n        selection_probabilities /= np.sum(selection_probabilities)  # Normalize for probability distribution\n        action_index = np.random.choice(range(action_count), p=selection_probabilities)\n    else:\n        # Exploitation: Select action with the highest normalized score\n        action_index = np.argmax(normalized_scores)\n\n    return action_index",
          "objective": 4153.838145558069,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))  # Decaying exploration parameter\n    action_indices = list(score_set.keys())\n    action_scores = []\n\n    for action in action_indices:\n        historical_scores = score_set[action]\n        if len(historical_scores) == 0:  # Handle actions that have never been selected\n            action_scores.append(0)  # Assume a baseline score for unselected actions\n        else:\n            average_score = np.mean(historical_scores)\n            action_scores.append(average_score)\n\n    if np.random.rand() < epsilon:  # Explore\n        action_index = np.random.choice(action_indices)\n    else:  # Exploit\n        action_index = action_indices[np.argmax(action_scores)]\n\n    return action_index",
          "objective": 18955.748092734368,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    \n    epsilon = max(0, 1 - (current_time_slot / total_time_slots))\n    explore_threshold = np.random.rand()\n\n    if explore_threshold < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        average_scores = np.zeros(num_actions)\n        confidence_intervals = np.zeros(num_actions)\n\n        for i, action_index in enumerate(action_indices):\n            scores = score_set[action_index]\n            selection_count = len(scores)\n            if selection_count > 0:\n                average_scores[i] = np.mean(scores)\n                # Calculate confidence interval (using standard error approach)\n                confidence_intervals[i] = np.std(scores) / np.sqrt(selection_count)\n            else:\n                average_scores[i] = 0\n                confidence_intervals[i] = float('inf')  # High uncertainty for unselected actions\n\n        upper_bounds = average_scores + confidence_intervals\n        action_index = action_indices[np.argmax(upper_bounds)]\n\n    return action_index",
          "objective": 24140.777442167662,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration rate\n    action_indices = list(score_set.keys())\n    \n    # Calculate average scores for each action\n    average_scores = [\n        np.mean(scores) if len(scores) > 0 else 0\n        for scores in score_set.values()\n    ]\n    \n    # Determine if we explore or exploit\n    if np.random.rand() < epsilon:\n        # Explore: select a random action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploit: select the action with the highest average score\n        max_score = max(average_scores)\n        best_actions = [i for i, score in enumerate(average_scores) if score == max_score]\n        action_index = np.random.choice(best_actions)\n\n    return action_index",
          "objective": 30886.650873186936,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    scores = []\n    exploration_factor = (current_time_slot + 1) / total_time_slots  # Encourage exploration early on\n\n    for action_index in action_indices:\n        if score_set[action_index]:\n            avg_score = np.mean(score_set[action_index])\n        else:\n            avg_score = 0\n        \n        exploration = 1 / (len(score_set[action_index]) + 1)  # Soft exploration term\n        score_with_exploration = avg_score + exploration_factor * exploration\n        \n        scores.append(score_with_exploration)\n\n    selected_action_index = np.argmax(scores)\n    return action_index",
          "objective": 32005.304569016705,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize parameters for exploration-exploitation balance\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))  # Keep a minimum level of exploration\n    explore_threshold = np.random.rand()\n    \n    # Extract scores and selection counts\n    actions = list(score_set.keys())\n    num_actions = len(actions)\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    for action_index in actions:\n        scores = score_set[action_index]\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n    \n    if explore_threshold < epsilon:\n        # Exploration: select an action randomly\n        action_index = np.random.choice(actions)  # Choose randomly from available actions\n    else:\n        # Exploitation\n        # Calculate Upper Confidence Bound (UCB) for each action\n        ucb_values = average_scores + np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1e-10))\n        action_index = np.argmax(ucb_values)  # Choose action with highest UCB value\n    \n    return action_index",
          "objective": 32115.973340172037,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1\n    action_indices = list(score_set.keys())\n    historical_scores = [np.mean(score_set[action]) if score_set[action] else 0 for action in action_indices]\n    \n    if np.random.rand() < epsilon:\n        # Exploration: Select a random action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploitation: Select the best historical score\n        best_action_index = np.argmax(historical_scores)\n        action_index = action_indices[best_action_index]\n\n    return action_index",
          "objective": 33763.05774486575,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate weighted mean for each action based on historical scores\n    expected_values = {}\n    \n    for action_index, scores in score_set.items():\n        if len(scores) > 0:\n            expected_values[action_index] = np.mean(scores)  # Exploitation\n        else:\n            expected_values[action_index] = 0  # No historical data for this action\n\n    # Add a small exploration factor\n    exploration_factor = np.log(total_selection_count + 1) / (np.array([len(score_set[i]) for i in range(8)]) + 1)\n    \n    # Combine expected values with exploration factor\n    exploration_adjusted_values = {\n        action_index: expected_values[action_index] + exploration_factor[i] \n        for i, action_index in enumerate(expected_values.keys())\n    }\n    \n    # Select action with maximum adjusted value\n    action_index = max(exploration_adjusted_values, key=exploration_adjusted_values.get)\n\n    return action_index",
          "objective": 35098.8030866623,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set[action_index]\n        if len(scores) == 0:\n            # If no scores recorded, assign a low average score and high exploration factor\n            average_score = 0\n            exploration_factor = 1.0\n        else:\n            average_score = np.mean(scores)\n            exploration_factor = (1 / (len(scores) + 1))  # Helps encourage exploration of less tried options\n        \n        # Calculate a combined score using weighted exploration and exploitation\n        combined_score = average_score + exploration_factor * (1 - current_time_slot / total_time_slots)\n        action_scores.append(combined_score)\n        \n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 36350.77328482621,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Handle zero selections with a small epsilon for stability\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Calculate UCB components with enhanced exploration factor\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    ucb_values = expected_values + (exploration_bonus * exploration_weight)\n\n    # Adjust exploration using \u03b5-greedy strategy\n    epsilon_greedy = 0.1\n    if np.random.rand() < epsilon_greedy:\n        action_index = np.random.randint(num_actions)  # Explore a random action\n    else:\n        action_index = np.argmax(ucb_values)  # Exploit the best known action\n\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Avoid division by zero by assigning a small positive value\n    counts_with_min = counts + 1e-5\n    average_scores = scores\n\n    # Calculate epsilon for exploration\n    epsilon = 1.0 / (current_time_slot + 1)\n    \n    # Select with epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(actions)  # Explore\n    else:\n        # Calculate exploration bonuses using UCB\n        exploration_factor = 2\n        exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n        adjusted_scores = average_scores + exploration_bonus\n        action_index = actions[np.argmax(adjusted_scores)]  # Exploit\n\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Handle zero selections with a small epsilon for stability\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # UCB calculation\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    combined_values = expected_values + (exploration_bonus * exploration_weight)\n\n    # Add a softmax exploration component\n    temperature = 0.5  # Adjust temperature for exploration-exploitation balance\n    softmax_probabilities = np.exp(combined_values / temperature) / np.sum(np.exp(combined_values / temperature))\n\n    # Select action probabilistically based on softmax probabilities\n    action_index = np.random.choice(num_actions, p=softmax_probabilities)\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Handle zero selections with a small epsilon\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Calculate UCB components\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n    \n    # Adaptive exploration adjustment based on time\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    combined_values = expected_values + exploration_weight * exploration_bonus\n\n    # Select action with maximum combined value\n    action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    \n    # Calculate mean scores and counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Handle cases with no selections\n    counts_with_min = counts + 1e-5\n    \n    # Calculate average scores\n    average_scores = scores\n    \n    # Softmax for action selection\n    temperature = 0.5  # Adjust temperature for exploration/exploitation\n    exp_scores = np.exp(average_scores / temperature)\n    softmax_probs = exp_scores / np.sum(exp_scores)\n\n    # Explore: Randomly choose with probabilities\n    action_index = np.random.choice(actions, p=softmax_probs)\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    counts_with_min = counts + 1e-5\n    average_scores = scores\n\n    # UCB with an adaptive exploration factor\n    exploration_factor = np.log(total_time_slots + 1) / (counts_with_min + 1e-5)\n    adjusted_scores = average_scores + exploration_factor\n\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Avoid division by zero with a small constant\n    counts_with_min = counts + 1e-5\n\n    # Calculate average scores\n    average_scores = scores\n\n    # Using UCB for exploration\n    exploration_factor = 2.0\n    ucb_values = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n\n    # Prefer higher average scores added with exploration\n    adjusted_scores = average_scores + ucb_values\n\n    # Select the action with the highest adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Apply a small constant to prevent division by zero\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Calculate UCB with an exploration factor\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    combined_values = expected_values + exploration_bonus * exploration_weight\n\n    # Select action based on maximum combined value\n    action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Avoid division by zero\n    total_action_counts = action_counts if np.all(action_counts > 0) else action_counts + 1e-5\n\n    # Compute UCB values\n    ucb_terms = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    \n    # Define combined metric\n    combined_values = expected_values + (ucb_terms * exploration_weight)\n\n    # Select action with the highest combined value\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n\n    # Handling zero counts with a minimum score estimate\n    min_score = 0.01  # A small constant to account for unexplored actions\n    action_counts += (action_counts == 0) * 1  # Increment counts for actions not yet taken\n\n    # Compute exploration term using UCB-like approach\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1) + 1) / action_counts)\n    \n    # Time-dependent normalization factor\n    time_factor = ((current_time_slot + 1) / total_time_slots) ** 0.5\n    \n    # Combined values for decision making\n    combined_values = expected_values + exploration_bonus * time_factor\n\n    # Select the action with the highest combined value\n    action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    num_actions = len(actions)\n    \n    # Initialize arrays for scores and counts\n    scores = np.zeros(num_actions)\n    counts = np.zeros(num_actions)\n    \n    # Populate scores and counts\n    for idx, action in enumerate(actions):\n        if score_set[action]:\n            scores[idx] = np.mean(score_set[action])\n            counts[idx] = len(score_set[action])\n        \n    # Handle cases with no selections\n    counts_with_min = counts + 1e-5\n    \n    # Calculate UCB scores\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n    \n    # Adjust exploration bonus according to time remaining\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    ucb_scores = scores + exploration_bonus * exploration_scaling\n    \n    # Select action with the highest UCB score\n    action_index = actions[np.argmax(ucb_scores)]\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    num_actions = len(actions)\n    \n    # Initialize scores and counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Avoid division by zero and implement a small constant for exploration\n    counts_with_min = counts + 1e-5\n    \n    # Calculate average scores\n    average_scores = scores\n    \n    # Calculate exploration bonuses using UCB\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / counts_with_min)\n    \n    # Calculate a decay factor for exploration as time progresses\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = average_scores + exploration_bonus * exploration_scaling\n    \n    # Implement an \u03b5-greedy strategy for exploration\n    exploration_rate = max(0.1, 1 - (current_time_slot / total_time_slots))  # Decreasing exploration over time\n    if np.random.rand() < exploration_rate:\n        action_index = np.random.choice(actions)\n    else:\n        action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    num_actions = len(actions)\n    \n    # Initialize scores and counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Avoid division by zero\n    counts_with_min = counts + 1e-5\n    \n    # Calculate average scores\n    average_scores = scores / counts_with_min\n    \n    # Calculate exploration factor (\u03b5)\n    epsilon = 1 / (current_time_slot + 1)\n    \n    # Select action based on \u03b5-greedy strategy\n    if np.random.rand() < epsilon:\n        # Exploration: randomly select one of the actions\n        action_index = np.random.choice(actions)\n    else:\n        # Exploitation: Select the action with the highest average score\n        action_index = actions[np.argmax(average_scores)]\n    \n    return action_index",
          "objective": -449.9999999999999,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate expected values and counts\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Small value to prevent division by zero\n    epsilon = 1e-5\n    \n    # Calculate total counts for exploration\n    exploration_bonus = np.sqrt(2 * np.log(total_selection_count + 1) / (action_counts + epsilon))\n    \n    # Combine expected values with exploration bonus\n    combined_values = expected_values + exploration_bonus\n    \n    # Adaptive exploration/exploitation balance\n    time_factor = (current_time_slot + 1) / total_time_slots\n    exploration_weight = 1 - time_factor\n    adjusted_values = (exploration_weight * combined_values) + (time_factor * expected_values)\n    \n    # Select the action with the highest adjusted value\n    action_index = np.argmax(adjusted_values)\n\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Calculate the UCB components\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    combined_values = expected_values + exploration_bonus * exploration_weight\n\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    \n    # Calculate average scores and selection counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Assign a small value to avoid division by zero\n    counts_with_min = counts + 1e-5\n\n    # Compute average scores and exploration bonuses\n    average_scores = scores\n    exploration_factor = 1.5\n    exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n    \n    # Scale exploration based on remaining time slots\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = average_scores + exploration_bonus * exploration_scaling\n\n    # Use softmax for selection based on adjusted scores\n    exp_scores = np.exp(adjusted_scores - np.max(adjusted_scores))  # stability improvement\n    probabilities = exp_scores / np.sum(exp_scores)\n    \n    action_index = np.random.choice(actions, p=probabilities)\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n    \n    # Calculate expected values and action counts\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Regularize counts to avoid division by zero\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Calculate the UCB component with adaptive exploration\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    \n    # Combine expected values and exploration bonuses\n    combined_values = expected_values + (exploration_bonus * exploration_weight)\n\n    # Select action based on maximum combined value\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Handle division safely with np.where for zero selections\n    counts_with_min = np.where(counts > 0, counts, 1e-5)\n\n    # Calculate average scores\n    average_scores = scores\n\n    # Calculate exploration bonus using UCB approach\n    exploration_factor = 1.5\n    exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n\n    # Scale exploration bonus according to the remaining time slots\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = average_scores + exploration_bonus * exploration_scaling\n\n    # Ensure we have a valid action by using np.argmax\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    \n    # Calculate mean scores and selection counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Safe division by adding a small constant\n    counts_with_min = counts + 1e-5\n\n    # Calculate average scores\n    average_scores = scores / counts_with_min\n    \n    # Explore-exploit factor: \u03b5-greedy method\n    exploration_prob = 0.1  # 10% of the time we explore\n    if np.random.rand() < exploration_prob:\n        # Explore: Randomly select an action\n        action_index = np.random.choice(actions)\n    else:\n        # Calculate UCB\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n        \n        # Time-scaled exploration bonus\n        exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_scores = average_scores + exploration_bonus * exploration_scaling\n\n        # Select action with the highest adjusted score\n        action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    epsilon = 1e-5\n\n    # Calculate the exploration term (UCB)\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1) + 1) / (action_counts + epsilon))\n\n    # Combined value for UCB selection\n    combined_values = expected_values + exploration_bonus\n\n    # Hybrid approach: Softmax probability distribution for exploration\n    temperature = total_time_slots - current_time_slot  # Encourage exploration earlier\n    softmax_probs = np.exp(combined_values / temperature) / np.sum(np.exp(combined_values / temperature))\n\n    # Randomly choose an action based on softmax probabilities\n    action_index = np.random.choice(num_actions, p=softmax_probs)\n\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n\n    # Handle zero selections by adding a small epsilon\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Calculate UCB components\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n    \n    # Dynamic exploration factor based on remaining time\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    combined_values = expected_values + (exploration_bonus * exploration_weight)\n\n    # Select action based on maximum combined value\n    action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    epsilon = 1e-5\n\n    # UCB calculation\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1) + 1) / (action_counts + epsilon))\n    combined_values = expected_values + exploration_bonus\n\n    # Decay factor for exploration\n    decay_factor = (current_time_slot / total_time_slots) ** 2\n    adjusted_values = (1 - decay_factor) * combined_values + decay_factor * expected_values\n\n    action_index = np.argmax(adjusted_values)\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate expected values and counts\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Avoid division by zero by adding a small epsilon\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Calculate UCB components\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n    \n    # Adjust exploration based on time remaining\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    combined_values = expected_values + exploration_bonus * exploration_weight\n\n    # Select action based on maximum combined value\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Avoid division by zero by adding a small value\n    min_count = 1e-5\n    total_action_counts = action_counts + min_count\n\n    # Calculate UCB components\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n\n    # Define dynamic exploration factor based on remaining time slots\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    combined_values = expected_values + exploration_bonus * exploration_weight\n\n    # Select action with the highest combined value\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    \n    # Initialize scores and counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Handle exploration with \u03b5-greedy approach\n    exploration_prob = max(0.1, 0.5 * (total_time_slots - current_time_slot) / total_time_slots)\n\n    if np.random.rand() < exploration_prob:\n        # Randomly select an action\n        action_index = np.random.choice(actions)\n    else:\n        # Calculate UCB based on adjusted scores\n        counts_with_min = counts + 1e-5\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n        \n        # Time decay factor for exploration\n        exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_scores = scores + exploration_bonus * exploration_scaling\n        \n        # Select action with the highest adjusted score\n        action_index = actions[np.argmax(adjusted_scores)]\n\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    num_actions = len(actions)\n\n    # Calculate scores and counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Safe division for exploration bonus\n    counts_with_min = np.where(counts > 0, counts, 1e-5)\n\n    # Calculate average scores\n    average_scores = scores\n\n    # Define exploration factor\n    exploration_factor = 1.5\n    exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n\n    # Scaling exploration bonus\n    remaining_time = total_time_slots - current_time_slot\n    exploration_scaling = remaining_time / total_time_slots\n    adjusted_scores = average_scores + exploration_bonus * exploration_scaling\n    \n    # Select action with the highest adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        action_count = len(scores)\n        action_counts[action_index] = action_count\n        if action_count > 0:\n            expected_values[action_index] = np.mean(scores)\n\n    # Handling zero counts by using a small constant\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Upper Confidence Bound (UCB) calculation\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / total_action_counts)\n    \n    # Time factor applied to exploration\n    time_factor = np.sqrt((current_time_slot + 1) / total_time_slots)\n    \n    # Combining expected values with exploration bonus\n    combined_values = expected_values + exploration_bonus * time_factor\n\n    # Select the action with the highest combined value\n    action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            selection_counts[action_index] = len(scores)\n\n    # Handle zero selections by using a small epsilon value\n    epsilon = 1e-5\n    adjusted_selection_counts = selection_counts + epsilon\n\n    # Calculate UCB components\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / adjusted_selection_counts)\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    combined_values = expected_values + exploration_bonus * exploration_weight\n\n    # Select action with maximum combined value\n    action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n    \n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n        action_counts[action_index] = len(scores)\n    \n    # Handle division by zero with a small epsilon\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Calculate exploration shift based on time factor\n    time_factor = (total_time_slots - current_time_slot) / total_time_slots\n    ucb_values = expected_values + np.sqrt(np.log(total_selection_count + 1) / total_action_counts)\n    combined_values = ucb_values * time_factor\n\n    # Perform selection\n    action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_counts = np.zeros(num_actions)\n    expected_values = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n\n    # Avoid division by zero by adding a small constant\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Upper Confidence Bound (UCB) calculation\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n    \n    # Time-dependent scaling factor for exploration\n    time_factor = ((current_time_slot + 1) / total_time_slots)\n\n    combined_values = expected_values + exploration_bonus * time_factor\n\n    # Select the action with the highest combined value\n    action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        action_count = len(scores)\n        action_counts[action_index] = action_count\n        if action_count > 0:\n            expected_values[action_index] = np.mean(scores)\n\n    # Handling zero counts with a small smoothing factor\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Dynamic exploration-exploitation\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1) + 1) / total_action_counts)\n    \n    # Time-dependent adjustment factor\n    time_factor = (np.sqrt(current_time_slot + 1) / np.sqrt(total_time_slots))\n\n    # Combine expected values with exploration term\n    combined_values = expected_values + exploration_bonus * time_factor\n\n    # Choose action with the highest combined value\n    action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": -449.9999999999999,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n    \n    # Handle zero selections with a small epsilon\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Calculate the UCB components\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    ucb_values = expected_values + (exploration_bonus * exploration_weight)\n\n    # Introduce a threshold for exploration purely based on the current time slot\n    exploration_threshold = (total_time_slots - current_time_slot) / total_time_slots\n    if np.random.rand() < exploration_threshold:\n        action_index = np.random.randint(num_actions)  # Random exploration\n    else:\n        action_index = np.argmax(ucb_values)  # Exploit the best action\n\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    \n    # Calculate the mean scores and counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Handling counts with a small additive to avoid division by zero\n    counts_with_min = counts + 1e-5\n\n    # Calculate the average scores\n    average_scores = scores\n\n    # Adaptive exploration factor: \u03b5-greedy method\n    exploration_prob = max(0.1, 0.5 * (1 - current_time_slot / total_time_slots))  # Gradually reduce exploration probability\n    if np.random.rand() < exploration_prob:\n        # Explore: Randomly select an action\n        action_index = np.random.choice(actions)\n    else:\n        # Exploit: Calculate UCB\n        exploration_bonus = np.sqrt(2 * np.log(total_selection_count + 1) / counts_with_min)\n        \n        # Adjust scores with exploration bonus\n        adjusted_scores = average_scores + exploration_bonus\n\n        # Select the action with the highest adjusted score\n        action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Calculate total counts to scale exploration properly\n    counts_with_min = counts + 1e-5\n    average_scores = scores / counts_with_min  # Normalize scores by counts\n\n    # Adaptive exploration factor\n    exploration_factor = np.log(total_selection_count + 1) / counts_with_min\n    exploration_bonus = np.sqrt(exploration_factor) * (total_time_slots - current_time_slot) / total_time_slots\n\n    # Calculate adjusted scores\n    adjusted_scores = average_scores + exploration_bonus\n\n    # Select action with the highest adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Collect scores and counts for each action\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n\n    # Handling zero counts\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Apply Upper Confidence Bound (UCB) strategy\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n    \n    # Time-dependent factor for exploration weighting\n    time_factor = ((current_time_slot + 1) / total_time_slots) ** 0.5\n    \n    # Combine the expected values with exploration bonus and time factor\n    combined_values = expected_values + exploration_bonus * time_factor\n\n    # Select the action with the highest value\n    action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    num_actions = len(actions)\n\n    # Calculate mean scores and counts for each action\n    mean_scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Avoid division by zero\n    counts_with_min = counts + 1e-5\n    \n    # Calculate UCB for each action\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n    adjusted_scores = mean_scores + exploration_bonus * (total_time_slots - current_time_slot) / total_time_slots\n    \n    # Explore-exploit: \u03b5-greedy\n    exploration_prob = 0.1\n    if np.random.rand() < exploration_prob:\n        action_index = np.random.choice(actions)\n    else:\n        action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n    \n    # Calculate expected values and counts\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Handle zero selections with a small epsilon\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Calculate UCB values\n    exploration_bonus = np.sqrt(2 * np.log(total_selection_count + 1) / total_action_counts)\n    \n    # Adaptive exploration factor\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    combined_values = expected_values + (exploration_bonus * exploration_weight)\n\n    # Select action based on maximum combined value\n    action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Populate expected_values and action_counts\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n\n    # Prevent division by zero\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Calculate the exploration-exploitation term\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / total_action_counts)\n\n    # Time-dependent normalization factor\n    time_factor = ((current_time_slot + 1) / total_time_slots) ** 0.5\n\n    # Compute combined values using Upper Confidence Bound (UCB)\n    combined_values = expected_values + exploration_bonus * time_factor\n\n    # Select the action with the highest combined value\n    action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Protect against division by zero\n    counts_with_min = counts + 1e-5\n    \n    # Calculate exploration-exploitation balance using UCB with dynamic exploration factor\n    exploration_factor = 1.5\n    exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n    \n    # Linearly decreased exploration based on remaining time slots\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = scores + exploration_bonus * exploration_scaling\n    \n    # Select action with the highest adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Avoid division by zero and ensure minimum counts\n    counts_with_min = counts + 1e-5\n    average_scores = scores\n\n    # UCB calculation with exploration term\n    exploration_factor = 2\n    exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n\n    # Adjust exploration bonus based on time\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = average_scores + exploration_bonus * exploration_scaling\n\n    # Select action with the highest adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    n_actions = len(actions)\n    \n    # Calculate mean scores and counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Handle division by zero and calculate adjusted scores\n    counts_with_min = counts + 1e-5\n    average_scores = scores\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n\n    # Scale exploration factor\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = average_scores + exploration_bonus * exploration_scaling\n\n    # Use \u03b5-greedy approach for exploration-exploitation trade-off\n    epsilon = 0.1  # Probability of exploring\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(actions)\n    else:\n        action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Prevent division by zero\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Calculate UCB components for exploration\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    combined_values = expected_values + exploration_bonus * exploration_weight\n\n    # Apply scoring normalization\n    normalized_values = (combined_values - np.min(combined_values)) / (np.max(combined_values) - np.min(combined_values))\n\n    # Select action based on maximum normalized value\n    action_index = np.argmax(normalized_values)\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    \n    # Calculate mean scores and selection counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Handle cases with no selections\n    counts_with_min = np.maximum(counts, 1)  # Avoid division by zero\n\n    # Calculate average scores\n    average_scores = scores / counts_with_min\n\n    # Calculate UCB\n    exploration_bonus = np.sqrt(2 * np.log(total_selection_count + 1) / counts_with_min)\n    \n    # Adjust exploration bonus based on remaining time\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = average_scores + exploration_bonus * exploration_scaling\n\n    # Select action using softmax for a smoother exploration-exploitation balance\n    exp_scores = np.exp(adjusted_scores - np.max(adjusted_scores))  # Stabilize softmax\n    probabilities = exp_scores / np.sum(exp_scores)\n    \n    # Sample an action based on calculated probabilities\n    action_index = np.random.choice(actions, p=probabilities)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        if action_counts[action_index] > 0:\n            expected_values[action_index] = np.mean(scores)\n\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Dynamic exploration factor\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    combined_values = expected_values + (exploration_bonus * exploration_weight)\n\n    # Select action based on maximum combined value\n    action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Handle zero selections with a small epsilon\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Calculate UCB components\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n\n    # Implement a dynamic exploration factor based on current time slot\n    exploration_weight = np.power((total_time_slots - current_time_slot) / total_time_slots, 2)\n    combined_values = expected_values + (exploration_bonus * exploration_weight)\n\n    # Select action based on maximum combined value\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Handle zero selections by adding a small value for exploration\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Calculate the exploration term using an adaptive strategy\n    exploration_bonus = np.sqrt(2 * np.log(total_selection_count + 1) / total_action_counts)\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    combined_values = expected_values + exploration_bonus * exploration_weight\n\n    # Select the action with the highest combined value\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Improved exploration-exploitation balance\n    ucb_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / total_action_counts)\n\n    # Normalization factor incorporating time\n    time_scaling = (current_time_slot + 1) / total_time_slots\n    normalized_exploration = ucb_bonus * time_scaling\n\n    # Combined values for selection\n    combined_values = expected_values + normalized_exploration\n\n    # Select the action with the highest value\n    action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    \n    # Calculate average scores, handling cases with no scores\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Prevent division by zero\n    counts_with_min = counts + 1e-5\n\n    # Calculate average scores\n    average_scores = scores / counts_with_min\n\n    # Adaptive exploration-exploitation balance\n    exploration_prob = 1 / (current_time_slot + 1)  # Decaying exploration probability\n    if np.random.rand() < exploration_prob:\n        # Explore: Randomly select an action\n        action_index = np.random.choice(actions)\n    else:\n        # Exploit: Calculate UCB\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n        adjusted_scores = average_scores + exploration_bonus\n        \n        # Select the action with the highest adjusted score\n        action_index = actions[np.argmax(adjusted_scores)]\n\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Calculate UCB with an enhanced exploration factor\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    \n    # Combined values with improved weighting for exploration\n    combined_values = expected_values + (exploration_bonus * exploration_weight)\n\n    # Select action based on maximum combined value\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Handle zero action counts gracefully\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Explore-exploit balance using UCB with a dynamic exploration factor\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n    exploration_adjustment = (total_time_slots - current_time_slot) / total_time_slots\n    combined_values = expected_values + exploration_bonus * exploration_adjustment\n\n    # Select action with the highest combined value\n    action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Precision to mitigate division by zero\n    epsilon = 1e-5\n\n    # UCB Exploration factor\n    exploration_factor = np.sqrt((2 * np.log(total_selection_count + 1)) / (action_counts + epsilon))\n    \n    # Calculate combined values using expected values and exploration factors\n    combined_values = expected_values + exploration_factor\n\n    # Adjust combined values for the time-dependent factors\n    time_weight = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_combined_values = (1 - time_weight) * combined_values + time_weight * expected_values\n\n    # Handle actions that haven't been explored yet\n    unselected_actions = np.where(action_counts == 0)[0]\n    if len(unselected_actions) > 0:\n        action_index = np.random.choice(unselected_actions)\n    else:\n        action_index = np.argmax(adjusted_combined_values)\n\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Handle zero counts to prevent division by zero\n    counts_with_min = np.maximum(counts, 1e-5)\n\n    # Calculate average scores and exploration bonus\n    average_scores = scores\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n    \n    # Dynamic exploration scaling\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = average_scores + exploration_bonus * exploration_scaling\n\n    # Select action with the highest adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Assign a minimum count to avoid division by zero\n    counts_with_min = counts + 1e-5\n    average_scores = scores\n\n    # Dynamic exploration factor based on time slot progression\n    exploration_factor = 1.0  # Adjust this to tune exploration\n    exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n\n    # Adaptive scaling based on proximity to the final time slot\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = average_scores + exploration_bonus * exploration_scaling\n\n    # Select the action with the highest adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    \n    # Calculate mean scores and selection counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Prevent division by zero in UCB calculations\n    counts_with_min = counts + 1e-5\n\n    # Adjusted scores based on UCB\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    \n    # Calculate final scores for actions based on UCB\n    adjusted_scores = scores + exploration_bonus * exploration_scaling\n\n    # Dynamic exploration probability based on time slot\n    exploration_prob = max(0.05, 0.1 * (total_time_slots - current_time_slot) / total_time_slots)\n    \n    if np.random.rand() < exploration_prob:\n        # Explore: Randomly select an action\n        action_index = np.random.choice(actions)\n    else:\n        # Exploit: Select the action with the highest adjusted score\n        action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n    \n    total_action_counts = action_counts + 1e-5\n\n    # Calculate exploration factor using UCB\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / total_action_counts)\n\n    # Dynamic adjustment for exploration-exploitation\n    time_factor = (current_time_slot + 1) / total_time_slots\n    combined_values = expected_values + exploration_bonus * time_factor\n\n    # Select action with the maximum combined value\n    action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        if action_counts[action_index] > 0:\n            expected_values[action_index] = np.mean(scores)\n\n    # Handling zero counts for exploration bonus\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # UCB strategy for exploration-exploitation\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / total_action_counts)\n    \n    # Time-dependent exploration factor adjustment\n    time_factor = ((current_time_slot + 1) / total_time_slots) ** 0.5\n\n    # Combine expected values with UCB exploration\n    combined_values = expected_values + exploration_bonus * time_factor\n    \n    # Select the action with the highest combined value\n    action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate expected values and action counts\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Handle division by zero with a small epsilon\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Calculate the exploration bonus using UCB\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / total_action_counts)\n    \n    # Normalize time factor for exploration weight\n    time_factor = (current_time_slot + 1) / total_time_slots\n    \n    # Apply a scaling factor to the exploration bonus based on time factor\n    scaled_exploration_bonus = exploration_bonus * (1 + time_factor)\n    \n    # Calculate combined values for exploitation and exploration\n    combined_values = expected_values + scaled_exploration_bonus\n\n    # Select the action with the maximum combined value\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    \n    # Compute average scores and counts for actions\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Avoid division by zero by adding a small constant\n    counts_with_min = counts + 1e-5\n    \n    # Calculate the exploration factor using UCB\n    exploration_factor = 1.5\n    exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n\n    # Use a dynamic scaling factor to prioritize exploration earlier\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = scores + exploration_bonus * exploration_scaling\n\n    # Select the action with the maximum adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate the expected values and action counts\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Avoid division by zero\n    epsilon = 1e-5\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1) + 1) / (action_counts + epsilon))\n\n    # Define exploration rate\n    exploration_rate = max(0.1, 1 - (current_time_slot / total_time_slots))\n\n    # Calculate adjusted values using exploration bonus and expected values\n    adjusted_values = expected_values + exploration_bonus\n    \n    # \u03b5-greedy strategy\n    if np.random.rand() < exploration_rate:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(adjusted_values)\n\n    return action_index",
          "objective": -449.9999999999999,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    \n    # Calculate scores and counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Handle cases with no selections\n    counts_with_min = counts + 1e-5\n    \n    # Calculate average scores\n    average_scores = scores / counts_with_min\n    \n    # Exploration factor using \u03b5-greedy\n    epsilon = 0.1\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(actions)\n    else:\n        # UCB calculation\n        exploration_bonus = np.sqrt(2 * np.log(total_selection_count + 1) / counts_with_min)\n        \n        # Time-scaling factor for exploration\n        exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n        ucb_scores = average_scores + exploration_bonus * exploration_scaling\n        \n        action_index = actions[np.argmax(ucb_scores)]\n\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Handle zero selections with a small epsilon\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Calculate UCB components with dynamic exploration factor\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n    prioritization_factor = (total_time_slots - current_time_slot) / total_time_slots\n    combined_values = expected_values + (exploration_bonus * prioritization_factor)\n\n    # Select action based on maximum combined value\n    action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Avoid division by zero by adding a small constant\n    counts_with_min = np.where(counts > 0, counts, 1e-5)\n\n    # Calculate average scores\n    average_scores = scores\n\n    # Exploration bonus using UCB strategy\n    exploration_factor = 2.0\n    exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n\n    # Adjust exploration bonus based on remaining time slots\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = average_scores + exploration_bonus * exploration_scaling\n\n    # Select action with the highest adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n    \n    # Calculate UCB components\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n    \n    # Adaptive exploration factor based on time slot\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    combined_values = expected_values + (exploration_bonus * exploration_weight)\n\n    # Select action based on the maximum combined value, breaking ties randomly\n    action_index = np.random.choice(np.flatnonzero(combined_values == np.max(combined_values)))\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Handle zero selections with a small epsilon to avoid division by zero\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Calculate the exploration factor using logarithm of total selections\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n    \n    # Dynamic exploration weight based on time left in slots\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    \n    # Calculate combined values\n    combined_values = expected_values + (exploration_bonus * exploration_weight)\n\n    # Select action based on maximum combined value\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    \n    # Calculate scores and counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0.0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Avoid division by zero\n    counts_with_min = np.maximum(counts, 1e-5)\n\n    # Compute average scores\n    average_scores = scores\n\n    # Calculate UCB exploration term\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / counts_with_min)\n\n    # Dynamically adjust exploration factor\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    combined_scores = average_scores + exploration_bonus * exploration_scaling\n\n    # Select action with the highest score\n    action_index = actions[np.argmax(combined_scores)]\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n    \n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n    \n    # Handling zero counts more gracefully\n    epsilon = 1e-6\n    adjusted_counts = action_counts + epsilon\n\n    # UCB calculation with exploration factor\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / adjusted_counts)\n    \n    # Normalized time factor\n    time_factor = ((current_time_slot + 1) / total_time_slots) ** 0.5\n\n    # Combine expected values and exploration bonus\n    ucb_values = expected_values + exploration_bonus * time_factor\n\n    # Select the action with the highest UCB value\n    action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n\n    # Adding a small constant to avoid division by zero\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Calculating exploration bonus using Upper Confidence Bound strategy\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1) + 1) / total_action_counts)\n\n    # Time-dependent scaling factor that encourages exploration in the early time slots\n    time_factor = (current_time_slot + 1) / total_time_slots\n\n    # Combining expected values with scaled exploration bonus\n    combined_values = expected_values + exploration_bonus * time_factor\n\n    # Selecting action with the highest combined value\n    action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": -449.9999999999999,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    \n    # Calculate mean scores and selection counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Avoid division by zero by setting a small value for counts\n    counts_with_min = counts + 1e-5\n    \n    # Calculate exploration bonuses\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n    \n    # Adjust exploration with a time decay factor\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    \n    # Calculate adjusted scores\n    adjusted_scores = scores + exploration_bonus * exploration_scaling\n    \n    # Probability of exploring\n    exploration_prob = 0.1  # 10% exploration\n    if np.random.rand() < exploration_prob:\n        action_index = np.random.choice(actions)\n    else:\n        action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    n_actions = len(actions)\n\n    # Initialize arrays to hold the average scores and counts\n    scores = np.zeros(n_actions)\n    counts = np.zeros(n_actions)\n\n    # Compute mean scores and counts\n    for i, action in enumerate(actions):\n        if score_set[action]:\n            scores[i] = np.mean(score_set[action])\n            counts[i] = len(score_set[action])\n\n    # Assign a minimum count to avoid division by zero\n    counts_with_min = counts + 1e-5\n    \n    # Compute exploration component using UCB\n    exploration_factor = 1.5\n    exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n\n    # Scale exploration bonus based on remaining time slots\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = scores + (exploration_bonus * exploration_scaling)\n\n    # If all actions have the same score, select randomly to encourage exploration\n    if np.all(adjusted_scores == adjusted_scores[0]):\n        action_index = np.random.choice(actions)\n    else:\n        action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate expected values and counts\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        if action_counts[action_index] > 0:\n            expected_values[action_index] = np.mean(scores)\n\n    # Prevent division by zero\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n    \n    # Compute exploration bonus and decay factor\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n    decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n    combined_values = expected_values + (exploration_bonus * decay_factor)\n\n    # Select action based on maximum combined value\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n    \n    # Calculate expected values and action counts\n    for action_index, scores in score_set.items():\n        if scores:  # If there are recorded scores for the action\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Avoid division by zero by adding a small epsilon\n    epsilon = 1e-10\n    \n    # Upper Confidence Bound calculation\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / (action_counts + epsilon))\n    \n    # Combine expected values with exploration bonus\n    combined_values = expected_values + exploration_bonus\n    \n    # Dynamic exploration adjustment\n    time_factor = (current_time_slot + 1) / total_time_slots\n    adjusted_values = (1 - time_factor) * combined_values + time_factor * expected_values\n    \n    # Select the action with the highest adjusted value\n    action_index = np.argmax(adjusted_values)\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    num_actions = len(actions)\n    \n    # Initialize scores and counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Avoid division by zero and implement a small constant for exploration\n    counts_with_min = counts + 1e-5\n    \n    # Calculate average scores\n    average_scores = scores\n    \n    # Calculate exploration using an adaptive epsilon-greedy approach\n    epsilon = max(0.1, (total_time_slots - current_time_slot) / total_time_slots)\n    \n    # Select action based on epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        # Explore: Random selection of an action\n        action_index = np.random.choice(actions)\n    else:\n        # Exploit: Select action with highest average score\n        action_index = actions[np.argmax(average_scores)]\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Assign a minimum count to avoid division by zero\n    counts_with_min = counts + 1e-5\n    total_scores = scores\n    exploration_weight = 1.5\n\n    # Calculate the dynamic exploration factor\n    exploration_bonus = exploration_weight * np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n\n    # Normalize exploration factor based on time slot\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = total_scores + exploration_bonus * exploration_scaling\n\n    # Select the action with the highest adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Handle zero selections with a small epsilon to avoid division by zero\n    epsilon = 1e-5\n    adjusted_action_counts = action_counts + epsilon\n\n    # Calculate UCB components with dynamic exploration factor\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / adjusted_action_counts)\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    combined_values = expected_values + (exploration_bonus * exploration_weight)\n\n    # Select action based on highest combined value\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n    \n    # Handling zero counts\n    epsilon = 1e-5\n    action_counts += epsilon\n\n    # Adaptive exploration via UCB\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / action_counts)\n    combined_values = expected_values + exploration_bonus\n    \n    # Normalize by time ready to favor exploration at earlier time slots\n    time_factor = (current_time_slot + 1) / total_time_slots\n    combined_values *= (1 - time_factor) + time_factor * 1.5  # Adjust for exploration preference\n\n    action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Handle division by zero with small epsilon\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Calculate UCB with a dynamic exploration weight\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / total_action_counts)\n    \n    # Dynamic exploration weight based on the time slot\n    time_weight = (total_time_slots - current_time_slot) / total_time_slots\n    \n    # Combined value\n    combined_values = expected_values + exploration_bonus * time_weight\n\n    # Select the action with the highest combined value\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Compute expected values and counts\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n\n    # Handle not yet selected actions\n    for i in range(num_actions):\n        if action_counts[i] == 0:\n            expected_values[i] = 0  # No prior score, remains 0\n\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # UCB calculation with adaptive approach\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    combined_values = expected_values + (exploration_bonus * exploration_weight)\n\n    # Select action based on maximum combined value\n    action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Assign a minimum count to avoid division by zero\n    counts_with_min = counts + 1e-5\n    average_scores = scores\n\n    # \u03b5-greedy strategy\n    exploration_probability = 1 / (current_time_slot + 1)\n    if np.random.rand() < exploration_probability:\n        # Explore: select a random action\n        action_index = np.random.choice(actions)\n    else:\n        # Exploit: calculate adjusted scores\n        exploration_factor = 1.5\n        ucb = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n        \n        # Scale the exploration bonus\n        exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_scores = average_scores + ucb * exploration_scaling\n        \n        # Select the action with the highest adjusted score\n        action_index = actions[np.argmax(adjusted_scores)]\n        \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Calculate exploration factor\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / total_action_counts)\n    \n    # Adjust exploration factor based on the current time slot\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    combined_values = expected_values + exploration_bonus * exploration_weight\n\n    # Introduce a small deterministic exploration mechanism\n    exploration_threshold = 0.1\n    if np.random.rand() < exploration_threshold:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Use a small epsilon to handle divisions by zero\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Calculate Upper Confidence Bound (UCB)\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n    remaining_exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    combined_values = expected_values + exploration_bonus * remaining_exploration_weight\n\n    # Select action based on the maximum value\n    action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Handle zero selections with a small epsilon\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # UCB calculation\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    combined_values = expected_values + exploration_bonus * exploration_weight\n\n    # Implement an \u03b5-greedy strategy\n    epsilon_exploration_rate = 0.1 * (total_time_slots - current_time_slot) / total_time_slots\n    if np.random.rand() < epsilon_exploration_rate:\n        action_index = np.random.randint(0, num_actions)\n    else:\n        action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Calculate UCB\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / total_action_counts)\n\n    # Adjust exploration rate dynamically\n    time_weight = (total_time_slots - current_time_slot) / total_time_slots\n    combined_values = expected_values + exploration_bonus * time_weight\n\n    # Use Thompson Sampling as an additional exploration strategy\n    beta_samples = np.random.beta(action_counts + 1, (total_selection_count - action_counts) + 1)\n    \n    # Choose the best value between combined values and Thompson samples\n    action_values = np.maximum(combined_values, beta_samples)\n\n    action_index = np.argmax(action_values)\n\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    num_actions = len(actions)\n    \n    # Calculate average scores and counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Use a small value for zero counts to avoid division by zero\n    counts_with_min = counts + 1e-5\n    \n    # Calculate the exploration factor\n    exploration_factor = np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n    \n    # Dynamic scaling for exploration based on time\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n\n    # Calculate total scores for selection\n    adjusted_scores = scores + exploration_factor * exploration_scaling\n    \n    # Select action with the highest adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        if action_counts[action_index] > 0:\n            expected_values[action_index] = np.mean(scores)\n\n    # Handling zero counts\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Improved exploration factor\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1) + 1) / total_action_counts)\n\n    # Time-dependent normalization factor for exploration\n    time_factor = (current_time_slot + 1) / total_time_slots\n\n    # Combining expected values with exploration bonus\n    combined_values = expected_values + exploration_bonus * time_factor\n\n    # Selecting action with the highest combined value\n    action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Add epsilon to avoid division by zero\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n    \n    # Calculate exploitation value (mean scores)\n    exploitation_value = expected_values\n\n    # Calculate exploration value (UCB with adaptive factor)\n    exploration_value = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n\n    # Apply a decay factor to exploration\n    decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n    exploration_value *= decay_factor\n    \n    # Combine exploitation and exploration values\n    combined_values = exploitation_value + exploration_value\n\n    # Select action with the maximum combined value\n    action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n    \n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n    \n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n    \n    # Calculate adaptive exploration factor\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    \n    # Combine expected values with exploration components\n    combined_values = expected_values + (exploration_bonus * exploration_weight)\n\n    # Select action based on maximum combined value\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    num_actions = len(actions)\n\n    # Initialize scores and counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Avoid division by zero and implement a small constant for exploration\n    counts_with_min = counts + 1e-5\n\n    # Calculate average scores and UCB bonuses\n    average_scores = scores\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / counts_with_min)\n\n    # Scale exploration based on remaining time slots\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = average_scores + exploration_bonus * exploration_scaling\n\n    # Apply \u03b5-greedy strategy\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n    if np.random.random() < epsilon:\n        action_index = np.random.choice(actions)  # Explore\n    else:\n        action_index = actions[np.argmax(adjusted_scores)]  # Exploit\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n    \n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n    \n    # Calculate exploration bonuses with a safeguard against division by zero\n    total_action_counts = action_counts + 1e-5\n    \n    # Calculate exploration and exploitation factors\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    \n    # Combine expected values with exploration bonus\n    combined_values = expected_values + (exploration_bonus * exploration_weight)\n    \n    # Select action with the highest combined value\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    num_actions = len(actions)\n\n    # Calculate average scores and selection counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Compute the total counts to avoid division by zero\n    counts_with_min = counts + 1e-5\n    \n    # Calculate average scores\n    average_scores = scores\n    \n    # Calculate exploration using UCB\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / counts_with_min)\n\n    # Adaptive exploration scaling\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = average_scores + exploration_bonus * exploration_scaling\n\n    # Select the action with the maximum adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    \n    # Calculate mean scores and counts while handling empty cases\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0.0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Adjust counts to avoid division by zero\n    adjusted_counts = counts + 1e-5\n    \n    # Calculate average scores\n    average_scores = scores\n    \n    # Exploration probability\n    exploration_prob = 0.15  # Increased exploration to allow for more diverse action settlings\n    if np.random.rand() < exploration_prob:\n        # Explore: randomly select an action\n        action_index = np.random.choice(actions)\n    else:\n        # Exploit: calculate UCB with an adaptive exploration bonus\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / adjusted_counts)\n        \n        # Dynamic scaling of exploration based on remaining time\n        exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n        \n        # Combine average scores with exploration bonus\n        adjusted_scores = average_scores + exploration_bonus * exploration_scaling\n        \n        # Select action with the highest adjusted score\n        action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    \n    # Calculate mean scores and selection counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Add a small value to counts to avoid division by zero\n    counts_with_min = counts + 1e-5\n\n    # Calculate average scores\n    average_scores = scores\n\n    # Exploration-exploitation strategy: Upper Confidence Bound (UCB) with decay\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n    \n    # Adjust the exploration factor based on remaining time slots\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = average_scores + exploration_bonus * exploration_scaling\n\n    # Select the action with the highest adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    \n    # Calculate mean scores and selection counts\n    mean_scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Handle cases with no selections\n    counts_with_min = counts + 1e-5\n\n    # Exploration factor\n    exploration_prob = 0.1  # 10% exploration\n\n    if np.random.rand() < exploration_prob:\n        action_index = np.random.choice(actions)\n    else:\n        # UCB calculation with decay\n        ucb_values = mean_scores + np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n        \n        # Adjust for time decay\n        time_decay = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_ucb = ucb_values * time_decay\n        \n        # Select the action with the highest adjusted UCB value\n        action_index = actions[np.argmax(adjusted_ucb)]\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Handle zero selections with a small epsilon\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Calculate UCB components\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    combined_values = expected_values + exploration_bonus * exploration_weight\n\n    # Select action based on maximum combined value\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    num_actions = len(actions)\n    \n    # Initialize scores and counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Avoid division by zero\n    counts_with_min = counts + 1e-5\n    \n    # Calculate average scores\n    average_scores = scores\n    \n    # Calculate exploration using UCB\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / counts_with_min)\n    \n    # Exploration scaling over time slots\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = average_scores + exploration_bonus * exploration_scaling\n    \n    # Select action with the highest adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    \n    # Calculate the mean scores and counts of selections\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Prevent division by zero by adding a small constant\n    counts_with_min = counts + 1e-5\n\n    # Calculate average scores\n    average_scores = scores / counts_with_min\n\n    # Define exploration-exploitation parameters\n    exploration_prob = 1 / ((current_time_slot + 1) ** 0.5)  # Decay exploration over time\n\n    if np.random.rand() < exploration_prob:\n        # Explore: Randomly select an action\n        action_index = np.random.choice(actions)\n    else:\n        # Exploit: Use UCB approach to select the best action\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n        \n        # Adjust the exploration bonus for more recent actions\n        adjusted_scores = average_scores + exploration_bonus\n        \n        # Select the action with the highest adjusted score\n        action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Handle zero selections with a small epsilon\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Calculate UCB with a focus on exploration and exploitation\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n    combined_values = expected_values + exploration_bonus\n\n    # Dynamic exploration based on remaining time slots\n    exploration_weight = np.sqrt(total_time_slots / (total_time_slots - current_time_slot + 1))\n    adjusted_combined_values = combined_values * exploration_weight\n\n    # Select action based on maximum adjusted combined value\n    action_index = np.argmax(adjusted_combined_values)\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    \n    # Calculate mean scores and selection counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Avoid division by zero\n    counts_with_min = counts + 1e-5\n    \n    # Calculate average scores\n    average_scores = scores / counts_with_min\n    \n    # UCB calculation\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n\n    # Adjust exploration bonus based on time\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = average_scores + exploration_scaling * exploration_bonus\n    \n    # Select action using \u03b5-greedy strategy\n    exploration_prob = 0.1  # 10% chance to explore\n    if np.random.rand() < exploration_prob:\n        action_index = np.random.choice(actions)\n    else:\n        action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    \n    # Calculate mean scores and selection counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Handle cases with no selections\n    counts_with_min = counts + 1e-5\n\n    # Calculate average scores\n    average_scores = scores\n\n    # Dynamic \u03b5-greedy factor\n    exploration_prob = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)  # Decrease exploration over time\n\n    if np.random.rand() < exploration_prob:\n        # Explore: Randomly select an action\n        action_index = np.random.choice(actions)\n    else:\n        # Exploit: Calculate UCB\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n        adjusted_scores = average_scores + exploration_bonus\n\n        # Select the action with the highest adjusted score\n        action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0.0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Avoid division by zero\n    counts_with_min = counts + 1e-5\n\n    # Calculate average scores\n    average_scores = scores\n\n    # Exploration factor using Upper Confidence Bound\n    exploration_factor = 1.5\n    exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n\n    # Scale exploration with a linear factor based on the remaining time slots\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = average_scores + exploration_bonus * exploration_scaling\n\n    # Select the action with the highest adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    \n    # Calculate average scores and counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Safe division\n    counts_with_min = np.where(counts > 0, counts, 1e-5)\n    \n    # Calculate average score\n    average_scores = scores / counts_with_min\n\n    # Calculate exploration bonus using UCB\n    exploration_factor = 1.5\n    exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n\n    # Scale exploration bonus based on remaining time slots\n    remaining_time_fraction = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = average_scores + exploration_bonus * remaining_time_fraction\n    \n    # Select action with the highest adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Handle zero selections\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Calculate the exploration bonus using UCB\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n    # Dynamic exploration based on remaining time slots\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    combined_values = expected_values + (exploration_bonus * exploration_weight)\n\n    # Select the action with the highest combined value\n    action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Calculate expected value of each action\n    expected_values = scores\n\n    # Assign a minimum count to avoid division by zero\n    counts_with_min = counts + 1e-5\n\n    # Explore-exploit balance using UCB\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / counts_with_min)\n    \n    # Implement a decaying exploration factor over time\n    time_scaling_factor = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = expected_values + exploration_bonus * time_scaling_factor\n\n    # Select action with the highest adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    epsilon = 1e-5\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1) + 1) / (action_counts + epsilon))\n    \n    exploration_weight = 1 - (current_time_slot / total_time_slots)\n    combined_values = expected_values + exploration_weight * exploration_bonus\n\n    # Incorporate a decay mechanism for exploration\n    decay_factor = 0.1\n    exploration_weight = np.exp(-decay_factor * current_time_slot)\n    adjusted_values = (1 - exploration_weight) * combined_values + exploration_weight * expected_values\n\n    action_index = np.argmax(adjusted_values)\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n\n    # Handle zero selections by assigning a minimum score\n    epsilon_score = 1e-5\n    expected_values = np.where(action_counts > 0, expected_values, epsilon_score)\n\n    # Calculate UCB components for exploration\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / (action_counts + 1e-5))\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    combined_values = expected_values + (exploration_bonus * exploration_weight)\n\n    # Select action based on maximum combined value\n    action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Handle zero selections\n    epsilon = 1e-5\n    action_counts += epsilon\n\n    # Calculate the UCB with a time-based exploration factor\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / action_counts)\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    combined_values = expected_values + exploration_bonus * exploration_weight\n\n    # Select action based on maximum combined value\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # To avoid division by zero, add a small epsilon value\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Calculate UCB components\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n    \n    # Adjust exploration more towards the beginning of the time slots\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    combined_values = expected_values + (exploration_bonus * exploration_weight)\n\n    # Select the action with the maximum combined value\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Avoid division by zero by adding a small epsilon\n    epsilon = 1e-5\n\n    # Calculate exploration bonus with UCB\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1) + 1) / (action_counts + epsilon))\n\n    # Combine expected values with exploration bonus\n    combined_values = expected_values + exploration_bonus\n\n    # Dynamic exploration factor based on time\n    time_weight = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_values = (combined_values * time_weight) + (expected_values * (1 - time_weight))\n\n    # Select the action with the highest adjusted value\n    action_index = np.argmax(adjusted_values)\n\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    \n    # Calculate mean scores and selection counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Add a small value to counts to avoid division by zero\n    counts_with_min = counts + 1e-5\n\n    # Calculate UCB adjusted scores\n    average_scores = scores\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n\n    # Dynamically adjust exploration factor based on the time evolution\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = average_scores + exploration_bonus * exploration_scaling\n\n    # Select action with the highest adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        if action_counts[action_index] > 0:\n            expected_values[action_index] = np.mean(scores)\n\n    # Avoid division by zero in case an action has never been selected\n    total_action_counts = action_counts + 1e-5\n\n    # Calculate UCB components\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    combined_values = expected_values + exploration_bonus * exploration_weight\n\n    # Select action based on maximum combined value\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    num_actions = len(actions)\n    \n    # Compute average scores and counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Handle division by zero using np.where\n    counts_with_min = np.where(counts > 0, counts, 1e-5)\n\n    # Exploitation: Average scores\n    average_scores = scores\n\n    # Exploration: UCB exploration bonus\n    exploration_factor = 1.0  # adjust this factor for exploration\n    exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n\n    # Scale exploration based on time left\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = average_scores + (exploration_bonus * exploration_scaling)\n\n    # Select action with the highest adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n    \n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Avoid division by zero by adding a small epsilon\n    epsilon = 1e-5\n\n    # Calculate UCB, incorporate historical selection counts\n    ucb_values = expected_values + np.sqrt((2 * np.log(total_selection_count + 1)) / (action_counts + epsilon))\n\n    # Adaptive exploration via \u03b5-greedy approach\n    exploration_rate = 1.0 - (current_time_slot / total_time_slots)\n    random_choice = np.random.rand() < exploration_rate\n    \n    if random_choice:\n        # Explore: Select an action randomly\n        action_index = np.random.randint(num_actions)\n    else:\n        # Exploit: Select action with the highest UCB value\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    \n    # Initialize scores and counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Handle cases with zero counts using a small constant\n    counts_with_min = counts + 1e-5\n    \n    # Calculate average scores\n    average_scores = scores / counts_with_min\n    \n    # Exploration-exploitation balance using \u03b5-greedy\n    exploration_prob = 0.1\n    if np.random.rand() < exploration_prob:\n        # Explore: Randomly select an action\n        action_index = np.random.choice(actions)\n    else:\n        # Exploit: Calculate UCB\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n        \n        # Decay exploration bonus over time\n        exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_scores = average_scores + exploration_bonus * exploration_scaling\n        \n        # Select the action with the highest adjusted score\n        action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n   \n    # Calculate the average score\n    average_scores = scores\n\n    # Avoid division by zero and set a small value for counts\n    counts_with_min = np.where(counts > 0, counts, 1e-5)\n    \n    # Exploration parameter: Adjust to balance exploration and exploitation\n    exploration_factor = 1.5\n    exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n    \n    # Scale exploration bonus according to remaining time slots\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = average_scores + exploration_bonus * exploration_scaling\n\n    # Introduce \u03b5-greedy strategy for better exploration\n    epsilon = 0.1\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(actions)\n    else:\n        action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # To avoid division by zero, we add a small constant to counts\n    counts_with_min = counts + 1e-5\n\n    # Calculate the average scores\n    average_scores = scores\n\n    # Define exploration factor dynamically (can be tuned)\n    exploration_factor = 2.0\n    exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n\n    # Scale exploration based on time left\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = average_scores + exploration_bonus * exploration_scaling\n\n    # Implement \u03b5-greedy exploration strategy\n    epsilon = 0.1\n    if np.random.rand() < epsilon:\n        # Explore: randomly select an action\n        action_index = np.random.choice(actions)\n    else:\n        # Exploit: select the action with the highest adjusted score\n        action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n\n    # Handle zero selections with a small epsilon\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Calculate the Upper Confidence Bound (UCB)\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n    \n    # Incorporate a weighted exploration factor based on remaining time slots\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    combined_values = expected_values + (exploration_bonus * exploration_weight)\n\n    # Select action with the highest combined value\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Calculate UCB components\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / total_action_counts)\n\n    # Adaptive exploration factor\n    exploration_weight = (total_time_slots - current_time_slot + 1) / total_time_slots\n    combined_values = expected_values + (exploration_bonus * exploration_weight)\n\n    # Select action based on maximum combined value\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Handle division by zero\n    epsilon = 1e-5\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1) + 1) / (action_counts + epsilon))\n\n    # Combine expected values with exploration bonus\n    combined_values = expected_values + exploration_bonus\n\n    # Calculate a decay factor for exploration over time\n    decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_values = combined_values * decay_factor + expected_values * (1 - decay_factor)\n\n    # Select the action with the highest adjusted value\n    action_index = np.argmax(adjusted_values)\n\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    \n    # Initialize lists for scores and counts\n    scores = []\n    counts = []\n    \n    for action in actions:\n        if score_set[action]:\n            scores.append(np.mean(score_set[action]))\n            counts.append(len(score_set[action]))\n        else:\n            scores.append(0)\n            counts.append(0)\n    \n    scores = np.array(scores)\n    counts = np.array(counts)\n\n    # Handle cases with no selections\n    counts_with_min = counts + 1e-5\n\n    # Calculate upper confidence bounds\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n    \n    # Adaptive scaling of the exploration bonus with respect to time\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = scores + exploration_bonus * exploration_scaling\n\n    # \u03b5-greedy exploration probability\n    exploration_prob = 0.1\n    if np.random.rand() < exploration_prob:\n        # Explore: Randomly select an action\n        action_index = np.random.choice(actions)\n    else:\n        # Exploit: Select the action with the highest adjusted score\n        action_index = actions[np.argmax(adjusted_scores)]\n\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    num_actions = len(actions)\n\n    # Compute means and counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Avoid division by zero by adding a small value\n    counts_with_min = counts + 1e-5\n\n    # Calculate average scores\n    average_scores = scores / counts_with_min\n\n    # Exploration factor\n    exploration_factor = 1.5\n    exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n\n    # Scale exploration by remaining time\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = average_scores + exploration_bonus * exploration_scaling\n\n    # Select the action with the maximum adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    \n    # Calculate the mean scores and selection counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Handle cases with no selections\n    counts_with_min = counts + 1e-5\n\n    # Implement UCB for exploration-exploitation balance\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n    \n    # Time-decayed exploration scaling\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = scores + (exploration_bonus * exploration_scaling)\n    \n    # Select the action with the highest adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Calculate exploration component\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / total_action_counts)\n\n    # Normalize time factor dynamically for exploration\n    time_factor = (current_time_slot + 1) / total_time_slots\n    \n    # Combined exploitation and exploration values\n    combined_values = expected_values + exploration_bonus * time_factor\n\n    # Handle the case of no selections yet. Prefer exploring actions that haven't been selected.\n    exploration_priority = (total_selection_count == 0).astype(int) * (1 / (1 + np.arange(num_actions)))\n    \n    final_values = combined_values + exploration_priority \n\n    action_index = np.argmax(final_values)\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    num_actions = len(actions)\n    \n    average_scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Avoid division by zero by adding a small constant\n    adjusted_counts = counts + 1e-5\n\n    # UCB parameter\n    exploration_factor = 1.5\n    exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / adjusted_counts)\n\n    # Scale exploration bonus based on remaining time slots\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = average_scores + exploration_bonus * exploration_scaling\n\n    # Select the action with the highest adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -449.9999999999999,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Calculate exploration factor using Upper Confidence Bound (UCB)\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / (action_counts + 1e-5))\n    \n    # Calculate combined values using expected values and exploration factors\n    combined_values = expected_values + exploration_bonus\n\n    # Apply dynamic decay based on the current time slot\n    decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n    combined_values = decay_factor * combined_values + (1 - decay_factor) * expected_values\n\n    # Select the action with the maximum combined value\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Safe counts to avoid division by zero\n    counts_with_min = np.maximum(counts, 1e-5)\n\n    # Calculate average scores\n    average_scores = scores\n\n    # Exploration factor with dynamic scaling\n    exploration_factor = 1.5 * (total_time_slots - current_time_slot) / total_time_slots\n    exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n\n    # Adjusted scores\n    adjusted_scores = average_scores + exploration_bonus\n\n    # Select action with the highest adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        if action_counts[action_index] > 0:\n            expected_values[action_index] = np.mean(scores)\n\n    total_action_counts = action_counts + 1e-5  # Avoid division by zero\n\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    combined_values = expected_values + exploration_bonus * exploration_weight\n\n    # Select action with a probability based on expected values and UCB exploration\n    total_expected_value = np.sum(np.exp(combined_values))\n    probabilities = np.exp(combined_values) / total_expected_value\n\n    # Implementing an \u03b5-greedy strategy\n    epsilon = 0.1\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(probabilities)\n\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Add a small value to avoid division by zero\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Calculating UCB with balanced exploration\n    exploration_factor = np.sqrt((2 * np.log(total_selection_count + 1)) / total_action_counts)\n    remaining_time_weight = (total_time_slots - current_time_slot) / total_time_slots  # Promotes exploration early on\n    \n    # Combining the expected value with the exploration factor\n    combined_values = expected_values + (exploration_factor * remaining_time_weight)\n\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Compute expected values and counts for each action\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Handle zero selections safely\n    action_counts += 1e-5  # To avoid division by zero\n\n    # Calculate exploration bonus using Upper Confidence Bound (UCB)\n    total_action_counts = action_counts + 1e-5\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n\n    # Incorporate a time-dependent exploration factor\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    combined_values = expected_values + exploration_bonus * exploration_weight\n\n    # Select action based on maximum combined value\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    num_actions = len(actions)\n\n    # Calculate average scores and counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Avoid division by zero\n    counts_with_min = counts + 1e-5\n\n    # Calculate the average score and the exploration bonus\n    average_scores = scores\n    exploration_bonus = np.sqrt(2 * np.log(total_selection_count + 1) / counts_with_min)\n\n    # Define an adaptive exploration factor\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = average_scores + exploration_factor * exploration_bonus\n\n    # Select action with the highest adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Safeguard against division by zero\n    counts_with_min = np.maximum(counts, 1e-5)\n\n    # Calculate average scores\n    average_scores = scores\n\n    # Use UCB for exploration\n    exploration_factor = 1.5\n    exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n\n    # Scale exploration bonus dynamically\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = average_scores + (exploration_bonus * exploration_scaling)\n\n    # Select action with the highest adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Adding a small epsilon to avoid division by zero\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Calculate exploration and exploitation trade-off\n    exploration_weight = np.sqrt(np.log(total_selection_count + 1) / total_action_counts)\n    \n    # Normalize time factor for exploration weight\n    time_factor = (current_time_slot + 1) / total_time_slots\n    \n    # Incorporate a decay factor for time to prioritize recent rewards\n    decay_factor = np.exp(-current_time_slot / total_time_slots)\n    \n    # Calculate combined values for better exploration and exploitation\n    combined_values = expected_values + (exploration_weight * time_factor * decay_factor)\n\n    # Select the action with the maximum combined value\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    num_actions = len(actions)\n    \n    # Initialize scores and counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Prevent division by zero\n    counts_with_min = counts + 1e-5\n    \n    # Compute the exploration term using softmax\n    exploration_factor = np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n    \n    # Combine scores and exploration factors\n    adjusted_scores = scores + exploration_factor\n    \n    # Use softmax to determine selection probabilities\n    exp_scores = np.exp(adjusted_scores - np.max(adjusted_scores))  # for numerical stability\n    probabilities = exp_scores / np.sum(exp_scores)\n    \n    # Stochastic action selection based on the computed probabilities\n    action_index = np.random.choice(actions, p=probabilities)\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    n_actions = len(actions)\n    \n    # Calculate mean scores and counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0.0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Handle counts with a small adjustment\n    counts_with_min = counts + 1e-5\n\n    # Calculate average scores\n    average_scores = scores\n\n    # Exploration probability using adaptive decay\n    exploration_prob = max(0.1 * (total_time_slots - current_time_slot) / total_time_slots, 0.01)\n\n    # Determine whether to explore or exploit\n    if np.random.rand() < exploration_prob:\n        # Explore: Randomly select an action\n        action_index = np.random.choice(actions)\n    else:\n        # Exploit: Calculate UCB\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n        adjusted_scores = average_scores + exploration_bonus\n\n        # Select the action with the highest adjusted score\n        action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Assign a small value to avoid division by zero\n    counts_with_min = counts + 1e-5\n    average_scores = scores\n\n    # Adaptive exploration factor\n    exploration_factor = 2.0\n    exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n\n    # Dynamic scaling based on remaining time slots\n    remaining_time_slots = total_time_slots - current_time_slot\n    exploration_scaling = remaining_time_slots / total_time_slots\n    adjusted_scores = average_scores + exploration_bonus * exploration_scaling\n\n    # Introduce Epsilon-Greedy for added exploration\n    epsilon = 0.1\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(actions)\n    else:\n        action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    historical_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        if scores:\n            historical_means[action_index] = np.mean(scores)\n    \n    # Handling zero selections\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Upper Confidence Bound calculation\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / total_action_counts)\n\n    # Combining historical means with exploration bonus\n    combined_values = historical_means + exploration_bonus\n    \n    # Time-dependent scaling to prioritize more recent actions\n    time_factor = (1 + current_time_slot) / (1 + total_time_slots)\n    combined_values *= time_factor\n\n    # Select the action with the highest combined value\n    action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    num_actions = len(actions)\n\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Add a small value to counts to prevent division by zero\n    counts_with_min = counts + 1e-5\n    \n    # Calculate average scores for each action\n    average_scores = scores / counts_with_min\n\n    # Exploration term (\u03b5-greedy approach)\n    epsilon = 1.0 / (1 + total_selection_count)  # Decay exploration over time\n    exploration_bonus = np.random.binomial(1, epsilon, num_actions)\n\n    # Calculate adjusted scores: exploit + explore\n    adjusted_scores = average_scores + exploration_bonus * (1 - average_scores) * (total_time_slots - current_time_slot) / total_time_slots\n  \n    # Select the action with the highest adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0.0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Handle zero counts safely\n    counts_with_min = np.where(counts > 0, counts, 1e-5)\n\n    # Exploration constant\n    exploration_constant = 1.0\n\n    # Calculate average scores\n    average_scores = scores\n\n    # Calculate UCB exploration bonus\n    ucb_bonus = exploration_constant * np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n\n    # Scale the exploration bonus according to remaining time slots\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = average_scores + ucb_bonus * exploration_scaling\n\n    # Select the action with the highest adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Avoid division by zero and handle zero selections with a small epsilon\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Calculate UCB components\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / total_action_counts)\n    \n    # Adjust exploration based on remaining time slots\n    exploration_weight = 1 - (current_time_slot / total_time_slots)\n    combined_values = expected_values + (exploration_bonus * exploration_weight)\n\n    # Select action based on maximum combined value\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Handle division safely with np.where for zero selections\n    counts_with_min = np.where(counts > 0, counts, 1e-5)\n\n    # Calculate the exploration bonus using an adaptive epsilon-greedy approach\n    epsilon = 0.1 * (total_time_slots - current_time_slot) / total_time_slots\n    exploration_values = np.random.rand(len(scores)) < epsilon\n    \n    # Calculate adjusted scores for exploitation\n    adjusted_scores = scores + (exploration_values.astype(float) * np.inf)\n\n    # Select action with the highest adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Handle zero selections with a small epsilon\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Calculate UCB components with adaptive exploration factor\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    combined_values = expected_values + (exploration_bonus * exploration_weight)\n\n    # Add a decay factor for exploration to prioritize immediate performance\n    decay_factor = 0.5 ** (current_time_slot / total_time_slots)\n    combined_values *= decay_factor\n\n    # Select action based on maximum combined value\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    \n    # Calculate mean scores and selection counts while handling empty action cases\n    mean_scores = np.array([np.mean(score_set[action]) if score_set[action] else 0.0 for action in actions])\n    selection_counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Avoid division by zero in exploration calculation\n    counts_with_min = selection_counts + 1e-5\n\n    # Calculate the exploration bonus using UCB\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n\n    # Implement a dynamic exploration strategy with time decay\n    time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = mean_scores + exploration_bonus * time_decay_factor\n    \n    # Original exploration probability using \u03b5-greedy\n    exploration_prob = 0.1\n    if np.random.rand() < exploration_prob:\n        # Explore: Select a random action\n        action_index = np.random.choice(actions)\n    else:\n        # Exploit: Select the action with the highest adjusted score\n        action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        if action_counts[action_index] > 0:\n            expected_values[action_index] = np.mean(scores)\n\n    total_action_counts = action_counts + 1e-5  # Handle division by zero\n    \n    # Compute UCB with an adaptive exploration factor\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    \n    combined_values = expected_values + exploration_bonus * exploration_weight\n    \n    # Select action based on maximum combined value\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": -449.9999999999999,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Handle division safely with np.where for zero selections\n    counts_with_min = np.where(counts > 0, counts, 1e-5)\n\n    # Calculate average scores\n    average_scores = scores\n\n    # Exploration factor decreasing based on the time slot\n    exploration_factor = 1.0 / (current_time_slot + 1)\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / counts_with_min) * exploration_factor\n\n    # Compute adjusted scores\n    adjusted_scores = average_scores + exploration_bonus\n\n    # Select action with the highest adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    num_actions = len(actions)\n\n    # Initialize scores and counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Prevent division by zero while calculating exploration bonuses\n    counts_with_min = counts + 1e-5\n\n    # Calculate average scores\n    average_scores = scores\n\n    # Enhanced exploration using UCB with adaptive scaling\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / counts_with_min)\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adj_exploration = exploration_bonus * exploration_scaling\n\n    # Implement \u03b5-greedy strategy for controlled exploration\n    epsilon = max(0.1 * (total_time_slots - current_time_slot) / total_time_slots, 0.05)\n    \n    if np.random.rand() < epsilon:\n        # Explore: select a random action\n        action_index = np.random.choice(actions)\n    else:\n        # Exploit: select action with the highest adjusted score\n        adjusted_scores = average_scores + adj_exploration\n        action_index = actions[np.argmax(adjusted_scores)]\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        if action_counts[action_index] > 0:\n            expected_values[action_index] = np.mean(scores)\n        else:\n            expected_values[action_index] = 0\n\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # UCB calculation with decaying exploration term\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    combined_values = expected_values + (exploration_bonus * exploration_weight)\n\n    # Select action with maximum combined value\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Add a small constant to avoid division by zero\n    epsilon = 1e-6\n    total_action_counts = action_counts + epsilon\n\n    # Calculate UCB\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n    \n    # Adjust exploration based on time left\n    time_factor = (total_time_slots - current_time_slot) / total_time_slots\n    combined_values = expected_values + exploration_bonus * time_factor\n\n    # Select action based on maximum combined value\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    num_actions = len(actions)\n    \n    # Initialize scores and counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Avoid division by zero and implement a small constant for exploration\n    counts_with_min = counts + 1e-5\n    \n    # Calculate average scores\n    average_scores = scores\n\n    # Calculate exploration bonus using UCB\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / counts_with_min)\n\n    # Dynamic exploration weight based on progress\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = average_scores + exploration_weight * exploration_bonus\n\n    # Handle the case where none of the actions have been selected\n    if total_selection_count == 0:\n        action_index = np.random.choice(actions)\n    else:\n        action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    num_actions = len(actions)\n    \n    # Initialize scores and counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Handle division by zero by adding a small constant\n    counts_with_min = counts + 1e-5\n    \n    # Calculate average scores\n    average_scores = scores\n    \n    # Calculate exploration bonuses using UCB\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / counts_with_min)\n    \n    # Calculate adaptive exploration scaling\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = average_scores + exploration_bonus * exploration_scaling\n\n    # Introduce \u03b5-greedy approach\n    epsilon = max(0.01, 1 - (current_time_slot / total_time_slots))  # \u03b5 decreases over time\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(actions)  # Explore\n    else:\n        action_index = actions[np.argmax(adjusted_scores)]  # Exploit\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate expected values and counts\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n\n    # Avoid division by zero by adding a small constant for counts\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Calculate exploration bonus using UCB\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / total_action_counts)\n\n    # Adjust exploration bonus based on time\n    time_factor = (current_time_slot + 1) / (total_time_slots + 1)\n\n    # Combine expected values with exploration bonus\n    combined_values = expected_values + exploration_bonus * time_factor\n\n    # Select the action with the highest combined value\n    action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n\n    # Calculate the mean scores and selection counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Handle cases with no selections by adding a small value\n    counts_with_min = counts + 1e-5\n\n    # Calculate average scores\n    average_scores = scores\n\n    # Explore-exploit factor: \u03b5-greedy method\n    exploration_prob = 0.1  # 10% of the time we explore\n    if np.random.rand() < exploration_prob:\n        action_index = np.random.choice(actions)\n    else:\n        # UCB calculation\n        ucb_values = average_scores + np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n\n        # Adjust UCB with a decay factor to promote exploration of all actions\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_ucb = ucb_values * time_decay_factor\n        \n        # Select the action with the highest adjusted UCB value\n        action_index = actions[np.argmax(adjusted_ucb)]\n\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    \n    # Calculate the mean scores and selection counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Handle counts safely to avoid division by zero\n    counts_with_min = counts + 1e-5\n\n    # Calculate average scores\n    average_scores = scores\n\n    # Exploration probability\n    exploration_prob = 0.1\n    if np.random.rand() < exploration_prob:\n        # Explore: random action\n        action_index = np.random.choice(actions)\n    else:\n        # Exploit: calculate UCB\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n        \n        # Time decay factor\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_scores = average_scores + exploration_bonus * time_decay_factor\n        \n        # Select the action with the highest adjusted score\n        action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate expected values and counts\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Handle zero selections by adding a small epsilon\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n    \n    # Calculate UCB components\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n    \n    # Adapt exploration parameter based on time left\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    ucb_values = expected_values + exploration_bonus * exploration_weight\n    \n    # Select the action with the highest UCB value\n    action_index = np.argmax(ucb_values)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n\n    # Handling zero counts with a small constant\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Calculate an exploration factor using UCB\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / total_action_counts)\n    \n    # Time-dependent normalization factor, for better exploration in early slots\n    time_factor = (current_time_slot + 1) / total_time_slots\n    \n    # Combining expected values with exploration bonus\n    combined_values = expected_values + exploration_bonus * time_factor\n\n    # Select the action with the highest combined value\n    action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Handle zero selections with a small epsilon\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Calculate UCB with exploration weight based on time remaining\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n    exploration_weight = np.maximum(0, (total_time_slots - current_time_slot) / total_time_slots)\n    combined_values = expected_values + exploration_bonus * exploration_weight\n\n    # Select action based on maximum combined value\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate expected values and counts\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Handle zero selections with a small epsilon\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Calculate UCB components with adaptive exploration factor\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    combined_values = expected_values + (exploration_bonus * exploration_weight)\n\n    # Select action based on maximum combined value\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    \n    # Calculate mean scores and selection counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Handle cases with no selections\n    counts_with_min = counts + 1e-5\n\n    # Calculate average scores\n    average_scores = scores / counts_with_min\n\n    # Exploration factor (\u03b5-greedy)\n    exploration_prob = 0.1  \n    if np.random.rand() < exploration_prob:\n        # Explore: Randomly select an action\n        action_index = np.random.choice(actions)\n    else:\n        # Exploit: Calculate UCB\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n        \n        # Adjust the exploration bonus based on remaining time slots\n        exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_scores = average_scores + exploration_bonus * exploration_scaling\n\n        # Select the action with the highest adjusted score\n        action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n    \n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n    \n    # Handling zero counts\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Upper Confidence Bound calculation\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / total_action_counts)\n    \n    # Time-dependent exploration factor\n    time_factor = ((current_time_slot + 1) / total_time_slots) ** 0.5\n\n    # Combined values calculation\n    combined_values = expected_values + (exploration_bonus * time_factor)\n\n    # Select action with the highest combined value\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_counts = np.zeros(num_actions)\n    expected_values = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n    \n    # To avoid division by zero\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Use UCB for exploration-exploitation balance\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / total_action_counts)\n\n    # Time-dependent normalization factor (For diminishing exploration)\n    time_factor = max(1 - (current_time_slot / total_time_slots), 0)\n\n    combined_values = expected_values + exploration_bonus * time_factor\n\n    # Select the action with the highest combined value\n    action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n    \n    # Calculate expected values and counts\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Apply epsilon for unselected actions to avoid division by zero\n    adjusted_action_counts = action_counts + 1e-5\n\n    # Calculate UCB components\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / adjusted_action_counts)\n    \n    # Normalize exploration weight based on remaining time slots\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n\n    # Calculate combined values\n    combined_values = expected_values + exploration_weight * exploration_bonus\n\n    # Select action based on maximum combined value\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Avoid division by zero by adding a small epsilon\n    epsilon = 1e-5\n\n    # Compute UCB values\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1) + 1) / (action_counts + epsilon))\n    ucb_values = expected_values + exploration_bonus\n\n    # Adaptive exploration strategy using \u03b5-greedy approach\n    base_exploration_rate = 0.1\n    adjusted_exploration_rate = base_exploration_rate * (1 - (current_time_slot / total_time_slots))\n    \n    # Random selection based on exploration rate\n    if np.random.rand() < adjusted_exploration_rate:\n        action_index = np.random.randint(0, num_actions)\n    else:\n        action_index = np.argmax(ucb_values)\n    \n    return action_index",
          "objective": -449.9999999999999,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    num_actions = len(actions)\n    \n    # Initialize scores and counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Calculate average scores and avoid division by zero in exploration adjustments\n    counts_with_min = np.maximum(counts, 1e-5)\n    \n    # Calculate exploration bonus using UCB\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / counts_with_min)\n    \n    # Implement an adaptive exploration factor influenced by time\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = scores + exploration_bonus * exploration_scaling\n\n    # Use softmax to encourage exploration while giving preference to higher scores\n    exp_scores = np.exp(adjusted_scores - np.max(adjusted_scores))  # Stabilize by subtracting max\n    probabilities = exp_scores / np.sum(exp_scores)\n    \n    # Select action based on probabilities\n    action_index = np.random.choice(actions, p=probabilities)\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    exploration_factor = np.sqrt((np.log(total_selection_count + 1) + 1) / (action_counts + 1e-5))\n\n    # Prioritize actions based on their historical performance and exploration\n    combined_values = expected_values + exploration_factor\n\n    # Apply decay weight to favor recent performance\n    decay_weight = np.exp(-current_time_slot / (total_time_slots + 1e-5))\n    adjusted_combined_values = (1 - decay_weight) * combined_values + decay_weight * expected_values\n\n    # Select the action with the maximum adjusted combined value\n    action_index = np.argmax(adjusted_combined_values)\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    num_actions = len(actions)\n    \n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Avoid division by zero\n    counts_with_min = counts + 1e-5\n    \n    # Calculating average scores and applying UCB for exploration\n    exploration_factor = 1.5\n    exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n    \n    # Apply a decay factor for exploration based on remaining time slots\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = scores + exploration_bonus * exploration_scaling\n\n    # Select action with the highest adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    epsilon = 1e-5  # Small constant to avoid division by zero\n    total_action_counts = action_counts + epsilon\n\n    # Calculate UCB values\n    ucb_values = expected_values + np.sqrt((2 * np.log(total_selection_count + 1)) / total_action_counts)\n\n    # Introduce a time-based decay factor for exploration\n    time_factor = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_ucb = ucb_values * (1 + time_factor)\n\n    # Select action based on maximum adjusted UCB value\n    action_index = np.argmax(adjusted_ucb)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    \n    # Calculate mean scores and selection counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Avoid division by zero\n    counts_with_min = counts + 1e-5\n\n    # Calculate average scores\n    average_scores = scores / counts_with_min\n\n    # Dynamic exploration factor\n    exploration_prob = np.clip(0.1 * (total_time_slots - current_time_slot) / total_time_slots, 0.01, 0.1)\n\n    if np.random.rand() < exploration_prob:\n        # Explore: Randomly select an action\n        action_index = np.random.choice(actions)\n    else:\n        # Exploit: UCB-based selection\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n        adjusted_scores = average_scores + exploration_bonus\n        \n        action_index = actions[np.argmax(adjusted_scores)]\n\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate expected values and action counts\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Handle zero selections\n    total_action_counts = action_counts + 1e-5  # small epsilon to prevent division by zero\n\n    # Compute the exploration bonus with a pivot towards late time slots\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / total_action_counts)\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n\n    # Combine expected values and exploration bonuses\n    combined_values = expected_values + (exploration_bonus * exploration_weight)\n\n    # Softmax action selection for a more exploratory approach\n    exp_values = np.exp(combined_values - np.max(combined_values))  # for numerical stability\n    probabilities = exp_values / np.sum(exp_values)\n\n    # Sample an action based on the computed probabilities\n    action_index = np.random.choice(num_actions, p=probabilities)\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Smoothing for zero selections\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Calculate UCB with exploration and exploitation balance\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n    exploration_weight = 1 - (current_time_slot / total_time_slots)\n    \n    combined_values = expected_values + (exploration_bonus * exploration_weight)\n\n    # Select the action with the highest value\n    action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    \n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    selection_counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Adding a small constant to avoid division by zero\n    counts_with_min = selection_counts + 1e-5\n\n    # Calculate average scores and UCB\n    average_scores = scores / counts_with_min\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n    \n    # Adjusting exploration based on the time slot\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = average_scores + exploration_bonus * exploration_scaling\n\n    # Select the action with highest score\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    \n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Handle zero counts by adding a small constant\n    counts_with_min = counts + 1e-5\n\n    # Calculate average scores\n    average_scores = scores\n\n    # \u03b5-greedy exploration probability\n    exploration_prob = 0.1\n    if np.random.rand() < exploration_prob:\n        action_index = np.random.choice(actions)\n    else:\n        # Calculate UCB with time decay\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n        exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n        \n        # Upper Confidence Bound adjusted scores\n        adjusted_scores = average_scores + exploration_bonus * exploration_scaling\n        \n        action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Handle zero selections with a small epsilon\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Calculate UCB components\n    ucb_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / total_action_counts)\n    combined_values = expected_values + ucb_bonus\n\n    # Calculate linear decay factor for exploration\n    decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n    combined_values *= decay_factor\n\n    # Select action based on maximum combined value\n    action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    num_actions = len(actions)\n    \n    # Calculate mean scores and counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Add a small constant to counts to avoid division by zero\n    counts_with_min = counts + 1e-5\n\n    # Calculate average scores and exploration bonus using UCB\n    average_scores = scores\n    exploration_factor = 1.5\n    exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n\n    # Adjust exploration based on time remaining\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = average_scores + exploration_bonus * exploration_scaling\n\n    # Select the action with the highest adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Handle actions that have not been selected yet\n    for action_index in range(num_actions):\n        if action_counts[action_index] == 0:\n            return action_index  # Select untested actions first\n\n    # Calculate exploration factor using UCB\n    exploration_factor = np.sqrt((2 * np.log(total_selection_count)) / action_counts)\n\n    # Calculate combined values using both expected values and exploration factors\n    combined_values = expected_values + exploration_factor\n\n    # Dynamic adjustment based on the current time slot\n    time_weight = current_time_slot / (total_time_slots + 1)\n    adjusted_combined_values = (1 - time_weight) * combined_values + time_weight * expected_values\n\n    # Select the action with the maximum adjusted combined value\n    action_index = np.argmax(adjusted_combined_values)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n    \n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Avoid division by zero\n    action_counts = np.maximum(action_counts, 1)  # Ensure no action count is zero\n\n    # Calculate exploration factor using UCB\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / action_counts)\n\n    # Introduce a diminishing exploration factor over time\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    combined_values = expected_values + exploration_bonus * exploration_weight\n\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    num_actions = len(actions)\n    \n    # Calculate scores and counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Avoid division by zero\n    counts_with_min = counts + 1e-5\n\n    # Calculate average scores\n    average_scores = scores / counts_with_min  # Normalized average to account for exploration\n\n    # Calculate exploration term using \u03b5-greedy strategy\n    epsilon = 1 / (current_time_slot + 1)  # Decaying epsilon over time\n    exploration_terms = np.random.rand(num_actions) < epsilon\n\n    # Combine exploitation and exploration\n    adjusted_scores = np.where(exploration_terms, np.random.rand(num_actions), average_scores)\n\n    # Select the action with the highest score\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Prevent division by zero by using a small constant\n    counts_with_min = np.maximum(counts, 1e-5)\n    \n    # Calculate exploitation scores (average scores)\n    exploitation_scores = scores\n    \n    # Exploration factor using UCB\n    exploration_factor = 1.5\n    exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n    \n    # Dynamic scaling of exploration based on the remaining time\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = exploitation_scores + exploration_bonus * exploration_scaling\n    \n    # Select the action with the highest adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate expected values and action counts\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Adding a small epsilon to avoid division by zero\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Calculate exploration component using UCB\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n\n    # Dynamic exploration factor based on time\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n\n    # Combined strategy of exploitation and exploration\n    combined_values = expected_values + (exploration_bonus * exploration_weight)\n\n    # Select the action with the highest value\n    action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    \n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n    \n    counts_with_min = counts + 1e-5\n\n    # Calculate average scores\n    average_scores = scores\n    \n    # Exploration probability scaling based on the current time slot\n    exploration_rate = max(0.01, (total_time_slots - current_time_slot) / total_time_slots)\n    \n    # \u03b5-greedy strategy\n    if np.random.rand() < exploration_rate:\n        # Explore: Randomly select an action\n        action_index = np.random.choice(actions)\n    else:\n        # UCB-based exploitation\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n        \n        # Calculate adjusted scores\n        adjusted_scores = average_scores + exploration_bonus\n        \n        # Select the action with the highest adjusted score\n        action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    \n    # Calculate mean scores and counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0.0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Handle zero selections\n    counts_with_min = counts + 1e-5  # Small constant to avoid division by zero\n    \n    # Calculate average scores\n    average_scores = scores / counts_with_min\n\n    # Exploration-exploitation trade-off using Upper Confidence Bound\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / counts_with_min)\n    \n    # Adjust exploration factor based on the time slot\n    decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n    ucb_scores = average_scores + decay_factor * exploration_bonus\n    \n    # Select action based on UCB scores\n    action_index = actions[np.argmax(ucb_scores)]\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        if action_counts[action_index] > 0:\n            expected_values[action_index] = np.mean(scores)\n\n    # Handle zero selections with a small epsilon to avoid division by zero\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Calculate UCB components with an adaptive exploration factor\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    combined_values = expected_values + exploration_bonus * exploration_weight\n\n    # Select action based on maximum combined value\n    action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    num_actions = len(actions)\n    \n    # Initialize scores and counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Avoid division by zero and implement a small constant for exploration\n    counts_with_min = counts + 1e-5\n    \n    # Calculate average scores\n    average_scores = scores / counts_with_min\n    \n    # Calculate exploration bonuses using a modified UCB\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / counts_with_min)\n\n    # Adjust exploration factor as time progresses\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = average_scores + exploration_bonus * exploration_scaling * 0.5\n\n    # Introduce \u03b5-greedy for more exploration\n    epsilon = 0.1\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(actions)\n    else:\n        action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate expected values and action counts from score set\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n\n    # To avoid division by zero and enhance exploration\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Explore-exploit strategy using UCB\n    ucb_values = expected_values + np.sqrt((2 * np.log(total_selection_count + 1)) / total_action_counts)\n\n    # Adjust for decay factor based on time\n    time_adjustment = (current_time_slot + 1) / total_time_slots\n    combined_scores = ucb_values * (1 - time_adjustment) + expected_values * time_adjustment\n\n    # Select action with the highest score\n    action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Safe handling for actions that haven't been selected yet\n    epsilon = 1e-5\n    counts = np.where(counts > 0, counts, epsilon)\n\n    # Calculate average scores\n    average_scores = scores\n\n    # Exploration parameter\n    exploration_factor = 1.0\n    exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / counts)\n\n    # Adjust exploration bonus scale based on remaining time slots\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = average_scores + exploration_bonus * exploration_scaling\n\n    # Select action with the highest adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Handle zero selections with a small epsilon\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Calculate UCB with adaptive exploration factor\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n    \n    # Dynamic exploration weight based on remaining time slots\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    combined_values = expected_values + (exploration_bonus * exploration_weight)\n\n    # Calculate softmax scores for more nuanced selection\n    exp_values = np.exp(combined_values - np.max(combined_values))  # stable softmax\n    probabilities = exp_values / np.sum(exp_values)\n\n    # Sample action based on probabilities\n    action_index = np.random.choice(num_actions, p=probabilities)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Prevent division by zero\n    adjusted_counts = np.where(counts > 0, counts, 1e-5)\n\n    # Calculate average scores\n    average_scores = scores\n\n    # UCB calculation\n    exploration_factor = 2.0\n    exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / adjusted_counts)\n\n    # Adaptive exploration scaling\n    remaining_time_slots = total_time_slots - current_time_slot\n    exploration_scaling = remaining_time_slots / total_time_slots\n    adjusted_scores = average_scores + exploration_bonus * exploration_scaling\n\n    # Select action with the maximum adjusted score safely\n    action_index = np.argmax(adjusted_scores)\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    \n    # Calculate the mean scores and selection counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Avoid potential division by zero\n    counts_with_min = counts + 1e-5\n    \n    # Calculate average scores\n    average_scores = scores\n\n    # Adjust exploration factor based on time\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n    \n    # Softmax function for action probabilities\n    scaled_scores = average_scores + exploration_factor * np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n    exp_scores = np.exp(scaled_scores - np.max(scaled_scores))  # Stability improvement\n    probabilities = exp_scores / np.sum(exp_scores)\n    \n    # Randomly select an action based on calculated probabilities\n    action_index = np.random.choice(actions, p=probabilities)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Set a small value for counts to avoid division by zero\n    counts_with_min = np.maximum(counts, 1e-5)\n\n    # Calculate average scores with safe handling\n    average_scores = scores / counts_with_min\n\n    # Calculate the exploration bonus using UCB\n    exploration_factor = 1.5\n    exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n    \n    # Scale exploration based on remaining time\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = average_scores + (exploration_bonus * exploration_scaling)\n\n    # Select action with the highest adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    \n    # Initialize arrays for scores and counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Handle counts by adding a small constant to avoid division by zero\n    counts_with_min = counts + 1e-5\n\n    # Calculate average scores\n    average_scores = scores / counts_with_min\n\n    # Parameters\n    exploration_prob = 0.1  # Exploration probability\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n\n    if np.random.rand() < exploration_prob:\n        # Explore: Random action selection\n        action_index = np.random.choice(actions)\n    else:\n        # Exploit: Calculate UCB for each action\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n        adjusted_scores = average_scores + (exploration_bonus * exploration_scaling)\n\n        # Select the action with the highest adjusted score\n        action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate expected values and action counts\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Handle zero selections with a small epsilon\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Modify exploration bonus using a decay factor based on time\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n    time_discount = (total_time_slots - current_time_slot) / total_time_slots\n    combined_values = expected_values + (exploration_bonus * time_discount)\n\n    # Select action based on maximum combined value\n    action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    num_actions = len(actions)\n    \n    # Initialize scores and counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Assign a minimum count to avoid division by zero\n    counts_with_min = counts + 1e-5\n\n    # Calculate exploitation scores (mean scores of actions)\n    exploitation_scores = scores\n    \n    # Adaptive exploration using UCB\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / counts_with_min)\n    adjusted_scores = exploitation_scores + exploration_bonus\n\n    # Normalize exploration impact over time\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores *= exploration_factor\n\n    # Select the action with the highest adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Handle actions that have never been selected\n    unexplored_actions = (action_counts == 0).astype(float)\n    exploration_bonus = unexplored_actions * (1 / (total_selection_count + 1e-5))\n\n    # Calculate effective exploration factor using UCB\n    exploration_factor = np.sqrt(2 * np.log(total_selection_count + 1) / (action_counts + 1e-5))\n\n    # Combine expected values and exploration factors\n    combined_values = expected_values + exploration_bonus + exploration_factor\n\n    # Apply a time decay factor to encourage timely selections\n    decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_values = combined_values * decay_factor + expected_values * (1 - decay_factor)\n\n    # Select action with maximum adjusted value\n    action_index = np.argmax(adjusted_values)\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Handle zero selections with a small epsilon\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Calculate UCB components\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n    combined_values = expected_values + exploration_bonus\n\n    # Calculate a dynamic exploration weight\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    weighted_combined_values = combined_values * exploration_weight\n\n    # Select action based on maximum weighted combined value\n    action_index = np.argmax(weighted_combined_values)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n    \n    # Calculate expected values and action counts from score_set\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Handle actions with zero selections\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n    \n    # Calculate Boltzmann exploration probabilities\n    temperature = 1.0 / (1 + (total_selection_count ** 0.5) / total_time_slots)\n    exp_values = np.exp(expected_values / temperature)\n    probabilities = exp_values / np.sum(exp_values)\n\n    # Select action based on multi-nomial sampling using Boltzmann probabilities\n    action_index = np.random.choice(num_actions, p=probabilities)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n    total_selection_count += 1  # To avoid division by zero\n\n    # Calculate the Upper Confidence Bound (UCB)\n    exploration_bonus = np.sqrt((np.log(total_selection_count)) / total_action_counts)\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    combined_values = expected_values + (exploration_bonus * exploration_weight)\n\n    # Select action based on maximum combined value\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n\n    # Handling zero counts to avoid division by zero\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Use UCB with a more aggressive exploration factor\n    ucb_exploration = np.sqrt(2 * np.log(total_selection_count + 1) / total_action_counts)\n    \n    # Time adjustment factor for better early exploration\n    time_adjustment = np.sqrt((current_time_slot + 1) / (total_time_slots + 1))\n\n    # Combine expected values and exploration\n    combined_values = expected_values + ucb_exploration * time_adjustment\n\n    # Select the action with the highest combined value\n    action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Handle zero selections with a small epsilon\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Calculate UCB components\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n\n    # Time-weighted exploration factor\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n\n    # Combine expected values and exploration bonus\n    combined_values = expected_values + (exploration_bonus * exploration_weight)\n\n    # Select action with the maximum combined value\n    action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": -449.9999999999999,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    num_actions = len(actions)\n    \n    # Initialize scores and counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Avoid division by zero and implement a small constant for exploration\n    counts_with_min = counts + 1e-5\n    \n    # Calculate average scores directly\n    average_scores = scores\n    \n    # Calculate exploration bonuses using UCB\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / counts_with_min)\n    \n    # Adjust exploration factor based on time\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = average_scores + exploration_bonus * exploration_scaling\n    \n    # To introduce randomness, implement \u03b5-greedy exploration\n    epsilon = 0.1 * exploration_scaling  # Reduce exploration as time increases\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(actions)\n    else:\n        action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    num_actions = len(actions)\n    \n    # Initialize scores and counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Avoid division by zero for counts\n    counts_with_min = counts + 1e-5\n    \n    # Calculate average scores\n    average_scores = scores\n    \n    # Exploration parameter (\u03b5)\n    exploration_rate = max(0.01, 1 - (current_time_slot / total_time_slots))  # Linearly decaying \u03b5\n    \n    # Choose a random number to decide exploration vs exploitation\n    if np.random.rand() < exploration_rate:\n        action_index = np.random.choice(actions)  # Explore\n    else:\n        # Calculate UCB scores\n        exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / counts_with_min)\n        adjusted_scores = average_scores + exploration_bonus\n        \n        action_index = actions[np.argmax(adjusted_scores)]  # Exploit\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate expected values and action counts\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Handle zero selections with a small epsilon\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # UCB with a decay factor based on remaining time slots\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n    time_factor = (total_time_slots - current_time_slot + 1) / total_time_slots\n    combined_values = expected_values + exploration_bonus * time_factor\n\n    # Select the action with the highest combined value\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    num_actions = len(actions)\n    \n    # Calculate mean scores and selection counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Assign a small value to counts to avoid division by zero\n    counts_with_min = counts + 1e-5\n    \n    # Average scores with exploration bonus (UCB approach)\n    exploration_factor = 1.5\n    exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n    \n    # Adapt exploration bonus based on remaining time\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = scores + exploration_bonus * exploration_scaling\n    \n    # Select the action with the highest adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n    \n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Handle zero counts with a minimum epsilon\n    epsilon = 1e-5\n    action_counts += epsilon\n\n    # Calculate UCB with adaptive exploration\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / action_counts)\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    combined_values = expected_values + exploration_bonus * exploration_weight\n\n    # Select the action with the maximum combined value\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n    \n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n    \n    # Handling zero counts\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Upper Confidence Bound (UCB) strategy\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / total_action_counts)\n\n    # Time-dependent normalization factor\n    time_factor = (current_time_slot + 1) / total_time_slots\n    \n    # Combining expected values with exploration bonus and time factor\n    combined_values = expected_values + exploration_bonus * time_factor\n\n    # Select the action with the highest combined value\n    action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Calculate total selections for exploitation\n    counts_with_min = counts + 1e-5\n    average_scores = scores\n\n    # Explore-exploit ratio\n    exploration_factor = 1.5\n    exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n\n    # Adjust exploration based on time\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = average_scores + exploration_bonus * exploration_scaling\n\n    # Dynamic exploration rate (\u03b5-greedy strategy)\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n    if np.random.rand() < epsilon:  # Explore\n        action_index = np.random.choice(actions)\n    else:  # Exploit\n        action_index = actions[np.argmax(adjusted_scores)]\n        \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    num_actions = len(actions)\n    \n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Avoid division by zero\n    counts_with_min = counts + 1e-5\n    \n    # Calculate exploration bonus based on UCB\n    exploration_factor = 1.5\n    exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n    \n    # Adjust bonuses dynamically based on the remaining time slots\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = scores + (exploration_bonus * exploration_scaling)\n\n    # Calculate probabilities for Softmax approach\n    exp_scores = np.exp(adjusted_scores - np.max(adjusted_scores))  # Stability improvement\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Select action based on probabilities\n    action_index = np.random.choice(actions, p=probabilities)\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Handle zero counts\n    counts_with_min = np.maximum(counts, 1)\n\n    # Calculate average scores\n    average_scores = scores\n\n    # \u03b5-greedy exploration factor\n    epsilon = max(0.1, (total_time_slots - current_time_slot) / total_time_slots)\n\n    # Probability to explore vs exploit\n    explore_probabilities = np.random.rand(len(actions)) < epsilon\n    \n    # Select randomly among actions if exploring\n    if np.any(explore_probabilities):\n        action_index = np.random.choice(np.array(actions)[explore_probabilities])\n    else:\n        # Calculate exploration bonus using UCB\n        adjustment_factor = np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n        adjusted_scores = average_scores + adjustment_factor\n        action_index = actions[np.argmax(adjusted_scores)]\n\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    \n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0.0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n    \n    counts_with_min = counts + 1e-5\n    \n    # Calculate average scores\n    average_scores = scores\n    \n    # Explore-exploit factor: \u03b5-greedy method\n    exploration_prob = 0.1  # 10% exploration\n    if np.random.rand() < exploration_prob:\n        # Explore: Randomly select an action\n        action_index = np.random.choice(actions)\n    else:\n        # Exploit: UCB modification\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n        exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_scores = average_scores + exploration_bonus * exploration_scaling\n        \n        action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    \"\"\"\n    Selects an action based on historical scores using an Upper Confidence Bound (UCB) approach with adaptive exploration.\n\n    Parameters:\n        score_set (dict): A dictionary of action indices (0-7) mapping to lists of historical scores.\n        total_selection_count (int): Total number of selections made across all actions.\n        current_time_slot (int): Current time slot index.\n        total_time_slots (int): Total number of time slots.\n\n    Returns:\n        int: The index of the selected action (0 to 7).\n    \"\"\"\n    \n    num_actions = len(score_set)  # Number of available actions\n    expected_values = np.zeros(num_actions)  # Store expected values for each action\n    action_counts = np.zeros(num_actions)  # Store counts of selections for each action\n\n    # Calculate expected values and selection counts\n    for action_index in score_set.keys():\n        scores = score_set[action_index]\n        action_counts[action_index] = len(scores)\n        if action_counts[action_index] > 0:\n            expected_values[action_index] = np.mean(scores)\n\n    # Handle the case of unselected actions\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Calculate exploration bonus using UCB\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1) + 1) / total_action_counts)\n\n    # Compute a time-dependent normalization factor for exploration\n    time_factor = ((current_time_slot + 1) / total_time_slots) ** 0.5\n\n    # Combine expected values with exploration bonus\n    combined_values = expected_values + exploration_bonus * time_factor\n\n    # Select the action with the highest combined value\n    action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    \n    # Calculate mean scores and counts of selections for each action\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Handle zero selections gracefully\n    counts_with_min = counts + 1e-5\n\n    # Calculate average scores\n    average_scores = scores / counts_with_min\n\n    # Define exploration probability and set adjustable parameter\n    exploration_prob = 0.1 if total_selection_count > 0 else 1.0  # Allow full exploration on first selection\n\n    if np.random.rand() < exploration_prob:\n        # Explore: Randomly select an action\n        action_index = np.random.choice(actions)\n    else:\n        # Exploit: Use UCB to calculate the adjusted scores\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n        exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_scores = average_scores + exploration_bonus * exploration_scaling\n\n        # Select the action with the highest adjusted score\n        action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(range(8))  # action indices from 0 to 7\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Prevent division by zero\n    counts_with_min = counts + 1e-5\n    average_scores = scores / counts_with_min\n\n    # Exploration-exploitation balance\n    exploration_prob = max(1 - current_time_slot / total_time_slots, 0.1)  # Decay exploration probability over time\n    if np.random.rand() < exploration_prob:\n        action_index = np.random.choice(actions)  # Explore\n    else:\n        ucb_bonus = np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n        adjusted_scores = average_scores + ucb_bonus\n        action_index = actions[np.argmax(adjusted_scores)]  # Exploit\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n\n    # Use a small constant to avoid division by zero\n    epsilon = 1e-5\n    adjusted_action_counts = action_counts + epsilon\n\n    # Upper Confidence Bound (UCB) strategy\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / adjusted_action_counts)\n    \n    # Time-dependent normalization factor\n    time_factor = ((current_time_slot + 1) / total_time_slots) ** 0.5\n    \n    # Combining expected values with exploration bonus\n    combined_values = expected_values + exploration_bonus * time_factor\n\n    # Select the action with the highest combined value\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    num_actions = len(actions)\n    \n    # Calculate average scores and selection counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Handle zero counts to avoid division by zero\n    counts_with_min = counts + 1e-5\n    \n    # Calculate average scores\n    average_scores = scores\n    \n    # Adaptive exploration - changing \u03b5 dynamically based on total selections\n    exploration_prob = max(0.1, 1 - (total_selection_count / (total_time_slots * 10)))  # Decrease \u03b5 over time\n    \n    if np.random.rand() < exploration_prob:\n        # Explore: Randomly select an action\n        action_index = np.random.choice(actions)\n    else:\n        # Exploit: Use UCB with adaptive bonus based on counts and remaining time\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n        exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_scores = average_scores + exploration_bonus * exploration_scaling\n        \n        # Select action with the highest adjusted score\n        action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Handle zero selections with a small epsilon\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Calculate UCB components\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n\n    # Decay exploration weight as time advances\n    exploration_weight = (total_time_slots - current_time_slot + 1) / total_time_slots\n    combined_values = expected_values + (exploration_bonus * exploration_weight)\n\n    # Select action based on maximum combined value\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Handle zero selections gracefully\n    total_action_counts = action_counts + 1  # Use 1 to avoid division by zero\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n\n    # Adjust exploration weight with consideration for time left\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    combined_values = expected_values + exploration_bonus * exploration_weight\n\n    # Select action based on maximum combined value\n    action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    \n    # Calculate the mean scores and selection counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Handle cases with no selections\n    counts_with_min = counts + 1e-5\n\n    # Calculate average scores\n    average_scores = scores\n    \n    # Dynamic exploration factor\n    exploration_probability = np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n    \n    # Decay exploration probability based on time slots\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    exploration_scores = exploration_probability * exploration_scaling\n    \n    # Combined score for action selection\n    adjusted_scores = average_scores + exploration_scores\n    \n    # Select the action with the highest adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Handle zero actions selected by adding a small epsilon\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Calculate UCB and exploration bonus\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n\n    # Combined value calculating exploitation and exploration\n    combined_values = expected_values + (exploration_bonus * exploration_weight)\n\n    # Select action by choosing the one with the highest combined value\n    action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    epsilon = 1e-5  # Small value to avoid division errors\n    total_action_counts = action_counts + epsilon\n\n    # Calculate UCB\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n    \n    # Adaptive exploration weight based on remaining time slots\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    combined_values = expected_values + exploration_bonus * exploration_weight\n\n    # Select action\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Small epsilon to avoid division by zero\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Calculate exploration bonus using UCB\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n\n    # Calculate weights for future exploration dynamics\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    \n    # Combine expected values and exploration bonuses\n    combined_scores = expected_values + (exploration_bonus * exploration_weight)\n\n    # Use a softmax approach for action selection\n    exp_scores = np.exp(combined_scores - np.max(combined_scores))  # Stability improvement\n    action_probabilities = exp_scores / np.sum(exp_scores)\n    \n    # Sample action based on computed probabilities\n    action_index = np.random.choice(num_actions, p=action_probabilities)\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    num_actions = len(actions)\n    \n    # Calculate scores and counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Prevent division by zero\n    counts_with_min = np.maximum(counts, 1e-5)\n    \n    # Calculate average scores\n    average_scores = scores\n    \n    # Exploration factor and scaling\n    exploration_factor = 1.5\n    exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n    \n    # Adaptive exploration scaling based on remaining time slots\n    remaining_slots = total_time_slots - current_time_slot\n    exploration_scaling = remaining_slots / total_time_slots\n    adjusted_scores = average_scores + exploration_bonus * exploration_scaling\n    \n    # Select action based on highest adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Avoid division by zero\n    counts_with_min = counts + 1e-5\n\n    # Calculate average scores\n    average_scores = scores\n\n    # Dynamic exploration-exploitation balance\n    exploration_factor = np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n    \n    # Weighted for exploration based on remaining time\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    \n    # Adjusted scores combining average score and exploration\n    adjusted_scores = average_scores + exploration_factor * exploration_weight\n\n    # Select the action with the highest adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n\n    # Handling zero counts with a small constant for exploration\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # UCB-based exploration term\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / total_action_counts)\n\n    # Time-dependent normalization factor\n    time_factor = ((current_time_slot + 1) / total_time_slots) ** 0.5\n    \n    # Combining expected values with exploration bonus\n    combined_values = expected_values + exploration_bonus * time_factor\n\n    # Select the action with the highest combined value\n    action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    \n    # Calculate scores and counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) if score_set[action] else 0 for action in actions])\n    \n    # Adjust counts to avoid division by zero\n    counts_with_min = counts + 1e-5\n    \n    # Calculate average scores\n    average_scores = scores\n    \n    # Exploration probability\n    exploration_prob = 0.2  # Adjusted exploration probability to encourage exploration\n    if np.random.rand() < exploration_prob:\n        # Explore: Randomly select an action\n        action_index = np.random.choice(actions)\n    else:\n        # Exploit: Calculate UCB\n        exploration_bonus = np.sqrt(2 * np.log(total_selection_count + 1) / counts_with_min)\n        \n        # Adjusting scaling factor based on how much time is left\n        time_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_scores = average_scores + exploration_bonus * time_factor\n\n        # Select the action with the highest adjusted score\n        action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Compute expected values and action counts\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Handle zero selections using a small offset\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Calculate UCB components\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n    \n    # Adaptive exploration weight based on remaining time slots\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    \n    # Combine expected values with exploration\n    combined_values = expected_values + (exploration_bonus * exploration_weight)\n\n    # Select action that maximizes combined value\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": -449.9999999999999,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n    \n    # Calculate UCB components\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n    \n    # Enhance exploration with total time slots consideration\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    \n    # Compute combined action values\n    combined_values = expected_values + exploration_bonus * exploration_weight\n\n    # Select the action with maximum combined value\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate expected values and action counts\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Handle zero selections with a small epsilon\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Calculate UCB components with adaptive exploration factor\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n\n    # Combine values for selection\n    combined_values = expected_values + (exploration_bonus * exploration_weight)\n\n    # Select action based on maximum combined value\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    total_action_counts = action_counts + 1e-5  # Handle zero selections with epsilon\n\n    # Calculate exploration bonus using UCB\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    \n    # Combine exploration and exploitation\n    combined_values = expected_values + exploration_weight * exploration_bonus\n\n    # Select action with maximum combined value\n    action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    num_actions = len(actions)\n\n    # Calculate average scores, using 0 for actions with no scores\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Avoid division by zero\n    counts_with_min = counts + 1e-5\n\n    # UCB calculation with exploration factor\n    exploration_factor = 1.5\n    exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n\n    # Linear decay factor to reduce exploration over time\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = scores + exploration_bonus * exploration_scaling\n\n    # Select the action with the highest adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    \n    # Calculate mean scores and counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Avoid division by zero by adding a small value\n    counts_with_min = counts + 1e-5\n\n    # Calculate exploration bonus using UCB\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n\n    # Dynamically adjust exploration based on remaining time\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = scores + exploration_bonus * exploration_scaling\n    \n    # Select the action with the highest adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Avoid division by zero using np.maximum\n    counts_with_min = np.maximum(counts, 1e-5)\n\n    # Calculate average scores\n    average_scores = scores\n\n    # Calculate exploration bonus (UCB)\n    exploration_factor = 1.5\n    exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n\n    # Scale exploration bonus with remaining time slots\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = average_scores + exploration_bonus * exploration_scaling\n\n    # Select action with the highest adjusted score\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    num_actions = len(actions)\n    \n    # Initialize scores and counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Avoid division by zero\n    counts_with_min = counts + 1e-5\n    \n    # Calculate average scores\n    average_scores = scores\n    \n    # Calculate exploration bonuses using UCB\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / counts_with_min)\n\n    # Adjust exploration scale based on time remaining\n    exploration_scaling = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = average_scores + exploration_bonus * exploration_scaling\n\n    # Use \u03b5-greedy method to provide a balance between exploration and exploitation\n    epsilon = 0.1 * (total_time_slots - current_time_slot) / total_time_slots  # Decay epsilon over time\n    if np.random.rand() < epsilon:  # Exploration\n        action_index = np.random.choice(actions)\n    else:  # Exploitation\n        action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    n_actions = len(actions)\n\n    # Calculate the mean scores and selection counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Add a small value to counts to avoid division by zero\n    counts_with_min = counts + 1e-5\n\n    # Calculate average scores\n    average_scores = scores\n\n    # Explore-exploit factor setup\n    exploration_prob = 1.0 / (current_time_slot + 1)  # Decreasing exploration probability over time\n    if np.random.rand() < exploration_prob:\n        # Explore: Randomly select an action\n        action_index = np.random.choice(actions)\n    else:\n        # Exploit: Calculate Upper Confidence Bound (UCB)\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n\n        # Adjust exploration bonus for diminishing returns based on time slot\n        adjusted_scores = average_scores + exploration_bonus * (total_time_slots - current_time_slot) / total_time_slots\n\n        # Select the action with the highest adjusted score\n        action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # Avoid division by zero by adding a small epsilon\n    epsilon = 1e-5\n\n    # Calculate UCB with exploration bonus\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1) + 1) / (action_counts + epsilon))\n    ucb_values = expected_values + exploration_bonus\n\n    # Dynamic balance between exploration and exploitation\n    exploration_parameter = 0.1  # Controls exploration level\n    adjusted_values = (1 - exploration_parameter) * ucb_values + exploration_parameter * expected_values\n\n    # Select action with the highest adjusted value\n    action_index = np.argmax(adjusted_values)\n\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    \n    # Calculate the mean scores and number of selections for each action\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n\n    # Handle cases with no selections\n    counts_with_min = counts + 1e-5\n\n    # Exploration-exploitation trade-off using UCB\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n    \n    # Decay the exploration bonus over time\n    time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = scores + exploration_bonus * time_decay_factor\n\n    # Incorporate randomness to balance exploration\n    epsilon = 0.05  # 5% chance to explore\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(actions)\n    else:\n        action_index = actions[np.argmax(adjusted_scores)]\n\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    expected_values = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate expected values and counts\n    for action_index, scores in score_set.items():\n        if scores:\n            expected_values[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n\n    # To avoid division by zero\n    epsilon = 1e-5\n    total_action_counts = action_counts + epsilon\n\n    # Calculate exploration term and dynamic weight\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1)) / total_action_counts)\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    combined_values = expected_values + (exploration_bonus * exploration_weight)\n\n    # Select the action with maximum combined value\n    action_index = np.argmax(combined_values)\n\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    counts = np.array([len(score_set[action]) for action in actions])\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n\n    # Handle cases with no selections\n    counts_with_min = counts + 1e-5\n    \n    # Calculate the average scores\n    average_scores = scores\n    \n    # UCB Calculation with exploration term\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / counts_with_min)\n    \n    # Time decay factor\n    time_decay = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = average_scores + exploration_bonus * time_decay\n\n    # Select action based on UCB scores\n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    num_actions = len(actions)\n\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    counts = np.array([len(score_set[action]) for action in actions])\n    \n    # Avoid division by zero\n    counts_with_min = counts + 1e-5 \n    \n    # Calculate the exploration factor using an \u03b5-greedy approach\n    epsilon = 0.1  # 10% exploration rate\n    adjusted_scores = np.where(np.random.rand(num_actions) < epsilon,\n                                np.random.rand(num_actions),  # Random exploration\n                                scores + np.sqrt((2 * np.log(total_selection_count + 1)) / counts_with_min))  # UCB exploitation\n    \n    action_index = actions[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     }
]