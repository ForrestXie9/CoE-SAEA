[
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration rate\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score, handling division by zero\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration term based on remaining time slots\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Calculate combined score for action selection\n        combined_score = average_score + (epsilon * exploration_value)\n        \n        # Add context based on current time slot\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    # Select action with highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration factor\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Handling case where no selections have been made for an action\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Exploration value\n        if selection_count > 0:\n            exploration_value = np.sqrt(np.log(total_selection_count) / selection_count)\n        else:\n            exploration_value = float('inf')  # Encourage exploration if action has never been selected\n        \n        # Combined score with epsilon for randomness in exploration\n        combined_score = average_score + epsilon * exploration_value\n        action_scores.append(combined_score)\n    \n    # Implement epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, 8)  # Explore: random selection\n    else:\n        action_index = np.argmax(action_scores)  # Exploit: choose best combined score\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.3  # Exploration parameter adjustment\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration term\n        if selection_count > 0:\n            exploration_value = np.sqrt(np.log(total_selection_count + 1) / selection_count)\n        else:\n            exploration_value = float('inf')  # Infinite exploration for untried actions\n        \n        # Compute combined score\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    # Return action with the maximum score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    epsilon = 0.1  # Exploration parameter\n    action_scores = []\n    \n    # Calculate the total possible exploration weight as a function of remaining time slots\n    time_weight = (total_time_slots - current_time_slot) / total_time_slots\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n\n        # UCB exploration value calculation\n        if selection_count > 0:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count)) / selection_count)\n        else:\n            exploration_value = float('inf')  # Allocate exploration to actions not yet tried\n        \n        # Calculate combined score with exploration weight\n        combined_score = average_score + epsilon * exploration_value * time_weight\n        action_scores.append(combined_score)\n    \n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 1.0  # Adjusted exploration parameter for better exploration-exploitation trade-off\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration term with a dynamic adjustment based on overall selections\n        if selection_count > 0:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_count)\n        else:\n            exploration_value = float('inf')  # Encourage exploration for untried actions\n        \n        # Compute combined score\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    # Return action with the maximum score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.1  # Exploration parameter\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0.0\n        \n        # Calculate exploration term\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combined score for action selection\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 1.0 / (current_time_slot + 1)  # Decaying exploration rate\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score and handle division by zero\n        average_score = np.mean(scores) if scores else 0\n        \n        # Exploration term using UCB method\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Epsilon-Greedy approach to balance exploration and exploitation\n        combined_score = (1 - epsilon) * average_score + epsilon * exploration_value\n        action_scores.append(combined_score)\n    \n    # Select action with the highest combined score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Define parameters for exploration/exploitation\n    epsilon = 0.1  # Epsilon for exploration\n    action_indices = list(range(len(score_set)))\n    \n    # Calculate the average scores and selection counts for each action\n    action_avg_scores = np.zeros(len(action_indices))\n    for action_index in action_indices:\n        scores = score_set.get(action_index, [])\n        action_avg_scores[action_index] = np.mean(scores) if scores else 0\n\n    # UCB exploration factor\n    exploration_values = np.zeros(len(action_indices))\n    for action_index in action_indices:\n        selection_count = len(score_set.get(action_index, []))\n        exploration_values[action_index] = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n\n    # Combine average scores with exploration using a weighted approach\n    combined_scores = (1 - epsilon) * action_avg_scores + epsilon * exploration_values\n    \n    # Select the action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.1  # Exploration parameter for balancing exploration and exploitation.\n    action_scores = []\n    total_actions = len(score_set)\n    \n    for action_index in range(total_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate the exploration term using a simple epsilon-greedy approach.\n        # Define epsilon that decreases over time to encourage more exploitation as time goes on.\n        epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n        \n        # Exploration term calculated using UCB\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Calculate adjusted score with both exploitation and exploration.\n        combined_score = (1 - epsilon) * average_score + epsilon * exploration_param * exploration_value\n        action_scores.append(combined_score)\n\n    # Select the action with the highest score.\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.1  # exploration coefficient\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration factor\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Calculate final score combining exploitation and exploration\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.1  # Exploration parameter\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score, handle division by zero\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Explore less selected actions with UCB\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combined score\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    epsilon = 0.1  # Exploration parameter\n    action_scores = []\n    exploration_factor = 1.0  # Adjustable factor for exploration weight\n\n    # Calculate time-based exploration weight\n    if total_time_slots > 1:\n        time_weight = (total_time_slots - current_time_slot) / total_time_slots\n    else:\n        time_weight = 1.0\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0.0\n\n        # Calculate exploration value using modified UCB\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n\n        # Combined score with a dynamic balance between exploration and exploitation\n        combined_score = average_score + (epsilon * exploration_value * time_weight)\n        action_scores.append(combined_score)\n\n    # Select action based on the highest combined score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.1  # Exploration parameter\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score, handling the case when selection_count is 0\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration term\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Calculate combined score for action selection\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Define a dynamic exploration factor that decreases over time\n    epsilon = max(0.01, 0.1 * (1 - current_time_slot / total_time_slots))\n    action_count = 8\n    action_scores = np.zeros(action_count)\n    \n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        average_score = np.mean(scores) if selection_count > 0 else 0\n        exploration_value = np.sqrt(np.log(total_selection_count) / (selection_count + 1e-5))\n        \n        action_scores[action_index] = average_score + epsilon * exploration_value\n    \n    # Implement an epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)  # Explore: random selection\n    else:\n        action_index = np.argmax(action_scores)  # Exploit: choose best combined score\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_factor = 0.1  # Exploration rate, adjust as needed\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration term using UCB\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combine average score with exploration value\n        combined_score = average_score + exploration_factor * exploration_value\n        \n        # Adjust score based on time decay\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    # Select the action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.1  # Exploration factor\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score with a safeguard against division by zero\n        average_score = np.mean(scores) if selection_count > 0 else 0.0\n        \n        # Calculate the exploration term with safeguards\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combined score for action selection\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    # Select action index with the maximum combined score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.5  # Increased exploration parameter for better exploration\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score with handling for untried actions\n        average_score = np.mean(scores) if selection_count > 0 else 0\n\n        # Use more robust exploration term\n        exploration_value = (np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1e-5)) \n                             if selection_count > 0 else float('inf'))\n        \n        # Compute combined score\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    # Return the action with the maximum score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 1.0 / (current_time_slot + 1)  # Decaying exploration rate\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score, handling division by zero\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Use a Bayesian-like exploration method based on the standard deviation\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combined score with exploration\n        combined_score = (1 - epsilon) * average_score + epsilon * exploration_value\n        action_scores.append(combined_score)\n    \n    # Select action with the highest combined score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_factor = 0.1\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Upper Confidence Bound calculation\n        if selection_count > 0:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count)) / selection_count)\n        else:\n            exploration_value = float('inf')  # Encourage exploration\n        \n        combined_score = average_score + exploration_factor * exploration_value\n        action_scores.append(combined_score)\n    \n    # Select the action with the maximum combined score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration rate\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate the average score, handling division by zero\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration term using UCB\n        if selection_count > 0:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1))\n        else:\n            exploration_value = float('inf')  # Favor exploration\n        \n        # Calculate adjusted score using a decay factor based on current time slot\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        combined_score = average_score + (epsilon * exploration_value)\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    # Select action with highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    epsilon = 0.1  # Exploration parameter\n    action_scores = []\n    \n    # Calculate the time-based weight for exploration\n    time_weight = (total_time_slots - current_time_slot) / total_time_slots\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n\n        # UCB exploration value calculation\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combined score with exploration weight\n        combined_score = average_score + epsilon * exploration_value * time_weight\n        action_scores.append(combined_score)\n    \n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.1  # Exploration parameter\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0.0\n        \n        # Calculate exploration term\n        if selection_count > 0:\n            exploration_value = np.sqrt(np.log(total_selection_count) / selection_count)\n        else:\n            exploration_value = float('inf')\n        \n        # Combined score for action selection\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n\n    # Select the action with the highest combined score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration rate\n    action_count = 8\n    action_scores = np.zeros(action_count)\n    \n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score, handling division by zero\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration term based on remaining time slots\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combine average score and exploration value\n        combined_score = average_score + (epsilon * exploration_value)\n        \n        # Apply time decay factor\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores[action_index] = adjusted_score\n    \n    # Select the action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration parameter\n    action_scores = []\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration term\n        exploration_value = (np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1e-5))\n                             if selection_count > 0 else float('inf'))\n\n        # Compute combined score using epsilon-greedy strategy\n        combined_score = (average_score * (1 - epsilon) +\n                          (np.random.rand() * epsilon) * exploration_value)\n        action_scores.append(combined_score)\n\n    # Select action with the maximum score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.5  # Exploration parameter adjustment\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration term\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1e-5))  # Small epsilon to avoid division by zero\n        \n        # Compute combined score\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    # Return action with the maximum score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Epsilon for exploration\n    epsilon = 0.1\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if scores else 0.0\n        \n        # Calculate exploration term (UCB)\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combine average score with exploration value\n        combined_score = average_score + (epsilon * exploration_value)\n        \n        # Apply time decay factor\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n\n        action_scores.append(adjusted_score)\n    \n    # Select action with highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    exploration_weight = min(1.0, 1.0 / (current_time_slot + 1))  # Epsilon for exploration decay\n    action_scores = []\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration term using UCB\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combined score calculation for exploration-exploitation balance\n        combined_score = (1 - exploration_weight) * average_score + exploration_weight * exploration_value\n        action_scores.append(combined_score)\n    \n    # Select action with the highest combined score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters\n    exploration_factor = 0.1  # Exploration probability\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Average score for the action\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Upper Confidence Bound calculation\n        if selection_count > 0:\n            confidence_interval = np.sqrt((2 * np.log(total_selection_count)) / selection_count)\n        else:\n            confidence_interval = float('inf')  # Encourage exploration for unselected actions\n        \n        # Combined score for UCB\n        combined_score = average_score + confidence_interval\n        \n        action_scores.append(combined_score)\n    \n    # Epsilon-greedy selection\n    if np.random.rand() < exploration_factor:\n        action_index = np.random.randint(0, 8)  # Random selection (exploration)\n    else:\n        action_index = np.argmax(action_scores)  # Greedy selection (exploitation)\n        \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    alpha = 0.1  # Exploration factor\n    action_count = 8\n    action_scores = np.zeros(action_count)\n    \n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score, avoiding division by zero\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Compute exploration bonus using Upper Confidence Bound (UCB)\n        if selection_count > 0:\n            exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_count)\n        else:\n            exploration_bonus = float('inf')  # Encourage exploration for untried actions\n        \n        # Combine average score and exploration bonus\n        combined_score = average_score + alpha * exploration_bonus\n        \n        # Incorporate time decay factor\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        action_scores[action_index] = combined_score * time_decay_factor\n\n    # Select the action with the highest adjusted score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Exploration factor\n    epsilon = 0.05  # Reduced epsilon for more exploitation\n    \n    # Calculate scores for each action\n    action_scores = np.zeros(8)\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score, handling division by zero\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Exploration term using adjusted UCB\n        if selection_count > 0:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count)) / selection_count)\n        else:\n            exploration_value = float('inf')  # Encourage exploration for unselected actions\n            \n        # Calculate combined score for action selection\n        combined_score = average_score + (epsilon * exploration_value)\n        \n        # Add time decay factor for diminishing future rewards\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores[action_index] = adjusted_score\n    \n    # Select action with the highest adjusted score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    beta = 0.2  # Exploration weight\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration term using UCB\n        exploration_value = (np.sqrt(2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Calculate combined score\n        combined_score = average_score + beta * exploration_value\n        \n        # Apply time decay factor to prioritize early selections\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    # Select the action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.5  # Adjust exploration parameter to balance exploration and exploitation\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score with fallback for untried actions\n        average_score = np.mean(scores) if scores else 0\n        \n        # Exploration term to encourage selection of underexplored actions\n        exploration_value = (np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1))) if selection_count > 0 else float('inf')\n        \n        # Combine average score with exploration factor\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    # Return the action index with the highest combined score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    beta = 0.1  # Exploitation-Exploration balance factor\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score, handling division by zero\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate confidence range (UCB)\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combined UCB score\n        ucb_score = average_score + beta * exploration_value\n        \n        # Apply time decay factor\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = ucb_score * time_decay_factor\n\n        action_scores.append(adjusted_score)\n        \n    # Select action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(range(8))\n    \n    # Calculate average scores for each action\n    action_avg_scores = np.zeros(len(action_indices))\n    for action_index in action_indices:\n        scores = score_set.get(action_index, [])\n        action_avg_scores[action_index] = np.mean(scores) if scores else 0\n\n    # Handle exploration with UCB or a modified approach\n    exploration_values = np.zeros(len(action_indices))\n    for action_index in action_indices:\n        selection_count = len(score_set.get(action_index, []))\n        exploration_values[action_index] = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n\n    # Combined score using weighted sum\n    weights = np.clip(1 - (current_time_slot / total_time_slots), 0, 1)  # Giving more weight to exploration as time progresses\n    combined_scores = weights * exploration_values + (1 - weights) * action_avg_scores\n    \n    # Select the action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 1.0\n    action_scores = []\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration term with dynamic modification\n        if selection_count > 0:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_count)\n        else:\n            exploration_value = float('inf')  # Fully explore untried actions\n        \n        # Compute combined score with a potential bias based on time slot\n        time_factor = (total_time_slots - current_time_slot) / total_time_slots\n        combined_score = average_score + exploration_param * exploration_value * time_factor\n        \n        action_scores.append(combined_score)\n    \n    # Return action with the maximum score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_weight = 1.0  # Exploration weight for UCB\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score, handling division by zero\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration term using UCB\n        exploration_value = np.sqrt((exploration_weight * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combined score leveraging both exploitation and exploration\n        combined_score = average_score + exploration_value\n        \n        # Adjust score based on time decay\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n\n    # Select action with the highest adjusted score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Constants for exploration\n    epsilon = 0.1  # Exploration factor\n    alpha = 0.03  # UCB exploration factor\n\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Explore or exploit\n        if selection_count > 0:\n            exploration_value = alpha * np.sqrt(np.log(total_selection_count) / selection_count)\n        else:\n            exploration_value = float('inf')  # Encourage exploration if action has never been selected\n        \n        # Combined score calculation\n        combined_score = average_score + exploration_value\n        action_scores.append(combined_score)\n    \n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, 8)  # Explore: random selection\n    else:\n        action_index = np.argmax(action_scores)  # Exploit: choose best combined score\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.5  # Adjusted exploration parameter for better balance\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration term\n        if selection_count > 0:\n            exploration_value = np.sqrt((np.log(total_selection_count + 1)) / selection_count)\n        else:\n            exploration_value = float('inf')  # Untried actions have infinite exploration value\n        \n        # Compute combined score using a modified UCB approach\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    # Return the action index with the maximum score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Define constants for exploration and exploitation balancing\n    epsilon_start = 1.0\n    epsilon_decay = 0.99\n    epsilon_threshold = 0.1\n    \n    # Compute the epsilon value based on decay\n    epsilon = max(epsilon_start * (epsilon_decay ** current_time_slot), epsilon_threshold)\n    \n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score with protection against division by zero\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Use UCB for exploration component\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combined score with an epsilon-greedy approach\n        combined_score = (1 - epsilon) * average_score + epsilon * exploration_value\n        action_scores.append(combined_score)\n    \n    # Select action with the highest combined score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Define exploration parameter\n    epsilon = 0.1\n    \n    # Prepare action scores\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score, handling division by zero\n        average_score = np.mean(scores) if scores else 0.0\n\n        # Calculate exploration term based on selection counts\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n\n        # Calculate the total score for the action combining average and exploration\n        total_score = average_score + (epsilon * exploration_value)\n\n        # Discount score based on time slot\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = total_score * time_decay_factor\n\n        # Append adjusted score for that action\n        action_scores.append(adjusted_score)\n\n    # Select the action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 1.0  # Exploration parameter\n    action_scores = []\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration term\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Compute combined score\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n\n    # Return action with the maximum score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    epsilon = 1.0 / (current_time_slot + 1)  # Decaying exploration rate\n    action_scores = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        selection_counts[action_index] = selection_count\n        \n        # Calculate average score and handle division by zero\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration term using UCB\n        if selection_count > 0:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1))\n        else:\n            exploration_value = float('inf')  # Infinite exploration for untried actions\n\n        # Combined score with exploration-exploitation trade-off\n        combined_score = (1 - epsilon) * average_score + epsilon * exploration_value\n        action_scores[action_index] = combined_score\n            \n    # Select action with the highest combined score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters for exploration vs exploitation\n    exploration_weight = 1.0  # Adjust this to tune exploration\n    decay_factor = 0.5  # Adjust how much to down-weight future potential\n    \n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score with safety check for division by zero\n        average_score = np.mean(scores) if scores else 0\n\n        # Compute exponential decay for future consideration\n        time_decay = decay_factor ** (total_time_slots - current_time_slot)\n        \n        # Calculate exploration term using UCB formula\n        if selection_count > 0:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count)) / selection_count)\n        else:\n            exploration_value = float('inf')  # Exploratory push for unselected actions\n        \n        # Combine scores for action selection\n        combined_score = average_score + (exploration_weight * exploration_value)\n        adjusted_score = combined_score * time_decay\n        \n        action_scores.append(adjusted_score)\n    \n    # Select the action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_factor = 1.5  # Factor to adjust exploration strength\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Handling case where no selections have been made for an action\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Compute exploration value\n        exploration_value = np.sqrt((np.log(total_selection_count + 1) / (selection_count + 1))) if selection_count > 0 else float('inf')\n        \n        # Compute combined score for action\n        combined_score = average_score + exploration_factor * exploration_value\n        action_scores.append(combined_score)\n    \n    # Select action with the highest score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.1  # Exploration parameter\n    action_scores = []\n    total_actions = len(score_set)\n\n    for action_index in range(total_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate epsilon that decreases over time\n        epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n        \n        # The exploration term using UCB\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combined score with hybrid weighting for exploration vs exploitation\n        combined_score = (1 - epsilon) * average_score + epsilon * exploration_param * exploration_value\n        action_scores.append(combined_score)\n\n    # Select the action with the highest score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration probability\n    action_scores = []\n    total_actions = 8\n    \n    for action_index in range(total_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate upper confidence bound\n        if selection_count > 0:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_count)\n        else:\n            exploration_value = float('inf')  # Prioritize untried actions\n            \n        # Combined score using UCB\n        combined_score = average_score + exploration_value\n        action_scores.append(combined_score)\n\n    # Epsilon-greedy strategy for selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(total_actions)  # Explore random action\n    else:\n        action_index = np.argmax(action_scores)  # Exploit best action\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Adjustable parameters\n    exploration_constant = 1.5  # Controls exploration level\n    epsilon = 0.1  # Exploration rate\n    \n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate the average score, defaulting to zero if no scores exist\n        average_score = np.mean(scores) if scores else 0\n\n        # Calculate exploration term based on UCB approach\n        exploration_value = (exploration_constant * np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1))) if selection_count > 0 else float('inf')\n        \n        # Combine average score with exploration value\n        combined_score = average_score + exploration_value\n        \n        # Adjust score based on the time decay factor\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    # Epsilon-greedy exploration \n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(range(8))  # randomly select an action\n    else:\n        action_index = np.argmax(action_scores)  # select action with highest score\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize parameters\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    # Populate selection counts and scores\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if scores else 0\n        action_scores[action_index] = average_score\n\n    # Total time slots for scaling purposes\n    scaling_factor = (total_time_slots - current_time_slot) / total_time_slots if total_time_slots > 0 else 1\n\n    # Exploration-exploitation balance\n    for action_index in range(num_actions):\n        if selection_counts[action_index] == 0:\n            exploration_value = float('inf')  # Encourage exploration for unselected actions\n        else:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts[action_index] + 1))\n        \n        # Combined score considers both average score and exploration\n        action_scores[action_index] += scaling_factor * exploration_value\n\n    # Select action with the highest combined score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Define exploration parameter\n    epsilon = 0.1  # Exploration factor\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score, handling division by zero\n        average_score = np.mean(scores) if scores else 0.0\n        \n        # Implement Upper Confidence Bound (UCB) for exploration\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combine scores for final action selection\n        combined_score = average_score + (epsilon * exploration_value)\n        \n        # Temporal decay factor to encourage earlier selections\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    # Select action with highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    alpha = 1.0  # prior belief of success\n    beta = 1.0   # prior belief of failure\n    k = len(score_set)  # number of actions is fixed to 8\n\n    action_scores = []\n    \n    for action_index in range(k):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score, handling empty scores to avoid division by zero\n        average_score = np.mean(scores) if scores else 0\n        \n        # Update prior beliefs\n        successful_count = sum(1 for s in scores if s > 0)\n        unsuccessful_count = selection_count - successful_count\n        \n        # Apply Thompson Sampling/Beta distribution\n        sampled_theta = np.random.beta(successful_count + alpha, unsuccessful_count + beta)\n        \n        # Combine average score with sampled theta\n        combined_score = average_score + sampled_theta\n        \n        # Adjust score for time decay\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n\n    # Select action with the highest adjusted score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_factor = 1.0\n    action_count = len(score_set)\n    action_scores = []\n    \n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration value using UCB\n        if selection_count > 0:\n            exploration_value = exploration_factor * np.sqrt(np.log(total_selection_count) / selection_count)\n        else:\n            exploration_value = float('inf')  # Encourage exploration if action has never been selected\n        \n        # Combined score\n        combined_score = average_score + exploration_value\n        action_scores.append(combined_score)\n    \n    # Select action with the highest score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    exploration_factor = 0.1  # Exploration parameter\n    action_scores = []\n\n    # Calculate time-based adjustment for exploitation\n    time_weight = (total_time_slots - current_time_slot) / total_time_slots\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score and handle zero selections\n        average_score = np.mean(scores) if selection_count > 0 else 0\n\n        # UCB exploration value calculation\n        exploration_value = (\n            np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1))\n            if selection_count > 0 else float('inf')  # Encourage exploration of untried actions\n        )\n\n        # Combine average score and exploration term\n        combined_score = average_score + exploration_factor * exploration_value * time_weight\n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)  # Select action with highest score\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters\n    exploration_factor = 0.1  # Small constant for exploration\n    exploration_weight = 2.0  # Weight for UCB explorations\n    \n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # UCB exploration value\n        exploration_value = (\n            np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1))\n            if selection_count > 0 else\n            float('inf')  # Encourage exploration if action has never been selected\n        )\n        \n        # Combined score for the action using UCB\n        combined_score = average_score + exploration_weight * exploration_value\n        action_scores.append(combined_score)\n    \n    # Select action by choosing the one with the highest combined score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Define exploration and exploitation weights\n    exploration_param = 1.0\n    exploitation_param = 0.5\n\n    action_scores = []\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0.0\n\n        # Calculate exploration term\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n\n        # Dynamic adjustment based on current time slot to balance exploration and exploitation\n        time_ratio = current_time_slot / total_time_slots\n        combined_score = (exploitation_param * average_score) + (exploration_param * (1 - time_ratio) * exploration_value)\n\n        action_scores.append(combined_score)\n\n    # Return action with the maximum score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_factor = 1.0  # Base exploration factor for the UCB strategy\n    total_actions = 8\n    action_scores = []\n    \n    for action_index in range(total_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Handling case where no selections have been made for an action\n        if selection_count > 0:\n            average_score = np.mean(scores)\n        else:\n            average_score = 0\n        \n        # UCB exploration term\n        exploration_value = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combined score for UCB\n        combined_score = average_score + exploration_value\n        action_scores.append(combined_score)\n    \n    # Select the action with the highest score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    alpha = 1.5  # Exploration exploitation trade-off\n    epsilon = 0.1  # Base epsilon-greedy exploration\n    action_scores = []\n\n    # Dynamic exploration weight adjustment\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n\n        # UCB exploration value calculation\n        if selection_count > 0:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count)) / selection_count)\n        else:\n            exploration_value = float('inf')  # Prioritize untried actions\n\n        # Calculate combined score with exploration weight\n        combined_score = average_score + (epsilon * exploration_value * exploration_weight * alpha)\n        action_scores.append(combined_score)\n    \n    action_index = np.argmax(action_scores)  # Select action with maximum score\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration rate\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)  # Number of times the action has been selected\n        \n        # Calculate average score for the action\n        average_score = np.mean(scores) if scores else 0.0\n        \n        # Compute exploration term using UCB approach\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Calculate adjusted score\n        combined_score = average_score + (epsilon * exploration_value)\n        \n        # Apply time decay factor to prioritize earlier actions\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    # Select action with the highest adjusted score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_rate = 0.1  # Initial exploration rate\n    adjusted_exploration_rate = exploration_rate * (1 - current_time_slot / total_time_slots)\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if scores else 0\n\n        # Calculate exploration term\n        if selection_count > 0:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count)) / selection_count)\n        else:\n            exploration_value = float('inf')  # Ensure exploration for unselected actions\n        \n        # Calculate combined score for action selection\n        combined_score = average_score + adjusted_exploration_rate * exploration_value\n        \n        # Add context based on time decay\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    # Select action with highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration rate\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score, handling division by zero\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration term based on remaining time slots and selection count\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Calculate combined score for action selection\n        combined_score = average_score + (epsilon * exploration_value)\n        \n        # Adjust score for time decay\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    # Select action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize variables\n    epsilon = max(0.1, 0.9 * (1 - current_time_slot / total_time_slots))  # Decaying exploration factor\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        average_score = np.mean(scores) if scores else 0\n        \n        # Use UCB for exploration\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Calculate combined score\n        combined_score = (1 - epsilon) * average_score + epsilon * exploration_value\n        action_scores.append(combined_score)\n    \n    # Select action with the highest combined score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration rate\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score carefully to handle empty scores\n        average_score = np.mean(scores) if scores else 0.0\n        \n        # Calculate exploration term based on selection counts\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Calculate decay factor based on time\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        \n        # Combine average score and exploration term with time decay\n        adjusted_score = (average_score + epsilon * exploration_value) * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    # Select the action with the highest adjusted score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration rate\n    action_count = len(score_set)  # Total number of actions\n    action_scores = np.zeros(action_count)\n    exploration_factors = np.zeros(action_count)\n    \n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score, handling division by zero\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration term based on remaining time slots\n        exploration_factor = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combine average score with exploration term\n        combined_score = average_score + (epsilon * exploration_factor)\n        \n        # Adjust for the time decay factor to prioritize early selections \n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        \n        # Store score for action\n        action_scores[action_index] = combined_score * time_decay_factor\n        \n    # Select action with highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Hyperparameters\n    exploration_param = 0.5  # Exploration factor for balancing\n    beta = 1.5  # Weighting factor for balancing exploration and exploitation\n    \n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration term\n        if selection_count > 0:\n            exploration_value = np.sqrt(np.log(total_selection_count + 1) / selection_count)\n        else:\n            exploration_value = float('inf')  # Mark untried actions as high exploration potential\n        \n        # Compute combined score with adjusted exploration parameter\n        combined_score = average_score + exploration_param * exploration_value\n        \n        # Apply a decay factor based on the remaining time slots to bias towards immediate rewards\n        time_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * (1 + beta * time_factor)\n        \n        action_scores.append(adjusted_score)\n    \n    # Return action with the maximum adjusted score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores and selection counts for each action\n    action_scores = []\n    total_actions = len(score_set)\n\n    for action_index in range(total_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        average_score = np.mean(scores) if selection_count > 0 else 0\n\n        # Epsilon that decreases over time\n        epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n\n        # UCB exploration value\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n\n        # Combine scores\n        combined_score = (1 - epsilon) * average_score + epsilon * exploration_value\n        action_scores.append(combined_score)\n\n    # Select action with the highest adjusted score\n    action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize action scores\n    action_scores = np.zeros(8)\n    action_counts = np.zeros(8)\n\n    # Iterate through each action to compute average scores and counts\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if action_counts[action_index] > 0 else 0\n        \n        # Apply UCB formula for exploration term\n        if action_counts[action_index] > 0:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / action_counts[action_index])\n        else:\n            exploration_value = float('inf')  # Encourage exploration for untried actions\n        \n        # Combined score: average score + exploration term\n        action_scores[action_index] = average_score + exploration_value\n\n    # Select action with the maximum score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    epsilon = 0.05  # Exploration parameter\n    alpha = 0.95  # Sensitivity to historical performance\n    action_scores = []\n    \n    # Calculate time decay factor \n    time_weight = (total_time_slots - current_time_slot) / total_time_slots\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score with a decay on older scores\n        if selection_count > 0:\n            average_score = np.mean(scores) * (1 - epsilon) + epsilon * np.random.uniform(0, 1)\n        else:\n            average_score = np.random.uniform(0, 1)  # Random value for untried actions\n\n        # UCB calculation\n        exploration_value = (np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) \n                             if selection_count > 0 else float('inf'))\n        \n        # Calculate combined score with exploitation and exploration\n        combined_score = average_score + exploration_value * time_weight\n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.3\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration term\n        if selection_count > 0:\n            exploration_value = np.sqrt(np.log(total_selection_count) / selection_count)\n        else:\n            exploration_value = float('inf')\n        \n        # Adjust exploration parameter based on time\n        adaptive_exploration = exploration_param * (total_time_slots - current_time_slot) / total_time_slots\n        \n        # Compute combined score\n        combined_score = average_score + adaptive_exploration * exploration_value\n        action_scores.append(combined_score)\n    \n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.1  # Exploration parameter\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0.0\n        \n        # Calculate exploration term using Upper Confidence Bound\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combined score for action selection\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    # Select action with the highest combined score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Define dynamic exploration parameter based on current_time_slot\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n    \n    action_scores = []\n    total_actions = len(score_set)\n    \n    for action_index in range(total_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        average_score = np.mean(scores) if selection_count > 0 else 0\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combine scores with exploration-exploitation adjustment\n        combined_score = average_score + (epsilon * exploration_value)\n        \n        action_scores.append(combined_score)\n    \n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_weight = 0.5  # Weight to balance between exploration and exploitation.\n    action_scores = []\n    total_actions = len(score_set)\n    \n    for action_index in range(total_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Epsilon decay function\n        epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n        \n        # UCB: Upper Confidence Bound\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combined score calculation\n        combined_score = (1 - exploration_weight) * average_score + exploration_weight * exploration_value\n\n        # Adding exploration based on epsilon\n        if np.random.rand() < epsilon:\n            combined_score += np.random.rand() * exploration_weight  # Encourage exploration\n        \n        action_scores.append(combined_score)\n\n    # Select the action with the highest score.\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.2  # Enhanced exploration parameter\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0.0\n        \n        # Calculate exploration term with a slightly different approach\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combined score for action selection\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.1  # Exploration parameter\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0.0\n        \n        # Calculate exploration term with a small epsilon adjustment for unexplored actions\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Balanced score for action selection\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    # Select action with the highest combined score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize parameters\n    num_actions = 8\n    epsilon = max(0.01, 1.0 - (current_time_slot / total_time_slots))  # Decaying exploration rate\n    action_scores = []\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score\n        average_score = np.mean(scores) if scores else 0.0\n        \n        # Exploration term: UCB\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combined score: include exploration with a decay in epsilon\n        combined_score = (1 - epsilon) * average_score + epsilon * exploration_value\n        action_scores.append(combined_score)\n    \n    # Select the action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.2  # Increased exploration coefficient for better balance\n    action_scores = []\n    \n    # Calculate the exploration term for each action\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if scores else 0\n        \n        # Handle cases with zero selections to avoid division by zero\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combined score for analysis\n        action_score = average_score + exploration_param * exploration_value\n        action_scores.append(action_score)\n    \n    # Select action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration factor\n    action_scores = []\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration value using UCB\n        if selection_count > 0:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_count)\n        else:\n            exploration_value = float('inf')\n        \n        # Use epsilon-greedy method for exploration-exploitation trade-off\n        if np.random.rand() < epsilon:\n            action_scores.append(float('inf'))  # Fully explore untried actions\n        else:\n            combined_score = average_score + exploration_value\n            action_scores.append(combined_score)\n\n    # Return action with the maximum score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Adaptive exploration rate\n    epsilon = 0.1\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        if selection_count > 0:\n            average_score = np.mean(scores)\n        else:\n            average_score = 0\n\n        # Calculate exploration term with UCB\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combined score\n        combined_score = average_score + (epsilon * exploration_value)\n\n        # Time decay factor\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores and selection counts\n    action_scores = []\n    total_actions = 8  # We have actions from 0 to 7\n\n    for action_index in range(total_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration factor using UCB\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n\n        # Compute final score using a weighted combination of average score and exploration value\n        combined_score = average_score + exploration_value\n        action_scores.append(combined_score)\n\n    # Select the action with the highest combined score.\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration rate\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score with safety for division by zero\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration term using Upper Confidence Bound (UCB)\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combined score calculation\n        combined_score = average_score + exploration_value\n        \n        # Time decay to adjust importance based on how far into the time slots we are\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    # Select action with highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters for exploration and exploitation\n    exploration_weight = 2.0  # Weight for exploration\n    epsilon = 0.1  # Probability of choosing a random action for exploration\n    \n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score and handle cases where no previous scores exist\n        average_score = np.mean(scores) if selection_count > 0 else 0.0\n        \n        # Calculate exploration term: Upper Confidence Bound (UCB)\n        exploration_value = np.sqrt((exploration_weight * np.log(total_selection_count + 1)) / (selection_count + 1))\n        \n        # Combine scores for selection\n        combined_score = average_score + exploration_value\n        action_scores.append(combined_score)\n    \n    # Epsilon-greedy strategy for exploration vs exploitation\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, 8)  # Explore\n    else:\n        action_index = np.argmax(action_scores)  # Exploit\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Define constants\n    epsilon = 0.1  # Exploration rate\n\n    # Initialize an array to hold action scores\n    action_scores = []\n    \n    # Calculate scores for each action\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration term to balance exploration and exploitation\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Compute combined score with exploration factor\n        combined_score = average_score + (epsilon * exploration_value)\n        \n        # Apply time decay to favor earlier selections\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n\n    # Select action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Define exploration factor\n    beta = 0.1  # Exploration factor for UCB or Thompson sampling-like behavior\n    \n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score handling the case of no selections\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration value using UCB\n        exploration_value = np.sqrt((2 * np.log(total_selection_count)) / (selection_count + 1e-5))\n        \n        # Combined score\n        combined_score = average_score + beta * exploration_value if selection_count > 0 else float('inf')\n        action_scores.append(combined_score)\n    \n    # Choose action with the highest combined score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters\n    exploration_factor = 1.5  # Exploration coefficient\n    \n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Mean score calculation\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # UCB calculation\n        if selection_count > 0:\n            ucb_value = exploration_factor * np.sqrt(np.log(total_selection_count) / selection_count)\n        else:\n            ucb_value = float('inf')  # Encourage exploration if action has never been selected\n        \n        # Combined score using UCB\n        combined_score = average_score + ucb_value\n        action_scores.append(combined_score)\n    \n    # Select the best action based on UCB values\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_factor = 1.0  # Factor to balance exploration\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Upper Confidence Bound\n        if selection_count > 0:\n            confidence_interval = exploration_factor * np.sqrt(np.log(total_selection_count) / selection_count)\n        else:\n            confidence_interval = float('inf')  # Encourage exploration if action has never been selected\n        \n        combined_score = average_score + confidence_interval\n        action_scores.append(combined_score)\n    \n    action_index = np.argmax(action_scores) if np.random.rand() >= 0.1 else np.random.randint(0, 8)  # Epsilon-greedy with 10% exploration\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_weight = 1.5  # Exploration factor for UCB\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration weight\n        if selection_count > 0:\n            exploration_value = exploration_weight * np.sqrt(np.log(total_selection_count) / selection_count)\n        else:\n            exploration_value = float('inf')  # Encouraging exploration for unselected actions\n        \n        # UCB combined score\n        combined_score = average_score + exploration_value\n        action_scores.append(combined_score)\n    \n    # Select action with the highest UCB score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    exploration_factor = 1.5  # Exploration multiplier\n    action_scores = []\n\n    # Calculate the time decay factor for exploration\n    time_decay = (total_time_slots - current_time_slot) / total_time_slots\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n\n        # Calculate exploration value using UCB\n        if selection_count > 0:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count)) / selection_count)\n        else:\n            exploration_value = float('inf')  # High exploration for untried actions\n\n        # Combine average score and exploration component\n        combined_score = average_score + exploration_factor * exploration_value * time_decay\n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)  # Select action with highest combined score\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.5  # Exploration parameter\n    action_scores = []\n    available_actions = 8\n    \n    for action_index in range(available_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration term\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Compute combined score using a balance of average score and exploration\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    # Return action index with the maximum combined score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters\n    exploration_param = 0.5  # Exploration weight adjustment\n    action_count = 8  # Total number of actions\n    epsilon = 0.1  # Epsilon for epsilon-greedy exploration\n\n    action_scores = []\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration term using UCB\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Compute combined score\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n\n    # Epsilon-greedy selection\n    if np.random.uniform(0, 1) < epsilon:\n        action_index = np.random.randint(0, action_count)  # Explore\n    else:\n        action_index = np.argmax(action_scores)  # Exploit\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.5  # Increased exploration parameter for better balance\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate the average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration term using Upper Confidence Bound approach\n        exploration_value = (np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) \n                             if selection_count > 0 else float('inf'))\n\n        # Combined score calculation\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    # Return action with the maximum combined score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters\n    epsilon = 0.1  # Exploration factor\n    beta = 2.0  # Adjust this factor to emphasize exploration\n\n    action_scores = []\n    action_counts = []\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score and count\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        action_counts.append(selection_count)\n        \n        # Exploration value (Upper Confidence Bound)\n        if selection_count > 0:\n            exploration_value = np.sqrt((beta * np.log(total_selection_count)) / selection_count)\n        else:\n            exploration_value = float('inf')  # Encourages exploring untried actions\n        \n        # Combined score\n        combined_score = average_score + exploration_value\n        action_scores.append(combined_score)\n\n    # Epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, 8)  # Explore: random selection\n    else:\n        action_index = np.argmax(action_scores)  # Exploit: choose the best score\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.5  # Higher exploration parameter\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration term\n        if selection_count > 0:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_count)  # UCB variant\n        else:\n            exploration_value = float('inf')  # Infinite exploration for untried actions\n        \n        # Compute combined score\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    # Return action with the maximum combined score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    alpha = 0.1  # Exploration rate\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score safely\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Use UCB for exploration factor\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Compute combined score with decay for time\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        combined_score = average_score + (alpha * exploration_value) * time_decay_factor\n        \n        action_scores.append(combined_score)\n    \n    # Select action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.5  # Exploration parameter adjustment\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration term\n        if selection_count > 0:\n            exploration_value = np.sqrt(np.log(total_selection_count + 1) / selection_count)\n        else:\n            exploration_value = float('inf')  # Infinite exploration for untried actions\n        \n        # Combine average score with exploration term\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    # Select action with the highest combined score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    epsilon = 0.1  # Exploration probability\n    action_scores = []\n    \n    # Calculate remaining exploration weight based on remaining time slots\n    time_weight = (total_time_slots - current_time_slot) / total_time_slots\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate the average score or set to zero if no selections made\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration value using the UCB approach\n        if selection_count > 0:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count)) / selection_count)\n        else:\n            exploration_value = float('inf')  # Prioritize untried actions\n\n        # Combined score with exploration weight\n        combined_score = average_score + epsilon * exploration_value * time_weight\n        action_scores.append(combined_score)\n    \n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 1.0  # Balance exploration and exploitation\n    action_scores = []\n    epsilon = 0.1  # Epsilon for epsilon-greedy approach\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration term\n        if selection_count > 0:\n            exploration_value = np.sqrt(np.log(total_selection_count) / selection_count)\n        else:\n            exploration_value = float('inf')  # Encourage exploration of unselected actions\n            \n        # Epsilon-greedy mechanism\n        if np.random.rand() < epsilon:\n            action_scores.append(float('inf'))  # Promote exploration\n        else:\n            combined_score = average_score + exploration_param * exploration_value\n            action_scores.append(combined_score)\n    \n    # Return action with the maximum score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.1  # Tuning parameter for exploration\n    action_scores = []\n    total_actions = len(score_set)\n\n    for action_index in range(total_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        average_score = np.mean(scores) if scores else 0\n        \n        # Compute exploration term using UCB approach\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Calculate combined score for action selection\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n\n    # Select the action with the highest score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.1  # Exploration parameter\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score, handling the case when selection_count is 0\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration term, ensuring no division by zero\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Calculate combined score for action selection\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.2  # Increased exploration factor for improved exploration\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration term\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Use a weighted score to balance exploration and exploitation\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    # Return action with the maximum score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Hyperparameters\n    exploration_param = 0.1\n    \n    # Initialize variables\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score, avoiding division by zero\n        average_score = np.mean(scores) if scores else 0\n        \n        # Compute exploration term with improved handling of selection counts\n        if selection_count > 0:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_count)\n        else:\n            exploration_value = float('inf')\n        \n        # Combine average score and exploration value\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    # Select action with the highest combined score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.1  # Exploration parameter\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score and handle division by zero\n        average_score = np.mean(scores) if scores else 0\n        \n        # Compute the exploration term using UCB method\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Enhanced combined score for action selection\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    # Select action with the highest combined score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.5  # Adjustable exploration factor\n    action_scores = []\n    epsilon = 0.1  # Epsilon for epsilon-greedy approach\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration term\n        if selection_count > 0:\n            exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1))\n        else:\n            exploration_value = float('inf')  # Prioritize exploring unselected actions\n            \n        # Epsilon-greedy mechanism\n        if np.random.rand() < epsilon:\n            action_scores.append(float('inf'))  # Promote exploration\n        else:\n            combined_score = average_score + exploration_param * exploration_value\n            action_scores.append(combined_score)\n    \n    # Return action with the maximum score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.1  # Exploration parameter\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score and handle division by zero\n        average_score = np.mean(scores) if scores else 0\n        \n        # Compute the exploration term using UCB method\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Calculate combined score for action selection\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.2  # Adjusted exploration parameter\n    action_scores = []\n    epsilon = 1.0 / (current_time_slot + 1)  # Decay exploration rate over time\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Compute the exploration term using Upper Confidence Bound method\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combine exploration and exploitation with epsilon for balanced selection\n        combined_score = average_score + (exploration_param * exploration_value) * (1 - epsilon)\n        action_scores.append(combined_score)\n\n    # Implement epsilon-greedy strategy to balance exploration\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(range(8))  # Explore: select a random action\n    else:\n        action_index = np.argmax(action_scores)  # Exploit: select the best action based on scores\n\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate epsilon for exploration rate\n    epsilon = 1.0 / (current_time_slot + 1)\n    \n    # Store average scores and selection counts\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # UCB exploration term\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Calculate combined score using exploration and exploitation\n        combined_score = average_score + exploration_value\n        \n        action_scores.append(combined_score)\n    \n    # Epsilon-greedy strategy for exploration\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(range(8))  # Explore\n    else:\n        action_index = np.argmax(action_scores)  # Exploit\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.5  # Increased exploration factor for better exploration\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration term\n        if selection_count > 0:\n            exploration_value = np.sqrt(np.log(total_selection_count) / selection_count)\n        else:\n            exploration_value = float('inf')  # Encourage exploration for unselected actions\n        \n        # Combine average score and exploration term\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    # Return action with the maximum score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.1  # Exploration parameter\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration term, handling division by zero\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Calculate combined score using UCB approach\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    # Select action with the maximum combined score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.1  # Exploration parameter\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score while handling division by zero\n        average_score = np.mean(scores) if scores else 0.0\n        \n        # Compute the exploration term using UCB method\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Enhanced combined score for action selection with a logarithmic decay based on time and history\n        time_discount = (total_time_slots - current_time_slot) / total_time_slots\n        combined_score = average_score + exploration_param * exploration_value * time_discount\n        action_scores.append(combined_score)\n    \n    # Select action with the highest combined score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.1  # Exploration parameter\n    num_actions = 8\n    action_scores = []\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration term\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Calculate combined score for action selection using UCB\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n\n    # Select action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.2  # Exploration rate\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if scores else 0\n        \n        # UCB calculation to balance exploration and exploitation\n        if selection_count > 0:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_count)\n        else:\n            exploration_value = float('inf')  # Explore unselected actions fully\n            \n        # Combined score with exploration factor\n        combined_score = average_score + exploration_value\n        \n        action_scores.append(combined_score)\n    \n    # Epsilon-greedy mechanism\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(range(8))  # Explore\n    else:\n        action_index = np.argmax(action_scores)  # Exploit\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Epsilon for epsilon-greedy strategy\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score and handle division by zero\n        average_score = np.mean(scores) if scores else 0\n        \n        # Exploration term based on UCB\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combined score calculation using exploration-exploitation balance\n        combined_score = average_score + exploration_value\n        \n        action_scores.append(combined_score)\n    \n    # Epsilon-greedy strategy for exploration\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, 8)\n    else:\n        action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = np.sqrt(2)  # Higher exploration constant\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        exploration_value = (exploration_param * np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1))) if selection_count > 0 else float('inf')\n        \n        combined_score = average_score + exploration_value\n        action_scores.append(combined_score)\n    \n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Epsilon for exploration\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score and handle division by zero\n        average_score = np.mean(scores) if scores else 0\n        \n        # Compute the exploration term\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Calculate combined score (exploitation + exploration)\n        combined_score = average_score + exploration_value\n        \n        # Append action score along with its selection count\n        action_scores.append((combined_score, selection_count))\n\n    # Epsilon-greedy choice to enhance exploration\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(range(8))  # Random selection\n    else:\n        action_index = np.argmax([score[0] for score in action_scores])  # Select based on score\n\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.2  # Exploration rate\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate the exploration term\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combine average score and exploration value\n        combined_score = average_score + exploration_value\n        \n        action_scores.append(combined_score)\n    \n    # Epsilon-greedy strategy for final selection with a balance of exploration\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(8)  # Explore: choose a random action\n    else:\n        action_index = np.argmax(action_scores)  # Exploit: choose the best scoring action\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Exploration probability\n    epsilon = 0.1  # Epsilon value for epsilon-greedy strategy\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score handling cases with no selections\n        average_score = np.mean(scores) if scores else 0.0\n        \n        # Calculate exploration term using UCB approach\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combine exploration and exploitation\n        combined_score = average_score + exploration_value\n        \n        # Add randomness for exploration\n        if np.random.rand() < epsilon:\n            combined_score += np.random.rand()  # Adding some randomness\n        \n        action_scores.append(combined_score)\n\n    # Select action index with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize constants\n    epsilon = 0.1  # Exploration rate\n    \n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score with handling division by zero\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration value using Upper Confidence Bound (UCB)\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combine average score and exploration term\n        combined_score = average_score + exploration_value\n        \n        # Apply time decay factor\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    # Select action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters for exploration\n    epsilon = 0.1  # Exploration factor\n    exploration_weight = 0.5  # Weight for exploration term\n\n    action_scores = []\n    \n    for action_index in range(8):\n        # Get the scores and selection count for the action\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score, handling division by zero\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration term based on selection counts\n        exploration_term = np.sqrt((np.log(total_selection_count + 1) / (selection_count + 1))) if selection_count > 0 else float('inf')\n        \n        # Combine exploitation and exploration scores\n        combined_score = average_score + (exploration_weight * exploration_term)\n        \n        # Incorporate time decay\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    # Select action with the highest adjusted score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.5  # Adjusted exploration factor\n    action_scores = []\n    action_count = 8  # Number of actions\n    \n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration term\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combined score: basic average with exploration\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    # Return action with the maximum combined score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.5  # Adjusted exploration factor for more exploration\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Exploration term\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combine exploration and exploitation\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    # Implement a softmax distribution for selecting the action\n    action_probs = np.exp(action_scores - np.max(action_scores))  # Numerical stability\n    action_probs /= np.sum(action_probs)  # Normalize to get probabilities\n    \n    # Randomly select an action based on calculated probabilities\n    action_index = np.random.choice(range(8), p=action_probs)\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_factor = np.sqrt(2)  # Exploration parameter for UCB\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0.0\n        \n        # Handle exploration term\n        if selection_count > 0:\n            exploration_value = exploration_factor * np.sqrt(np.log(total_selection_count) / selection_count)\n        else:\n            exploration_value = float('inf')  # Force exploration of unselected action\n        \n        # Combined UCB score\n        combined_score = average_score + exploration_value\n        action_scores.append(combined_score)\n    \n    # Select action with the highest score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.1  # Exploration parameter adjustment\n    action_scores = []\n    exploration_values = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration term\n        if selection_count > 0:\n            exploration_value = np.sqrt(np.log(total_selection_count) / selection_count)\n        else:\n            exploration_value = float('inf')  # Maximum exploration for untried actions\n        \n        # Compute combined score\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    # Return action with the maximum score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.4  # Adjusted exploration parameter\n    action_scores = []\n    unselected_penalty = 0.1  # Penalize unselected actions for better exploration\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration term\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Compute combined score using a modified formula\n        combined_score = average_score + exploration_param * exploration_value - (unselected_penalty if selection_count == 0 else 0)\n        action_scores.append(combined_score)\n    \n    # Select the action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize parameters\n    exploration_factor = 1.5  # Adjust exploration strength\n    action_scores = np.zeros(8)\n    selection_counts = np.zeros(8, dtype=int)\n\n    for action_index, scores in score_set.items():\n        selection_count = len(scores)\n        selection_counts[action_index] = selection_count\n        \n        # Calculate average score and handle division by zero\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # UCB-based exploration value\n        if selection_count > 0:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count)) / selection_count)\n        else:\n            exploration_value = float('inf')  # Encourage exploration if action has never been selected\n        \n        # Combined score evaluation\n        action_scores[action_index] = average_score + exploration_factor * exploration_value\n\n    # Implement a softmax selection strategy for better exploration\n    exp_scores = np.exp(action_scores - np.max(action_scores))  # For numerical stability\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Randomly select an action based on the calculated probabilities\n    action_index = np.random.choice(range(8), p=probabilities)\n\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.5  # Adjusted exploration parameter for better balance\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n\n        # Calculate exploration term\n        if selection_count > 0:\n            exploration_value = np.sqrt(np.log(total_selection_count) / selection_count)\n        else:\n            exploration_value = float('inf')  # Prioritize untried actions\n\n        # Incorporate time decay to emphasize more recent selections\n        time_decay = (1 - (current_time_slot / total_time_slots)) * average_score\n\n        # Compute combined score\n        combined_score = average_score + exploration_param * exploration_value + time_decay\n        action_scores.append(combined_score)\n    \n    # Return action with the maximum score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Dynamic exploration-exploitation parameter\n    exploration_factor = np.log(total_time_slots + 1)  # Log based on total time slots\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Average score calculation\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Exploration value based on selection count\n        exploration_value = np.sqrt(exploration_factor / (selection_count + 1e-5)) if selection_count > 0 else np.inf\n        \n        # UCB score calculation\n        combined_score = average_score + exploration_value\n        action_scores.append(combined_score)\n    \n    # Select the action with the highest UCB score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Hyperparameter for exploration vs exploitation\n    exploration_param = 2.0  # Increased exploration factor\n    epsilon = 0.1  # Epsilon for epsilon-greedy approach\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration term\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Epsilon-greedy mechanism\n        if np.random.rand() < epsilon:\n            action_scores.append(float('inf'))  # Promote exploration\n        else:\n            combined_score = average_score + exploration_param * exploration_value\n            action_scores.append(combined_score)\n    \n    # Return action with the maximum score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters\n    exploration_param = 1.0  # Balance exploration and exploitation\n    epsilon = 0.1  # Epsilon for epsilon-greedy approach\n    num_actions = 8  # Number of actions\n\n    action_scores = []\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration term\n        if selection_count > 0:\n            exploration_value = np.sqrt(np.log(total_selection_count) / selection_count)\n        else:\n            exploration_value = float('inf')  # Encourage exploration of unselected actions\n\n        # Epsilon-greedy mechanism\n        if np.random.rand() < epsilon:\n            action_scores.append(float('inf'))  # Promote exploration\n        else:\n            combined_score = average_score + exploration_param * exploration_value\n            action_scores.append(combined_score)\n\n    # Return action with the maximum score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score and handle division by zero\n        average_score = np.mean(scores) if scores else 0\n        \n        # Update action counts\n        action_counts[action_index] = selection_count\n        \n        # Exploration term using UCB method\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combine average score with the exploration value\n        action_scores[action_index] = average_score + exploration_value\n    \n    # Select action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    epsilon = 1.0 / (current_time_slot + 1)  # Decaying exploration rate\n    action_scores = np.zeros(action_count)\n    \n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score and handle division by zero\n        average_score = np.mean(scores) if scores else 0\n        \n        # Exploration term using UCB method\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Weighted score combining average and exploration\n        action_scores[action_index] = (1 - epsilon) * average_score + epsilon * exploration_value\n    \n    # Select action with the highest combined score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.5  # Increased exploration parameter\n    discount_factor = 0.9  # Discount factor for historical performance\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score with discounting for recency\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration term using the Upper Confidence Bound (UCB) method\n        if selection_count > 0:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_count)\n        else:\n            exploration_value = float('inf')  # Explore untried actions infinitely\n        \n        # Compute combined score using a more weighted approach\n        combined_score = average_score + (exploration_param * exploration_value)\n        action_scores.append(combined_score)\n    \n    # Select the action with the maximum combined score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Set a constant for exploration adjustment\n    exploration_weight = 1.0\n    \n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score, handling cases where there are no scores\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration value using UCB\n        if selection_count > 0:\n            exploration_value = np.sqrt(exploration_weight * (np.log(total_selection_count) / selection_count))\n        else:\n            exploration_value = float('inf')  # To explore unselected actions\n        \n        # Combined score based on average and exploration value\n        combined_score = average_score + exploration_value\n        \n        # Decay score based on time slot\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    # Select action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize parameters\n    action_count = 8\n    exploration_factor = np.sqrt(2)  # Exploration factor for UCB\n    selection_counts = np.array([len(score_set.get(i, [])) for i in range(action_count)])\n    \n    # Calculate average scores and initialize UCB values\n    average_scores = np.array([\n        np.mean(score_set.get(i, [0])) if selection_counts[i] > 0 else 0\n        for i in range(action_count)\n    ])\n    \n    # UCB calculation\n    ucb_values = average_scores + exploration_factor * np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-6))\n    \n    # Select action based on UCB values\n    action_index = np.argmax(ucb_values)\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration rate\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score, handling division by zero\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration factor using UCB\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Calculate time decay factor\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        \n        # Combined score\n        combined_score = average_score + (epsilon * exploration_value) * time_decay_factor\n        \n        # Store score\n        action_scores.append(combined_score)\n    \n    # Select action with highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration rate\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score, handling division by zero\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration term based on UCB\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Update combined score with exploration\n        combined_score = average_score + exploration_value\n        \n        # Time decay factor for adjusting influence over time\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    # Select action with highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        average_score = np.mean(scores) if scores else 0\n        \n        # Adjusted exploration using Thompson Sampling\n        if selection_count > 0:\n            alpha = average_score * selection_count + 1\n            beta = (1 - average_score) * selection_count + 1\n            exploration_value = np.random.beta(alpha, beta)\n        else:\n            exploration_value = 1.0  # If the action has not been selected, it gets a maximum exploration value\n        \n        # Balance exploration and exploitation using a weighted approach\n        weighted_score = 0.7 * average_score + 0.3 * exploration_value\n        action_scores.append(weighted_score)\n    \n    # Select the action with the highest weighted score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters for exploration and exploitation\n    epsilon = 0.1\n    exploration_weight = 1.5  # Weight for exploration in UCB\n    \n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Handling case where no selections have been made for an action\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # UCB exploration value\n        if selection_count > 0:\n            exploration_value = exploration_weight * np.sqrt(np.log(total_selection_count) / selection_count)\n        else:\n            exploration_value = float('inf')  # Encourage exploration if action has never been selected\n        \n        # Combined score\n        combined_score = average_score + exploration_value\n        action_scores.append(combined_score)\n    \n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, 8)  # Explore: random selection\n    else:\n        action_index = np.argmax(action_scores)  # Exploit: choose best combined score\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration rate\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score, handling division by zero\n        average_score = np.mean(scores) if scores else 0\n\n        # UCB exploration term\n        exploration_value = (np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1))\n                             if selection_count > 0 else float('inf'))\n        \n        # Combined score\n        combined_score = average_score + (epsilon * exploration_value)\n\n        # Time decay factor\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    # Select action with highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 1.0  # Increased exploration factor\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration term\n        if selection_count > 0:\n            exploration_value = np.sqrt(np.log(total_selection_count) / selection_count)\n        else:\n            exploration_value = float('inf')  # Prioritize untried actions\n        \n        # Compute combined score (Exploitation + Exploration)\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    # Return action with the maximum combined score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.5  # Adjusted exploration parameter\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0.0\n        \n        # Calculate exploration term using UCB\n        if selection_count > 0:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_count)\n        else:\n            exploration_value = float('inf')  # Force exploration for untried actions\n        \n        # Compute combined score\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    # Select the action with the maximum combined score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_weight = 1.5  # Tunable parameter for exploration weight\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate the average score for the action\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration value using Upper Confidence Bound (UCB) approach\n        if selection_count > 0:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_count)\n        else:\n            exploration_value = float('inf')  # Ensure exploration for unselected actions\n        \n        # Compute combined score with the exploration term\n        combined_score = average_score + exploration_weight * exploration_value\n        action_scores.append(combined_score)\n    \n    # Select action with the maximum combined score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters\n    exploration_factor = 0.1  # Exploration factor\n    action_count = 8  # Total actions available\n    \n    # Initialize lists for scores and selection counts\n    average_scores = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n    \n    # Calculate average scores and selection counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        average_scores[action_index] = np.mean(scores) if scores else 0  # Handle division by zero\n    \n    # Time decay factor\n    time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n    \n    # Calculate combined scores\n    combined_scores = average_scores * time_decay_factor\n    \n    # Add exploration value to the combined scores\n    for action_index in range(action_count):\n        if selection_counts[action_index] > 0:\n            exploration_value = (exploration_factor * np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts[action_index] + 1)))\n        else:  # If action has never been selected, give it a high exploration value\n            exploration_value = float('inf')\n        combined_scores[action_index] += exploration_value\n    \n    # Select action with highest combined score\n    action_index = np.argmax(combined_scores)\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Hyperparameters to control exploration-exploitation\n    exploration_param = 1.0\n    min_exploration_value = 1e-5  # A small constant to handle exploration for untried actions\n    \n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration term\n        if selection_count > 0:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_count)\n        else:\n            exploration_value = float('inf')  # Encourage exploration for untried actions\n        \n        # Compute combined score\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    # Return action with the maximum score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions_count = 8\n    action_scores = np.zeros(actions_count)\n    selection_counts = np.zeros(actions_count)\n\n    for action_index in range(actions_count):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        average_score = np.mean(scores) if scores else 0.0\n        \n        # Adjust exploration factor based on the total number of selections\n        exploration_factor = np.sqrt((np.log(total_selection_count + 1) + (total_time_slots - current_time_slot)) / (selection_counts[action_index] + 1)) if selection_counts[action_index] > 0 else float('inf')\n        \n        # Combine exploration and exploitation\n        action_scores[action_index] = average_score + exploration_factor\n\n    # Select action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_rate = 1.0  # Adjustable exploration factor\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score, handling division by zero\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate UCB exploration term\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combined score with a time decay factor\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        combined_score = average_score + exploration_rate * exploration_value\n        \n        # Adjusting combined score based on time decay\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    # Select action with highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    exploration_weight = 1.0 / (1 + total_selection_count)  # Decay exploration over time\n    epsilon = 0.1  # Epsilon for epsilon-greedy exploration\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate confidence bounds\n        if selection_count > 0:\n            confidence_bound = np.sqrt((2 * np.log(total_selection_count)) / selection_count)\n        else:\n            confidence_bound = float('inf')  # Infinite confidence if not selected\n        \n        # Calculate exploratory score\n        action_scores[action_index] = average_score + exploration_weight * confidence_bound\n    \n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(num_actions)\n    else:\n        action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -449.9999999999999,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.1  # Exploration parameter\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score safely\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Compute the exploration term using Upper Confidence Bound method\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Calculate combined score for action selection\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.9999999999999,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_factor = 0.2\n    average_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score with np.mean and handle empty score lists\n        average_score = np.mean(scores) if scores else 0\n\n        # Calculate exploration bonus to encourage exploration of less-selected actions\n        exploration_bonus = (\n            exploration_factor * np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1))\n            if selection_count > 0 else exploration_factor\n        )\n        \n        adjusted_score = average_score + exploration_bonus\n        average_scores.append(adjusted_score)\n    \n    action_index = np.argmax(average_scores)  # Select the action with the highest adjusted score\n    return action_index",
          "objective": -449.9999999999999,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Exploration parameters\n    exploration_param = 0.5\n    epsilon = max(0.1, 1.0 - (current_time_slot / total_time_slots))\n\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Compute the exploration term using Upper Confidence Bound method\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Calculate combined score with exploration parameter\n        combined_score = average_score + (exploration_param * exploration_value)\n        action_scores.append(combined_score)\n\n    # Epsilon-greedy strategy to balance exploration\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(range(8))  # Explore: select a random action\n    else:\n        action_index = np.argmax(action_scores)  # Exploit: select the best action based on scores\n\n    return action_index",
          "objective": -449.9999999999999,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Epsilon value for exploration\n    epsilon = 0.1\n    \n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score with safe handling of division by zero\n        average_score = np.mean(scores) if scores else 0\n        \n        # Compute exploration factor\n        if selection_count > 0:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count)) / selection_count)\n        else:\n            exploration_value = float('inf')  # Encourages exploration for untried actions\n        \n        # Combined score using exploration/exploitation balance\n        combined_score = average_score + exploration_value\n        action_scores.append(combined_score)\n    \n    # Select action with the max combined score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.9999999999999,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.5  # Balanced exploration factor\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration term using Upper Confidence Bound (UCB)\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combine average score and exploration term\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    # Select action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.9999999999999,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 1.0  # Increased exploration factor for better exploration\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration term\n        if selection_count > 0:\n            exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1))\n        else:\n            exploration_value = float('inf')  # High value for unselected actions\n        \n        # Use a weighted score to balance exploration and exploitation\n        combined_score = average_score + exploration_param * exploration_value\n        \n        action_scores.append(combined_score)\n    \n    # Normalize the scores first\n    normalized_scores = (action_scores - np.min(action_scores)) / (np.max(action_scores) - np.min(action_scores) + 1e-10)\n    \n    # Select action based on softmax probabilities for a stochastic approach\n    probabilities = np.exp(normalized_scores) / np.sum(np.exp(normalized_scores))\n    \n    # Randomly select an action based on probabilities\n    action_index = np.random.choice(range(8), p=probabilities)\n    \n    return action_index",
          "objective": -449.9999999999999,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Define parameters for exploration/exploitation\n    epsilon = 0.1  # Epsilon for exploration\n\n    action_indices = list(range(8))\n    action_avg_scores = np.zeros(len(action_indices))\n    action_counts = np.zeros(len(action_indices))\n\n    # Calculate average scores and counts\n    for action_index in action_indices:\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_avg_scores[action_index] = np.mean(scores) if scores else 0\n\n    # UCB exploration factor\n    exploration_values = np.zeros(len(action_indices))\n    for action_index in action_indices:\n        if action_counts[action_index] > 0:\n            exploration_values[action_index] = np.sqrt((2 * np.log(total_selection_count)) / action_counts[action_index])\n        else:\n            exploration_values[action_index] = float('inf')  # High value for unselected actions\n\n    # Combine average scores with exploration using a weighted approach\n    combined_scores = (1 - epsilon) * action_avg_scores + epsilon * exploration_values\n\n    # Select the action with the highest combined score\n    action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": -449.9999999999999,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize parameters\n    n_actions = 8\n    exploration_weight = 1.5  # Weight to adjust exploration factor\n    \n    action_scores = np.zeros(n_actions)\n    \n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate the average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Compute exploration term\n        if selection_count > 0:\n            exploration_value = exploration_weight * np.sqrt(np.log(total_selection_count) / selection_count)\n        else:\n            exploration_value = float('inf')  # Maximum exploration for actions not yet selected\n        \n        # Update combined score\n        action_scores[action_index] = average_score + exploration_value\n\n    # Use a softmax approach to balance exploration and exploitation\n    exp_scores = np.exp(action_scores - np.max(action_scores))  # for numerical stability\n    action_probabilities = exp_scores / np.sum(exp_scores)  # normalize to create probabilities\n    \n    # Select action based on computed probabilities\n    action_index = np.random.choice(n_actions, p=action_probabilities)\n    \n    return action_index",
          "objective": -449.9999999999999,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    epsilon = 0.1  # Exploration parameter\n    action_scores = []\n    \n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Explore less-searched actions\n        if selection_count > 0:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count)) / selection_count)\n        else:\n            exploration_value = float('inf')  # Prioritize unexplored actions\n        \n        # Calculate the decay of exploration based on remaining time slots\n        time_weight = (total_time_slots - current_time_slot) / total_time_slots\n        \n        # Softmax transformation\n        adjusted_score = average_score + epsilon * exploration_value * time_weight\n        action_scores.append(adjusted_score)\n    \n    # Normalize scores using softmax\n    exp_scores = np.exp(action_scores - np.max(action_scores))\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Select action based on probabilities\n    action_index = np.random.choice(range(action_count), p=probabilities)\n    return action_index",
          "objective": -449.9999999999999,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.5  # Increased exploration factor\n    action_scores = []\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n\n        # Calculate exploration term\n        exploration_value = (\n            np.sqrt(2 * np.log(total_selection_count + 1) / (selection_count + 1e-6))\n            if selection_count > 0 else float('inf')\n        )\n\n        # Compute combined score\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n\n    # Return action with the maximum combined score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.9999999999999,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters\n    exploration_param = 0.3\n    action_count = 8\n    action_scores = []\n    \n    # Dynamic epsilon decay\n    epsilon = max(0.05, 1.0 - (current_time_slot / total_time_slots))  # Decay epsilon over time\n    \n    # Calculate scores for each action\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Upper Confidence Bound computation\n        exploration_value = np.sqrt((np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combined score\n        combined_score = average_score + (exploration_param * exploration_value)\n        action_scores.append(combined_score)\n\n    # Epsilon-greedy decision\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(range(action_count))  # Explore\n    else:\n        action_index = np.argmax(action_scores)  # Exploit\n\n    return action_index",
          "objective": -449.99999999999983,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    epsilon = 0.1  # Epsilon for exploration\n    decay_factor = 1.0 / (1 + total_selection_count)  # Decay exploration over time\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate Upper Confidence Bound (UCB)\n        if selection_count > 0:\n            confidence_bound = np.sqrt((2 * np.log(total_selection_count)) / selection_count)\n        else:\n            confidence_bound = float('inf')  # Infinite confidence if not selected\n        \n        # Combining average score and confidence bound\n        action_scores[action_index] = average_score + decay_factor * confidence_bound\n    \n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(num_actions)\n    else:\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -449.99999999999983,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_factor = 0.1\n    epsilon = 0.1  # Epsilon for exploration vs exploitation\n    average_scores = []\n    \n    for action_index in range(8):\n        scores = score_set[action_index]\n        selection_count = len(scores)\n        \n        # Calculate average score, ensuring safety against division by zero\n        average_score = np.mean(scores) if scores else 0.0\n        \n        # Calculate exploration bonus using UCB\n        exploration_bonus = (exploration_factor * \n                             np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1))) if selection_count > 0 else 1.0\n        \n        # Combine average score and exploration bonus\n        adjusted_score = average_score + exploration_bonus\n        average_scores.append(adjusted_score)\n    \n    # Epsilon-greedy strategy for exploration\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(range(8))  # Select a random action\n    else:\n        action_index = np.argmax(average_scores)  # Select the action with the highest adjusted score\n    \n    return action_index",
          "objective": -449.99999999999983,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    min_selection_count = 5  # Minimum selections to consider for exploitation\n    exploration_param = 0.5  # Tuning parameter for exploration, increased for more exploration\n    num_actions = 8\n    action_scores = []\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        average_score = np.mean(scores) if scores else 0\n        \n        # Adjust exploration factor based on selection count and current total selection count\n        if selection_count < min_selection_count:\n            exploration_value = float('inf')  # Encourage exploration for under-selected actions\n        else:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_count)\n        \n        # Combine scores for selection criteria\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n\n    # Select the action with the highest score\n    action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -449.99999999999983,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 1.0  # Exploration parameter\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0.0\n        \n        # Calculate exploration term using UCB\n        if selection_count > 0:\n            exploration_value = np.sqrt(np.log(total_selection_count) / selection_count)\n        else:\n            exploration_value = float('inf')  # Encourage exploration for unselected actions\n        \n        # Combined score for action selection\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -449.99999999999983,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 1.0  # Adjusted exploration parameter for balance\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score and handle division by zero\n        average_score = np.mean(scores) if scores else 0\n        \n        # Compute exploration term using a variant of UCB\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combine average score and exploration value\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    # Select the action with the highest score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -449.9999999999998,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 1.0  # Scale for exploration\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score, with handling for zero selections\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration term, handling division by zero\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combined score leveraging exploration and average score\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    # Using np.argmax to select action index with the highest combined score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.9999999999998,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 1.0  # Adjusted exploration factor for more emphasis on exploration\n    action_scores = []\n    epsilon = 0.2  # Increased epsilon for diversified exploration\n    exploration_factor = 1.5  # Factor to encourage exploring less-selected actions\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration term\n        exploration_value = np.sqrt((np.log(total_selection_count + 1) + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Epsilon-greedy mechanism\n        if np.random.rand() < epsilon:\n            combined_score = exploration_factor * exploration_value  # Promote exploration\n        else:\n            combined_score = average_score + exploration_param * exploration_value\n            \n        action_scores.append(combined_score)\n    \n    # Return action with the maximum score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.9999999999997,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))  # Decay epsilon over time\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score and handle division by zero\n        average_score = np.mean(scores) if scores else 0\n        \n        # Exploration bonus\n        if selection_count == 0:\n            exploration_value = float('inf')  # Encourage exploration for unselected actions\n        else:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count)) / selection_count)\n\n        # Compute final score with epsilon-greedy approach\n        if np.random.rand() < epsilon:\n            action_scores.append(exploration_value)  # Explore\n        else:\n            action_scores.append(average_score)  # Exploit\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.9999999999996,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.5  # Exploration weight\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        average_score = np.mean(scores) if selection_count > 0 else 0.0\n        \n        # Compute exploration term\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Calculate combined score leveraging exploration and exploitation\n        combined_score = average_score + (exploration_param * exploration_value)\n        action_scores.append(combined_score)\n\n    # Normalize scores to prevent extreme values skewing the selection\n    action_scores = np.array(action_scores)\n    action_scores -= np.max(action_scores)  # Shifting max to 0 for stability\n    probabilities = np.exp(action_scores)  # Softmax transformation\n    probabilities /= np.sum(probabilities)  # Normalize to probability distribution\n    \n    # Sample action based on computed probabilities\n    action_index = np.random.choice(range(8), p=probabilities)\n\n    return action_index",
          "objective": -449.99999999999955,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_factor = 0.1\n    average_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score, handling the case where there are no scores\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate the exploration bonus\n        exploration_bonus = (exploration_factor * \n                             np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) \n                             if selection_count > 0 else 1)\n        \n        # Adjusted score considering exploration\n        adjusted_score = average_score + exploration_bonus\n        average_scores.append(adjusted_score)\n    \n    action_index = np.argmax(average_scores)  # Select the action with the highest adjusted score\n    return action_index",
          "objective": -449.99999999999943,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    epsilon = max(0.01, 0.1 * (1 - current_time_slot / total_time_slots))  # Decay epsilon over time\n    exploration_factor = 1.0\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate confidence bound\n        confidence_bound = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combining average score and confidence bound\n        action_scores[action_index] = average_score + exploration_factor * confidence_bound\n\n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -449.9999999999992,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.2\n    action_scores = []\n    epsilon = max(0.1, 1.0 - (current_time_slot / total_time_slots))  # Decay epsilon over time\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Computing exploration term using the Upper Confidence Bound method\n        exploration_value = np.sqrt((np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combined score\n        combined_score = average_score + (exploration_param * exploration_value)\n        action_scores.append(combined_score)\n\n    # Epsilon-greedy decision\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(range(8))  # Explore\n    else:\n        action_index = np.argmax(action_scores)  # Exploit\n\n    return action_index",
          "objective": -449.9999999999991,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Epsilon for exploration in epsilon-greedy strategy\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score safely\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Epsilon-greedy exploration\n        if np.random.rand() < epsilon:\n            combined_score = float('inf')  # Encourage exploration by choosing a random action\n        else:\n            # Compute exploration term using UCB\n            exploration_value = np.sqrt(np.log(total_selection_count) / (selection_count + 1)) if selection_count > 0 else float('inf')\n            combined_score = average_score + exploration_value\n        \n        action_scores.append(combined_score)\n    \n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.999999999999,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_factor = 0.1\n    average_scores = []\n    \n    for action_index in range(8):\n        scores = score_set[action_index]\n        selection_count = len(scores)\n        \n        # Calculate average score, handling the case where there are no scores\n        average_score = np.mean(scores) if scores else 0\n\n        # Calculate exploration bonus, with consideration for total selections\n        exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else 1\n        \n        adjusted_score = average_score + exploration_bonus\n        average_scores.append(adjusted_score)\n    \n    action_index = np.argmax(average_scores)  # Select the action with the highest adjusted score\n    return action_index",
          "objective": -449.9999999999988,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8  # Fixed number of actions\n    average_scores = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n    \n    # Calculate average scores and selection counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        average_scores[action_index] = np.mean(scores) if selection_counts[action_index] > 0 else 0.0\n    \n    # Compute the exploration term\n    exploration_term = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-6))\n    \n    # Combine scores with exploration term (UCB-like strategy)\n    adjusted_scores = average_scores + exploration_term\n    \n    # Dynamic epsilon-greedy selection\n    epsilon = max(0.01, 0.1 * (0.95 ** current_time_slot))\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(range(action_count))  # Explore\n    else:\n        action_index = np.argmax(adjusted_scores)  # Exploit\n\n    return action_index",
          "objective": -449.9999999999985,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        average_scores[action_index] = np.mean(scores) if selection_counts[action_index] > 0 else 0.0\n\n    # Upper Confidence Bound (UCB) calculation\n    ucb_values = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        if selection_counts[action_index] == 0:\n            ucb_values[action_index] = float('inf')  # Allow full confidence for unselected actions\n        else:\n            confidence_bound = np.sqrt(np.log(total_selection_count) / selection_counts[action_index])\n            ucb_values[action_index] = average_scores[action_index] + confidence_bound\n\n    # Combine UCB with epsilon-greedy strategy\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01)\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(range(num_actions))  # Explore\n    else:\n        action_index = np.argmax(ucb_values)  # Exploit\n\n    return action_index",
          "objective": -449.99999999999835,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    \n    # Setting epsilon and temperature for softmax\n    epsilon = 0.1\n    temperature = 1.0\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        average_score = np.mean(scores) if selection_count > 0 else 0\n\n        # Calculate Upper Confidence Bound (UCB) component\n        if selection_count > 0:\n            confidence_bound = np.sqrt((2 * np.log(total_selection_count)) / selection_count)\n        else:\n            confidence_bound = float('inf')\n\n        # Combine average score and confidence bound\n        action_scores[action_index] = average_score + confidence_bound\n    \n    # Softmax selection for better exploration-exploitation balance\n    exp_scores = np.exp(action_scores / temperature)\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(num_actions)\n    else:\n        action_index = np.random.choice(num_actions, p=probabilities)\n\n    return action_index",
          "objective": -449.9999999999977,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    exploration_factor = 1.0 / (1 + total_selection_count)  # Decaying exploration weight\n    epsilon = 0.1  # Exploration rate\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        if selection_count > 0:\n            average_score = np.mean(scores)\n            variance = np.var(scores) if selection_count > 1 else 0\n            confidence_bound = np.sqrt((2 * np.log(total_selection_count)) / selection_count)\n            action_scores[action_index] = average_score + exploration_factor * (confidence_bound + variance)\n        else:\n            action_scores[action_index] = float('inf')  # Encourage selection of untried actions\n\n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(num_actions)\n    else:\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -449.9999999999976,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_factor = 1.0\n    adjusted_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate the average score with a fallback for empty score lists\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration bonus using UCB-like approach\n        if selection_count > 0:\n            exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / selection_count)\n        else:\n            exploration_bonus = 10  # Encourage exploration if action hasn't been selected\n        \n        adjusted_score = average_score + exploration_bonus\n        adjusted_scores.append(adjusted_score)\n    \n    action_index = np.argmax(adjusted_scores)\n    return action_index",
          "objective": -449.9999999999973,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters for exploration\n    exploration_factor = 1.5\n    epsilon = 0.1\n    \n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Safely calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Epsilon-greedy exploration\n        if np.random.rand() < epsilon:\n            action_index = np.random.randint(0, 8)  # Randomly select among actions\n            return action_index\n        \n        # Calculate exploration value\n        if selection_count == 0:\n            exploration_value = float('inf')  # Force exploration for unselected actions\n        else:\n            exploration_value = exploration_factor * np.sqrt(np.log(total_selection_count) / selection_count)\n        \n        # Calculate combined score\n        combined_score = average_score + exploration_value\n        action_scores.append(combined_score)\n    \n    # Select action with the highest combined score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -449.9999999999957,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration rate\n    adjusted_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate the average score with a fallback for empty score lists\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration factor based on selection count\n        if selection_count > 0:\n            exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_count)\n        else:\n            exploration_bonus = 10  # Maximum exploration bonus for unselected actions\n        \n        adjusted_score = average_score + exploration_bonus\n        \n        # Implement epsilon-greedy approach\n        if np.random.rand() < epsilon:\n            # Encourage exploration by picking a random action\n            adjusted_score += np.random.rand() * exploration_bonus\n        \n        adjusted_scores.append(adjusted_score)\n    \n    action_index = np.argmax(adjusted_scores)\n    return action_index",
          "objective": -449.9999999999938,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_factor = 1.0\n    average_scores = []\n    \n    for action_index in range(8):\n        scores = score_set[action_index]\n        selection_count = len(scores)\n\n        # Calculate average score, avoiding division by zero\n        average_score = np.mean(scores) if scores else 0\n\n        # Implement Upper Confidence Bound (UCB)\n        if selection_count == 0:\n            exploration_bonus = float('inf')  # Ensure exploration for unselected actions\n        else:\n            exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count) / selection_count)\n        \n        adjusted_score = average_score + exploration_bonus\n        average_scores.append(adjusted_score)\n    \n    action_index = np.argmax(average_scores)  # Select the action with the highest adjusted score\n    return action_index",
          "objective": -449.9999999999928,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        average_scores[action_index] = np.mean(scores) if selection_counts[action_index] > 0 else 0.0\n\n    # Upper Confidence Bound (UCB) calculation with improved exploration\n    ucb_values = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        if selection_counts[action_index] == 0:\n            ucb_values[action_index] = float('inf')  # Full exploration for unselected actions\n        else:\n            confidence_bound = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[action_index])\n            ucb_values[action_index] = average_scores[action_index] + confidence_bound\n\n    # Dynamic epsilon adjustment for exploration\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01)\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(range(num_actions))  # Explore\n    else:\n        action_index = np.argmax(ucb_values)  # Exploit based on UCB\n\n    return action_index",
          "objective": -449.9999999999767,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Exploration term using Thompson Sampling approach\n        alpha = 1 + np.sum(scores)  # Shape parameter for Beta distribution\n        beta = 1 + selection_count - np.sum(scores)  # Shape parameter for Beta distribution\n        exploration_value = np.random.beta(alpha, beta)  # Sample from Beta distribution\n        \n        # Combine and normalize for selection\n        action_scores[action_index] = average_score + exploration_value\n    \n    # Select the action with the maximum combined score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -449.99999999997254,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_weight = 2.0\n    average_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score and handle no scores scenario\n        average_score = np.mean(scores) if scores else 0.0\n        \n        # Compute exploration bonus using UCB method\n        exploration_bonus = (\n            exploration_weight * np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1))\n            if selection_count > 0 else exploration_weight * np.sqrt(np.log(total_selection_count + 1))\n        )\n        \n        adjusted_score = average_score + exploration_bonus\n        average_scores.append(adjusted_score)\n    \n    action_index = np.argmax(average_scores)  # Select action with the highest adjusted score\n    return action_index",
          "objective": -449.99999999997175,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        average_scores[action_index] = np.mean(scores) if selection_counts[action_index] > 0 else 0.0\n\n    # Dynamic epsilon based on the time slot\n    initial_epsilon = 0.1\n    epsilon = max(initial_epsilon * (1 - current_time_slot / total_time_slots), 0.01)\n\n    # UCB values calculation\n    ucb_values = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        if selection_counts[action_index] == 0:\n            ucb_values[action_index] = float('inf')  # Favor unselected actions\n        else:\n            confidence_bound = np.sqrt(np.log(total_selection_count) / selection_counts[action_index])\n            ucb_values[action_index] = average_scores[action_index] + confidence_bound\n    \n    # Exploration vs Exploitation\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(range(num_actions))  # Explore\n    else:\n        action_index = np.argmax(ucb_values)  # Exploit\n\n    return action_index",
          "objective": -449.9999999999527,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    \n    # Dynamic exploration-exploitation balance based on historical data\n    epsilon = 1.0 / (1 + current_time_slot / total_time_slots)  # Decay epsilon based on current time slot\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate upper confidence bounds\n        if selection_count > 0:\n            confidence_bound = np.sqrt((2 * np.log(total_selection_count)) / selection_count)\n        else:\n            confidence_bound = float('inf')  # No prior selection gives infinite confidence\n        \n        # Calculate action score combining average score and confidence\n        action_scores[action_index] = average_score + confidence_bound\n    \n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(num_actions)\n    else:\n        action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -449.9999999999104,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Epsilon decay factor based on the current time slot\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n    \n    # Calculate average scores and selection counts\n    average_scores = np.zeros(8)\n    selection_counts = np.zeros(8)\n    \n    for action_index, scores in score_set.items():\n        if scores:\n            average_scores[action_index] = np.mean(scores)\n            selection_counts[action_index] = len(scores)\n\n    # Handle cases where selection_counts are zero\n    selection_counts = np.where(selection_counts > 0, selection_counts, 1)\n    \n    # Calculate exploration values\n    normalized_counts = selection_counts / (total_selection_count + 1e-6)\n    exploration_values = (1 - normalized_counts) * (1 / (total_selection_count + 1e-6))\n    \n    # Compute combined action values\n    action_values = (1 - epsilon) * average_scores + epsilon * exploration_values\n\n    # Select action with the highest value\n    action_index = np.argmax(action_values)\n    \n    return action_index",
          "objective": -449.99999999990047,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize parameters\n    num_actions = 8\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        average_scores[action_index] = np.mean(scores) if selection_counts[action_index] > 0 else 0.0\n    \n    # Compute an exploration bonus using UCB\n    exploration_bonuses = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        if selection_counts[action_index] > 0:\n            confidence_bound = np.sqrt(np.log(total_selection_count) / selection_counts[action_index])\n        else:\n            confidence_bound = float('inf')  # Full exploration for actions never selected\n        exploration_bonuses[action_index] = confidence_bound\n    \n    # Adjust scores for exploration\n    adjusted_scores = average_scores + exploration_bonuses\n\n    # Epsilon-greedy strategy\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01)\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(range(num_actions))  # Explore\n    else:\n        action_index = np.argmax(adjusted_scores)  # Exploit\n    \n    return action_index",
          "objective": -449.9999999995408,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_factor = 1.0  # High exploration factor to encourage trying less-selected actions\n    average_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score, substituting with a small epsilon for smoothness\n        average_score = np.mean(scores) if scores else 0.01  # Avoiding division by zero\n        \n        # Calculate exploration bonus using Upper Confidence Bound (UCB)\n        exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else np.inf\n        \n        adjusted_score = average_score + exploration_bonus\n        average_scores.append(adjusted_score)\n    \n    action_index = np.argmax(average_scores)  # Select the action with the highest adjusted score\n    return action_index",
          "objective": -449.9999999992034,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Hyperparameters\n    epsilon_decay = 0.95\n    min_epsilon = 0.01\n    initial_epsilon = 0.1\n    exploration_factor = 1.0\n    decay_rate = 0.99\n    \n    # Calculate average scores, selection counts, and adjusted scores for each action\n    action_count = len(score_set)  # Assuming there are always 8 actions\n    average_scores = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n    \n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        selection_counts[action_index] = selection_count\n        average_scores[action_index] = np.mean(scores) if selection_count > 0 else 0.0\n    \n    # Calculate exploration bonuses using UCB\n    confidence_bounds = np.zeros(action_count)\n    for action_index in range(action_count):\n        if selection_counts[action_index] > 0:\n            confidence_bounds[action_index] = np.sqrt(np.log(total_selection_count) / selection_counts[action_index])\n        else:\n            confidence_bounds[action_index] = float('inf')  # Full confidence for never-selected actions\n\n    # Combine scores with exploration bonuses\n    adjusted_scores = average_scores + confidence_bounds\n\n    # Epsilon-greedy approach\n    epsilon = max(min_epsilon, initial_epsilon * (epsilon_decay ** current_time_slot))\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(range(action_count))  # Explore\n    else:\n        action_index = np.argmax(adjusted_scores)  # Exploit\n\n    return action_index",
          "objective": -449.9999999950004,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.1  # Epsilon value for exploration\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration term\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Calculate the combined score considering exploration\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.9999999895789,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_factor = 1.0  # Starting exploration factor\n    decay_rate = 0.99  # Decay rate for exploration factor\n    \n    # Calculate average scores and selection counts\n    average_scores = []\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        average_score = np.mean(scores) if selection_count > 0 else 0.0\n        \n        # Calculate exploration bonus using UCB\n        if selection_count > 0:\n            confidence_bound = np.sqrt(np.log(total_selection_count) / selection_count)\n        else:\n            confidence_bound = float('inf')  # Full confidence for never-selected actions\n        \n        # Adjust average score with exploration factor\n        adjusted_score = average_score + confidence_bound\n        average_scores.append(adjusted_score)\n    \n    # Epsilon-greedy approach to balance exploration and exploitation\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01)  # Decay epsilon over time\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(range(8))  # Explore\n    else:\n        action_index = np.argmax(average_scores)  # Exploit\n    \n    # Update exploration factor dynamically\n    exploration_factor *= decay_rate\n    \n    return action_index",
          "objective": -449.999999989232,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initializing variables\n    actions = 8\n    average_scores = np.zeros(actions)\n    selection_counts = np.zeros(actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        selection_counts[action_index] = selection_count\n        average_scores[action_index] = np.mean(scores) if selection_count > 0 else 0.0\n\n    # Calculate confidence bounds using UCB\n    confidence_bounds = np.zeros(actions)\n    for action_index in range(actions):\n        if selection_counts[action_index] > 0:\n            confidence_bounds[action_index] = np.sqrt(np.log(total_selection_count) / selection_counts[action_index])\n        else:\n            confidence_bounds[action_index] = float('inf')  # Full confidence for unexplored actions\n\n    # Adjust scores for exploration\n    adjusted_scores = average_scores + confidence_bounds\n\n    # Epsilon-greedy strategy\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01)\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(range(actions))  # Explore\n    else:\n        action_index = np.argmax(adjusted_scores)  # Exploit\n\n    return action_index",
          "objective": -449.99999998739344,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize parameters\n    num_actions = 8\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        average_scores[action_index] = np.mean(scores) if selection_counts[action_index] > 0 else 0.0\n        \n    # Calculate the exploration bonus using Upper Confidence Bound (UCB)\n    ucb_values = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        if selection_counts[action_index] > 0:\n            confidence_bound = np.sqrt((2 * np.log(total_selection_count)) / selection_counts[action_index])\n            ucb_values[action_index] = average_scores[action_index] + confidence_bound\n        else:\n            ucb_values[action_index] = float('inf')  # Prioritize unselected actions\n\n    # Epsilon-greedy approach with dynamic epsilon\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01)\n    \n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(range(num_actions))  # Explore\n    else:\n        action_index = np.argmax(ucb_values)  # Exploit based on UCB\n    \n    return action_index",
          "objective": -449.99999998725775,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = 8\n    average_scores = np.zeros(actions)\n    selection_counts = np.zeros(actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        selection_counts[action_index] = selection_count\n        average_scores[action_index] = np.mean(scores) if selection_count > 0 else 0.0\n\n    # Calculate confidence bounds using UCB\n    confidence_bounds = np.zeros(actions)\n    for action_index in range(actions):\n        if selection_counts[action_index] > 0:\n            confidence_bounds[action_index] = np.sqrt((2 * np.log(total_selection_count)) / selection_counts[action_index])\n        else:\n            confidence_bounds[action_index] = float('inf')  # Full confidence for unexplored actions\n\n    # Adjust scores for exploration\n    adjusted_scores = average_scores + confidence_bounds\n\n    # Epsilon-greedy strategy\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01)\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(range(actions))  # Explore\n    else:\n        action_index = np.argmax(adjusted_scores)  # Exploit\n\n    return action_index",
          "objective": -449.99999998529665,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    k = 8  # Number of actions\n    exploration_factor = 1.0\n    epsilon = 0.1  # Exploration rate\n    \n    average_scores = np.zeros(k)\n    counts = np.zeros(k)\n\n    for action_index in range(k):\n        scores = score_set.get(action_index, [])\n        counts[action_index] = len(scores)\n        average_scores[action_index] = np.mean(scores) if scores else 0.0\n\n    # Calculate UCB for each action\n    ucb_scores = np.zeros(k)\n    for action_index in range(k):\n        if counts[action_index] > 0:\n            ucb_scores[action_index] = average_scores[action_index] + \\\n                                       exploration_factor * np.sqrt(np.log(total_selection_count) / counts[action_index])\n        else:\n            ucb_scores[action_index] = float('inf')  # Encourage exploration for unselected actions\n\n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(range(k))  # Select a random action\n    else:\n        action_index = np.argmax(ucb_scores)  # Select action with the highest UCB score\n\n    return action_index",
          "objective": -449.9999999815887,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Epsilon-greedy strategy for balance\n        epsilon = 0.1  # Exploration parameter\n        exploration_value = np.random.rand() * (1 - average_score) if np.random.rand() < epsilon else 0\n        \n        # Combine average score and exploration\n        action_scores[action_index] = average_score + exploration_value\n    \n    # Select the action with the maximum combined score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -449.9999999521729,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Hyperparameters\n    epsilon_decay = 0.95\n    min_epsilon = 0.01\n    initial_epsilon = 0.1\n    c = 2.0  # Exploration factor for UCB\n\n    action_count = len(score_set)  # Always 8 actions\n    average_scores = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n\n    # Calculate average scores and selection counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        selection_counts[action_index] = selection_count\n        average_scores[action_index] = np.mean(scores) if selection_count > 0 else 0.0\n\n    # Calculate the uncertainty for each action using UCB\n    confidence_bounds = np.sqrt((c * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n\n    # Combine scores with exploration bonuses\n    adjusted_scores = average_scores + confidence_bounds\n\n    # Epsilon-greedy approach for balanced exploration and exploitation\n    epsilon = max(min_epsilon, initial_epsilon * (epsilon_decay ** current_time_slot))\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(range(action_count))  # Explore\n    else:\n        action_index = np.argmax(adjusted_scores)  # Exploit\n\n    return action_index",
          "objective": -449.99999994859394,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        average_scores[action_index] = np.mean(scores) if selection_counts[action_index] > 0 else 0.0\n\n    # Softmax exploration factor\n    temperature = max(0.1, 1.0 - (current_time_slot / total_time_slots))\n    exp_scores = np.exp(average_scores / temperature)\n    softmax_probabilities = exp_scores / np.sum(exp_scores)\n\n    # Epsilon-greedy adjustment\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01)\n    \n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(range(num_actions))  # Explore\n    else:\n        action_index = np.random.choice(range(num_actions), p=softmax_probabilities)  # Exploit using softmax probabilities\n    \n    return action_index",
          "objective": -449.9999999005748,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Adaptive epsilon based on the current time slot\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n    \n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(8)\n    selection_counts = np.zeros(8)\n    \n    # Collect data on average scores and counts\n    for action_index, scores in score_set.items():\n        if scores:\n            average_scores[action_index] = np.mean(scores)\n            selection_counts[action_index] = len(scores)\n\n    # Avoid division by zero by replacing zero counts with a small value\n    selection_counts += 1e-6\n    \n    # Calculate exploration term (UCB variant)\n    exploration_values = (np.sqrt(np.log(total_selection_count) / selection_counts))\n    \n    # Update the action values using a weighted combination of average scores and exploration values\n    action_values = (1 - epsilon) * average_scores + epsilon * exploration_values\n\n    # Select action by finding the index of the maximum action value\n    action_index = np.argmax(action_values)\n    \n    return action_index",
          "objective": -449.9999993127621,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize variables\n    num_actions = 8\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        average_scores[action_index] = np.mean(scores) if selection_counts[action_index] > 0 else 0.0\n    \n    # Calculate exploration term using UCB\n    ucb_values = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))  # Add small constant to prevent division by zero\n    ucb_values[selection_counts == 0] = float('inf')  # Full exploration for actions that have never been chosen\n\n    # Adjust scores by UCB for balancing exploration and exploitation\n    adjusted_scores = average_scores + ucb_values\n    \n    # Epsilon-greedy strategy with adaptive epsilon\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01)\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(range(num_actions))  # Explore\n    else:\n        action_index = np.argmax(adjusted_scores)  # Exploit\n\n    return action_index",
          "objective": -449.9999991313356,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        average_scores[action_index] = np.mean(scores) if selection_counts[action_index] > 0 else 0.0\n\n    # Upper Confidence Bound (UCB) calculation\n    ucb_values = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        if selection_counts[action_index] == 0:\n            ucb_values[action_index] = float('inf')\n        else:\n            confidence_bound = np.sqrt(np.log(total_selection_count) / selection_counts[action_index])\n            ucb_values[action_index] = average_scores[action_index] + confidence_bound\n\n    # Softmax action selection for exploring actions based on their scores\n    temperature = max(1.0 - (current_time_slot / total_time_slots), 0.1)\n    scaled_scores = np.exp(ucb_values / temperature)\n    probabilities = scaled_scores / np.sum(scaled_scores)\n    action_index = np.random.choice(range(num_actions), p=probabilities)\n\n    return action_index",
          "objective": -449.9999985649834,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration term using Upper Confidence Bound (UCB)\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1) / (selection_count + 1))) if selection_count > 0 else float('inf')\n        \n        # Combine average score and exploration value\n        action_scores[action_index] = average_score + exploration_value\n    \n    # Select the action with the maximum combined score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -449.9999984955775,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Adaptive exploration factor that decreases over time\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01)\n    \n    average_scores = []\n    exploration_bonus = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Confidence interval term\n        if selection_count > 0:\n            confidence_interval = np.sqrt((2 * np.log(total_selection_count)) / selection_count)\n        else:\n            confidence_interval = np.inf  # High uncertainty for unselected actions\n        \n        # Append average score and confidence interval\n        average_scores.append(average_score)\n        exploration_bonus.append(confidence_interval)\n\n    adjusted_scores = np.array(average_scores) + np.array(exploration_bonus)\n\n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, 8)  # Random selection\n    else:\n        action_index = np.argmax(adjusted_scores)  # Greedy selection based on UCB\n    \n    return action_index",
          "objective": -449.99999775749745,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration factor\n    average_scores = []\n    \n    # Calculate the upper confidence bound for each action\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score or set to zero if no scores are available\n        average_score = np.mean(scores) if selection_count > 0 else 0.0\n        \n        # Confidence interval term\n        if selection_count > 0:\n            confidence_interval = np.sqrt((2 * np.log(total_selection_count)) / selection_count)\n        else:\n            confidence_interval = np.inf  # No previous selections means high uncertainty\n        \n        # Adjusted score with UCB\n        adjusted_score = average_score + confidence_interval\n        \n        average_scores.append(adjusted_score)\n\n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        # Explore: randomly select an action\n        action_index = np.random.randint(0, 8)\n    else:\n        # Exploit: select the action with the highest adjusted score\n        action_index = np.argmax(average_scores)\n    \n    return action_index",
          "objective": -449.9999974349343,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Dynamic exploration-exploitation balance using Epsilon-Greedy method\n        epsilon = 1.0 / (current_time_slot + 1)  # Decaying exploration probability\n        \n        if np.random.rand() < epsilon:\n            action_scores[action_index] = np.random.rand()  # Explore: random score\n        else:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count + 1) / (selection_count + 1))) if selection_count > 0 else float('inf')\n            action_scores[action_index] = average_score + exploration_value  # Exploit: combine score and exploration\n    \n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -449.9999954845304,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon = 0.1  # Exploration rate\n    action_scores = np.zeros(num_actions)\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration component using an improved UCB method\n        if selection_count > 0:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_count)\n        else:\n            exploration_value = float('inf')\n        \n        # Combining average score and exploration value\n        action_scores[action_index] = average_score + exploration_value\n    \n    # Epsilon-greedy strategy to enhance exploration\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)\n    else:\n        # Softmax scaling for selection\n        exp_scores = np.exp(action_scores - np.max(action_scores))  # Stability factor\n        action_probabilities = exp_scores / np.sum(exp_scores)\n        action_index = np.random.choice(num_actions, p=action_probabilities)\n    \n    return action_index",
          "objective": -449.99999291638017,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # exploration factor\n    decay_rate = 0.99  # decay rate for exploration factor\n    adjusted_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score while handling division by zero\n        average_score = np.mean(scores) if scores else 0\n        \n        if selection_count == 0:\n            exploration_bonus = 1.0  # full bonus for unselected actions\n        else:\n            exploration_bonus = np.log(total_selection_count) / (selection_count + 1)  # logarithmic exploration bonus\n\n        # Adjust the exploration bonus with time-based factor\n        time_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = average_score + time_factor * exploration_bonus\n        \n        adjusted_scores.append(adjusted_score)\n    \n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(range(8))  # Explore\n    else:\n        action_index = np.argmax(adjusted_scores)  # Exploit\n    \n    return action_index",
          "objective": -449.9999924992982,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Adaptive parameters\n    epsilon = 0.1  # Exploration probability\n    alpha = 0.5    # Softmax adjustment parameter\n    \n    average_scores = []\n    action_counts = []\n    \n    # Calculate average scores and counts\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_count = len(scores)\n        \n        # Compute average score\n        average_score = np.mean(scores) if action_count > 0 else 0.0\n        average_scores.append(average_score)\n        action_counts.append(action_count)\n\n    # Exploration-exploitation decision\n    if np.random.rand() < epsilon:\n        # Explore: randomly select an action\n        action_index = np.random.randint(0, 8)\n    else:\n        # Softmax for exploitation\n        exp_scores = np.exp(np.array(average_scores) / alpha)\n        probabilities = exp_scores / np.sum(exp_scores)\n        action_index = np.random.choice(range(8), p=probabilities)\n        \n    return action_index",
          "objective": -449.99999220361127,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize parameters\n    exploration_factor = 0.5\n    confidence_level = 1.5  # Confidence level for UCB\n    average_scores = []\n    \n    for action_index in range(8):\n        scores = score_set[action_index]\n        selection_count = len(scores)\n        \n        # Compute average score, handling edge case of no scores\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Upper Confidence Bound (UCB)\n        exploration_bonus = confidence_level * np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Adjusted score\n        adjusted_score = average_score + exploration_bonus\n        \n        # Implement a decay factor based on the current time slot\n        decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score *= decay_factor\n        \n        average_scores.append(adjusted_score)\n    \n    action_index = np.argmax(average_scores)  # Select the action with the highest adjusted score\n    return action_index",
          "objective": -449.99998401089994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 1.0  # Adjust exploration parameter as needed\n    action_scores = []\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate the exploration term using UCB\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Calculate the combined score considering exploration\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)  # Select the action with the highest combined score\n    return action_index",
          "objective": -449.9999261842173,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.1  # Exploration parameter\n    number_of_actions = 8\n    action_scores = []\n\n    for action_index in range(number_of_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score, handling the case when selection_count is 0\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration term, ensuring no division by zero\n        if selection_count > 0:\n            exploration_value = np.sqrt(np.log(total_selection_count) / selection_count)\n        else:\n            exploration_value = float('inf')  # All actions are explored at least once\n\n        # Combined score for exploration-exploitation balance\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    # Use softmax to introduce randomness and exploration\n    action_scores = np.array(action_scores)\n    exp_scores = np.exp(action_scores - np.max(action_scores))  # For numerical stability\n    probabilities = exp_scores / np.sum(exp_scores)\n    \n    # Sample an action based on calculated probabilities\n    action_index = np.random.choice(number_of_actions, p=probabilities)\n    return action_index",
          "objective": -449.9997923787994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Dynamic parameters\n    epsilon = 0.1  # Exploration probability\n    confidence_weight = 1.0  # Weight for the upper confidence bounds\n    \n    average_scores = []\n    action_counts = []\n    \n    # Calculate average scores and counts\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_count = len(scores)\n        \n        # Compute average score\n        average_score = np.mean(scores) if action_count > 0 else 0.0\n        average_scores.append(average_score)\n        action_counts.append(action_count)\n\n    # Exploration-exploitation decision using Upper Confidence Bound (UCB)\n    if total_selection_count == 0 or np.random.rand() < epsilon:\n        # Explore: randomly select an action\n        action_index = np.random.randint(0, 8)\n    else:\n        ucb_values = []\n        for i in range(8):\n            if action_counts[i] > 0:\n                ucb = average_scores[i] + confidence_weight * np.sqrt(np.log(total_selection_count) / action_counts[i])\n            else:\n                ucb = float('inf')  # Prioritize unexplored actions\n            ucb_values.append(ucb)\n\n        action_index = np.argmax(ucb_values)\n        \n    return action_index",
          "objective": -449.9997489068736,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    exploration_param = 0.5  # Exploration weight\n    action_scores = np.zeros(num_actions)\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration component to ensure less selected actions are considered\n        if selection_count > 0:\n            exploration_value = np.sqrt((np.log(total_selection_count + 1) / (selection_count + 1)))\n        else:\n            exploration_value = float('inf')\n        \n        # Combine average score and exploration value\n        action_scores[action_index] = average_score + exploration_param * exploration_value\n    \n    # Softmax scaling to favor higher scores while allowing exploration\n    exp_scores = np.exp(action_scores - np.max(action_scores))  # Stability factor for numerical stability\n    action_probabilities = exp_scores / np.sum(exp_scores)\n    \n    # Select action based on the computed probabilities\n    action_index = np.random.choice(num_actions, p=action_probabilities)\n    \n    return action_index",
          "objective": -449.9991799088752,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.1  # Epsilon value for exploration\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration term\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combine average score and exploration value\n        action_scores[action_index] = average_score + exploration_param * exploration_value\n    \n    # Select the action with the maximum combined score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -449.9963163601071,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Exploration factor that decreases over time\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n    \n    adjusted_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score while handling division by zero\n        average_score = np.mean(scores) if scores else 0\n        \n        # Confidence interval component\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else 1.0\n        \n        # Time decay factor\n        time_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = average_score + time_factor * exploration_bonus\n        \n        adjusted_scores.append(adjusted_score)\n    \n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(range(8))  # Explore\n    else:\n        action_index = np.argmax(adjusted_scores)  # Exploit\n    \n    return action_index",
          "objective": -449.99576384394885,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_factor = 0.5\n    average_scores = []\n    \n    for action_index in range(8):\n        scores = score_set[action_index]\n        if len(scores) == 0:\n            average_score = 0  # No selections yet, treat as zero score\n        else:\n            average_score = np.mean(scores)  # Compute average score\n        \n        # Calculate selection rate to incorporate exploration\n        selection_count = len(scores)\n        exploration_bonus = exploration_factor * (1 / (selection_count + 1))  # Add slight bonus for less selected actions\n        \n        adjusted_score = average_score + exploration_bonus\n        average_scores.append(adjusted_score)\n    \n    action_index = np.argmax(average_scores)  # Select the action with the highest adjusted score\n    return action_index",
          "objective": -449.99566198076116,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters\n    exploration_weight = 0.2  # Weight for exploration factor\n    exploitation_weight = 1.0  # Weight for exploitation factor\n    epsilon = 0.1  # Probability of choosing exploration over exploitation\n    \n    average_scores = []\n    action_counts = np.zeros(8)  # To keep track of the number of selections for each action\n    \n    # Compute average scores and selection counts\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        average_score = np.mean(scores) if scores else 0  # Avoid division by zero\n        average_scores.append(average_score)\n\n    # Compute selection probabilities\n    probabilities = np.array(average_scores) * exploitation_weight\n    total_scores = np.sum(probabilities) if np.sum(probabilities) > 0 else 1  # Avoid division by zero\n    \n    # Normalize for probabilities\n    probabilities = probabilities / total_scores if total_scores > 0 else probabilities\n    \n    # Apply exploration strategy\n    if np.random.rand() < epsilon:\n        # Choose an action uniformly at random for exploration\n        action_index = np.random.randint(0, 8)\n    else:\n        # Choose based on the computed probabilities for exploitation\n        action_index = np.random.choice(range(8), p=probabilities)\n    \n    return action_index",
          "objective": -449.86311246743526,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Epsilon decay factor based on current time slot\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n    \n    # Calculate average scores and selection counts\n    average_scores = np.zeros(8)\n    selection_counts = np.zeros(8)\n    \n    for action_index, scores in score_set.items():\n        if scores:\n            average_scores[action_index] = np.mean(scores)\n            selection_counts[action_index] = len(scores)\n    \n    # Normalizing selection counts to avoid division by zero\n    normalized_counts = selection_counts / (np.sum(selection_counts) + 1e-6)\n    \n    # Exploration term\n    exploration_term = (1 - normalized_counts) * (1 / (total_selection_count + 1e-6))\n    \n    # Combine average scores and exploration term\n    action_values = (1 - epsilon) * average_scores + epsilon * exploration_term\n    \n    # Select action with the highest value\n    action_index = np.argmax(action_values)\n    \n    return action_index",
          "objective": -449.76844834527213,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration probability\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate the exploration term using UCB\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Calculate combined score\n        combined_score = average_score + exploration_value\n\n        # Add exploration parameter\n        if np.random.rand() < epsilon:\n            action_scores.append((float('inf'), action_index))  # Encourage exploration\n        else:\n            action_scores.append((combined_score, action_index))\n\n    # Select action with highest scoring\n    action_index = max(action_scores, key=lambda x: x[0])[1]\n    return action_index",
          "objective": -449.5758609930605,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 1.0 - (current_time_slot / total_time_slots)  # Decay epsilon over time\n    epsilon = max(epsilon, 0.1)  # Ensure epsilon is not less than 0.1 for exploration\n\n    action_values = []\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        if scores:\n            average_score = np.mean(scores)\n        else:\n            average_score = 0\n            \n        action_values.append(average_score)\n\n    if np.random.rand() < epsilon:\n        # Exploration: Select a random action\n        action_index = np.random.choice(range(8))\n    else:\n        # Exploitation: Select the action with the highest average score\n        action_index = np.argmax(action_values)\n\n    return action_index",
          "objective": -446.51809602159886,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8  # Number of actions (0 to 7)\n    exploration_factor = np.log(total_selection_count + 1)  # Encourages exploration based on historical attempts\n    \n    # Initialize scores and counts\n    scores = np.zeros(n_actions)\n    counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(n_actions):\n        action_scores = score_set.get(action_index, [])\n        counts[action_index] = len(action_scores)\n        if counts[action_index] > 0:\n            scores[action_index] = np.mean(action_scores)\n\n    # Ensure action selection considers both scores and counts\n    ucb_values = np.zeros(n_actions)\n    for action_index in range(n_actions):\n        if counts[action_index] == 0:\n            ucb_values[action_index] = float('inf')  # Favor untried actions\n        else:\n            ucb_values[action_index] = scores[action_index] + np.sqrt((2 * exploration_factor) / counts[action_index])\n\n    # Select action index with the highest UCB value\n    action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -440.6691373529234,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_factor = 0.5\n    action_indices = list(score_set.keys())\n    \n    # Initialize lists to hold average scores and selection counts\n    avg_scores = []\n    selection_counts = []\n\n    for index in action_indices:\n        scores = score_set[index]\n        selection_count = len(scores)\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        avg_scores.append(average_score)\n        selection_counts.append(selection_count)\n\n    # Calculate exploration bonus based on selection counts\n    exploration_bonuses = [\n        exploration_factor * (1 / (count + 1)) for count in selection_counts\n    ]\n\n    # Combined score is average score plus exploration bonus\n    combined_scores = np.array(avg_scores) + np.array(exploration_bonuses)\n    \n    # Normalize based on current time slot vs total time slots for dynamic adjustment\n    time_adjustment_factor = (current_time_slot + 1) / total_time_slots\n    final_scores = combined_scores * time_adjustment_factor\n    \n    # Select the action with the maximum final score\n    action_index = np.argmax(final_scores)\n    \n    return action_index",
          "objective": -433.0315339308396,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8  # Number of actions (0 to 7)\n    exploration_factor = 0.1  # Exploration factor for UCB\n    scores = np.zeros(n_actions)\n    counts = np.zeros(n_actions)\n    \n    # Calculate the scores and counts\n    for action_index in score_set:\n        action_scores = score_set[action_index]\n        counts[action_index] = len(action_scores)\n        if counts[action_index] > 0:\n            scores[action_index] = np.mean(action_scores)\n\n    # Avoid division by zero for UCB\n    total_counts = np.sum(counts)\n    \n    # Calculate UCB\n    ucb_values = np.zeros(n_actions)\n    \n    for action_index in range(n_actions):\n        if counts[action_index] == 0:\n            ucb_values[action_index] = float('inf')  # Infinite UCB for unexplored actions\n        else:\n            ucb_values[action_index] = scores[action_index] + exploration_factor * np.sqrt(np.log(total_counts) / counts[action_index])\n    \n    # Select the action with the highest UCB value\n    action_index = np.argmax(ucb_values)\n    \n    return action_index",
          "objective": -421.67962783216586,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 1  # Tunable parameter for exploration\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration term\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combined score for action selection\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    # Optionally introduce softmax to enhance selection diversity\n    exp_scores = np.exp(action_scores - np.max(action_scores))  # for numerical stability\n    probabilities = exp_scores / np.sum(exp_scores)\n    \n    # Sample action based on calculated probabilities\n    action_index = np.random.choice(range(8), p=probabilities)\n    \n    return action_index",
          "objective": -413.4178792096898,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Setting the exploration rate\n    epsilon = max(0.1, 1.0 - (current_time_slot / total_time_slots))\n    \n    action_values = []\n    action_counts = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        average_score = np.mean(scores) if scores else 0\n        action_count = len(scores)\n        \n        action_values.append(average_score)\n        action_counts.append(action_count)\n\n    # Confidence interval adjustments using total selections\n    confidence_factors = [(1 + (count / (total_selection_count + 1))) for count in action_counts]\n    weighted_values = np.array(action_values) * np.array(confidence_factors)\n\n    if np.random.rand() < epsilon:\n        # Exploration: Select a random action\n        action_index = np.random.choice(np.arange(8), p=(np.array(action_counts) + 1) / np.sum(np.array(action_counts) + 1))\n    else:\n        # Exploitation: Select the action with the highest weighted average score\n        action_index = np.argmax(weighted_values)\n        \n    return action_index",
          "objective": 159.48384475039143,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8  # Number of actions (0 to 7)\n    exploration_factor = 1.0  # Exploration factor to balance exploration and exploitation\n    epsilon = 0.1  # Epsilon for epsilon-greedy strategy\n    \n    scores = np.zeros(n_actions)\n    counts = np.zeros(n_actions)\n    \n    for action_index in range(n_actions):\n        action_scores = score_set.get(action_index, [])\n        counts[action_index] = len(action_scores)\n        if counts[action_index] > 0:\n            scores[action_index] = np.mean(action_scores)\n    \n    total_counts = np.sum(counts)\n    \n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        # Explore: randomly select one of the actions\n        action_index = np.random.randint(n_actions)\n    else:\n        # Exploit: compute UCB values to select the best action\n        ucb_values = np.zeros(n_actions)\n        \n        for action_index in range(n_actions):\n            if counts[action_index] == 0:\n                ucb_values[action_index] = float('inf')  # Infinite UCB for unexplored actions\n            else:\n                ucb_values[action_index] = scores[action_index] + exploration_factor * np.sqrt(np.log(total_counts) / counts[action_index])\n        \n        action_index = np.argmax(ucb_values)\n    \n    return action_index",
          "objective": 1011.0804093183372,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    action_scores = np.zeros(n_actions)\n    \n    # Calculate the average scores for each action\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        if scores:\n            action_scores[action_index] = np.mean(scores)\n    \n    # Exploration factor based on total selections and current time slot\n    if total_selection_count == 0:  # No actions selected yet\n        return np.random.choice(n_actions)  # Random selection on the first action\n\n    # Dynamic epsilon that decreases with more selections\n    epsilon = max(0.1, 1.0 - (current_time_slot / total_time_slots))\n\n    # Epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(n_actions)\n    else:\n        action_index = np.argmax(action_scores)\n    \n    # Increase the chances of selecting less explored actions\n    selection_counts = np.array([len(score_set.get(i, [])) for i in range(n_actions)])\n    selection_counts[selection_counts == 0] = 1e-5  # Prevent division by zero\n    exploration_bonus = (total_selection_count / selection_counts) ** 0.5\n    adjusted_scores = action_scores + exploration_bonus\n    \n    # Return the action index based on the adjusted scores\n    return action_index",
          "objective": 1787.7221777498548,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8  # Number of actions (0 to 7)\n    epsilon_max = 0.5  # Maximum exploration probability\n    epsilon_min = 0.1  # Minimum exploration probability\n    decay_rate = 0.05  # Decay rate for epsilon\n    epsilon = max(epsilon_min, epsilon_max * (1 - (total_selection_count / (total_time_slots * decay_rate))))  \n    \n    scores = np.zeros(n_actions)\n    counts = np.zeros(n_actions)\n\n    # Calculate average scores and counts\n    for action_index in range(n_actions):\n        action_scores = score_set.get(action_index, [])\n        counts[action_index] = len(action_scores)\n        if counts[action_index] > 0:\n            scores[action_index] = np.mean(action_scores)\n\n    # Select action based on epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        # Exploration: Select a random action\n        action_index = np.random.choice(np.arange(n_actions))\n    else:\n        # Exploitation: Select the action with the highest average score\n        action_index = np.argmax(scores)\n\n    return action_index",
          "objective": 3274.5649072290053,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration factor\n    action_count = len(score_set)\n    \n    if total_selection_count == 0:\n        return np.random.randint(action_count)\n    \n    average_scores = []\n    for action in range(action_count):\n        scores = score_set.get(action, [])\n        if len(scores) > 0:\n            average_score = np.mean(scores)\n            average_scores.append(average_score)\n        else:\n            average_scores.append(0)  # Assign 0 if no scores are available\n    \n    if np.random.rand() < epsilon:\n        return np.random.randint(action_count)  # Random action for exploration\n    \n    # Selecting action based on UCB\n    ucb_values = []\n    for action in range(action_count):\n        n_i = len(score_set.get(action, []))\n        # If the action was never selected, give it a high UCB value\n        if n_i == 0:\n            ucb_value = float('inf')  # Inf for unselected actions promotes them being chosen first\n        else:\n            average_score = average_scores[action]\n            ucb_value = average_score + np.sqrt((2 * np.log(total_selection_count)) / n_i)\n        ucb_values.append(ucb_value)\n    \n    action_index = np.argmax(ucb_values)\n    return action_index",
          "objective": 8109.6659192035695,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # exploration probability\n    n_actions = 8   # total number of actions\n    action_scores = np.zeros(n_actions)\n    \n    # Calculate the average scores for each action\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        if len(scores) > 0:\n            action_scores[action_index] = np.mean(scores)\n        else:\n            action_scores[action_index] = 0  # No score if not selected yet\n    \n    # Epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(n_actions)\n    else:\n        action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": 11674.570136823202,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    scores = np.zeros(len(action_indices))\n    \n    for i, index in enumerate(action_indices):\n        action_scores = score_set[index]\n        if len(action_scores) > 0:\n            average_score = np.mean(action_scores)\n            selection_count = len(action_scores)\n            exploration_factor = (1 / (selection_count + 1))  # adding 1 to avoid division by zero\n            decay_factor = current_time_slot / total_time_slots\n            scores[i] = (average_score * decay_factor) + exploration_factor\n        else:\n            scores[i] = 1  # High exploration for actions never selected\n    \n    action_index = np.argmax(scores)\n    return action_index",
          "objective": 13078.557195218253,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_rate = 0.1\n    action_indices = list(score_set.keys())\n    \n    # Initialize arrays for average scores and selection counts\n    avg_scores = np.zeros(len(action_indices))\n    selection_counts = np.zeros(len(action_indices))\n    \n    for index in action_indices:\n        scores = score_set[index]\n        selection_counts[index] = len(scores)\n        avg_scores[index] = np.mean(scores) if selection_counts[index] > 0 else 0\n\n    # Calculate exploration probabilities (Epsilon-Greedy strategy)\n    exploration_probs = exploration_rate / len(action_indices) * np.ones(len(action_indices))\n    \n    # Adjust exploration based on selection counts\n    for i in range(len(action_indices)):\n        if selection_counts[i] > 0:\n            exploration_probs[i] += (1 - exploration_rate) * (avg_scores[i] / np.max(avg_scores))\n\n    # Select action based on exploration probabilities\n    action_index = np.random.choice(action_indices, p=exploration_probs / np.sum(exploration_probs))\n    \n    return action_index",
          "objective": 19184.925492570103,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores and selection counts\n    avg_scores = []\n    selection_counts = []\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        avg_score = np.mean(scores) if scores else 0.0\n        avg_scores.append(avg_score)\n        selection_counts.append(len(scores))\n\n    # Epsilon-greedy exploration factor\n    epsilon = 0.1\n    if np.random.rand() < epsilon:\n        # Exploration: choose a random action\n        action_index = np.random.choice(range(8))\n    else:\n        # Exploitation: Upper Confidence Bound (UCB) strategy\n        ucb_values = []\n        for action in range(8):\n            if selection_counts[action] == 0:\n                ucb_value = float('inf')  # Prioritize unselected actions\n            else:\n                confidence_interval = np.sqrt((2 * np.log(total_selection_count)) / selection_counts[action])\n                ucb_value = avg_scores[action] + confidence_interval\n            ucb_values.append(ucb_value)\n\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 20716.104424441277,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Adaptive exploration rate\n    epsilon = 0.1 * (total_time_slots - current_time_slot) / total_time_slots\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score, ensuring no division by zero\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration term using Upper Confidence Bound\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Calculate final score by combining exploitation and exploration\n        combined_score = average_score + (epsilon * exploration_value)\n        \n        # Include a time decay factor\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    # Select action with the highest adjusted score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration rate\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score, handling division by zero\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration term to balance exploration and exploitation\n        if selection_count > 0:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1))\n        else:\n            exploration_value = float('inf')  # Encourage exploration of unselected actions\n        \n        # Combine exploitation and exploration components\n        action_scores[action_index] = average_score + (epsilon * exploration_value)\n    \n    # Incorporate time decay factor\n    time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = action_scores * time_decay_factor\n\n    # Select action with highest adjusted score\n    action_index = np.argmax(adjusted_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_factor = 0.1  # Exploration parameter\n    num_actions = 8\n    \n    # Calculate average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        average_scores[action_index] = np.mean(scores) if selection_counts[action_index] > 0 else 0\n\n    # Calculate exploration values using UCB\n    exploration_values = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        if selection_counts[action_index] > 0:\n            exploration_values[action_index] = np.sqrt(np.log(total_selection_count) / selection_counts[action_index])\n        else:\n            exploration_values[action_index] = float('inf')  # Encouraging exploration if never selected\n\n    # Combine scores for selection\n    combined_scores = average_scores + exploration_factor * exploration_values\n\n    # Epsilon-greedy selection\n    epsilon = max(0.05, 0.1 * (total_time_slots - current_time_slot) / total_time_slots)  # Decrease epsilon over time\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, num_actions)  # Explore\n    else:\n        action_index = np.argmax(combined_scores)  # Exploit\n\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate the average score for each action\n    action_scores = []\n    selection_counts = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score and handle cases with no selection\n        average_score = np.mean(scores) if scores else 0\n        selection_counts.append(selection_count)\n        \n        # UCB exploration term\n        if selection_count > 0:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_count)\n        else:\n            exploration_value = float('inf')  # Prioritize exploration for unselected actions\n        \n        # Combined score calculation (prioritizing exploitation with a modicum of exploration)\n        combined_score = average_score + exploration_value\n        action_scores.append(combined_score)\n    \n    # Normalize scores for all actions to maintain a sound basis for decision-making\n    max_action_score = max(action_scores)\n    scaled_scores = [(score - max_action_score + 1e-5) for score in action_scores]\n    \n    # Softmax for converting scores to selection probabilities\n    exp_scores = np.exp(scaled_scores)\n    probabilities = exp_scores / np.sum(exp_scores)\n    \n    # Select action based on the calculated probabilities\n    action_index = np.random.choice(range(8), p=probabilities)\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.5  # Adjusted exploration factor\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score with safety check\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Compute exploration term with safety check\n        exploration_value = (\n            np.sqrt((np.log(total_selection_count + 1) / (selection_count + 1)))\n            if selection_count > 0 else float('inf')\n        )\n        \n        # Compute combined score\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    # Return action with the maximum combined score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_factor = 1.5  # Modify exploration factor for UCB\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        if selection_count > 0:\n            # Upper Confidence Bound Calculation\n            confidence_interval = exploration_factor * np.sqrt(np.log(total_selection_count) / selection_count)\n        else:\n            confidence_interval = float('inf')  # Encourage exploration for unselected actions\n        \n        # Combined score for UCB\n        combined_score = average_score + confidence_interval\n        action_scores.append(combined_score)\n    \n    action_index = np.argmax(action_scores)  # Exploit the action with the highest score\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.1  # exploration coefficient\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate dynamic exploration factor based on time progression\n        epsilon = 1.0 / (current_time_slot + 1)  # Decreasing epsilon over time\n        \n        # Calculate exploration value using UCB approach\n        if selection_count > 0:\n            exploration_value = np.sqrt(np.log(total_selection_count + 1) / selection_count)\n        else:\n            exploration_value = float('inf')  # Give unselected actions more exploration potential\n        \n        # Combine average score with exploration adjusted by epsilon for exploration-exploitation balance\n        combined_score = average_score * (1 - epsilon) + epsilon * exploration_value\n        action_scores.append(combined_score)\n    \n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate UCB parameter\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score and handle division by zero\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration term\n        exploration_value = np.sqrt((2 * np.log(max(total_selection_count, 1))) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Decay exploration as a function of current time slot\n        decay_factor = 1 - (current_time_slot / total_time_slots) if total_time_slots > 0 else 1\n        combined_score = (decay_factor * average_score) + (1 - decay_factor) * exploration_value\n        \n        action_scores.append(combined_score)\n    \n    # Select action with the highest combined score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Constants\n    epsilon = 0.05  # Exploration rate\n    exploration_scale = 2  # Scaling factor for exploration\n    \n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score, handle division by zero\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate the exploration term\n        exploration_value = (exploration_scale * np.log(total_selection_count + 1)) / (selection_count + 1) if selection_count > 0 else float('inf')\n        \n        # Combined score incorporating exploration\n        combined_score = average_score + (epsilon * exploration_value)\n        \n        # Time decay factor for relevance\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    # Select action with the maximum adjusted score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    exploration_factor = 1.5  # A factor to emphasize exploration\n    epsilon = 0.1              # Exploration parameter\n    action_scores = []\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n\n        # UCB exploration value calculation\n        if selection_count > 0:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count)) / selection_count)\n        else:\n            exploration_value = float('inf')  # Full exploration for untried actions\n        \n        # Dynamic exploration weight based on time remaining\n        time_weight = (total_time_slots - current_time_slot) / total_time_slots\n        \n        # Combined score with exploration emphasis\n        combined_score = average_score + exploration_factor * exploration_value * time_weight * epsilon\n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Adjust exploration parameters\n    epsilon = 0.1\n    action_scores = []\n    action_counts = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        average_score = np.mean(scores) if scores else 0\n        \n        # Store counts for UCB calculations\n        action_counts.append(selection_count)\n\n        # UCB exploration term\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combined score\n        combined_score = average_score + (epsilon * exploration_value)\n        \n        # Time decay factor\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n\n        action_scores.append(adjusted_score)\n\n    # Select action with highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Dynamic parameters for improved adaptation\n    epsilon_start = 0.1\n    decay_rate = 0.99\n    exploration_pressure = 0.1\n    \n    # Explore-exploit balancing\n    epsilon = epsilon_start * (decay_rate ** (current_time_slot / total_time_slots))\n    \n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score safely\n        average_score = np.mean(scores) if scores else 0\n        \n        # Exploration term adjustment\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combined score with a new weighting strategy\n        combined_score = average_score + (exploration_pressure * exploration_value)\n        \n        # Time decay factor for responsiveness\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        # Final score calculation with epsilon consideration\n        final_score = (1 - epsilon) * average_score + epsilon * adjusted_score\n        action_scores.append(final_score)\n\n    # Select action with highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Dynamic exploration rate\n    epsilon = max(0.1, 1.0 - (current_time_slot / total_time_slots))  # Ensure a minimum level of exploration\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score and handle division by zero\n        average_score = np.mean(scores) if scores else 0\n        \n        # UCB exploration term\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combine exploration and exploitation values\n        combined_score = (1 - epsilon) * average_score + epsilon * exploration_value\n        action_scores.append(combined_score)\n    \n    # Select action with the highest combined score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration factor\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score, handling cases where selection_count is zero\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration term, enhancing exploration for untried actions\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1e-5))\n        \n        # Calculate adaptive exploration parameter based on current time slot\n        exploration_adjustment = epsilon * (total_time_slots - current_time_slot) / total_time_slots\n        \n        # Compute combined score: stronger emphasis on exploitation\n        combined_score = average_score + (exploration_adjustment * exploration_value)\n        action_scores.append(combined_score)\n    \n    # Return action with the maximum combined score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # exploration probability\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration factor\n        if selection_count > 0:\n            exploration_value = np.sqrt(np.log(total_selection_count) / selection_count)\n        else:\n            exploration_value = float('inf')\n        \n        # Combine exploitation and exploration\n        combined_score = average_score + exploration_value\n        action_scores.append(combined_score)\n    \n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, 8)\n    else:\n        action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters\n    epsilon = 0.1  # Exploration rate\n    \n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if scores else 0\n        \n        # Compute exploration term only if action has been taken before\n        exploration_value = (np.sqrt(2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combine average score and exploration\n        combined_score = average_score + (epsilon * exploration_value)\n        \n        # Adjust score based on time decay\n        time_decay = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay\n        \n        action_scores.append(adjusted_score)\n    \n    # Select action with highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration rate\n    action_scores = []\n    \n    # Calculate scores and confidence bounds for each action\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score, handling division by zero\n        average_score = np.mean(scores) if scores else 0\n        \n        # Exploration term based on the Upper Confidence Bound formula\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Calculate adjusted score combining average and exploration\n        combined_score = average_score + (epsilon * exploration_value)\n\n        # Apply a time decay factor to favor early time slots\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    # Select action with the highest adjusted score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Exploration rate\n    epsilon = 0.1  \n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score, avoiding division by zero\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration term\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Calculate combined score\n        combined_score = average_score + (epsilon * exploration_value)\n        \n        # Incorporate time decay factor\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    # Select action with highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_weight = 1.5  # weight to encourage exploration\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration value using Upper Confidence Bound\n        exploration_value = 0\n        if selection_count > 0:\n            exploration_value = exploration_weight * np.sqrt(np.log(total_selection_count) / selection_count)\n        else:\n            exploration_value = float('inf')  # Encourage exploration if an action has never been selected\n        \n        # Combined score\n        combined_score = average_score + exploration_value\n        action_scores.append(combined_score)\n    \n    # Select the action with the highest combined score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    exploration_probability = 0.2  # Exploration parameter\n    action_scores = []\n    \n    # Dynamic scaling parameter to encourage exploration based on time left\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score for the action\n        average_score = np.mean(scores) if selection_count > 0 else 0\n\n        # UCB exploration value calculation\n        if selection_count > 0:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count)) / selection_count)\n        else:\n            exploration_value = float('inf')  # High exploration value for untried actions\n        \n        # Combine average score and exploration weight\n        combined_score = average_score + (exploration_probability * exploration_value * exploration_weight)\n        action_scores.append(combined_score)\n    \n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize parameters\n    action_count = 8\n    action_scores = np.zeros(action_count)\n    \n    # Calculate current time decay factor\n    time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n    \n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if scores else 0\n        \n        # Define UCB exploration term\n        if selection_count > 0:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count)) / selection_count)\n        else:\n            exploration_value = float('inf')  # Ensure unselected actions are prioritized\n        \n        # Calculate combined score, incorporating time decay\n        combined_score = average_score + exploration_value\n        adjusted_score = combined_score * time_decay_factor\n        \n        # Store adjusted score for each action\n        action_scores[action_index] = adjusted_score\n    \n    # Select action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    exploration_prob = 0.1  # Epsilon for exploration\n    \n    action_scores = []\n    \n    # Calculate time decay factor based on remaining time slots\n    time_weight = (total_time_slots - current_time_slot) / total_time_slots\n    \n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration term\n        exploration_value = (np.sqrt(float(total_selection_count)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n\n        # Combine average score with exploration potential\n        combined_score = average_score + exploration_prob * exploration_value * time_weight\n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters\n    epsilon = 0.1  # Exploration rate\n    exploration_factor = np.sqrt(2)  # Factor for exploration term\n    \n    action_scores = []\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score with safety against division by zero\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration term\n        if selection_count > 0:\n            exploration_value = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1))\n        else:\n            exploration_value = float('inf')  # This action has never been selected\n        \n        # Combined UCB score\n        combined_score = average_score + exploration_value\n        \n        # Adjust score with time decay\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n\n    # Select the action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Hyperparameters\n    exploration_weight = 1.0\n    min_score = 0.01  # Minimum score to avoid division by zero\n    \n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Handling case where no selections have been made for an action\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Exploration value using UCB \n        exploration_value = np.sqrt((exploration_weight * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Calculate combined score\n        combined_score = average_score + exploration_value\n        action_scores.append(combined_score)\n    \n    # Select action based on the highest UCB score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_indices = list(range(num_actions))\n    \n    # Calculate average scores\n    action_avg_scores = np.array([np.mean(score_set.get(i, [0])) for i in action_indices])\n    \n    # Count selections and handle exploration\n    action_counts = np.array([len(score_set.get(i, [])) for i in action_indices])\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / (action_counts + 1))\n    \n    # Avoid division by zero and ensure exploration for unselected actions\n    exploration_bonus[action_counts == 0] = float('inf')\n\n    # Combine average scores and exploration bonuses\n    combined_scores = action_avg_scores + exploration_bonus\n\n    # Select the action with the highest combined score\n    action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters\n    epsilon = 0.1  # Exploration rate\n    alpha = 0.5    # Decay factor for time influence \n\n    # Prepare to accumulate scores\n    action_scores = []\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score, preventing division by zero\n        average_score = np.mean(scores) if scores else 0\n        \n        # UCB exploration term\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n\n        # Combine scores: exploit high average scores & explore less selected actions\n        combined_score = average_score + (epsilon * exploration_value)\n\n        # Adjust score with time decay\n        time_decay_factor = np.power((total_time_slots - current_time_slot) / total_time_slots, alpha)\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    # Select action with highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    \"\"\"\n    Selects an action index based on historical scores and selection counts using a modified UCB strategy.\n    \n    Parameters:\n    - score_set (dict): Historical scores for each action (key: action index, value: list of scores).\n    - total_selection_count (int): Total number of selections made across all actions.\n    - current_time_slot (int): Current selection period.\n    - total_time_slots (int): Total number of available time slots.\n    \n    Returns:\n    - action_index (int): The index of the selected action.\n    \"\"\"\n    \n    # UCB exploration factor\n    exploration_constant = 1.5\n    average_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # UCB formula\n        if selection_count > 0:\n            exploration_value = exploration_constant * np.sqrt(np.log(total_selection_count) / selection_count)\n        else:\n            exploration_value = float('inf')  # Infinite for unselected actions\n        \n        combined_score = average_score + exploration_value\n        average_scores.append(combined_score)\n    \n    # Select action with the highest UCB score\n    action_index = np.argmax(average_scores)\n    \n    # Log the decision for tracking\n    print(f\"Current Time Slot: {current_time_slot}, Selected Action: {action_index}, Scores: {average_scores}\")\n\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration rate\n    num_actions = 8\n    action_scores = []\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if scores else 0\n        \n        # Exploration term based on selection count and total selections\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n\n        # Time decay factor to prioritize earlier time slots\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        \n        # Combined score for action selection\n        adjusted_score = (average_score + (epsilon * exploration_value)) * time_decay_factor\n        action_scores.append(adjusted_score)\n\n    # Select action with highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 1.5  # Modify this to adjust exploration power\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Dynamic exploration factor\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_count) if selection_count > 0 else float('inf')\n        \n        # Compute combined score using the UCB approach\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    # Select action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration rate\n    action_count = 8  # Total number of actions\n    action_scores = np.zeros(action_count)\n    \n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score, handling division by zero\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration term based on remaining time slots\n        if selection_count > 0:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1))\n        else:\n            exploration_value = float('inf')  # Encourage exploration for unselected actions\n        \n        # Combine average score with exploration\n        combined_score = average_score + (epsilon * exploration_value)\n        \n        # Apply time decay factor to adjust scores based on remaining time\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores[action_index] = adjusted_score\n    \n    # Select action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Throttle exploration-exploitation trade-off using UCB\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate the average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration value\n        exploration_value = (np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1))\n                             if selection_count > 0 else float('inf'))\n        \n        # Combined score with exploitation and exploration\n        combined_score = average_score + exploration_value\n        action_scores.append(combined_score)\n    \n    # Select action with the highest combined score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.5  # Increased exploration parameter\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration term using Upper Confidence Bound (UCB)\n        if selection_count > 0:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_count)\n        else:\n            exploration_value = float('inf')  # Prioritize untried actions\n        \n        # Compute combined score\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    # Return action with the maximum score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    exploration_factor = 0.2  # Adjustable parameter for exploration\n    action_scores = []\n    \n    # Calculate time decay factor for exploration\n    time_weight = (total_time_slots - current_time_slot) / total_time_slots\n    \n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if scores else 0\n        \n        # Exploration value using the square root of selection count\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combined score with exploration weight\n        combined_score = average_score + exploration_factor * exploration_value * time_weight\n        action_scores.append(combined_score)\n    \n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Ensure stability and readability in parameters\n    n_actions = 8\n    epsilon = 0.1  # Exploration rate\n    action_scores = []\n    \n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score, handling division by zero\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration term using Upper Confidence Bound (UCB)\n        if selection_count > 0:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count))\n        else:\n            exploration_value = float('inf')  # Assign inf to encourage exploration of untested actions\n        \n        # Combined score balancing exploitation and exploration\n        combined_score = average_score + epsilon * exploration_value\n        \n        # Incorporate time decay into score\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    # Select the action with the highest adjusted score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters\n    exploration_rate = 0.1  # Exploration parameter for epsilon-greedy strategy\n    \n    action_scores = []\n    \n    # Calculate current time decay factor\n    time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score, handling division by zero\n        average_score = np.mean(scores) if scores else 0\n        \n        # Exploration value using Upper Confidence Bound (UCB)\n        if selection_count > 0:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count)) / selection_count)\n        else:\n            exploration_value = float('inf')  # Prioritize exploring unselected actions\n        \n        # Combined score with exploration and time decay\n        combined_score = average_score + exploration_rate * exploration_value\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    # Select action with highest computed score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters\n    epsilon = 0.1  # Exploration rate\n    action_count = 8  # Number of actions\n\n    action_scores = []\n    total_actions = len(score_set)\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration term (UCB approach)\n        if selection_count > 0:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_count)\n        else:\n            exploration_value = float('inf')  # If no selections, maximize exploration\n        \n        # Calculate combined score for action selection\n        combined_score = average_score + (epsilon * exploration_value)\n\n        # Time decay factor for action scores\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n\n    # Select action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration rate\n    action_count = len(score_set)\n    action_scores = []\n    \n    # Normalize time decay factor\n    time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n    \n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration term\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Calculate combined score\n        combined_score = average_score + (epsilon * exploration_value)\n        \n        # Adjust score with time decay\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    # Select action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.1  # Exploration parameter\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0.0\n        \n        # Calculate a confidence interval for the action\n        if selection_count > 0:\n            exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1))\n        else:\n            exploration_value = float('inf')  # Allow exploration if the action has never been selected\n        \n        # Combined score for action selection\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    # Use softmax to introduce stochastic selection among similarly scored actions\n    action_probabilities = np.exp(action_scores - np.max(action_scores))\n    action_probabilities /= np.sum(action_probabilities)\n    \n    action_index = np.random.choice(range(8), p=action_probabilities)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Define constants\n    exploration_weight = 1.5  # Weight for exploration term\n    min_selections = 1  # Minimum selections to start considering an action\n    action_count = 8  # The number of actions\n    \n    action_scores = []\n    \n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Compute exploration value\n        if selection_count > 0:\n            exploration_value = np.sqrt((np.log(total_selection_count) + 1) / selection_count)\n        else:\n            exploration_value = float('inf')  # Encourage exploration for unselected actions\n        \n        # Combined score using UCB\n        combined_score = average_score + exploration_weight * exploration_value\n        action_scores.append(combined_score)\n    \n    # Select action based on combined scores\n    action_index = np.argmax(action_scores)  # Exploit: choose best combined score\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Hyperparameters\n    epsilon = 0.1  # Exploration rate\n    exploration_weight = 1.5  # Adjust exploration impact on total score\n    \n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score, handling division by zero\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration term using Upper Confidence Bound (UCB) method\n        if selection_count > 0:\n            exploration_value = exploration_weight * np.sqrt((2 * np.log(total_selection_count + 1)) / selection_count)\n        else:\n            exploration_value = float('inf')\n\n        # Calculate combined score for action selection\n        combined_score = average_score + exploration_value\n        \n        # Add context based on current time slot with linear time decay\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    # Select action with highest adjusted score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_coefficient = 1.41  # Exploration coefficient for UCB\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score, handling division by zero\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate upper confidence bound (UCB)\n        exploration_value = (\n            exploration_coefficient * np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1))\n        ) if selection_count > 0 else float('inf')\n        \n        # Combined score for action selection\n        combined_score = average_score + exploration_value\n        \n        # Adjust score based on time decay\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    # Select action with highest adjusted score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters\n    epsilon = 0.1  # Exploration factor\n    exploration_weight = np.sqrt(2)  # For UCB\n\n    # Calculating average scores and selection counts for each action\n    action_scores = []\n    action_counts = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Store average score and count for each action\n        action_scores.append(average_score)\n        action_counts.append(selection_count)\n\n    # UCB calculation\n    ucb_values = []\n    for action_index in range(8):\n        if action_counts[action_index] > 0:\n            ucb_value = action_scores[action_index] + exploration_weight * np.sqrt(np.log(total_selection_count) / action_counts[action_index])\n        else:\n            ucb_value = float('inf')  # Encourage exploration for unselected actions\n        ucb_values.append(ucb_value)\n\n    # Implement epsilon-greedy combined with UCB\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, 8)  # Explore: random selection\n    else:\n        action_index = np.argmax(ucb_values)  # Exploit: choose best UCB value\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Epsilon for epsilon-greedy strategy\n    exploration_param = 0.5  # Exploration parameter adjustment\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Compute exploration term\n        if selection_count > 0:\n            exploration_value = np.sqrt(np.log(total_selection_count + 1) / selection_count)\n        else:\n            exploration_value = float('inf')  # Infinite exploration for untried actions\n        \n        # Compute combined score\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    # Epsilon-greedy selection: explore with probability epsilon\n    if np.random.rand() < epsilon:\n        # Select a random action among those not selected before or a random action\n        unexplored_actions = [i for i in range(8) if len(score_set.get(i, [])) == 0]\n        if unexplored_actions:\n            action_index = np.random.choice(unexplored_actions)\n        else:\n            action_index = np.random.randint(0, 8)\n    else:\n        # Select the action with the maximum combined score\n        action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 1.0  # Exploration parameter for the strategy\n    action_scores = []\n    exploration_values = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score and handle the case of zero selections\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration term\n        exploration_value = (np.sqrt(2 * np.log(total_selection_count + 1)) / (selection_count + 1e-5)) if selection_count > 0 else float('inf')\n        \n        # Compute combined score (exploiting historical performance and encouraging exploration)\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    # Return action with the maximum combined score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 1.0  # Exploration parameter\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0.0\n        \n        # Calculate exploration term with Laplace smoothing to handle zero selections gracefully\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combined score for action selection\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_factor = 0.1  # Exploration component\n    action_scores = []\n    \n    # Calculate scores for each action\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score or set default if no selections\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Use Upper Confidence Bound (UCB) for exploration\n        if selection_count > 0:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count)) / selection_count)\n        else:\n            exploration_value = float('inf')  # Encourage exploration for unselected actions\n        \n        # Combined score\n        combined_score = average_score + exploration_factor * exploration_value\n        action_scores.append(combined_score)\n    \n    # Epsilon-greedy strategy for exploration vs. exploitation\n    epsilon = 0.1\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, 8)  # Random selection\n    else:\n        action_index = np.argmax(action_scores)  # Best action based on computed scores\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.1  # exploration coefficient\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration factor using UCB\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Calculate final score combining exploitation and exploration\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    # Select the action with the highest combined score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Dynamic epsilon value, decreases over time to shift from exploration to exploitation\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score, handling division by zero\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration term using UCB\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combined score calculation with time decay\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        combined_score = average_score + (epsilon * exploration_value * time_decay_factor)\n\n        action_scores.append(combined_score)\n    \n    # Select action with the highest combined score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Define exploration rate\n    epsilon = 0.2  # Increase exploration rate for more interaction with lesser-used actions\n    exploration_weight = 1.5  # Weight for exploration term\n    \n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration term to encourage trying less frequent actions\n        exploration_term = np.sqrt(exploration_weight * (np.log(total_selection_count + 1) / (selection_count + 1))) if selection_count > 0 else float('inf')\n        \n        # Combined score with weighted exploration\n        combined_score = average_score + (epsilon * exploration_term)\n        \n        # Temporal consideration for decay in potential scores towards the end of the time slots\n        time_decay = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay\n        \n        action_scores.append(adjusted_score)\n    \n    # Select action with highest adjusted score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.5  # Exploration parameter adjustment\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration term\n        exploration_value = np.sqrt(2 * np.log(total_selection_count + 1) / (selection_count + 1e-5))\n        \n        # Compute combined score using UCB approach\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    # Return action with the maximum score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    num_actions = 8\n    exploration_factor = np.sqrt(np.log(total_selection_count + 1) / (np.array([len(score_set.get(i, [])) for i in range(num_actions)]) + 1))\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate the combined score using a hybrid method\n        combined_score = average_score + exploration_factor[action_index] * (total_time_slots / (current_time_slot + 1))\n        action_scores.append(combined_score)\n    \n    # Select action with the highest combined score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Exploration rate for balancing exploration and exploitation\n    epsilon = 0.1  \n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if scores else 0\n        \n        # Exploration term\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combined score calculation\n        combined_score = average_score + (epsilon * exploration_value)\n        \n        # Time decay factor\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    # Select action with the highest adjusted score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    exploration_factor = 0.2  # Adjusted exploration factor for scaling\n    \n    action_scores = []\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Average score calculation\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Upper Confidence Bound metric\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combined score\n        combined_score = average_score + exploration_factor * exploration_value\n        action_scores.append(combined_score)\n\n    # Softmax to convert scores to probabilities\n    exp_scores = np.exp(action_scores - np.max(action_scores))  # Stability in softmax computation\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Select action based on probabilities\n    action_index = np.random.choice(range(action_count), p=probabilities)\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.5  # Exploration parameter that can be tuned\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Compute exploration term\n        if selection_count > 0:\n            exploration_value = np.sqrt(np.log(total_selection_count) / selection_count)\n        else:\n            exploration_value = float('inf')  # Penalize untried actions\n        \n        # Dynamic adjustment of exploration based on time slot\n        remaining_time_slots = total_time_slots - current_time_slot\n        exploration_weight = (remaining_time_slots / total_time_slots)  # Decays over time\n        \n        # Compute combined score\n        combined_score = average_score + exploration_param * exploration_weight * exploration_value\n        action_scores.append(combined_score)\n    \n    # Return action with the maximum score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.1  # exploration coefficient\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration factor\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Calculate final score combining exploitation and exploration\n        combined_score = average_score + exploration_param * exploration_value\n        \n        # Use a normalization factor based on total time slots\n        normalization_factor = 1 - (current_time_slot / total_time_slots)\n        adjusted_score = combined_score * normalization_factor\n        \n        action_scores.append(adjusted_score)\n    \n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration rate\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score safely\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration term using UCB\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combine the average score and exploration value\n        combined_score = average_score + exploration_value\n        \n        # Adjust score based on time decay\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    # Select action with highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters\n    epsilon = 0.1  # Exploration rate\n    num_actions = 8\n    \n    # Calculate action scores\n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        avg_scores[action_index] = np.mean(scores) if scores else 0\n    \n    # Calculate exploration value and adjusted scores\n    ucb_scores = avg_scores.copy()\n    \n    for action_index in range(num_actions):\n        if selection_counts[action_index] > 0:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts[action_index] + 1))\n        else:\n            exploration_value = float('inf')  # Encourage exploration for unselected actions\n        \n        ucb_scores[action_index] += epsilon * exploration_value\n    \n    # Adjust scores based on remaining time slots\n    time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = ucb_scores * time_decay_factor\n    \n    # Select action with the highest adjusted score\n    action_index = np.argmax(adjusted_scores)\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Exploration factor\n    exploration_weight = 2.0\n    action_scores = []\n    actions = len(score_set)\n\n    for action_index in range(actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration value\n        exploration_value = (exploration_weight * np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1))) if selection_count > 0 else float('inf')\n\n        # Combine average score and exploration value\n        combined_score = average_score + exploration_value\n        action_scores.append(combined_score)\n    \n    # Select action based on the highest combined score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.2  # Exploration parameter (tuned for balance)\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score with handling for no scores\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration factor with handling for untested actions\n        exploration_value = (np.sqrt(np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else np.inf\n        \n        # Incorporate both exploitation and exploration\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    # Select the action with the highest combined score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Hyperparameters\n    epsilon = 0.1  # Exploration rate\n    alpha = 0.5  # Weight for the time decay factor\n    \n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration term\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combined score considering both exploration and exploitation\n        combined_score = average_score + (epsilon * exploration_value)\n        \n        # Adjust score with time decay factor\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * (time_decay_factor ** alpha)\n        \n        action_scores.append(adjusted_score)\n    \n    # Select action with highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.1  # Exploration parameter adjustment\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score for each action\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration term: UCB factor\n        if selection_count > 0:\n            exploration_value = np.sqrt(2 * np.log(total_selection_count + 1) / selection_count)\n        else:\n            exploration_value = float('inf')  # Favor untried actions\n        \n        # Compute combined score\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    # Return action with the maximum score, while allowing for ties\n    action_index = np.random.choice(np.flatnonzero(action_scores == np.max(action_scores)))\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration rate\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score safely\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration term with safety checks\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combined score with an exploration factor\n        combined_score = average_score + (epsilon * exploration_value)\n        \n        # Time decay factor\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    # Select action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_factor = 0.1\n    action_scores = []\n    action_counts = []\n    \n    # Calculate average scores and counts for each action\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        action_counts.append(selection_count)\n        \n        # Average score with handling for zero selections\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Upper Confidence Bound calculation\n        if selection_count > 0:\n            exploration_value = np.sqrt(np.log(total_selection_count) / selection_count)\n        else:\n            exploration_value = float('inf')  # Encourage exploration for unselected actions\n        \n        # Combined score\n        combined_score = average_score + exploration_factor * exploration_value\n        action_scores.append(combined_score)\n    \n    # Select action using UCB based deliberate strategy\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_factor = 1.5  # Adjust as necessary for exploration depth\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)  # Number of times the action has been selected\n\n        # Calculate average score and handle cases with no selections\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Upper Confidence Bound for exploration component\n        if selection_count > 0:\n            exploration_value = exploration_factor * np.sqrt(np.log(total_selection_count) / selection_count)\n        else:\n            exploration_value = float('inf')  # Encourage exploration for unselected actions\n            \n        # Combined score\n        combined_score = average_score + exploration_value\n        action_scores.append(combined_score)\n    \n    # Select the action based on the scores\n    action_index = np.argmax(action_scores)  # Always exploit the action with the highest combined score\n\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Constants\n    epsilon = 0.1  # Exploration factor for epsilon-greedy strategy\n    \n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score, handling division by zero\n        average_score = np.mean(scores) if scores else 0\n        \n        # Use a modified UCB approach for exploration\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        combined_score = average_score + exploration_value\n        \n        # Apply time decay factor to encourage timely scoring\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    # Select action with highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Define parameters for the selection strategy\n    exploration_factor = 1.0  # UCB exploration multiplier\n    confidence_level = 1.96   # For 95% confidence in UCB calculation\n\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Average score calculation, handling cases where no scores are present\n        average_score = np.mean(scores) if selection_count > 0 else 0.0\n        \n        # UCB calculation\n        if selection_count > 0:\n            exploration_value = np.sqrt((confidence_level * np.log(total_selection_count)) / selection_count)\n        else:\n            exploration_value = float('inf')  # Encourage exploration for untried actions\n        \n        combined_score = average_score + exploration_factor * exploration_value\n        action_scores.append(combined_score)\n    \n    # Selecting the action with the highest combined score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = np.sqrt(np.log(total_selection_count + 1))  # Exploration factor based on log\n    action_scores = []\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score with handling of empty score lists\n        average_score = np.mean(scores) if selection_count > 0 else 0.0\n\n        # Calculate exploration term with handling for unselected actions\n        exploration_value = exploration_param / (selection_count + 1) if selection_count > 0 else float('inf')\n        \n        # Define a combined score for action selection\n        combined_score = average_score + exploration_value\n        action_scores.append(combined_score)\n\n    # Selecting the action that has the highest combined score\n    action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Exploration rate\n    epsilon = 0.1\n    action_scores = []\n    \n    # Calculate the maximum allowed exploration factor\n    exploration_factor = np.sqrt(2)  # This is a common factor used in UCB strategies\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration term (only if the action has been selected before)\n        if selection_count > 0:\n            exploration_value = exploration_factor * np.sqrt(np.log(total_selection_count) / selection_count)\n        else:\n            exploration_value = float('inf')  # Encourage exploration of untried actions\n        \n        # Compute combined score for action selection\n        combined_score = average_score + (epsilon * exploration_value)\n        \n        # Time decay adjustment\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    # Select action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration rate\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score, safely handle division by zero\n        average_score = np.mean(scores) if scores else 0.0\n        \n        # Compute exploration component\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combined score with time decay factor\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        combined_score = (average_score + epsilon * exploration_value) * time_decay_factor\n        \n        action_scores.append(combined_score)\n    \n    # Select the action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    beta = 0.2  # Adjust the exploration factor based on remaining time slots\n    action_scores = []\n    \n    # Time decay factor\n    decay_factor = (total_time_slots - current_time_slot) / total_time_slots if total_time_slots > current_time_slot else 1\n    \n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration value\n        if selection_count > 0:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count)) / selection_count) * decay_factor\n        else:\n            exploration_value = float('inf')  # Encourage exploration of untried actions\n        \n        # Combine average score and exploration value\n        combined_score = average_score + beta * exploration_value\n        action_scores.append(combined_score)\n    \n    # Select action with the highest combined score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.5  # Increased exploration parameter\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration term using a logarithmic approach\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Compute combined score\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    # Choose action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize parameters\n    exploration_param = 1.5  # Exploration weight\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration term\n        if selection_count > 0:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_count)\n        else:\n            exploration_value = float('inf')  # Encourage exploration for untried actions\n        \n        # Calculate dynamic weight for exploration based on the current time slot\n        exploration_decay = 1 - (current_time_slot / total_time_slots)\n        combined_score = average_score + exploration_param * exploration_value * exploration_decay\n        \n        action_scores.append(combined_score)\n    \n    # Return action with the maximum computed score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize parameters\n    num_actions = 8\n    epsilon = 0.1  # Exploration rate\n\n    # Initialize lists to store average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        selection_counts[action_index] = selection_count\n        average_scores[action_index] = np.mean(scores) if scores else 0\n\n    # Calculate exploration values based on UCB\n    combined_scores = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        exploration_value = (np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts[action_index] + 1))\n                             if selection_counts[action_index] > 0 else float('inf'))\n        combined_scores[action_index] = average_scores[action_index] + (epsilon * exploration_value)\n    \n    # Apply time decay factor\n    time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = combined_scores * time_decay_factor\n    \n    # Select action with highest score\n    action_index = np.argmax(adjusted_scores)\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.5  # Increased exploration parameter for balance\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration term\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1e-5))  # Added a small constant to avoid division by zero\n        \n        # Compute combined score\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    # Return action with the maximum score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Constants\n    epsilon = 0.1  # Exploration rate\n\n    # Initialize action scores\n    action_scores = []\n    \n    # Loop over each action index from 0 to 7\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score\n        average_score = np.mean(scores) if scores else 0.0\n        \n        # Calculate exploration term using Upper Confidence Bound (UCB)\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n\n        # Combine exploration and exploitation scores\n        combined_score = average_score + (epsilon * exploration_value)\n\n        # Apply time decay to the score\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n\n        action_scores.append(adjusted_score)\n\n    # Identify the action with the maximum score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_weight = 1.5  # Trade-off parameter for UCB\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate the average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n\n        # Calculate exploration value using UCB formula\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n\n        # Combined score\n        combined_score = average_score + exploration_weight * exploration_value\n        action_scores.append(combined_score)\n    \n    # Select action based on the highest combined score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.1  # Exploration parameter\n    action_scores = []\n    number_of_actions = 8\n\n    for action_index in range(number_of_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0.0\n        \n        # Calculate exploration term using UCB\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combined score for action selection\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores and handle division by zero\n    action_averages = []\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        average_score = np.mean(scores) if scores else 0\n        action_averages.append(average_score)\n\n    # Calculate temperature for Softmax\n    temperature = 1.0 / (current_time_slot + 1)\n    \n    # Exponentiated scores to calculate Softmax probabilities\n    exp_scores = np.exp(np.array(action_averages) / temperature)\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Select action based on probabilities\n    action_index = np.random.choice(range(8), p=probabilities)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters for exploration-exploitation\n    epsilon = 0.1  # Epsilon-greedy parameter\n    exploration_param = 1.0  # Exploration coefficient\n\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score safely\n        average_score = np.mean(scores) if selection_count > 0 else 0.0\n        \n        # Adjust exploration based on the time slot and total selections\n        exploration_value = (exploration_param * np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1))) if selection_count else float('inf')\n        \n        # Combine average score and exploration factor\n        combined_score = average_score + exploration_value\n        \n        # Add to scores list\n        action_scores.append(combined_score)\n\n    # Epsilon-greedy decision making to balance exploration and exploitation\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, 8)  # Explore\n    else:\n        action_index = np.argmax(action_scores)  # Exploit\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.1  # Exploration parameter for balancing exploration and exploitation.\n    action_scores = []\n    total_actions = len(score_set)\n    \n    for action_index in range(total_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration probability (epsilon) dynamically decreasing over time.\n        epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n        \n        # Choose UCB method for exploration.\n        exploration_value = np.sqrt(2 * np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combine average score with exploration value.\n        combined_score = (1 - epsilon) * average_score + epsilon * exploration_param * exploration_value\n        action_scores.append(combined_score)\n\n    # Select the action with the highest score.\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Constants\n    epsilon = 0.1  # Exploration rate\n    alpha = 2.0  # Exploration weight\n    action_count = 8  # Total number of actions\n\n    action_scores = []\n    \n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration term\n        exploration_value = np.sqrt((alpha * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Dynamic time decay factor, amplified exploration for earlier slots\n        time_decay_factor = ((total_time_slots - current_time_slot + 1) / (total_time_slots + 1)) ** 2\n        \n        # Calculate combined score\n        combined_score = average_score + (epsilon * exploration_value * time_decay_factor)\n        \n        action_scores.append(combined_score)\n    \n    # Select action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    epsilon = 0.1  # Exploration parameter\n    action_scores = []\n\n    # Dynamic temperature parameter to adjust exploration-exploitation trade-off\n    temperature = total_time_slots / (current_time_slot + 1)\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if scores else 0\n        \n        # UCB exploration value calculation\n        if selection_count > 0:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count)) / selection_count)\n        else:\n            exploration_value = float('inf')  # Allocate exploration to actions not yet tried\n        \n        # Calculate combined score with adaptive exploration weight\n        combined_score = average_score + epsilon * exploration_value * temperature / total_time_slots\n        action_scores.append(combined_score)\n    \n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 1.5  # Adjust exploration parameter to increase exploration\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration term\n        if selection_count > 0:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_count)\n        else:\n            exploration_value = float('inf')  # Encourage exploration for untried actions\n        \n        # Calculate adjustments for time decay to favor more recent scores\n        decay_factor = current_time_slot / total_time_slots\n        adjusted_average_score = average_score * decay_factor\n        \n        # Compute combined score\n        combined_score = adjusted_average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    # Return action with the maximum score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Define hyperparameters\n    exploration_factor = 1.5\n    exploration_rate = 0.1\n    \n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Average score calculation\n        average_score = np.mean(scores) if scores else 0\n\n        # Exploration term with UCB approach\n        if selection_count > 0:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1))\n        else:\n            exploration_value = float('inf')  # Fully explore actions that have never been selected\n\n        # Combined score for UCB strategy\n        combined_score = average_score + exploration_factor * exploration_value\n        \n        # Time decay factor\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n\n    # Select action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 1.0  # Exploration parameter\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0.0\n        \n        # Calculate exploration term\n        if selection_count > 0:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_count)\n        else:\n            exploration_value = float('inf')  # Encourage exploration for untried actions\n        \n        # Compute combined score\n        combined_score = average_score + exploration_param * exploration_value\n        \n        # Penalize for late-time slots to prioritize actions with higher historical scores early on\n        time_penalty = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * (1 - time_penalty)\n        \n        action_scores.append(adjusted_score)\n    \n    # Return action with the maximum adjusted score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.5  # Adjusted for a better exploration/exploitation balance\n    action_scores = []\n    action_indices = range(8)\n\n    for action_index in action_indices:\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration term\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1e-6))\n        \n        # Compute combined score\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n\n    # Implementing a temperature parameter to govern exploration\n    temperature = 1 / (current_time_slot + 1)  # Decays exploration over time\n\n    # Normalize scores for a softmax-like selection (to enable exploration)\n    exp_scores = np.exp(np.array(action_scores) / temperature)\n    probabilities = exp_scores / np.sum(exp_scores)\n    \n    # Sample an action according to the calculated probabilities\n    action_index = np.random.choice(action_indices, p=probabilities)\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # exploration parameter\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if scores else 0.0\n        \n        # Calculate exploration value\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Apply epsilon-greedy approach\n        if np.random.rand() < epsilon:\n            combined_score = exploration_value  # Favor exploration\n        else:\n            combined_score = average_score + exploration_value  # Favor exploitation\n        \n        action_scores.append(combined_score)\n    \n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Define epsilon to adjust the exploration rate dynamically based on time\n    epsilon = max(0.1, (total_time_slots - current_time_slot) / total_time_slots * 0.5)\n    \n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score, handling division by zero\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration term based on the Upper Confidence Bound formula\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combined score incorporating both the average score and exploration\n        combined_score = average_score + (epsilon * exploration_value)\n        \n        # Time decay factor to prioritize urgent selections\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    # Select the action with the highest adjusted score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # exploration rate\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score\n        average_score = np.mean(scores) if scores else 0\n\n        # Calculate exploration factor with safety for division by zero\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n\n        # Combine average score and exploration value\n        combined_score = average_score + epsilon * exploration_value\n        action_scores.append(combined_score)\n\n    # Select the action with the highest combined score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    def calculate_average_scores(scores):\n        return np.mean(scores) if scores else 0\n    \n    def calculate_exploration_bonus(selection_count):\n        if selection_count == 0:\n            return float('inf')  # Encourage exploration for unselected actions\n        return np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1))\n        \n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        average_score = calculate_average_scores(scores)\n        exploration_value = calculate_exploration_bonus(selection_count)\n        \n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = average_score + exploration_value * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    epsilon = 0.1  # Exploration parameter\n    action_scores = []\n    \n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n\n        # UCB exploration value calculation\n        if selection_count > 0:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count)) / selection_count)\n        else:\n            exploration_value = float('inf')  # Encourage exploration for untried actions\n        \n        # Time-based weighting\n        time_fraction = (total_time_slots - current_time_slot) / total_time_slots\n\n        # Combined score calculation\n        combined_score = average_score + (epsilon * exploration_value * time_fraction)\n        action_scores.append(combined_score)\n    \n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Exploration parameter that decays over time slots to balance exploration and exploitation\n    exploration_param = np.sqrt(np.log(total_time_slots) / (current_time_slot + 1))\n\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Dynamic exploration term\n        exploration_value = np.sqrt(2 * np.log(total_selection_count + 1) / (selection_count + 1e-5))\n        \n        # Compute combined score\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    # Select the action with the maximum score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration parameter for epsilon-greedy strategy\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration value\n        exploration_value = (np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) \n                             if selection_count > 0 else float('inf'))\n        \n        # Compute combined score using weighted average of exploitation and exploration\n        combined_score = (1 - epsilon) * average_score + epsilon * exploration_value\n        action_scores.append(combined_score)\n\n    # Epsilon-greedy selection: explore with probability epsilon, otherwise exploit\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(range(8))  # Randomly select an action for exploration\n    else:\n        action_index = np.argmax(action_scores)  # Select the action with the highest score\n\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize parameters\n    num_actions = 8\n    epsilon = 1.0 / (current_time_slot + 1)  # Decaying exploration rate\n    action_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        average_score = np.mean(scores) if scores else 0\n\n        # UCB exploration term\n        if selection_counts[action_index] > 0:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts[action_index] + 1))\n        else:\n            exploration_value = float('inf')\n\n        # Combined score using epsilon-greedy balancing\n        action_scores[action_index] = (1 - epsilon) * average_score + epsilon * exploration_value\n\n    # Select action with the highest combined score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Adaptive epsilon for exploration\n    epsilon = max(0.1, 0.9 * (total_time_slots - current_time_slot) / total_time_slots)\n    \n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration term based on Upper Confidence Bound (UCB) method\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Calculate combined score with time decay factor\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        combined_score = average_score + (epsilon * exploration_value)\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    # Select action with highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.2  # Increased exploration coefficient\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score with a fallback for unselected actions\n        average_score = np.mean(scores) if scores else 0\n        \n        # Improved exploration factor calculation\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combined score balancing exploitation and exploration\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_weight = 0.1  # Weight for exploration\n    action_scores = []\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score, handle division by zero\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate UCB exploration term\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combine scores for action selection\n        combined_score = average_score + exploration_weight * exploration_value\n        \n        # Apply time decay to the combined score\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    # Select action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    exploration_weight = 0.2  # Weight of exploration in the final score calculation\n    decay_factor = 0.5  # Factor to decay exploration over time for less aggressive exploration\n    \n    action_scores = []\n    \n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration value based on UCB\n        exploration_value = (np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1))\n                             if selection_count > 0 else float('inf'))\n        \n        # Combine scores: higher weight on average score with a touch of exploration\n        combined_score = average_score * (1 - exploration_weight) + exploration_weight * exploration_value\n        \n        # Incorporate decay in exploration value over time\n        if current_time_slot > 0:\n            combined_score *= (1 - decay_factor * (current_time_slot / total_time_slots))\n        \n        action_scores.append(combined_score)\n    \n    # Select action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters\n    epsilon = 0.1  # Exploration rate\n    alpha = 0.5    # Weighting factor for temporal decay influence\n\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score, with a fallback for division by zero\n        average_score = np.mean(scores) if scores else 0.0\n        \n        # Calculate exploration value using UCB-inspired exploration\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combine average score with exploration term, adjusted by current time slot\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        combined_score = (average_score + epsilon * exploration_value) * (time_decay_factor ** alpha)\n\n        action_scores.append(combined_score)\n    \n    # Select action with the highest score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 1.0  # Exploration parameter for the UCB method.\n    action_scores = []\n    total_actions = len(score_set)\n\n    for action_index in range(total_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate confidence interval for UCB\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # UCB score combines average score and exploration value\n        ucb_score = average_score + exploration_param * exploration_value\n        action_scores.append(ucb_score)\n\n    # Select the action with the highest UCB score.\n    action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters for adaptive exploration\n    epsilon_min = 0.05  # Minimum exploration factor\n    epsilon_max = 0.2  # Maximum exploration factor\n    exploration_decay = 0.01  # Decay factor for exploration\n\n    # Calculate dynamic epsilon based on the total selection count\n    epsilon = max(epsilon_max - exploration_decay * total_selection_count, epsilon_min)\n    \n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score or consider a baseline for unselected actions\n        average_score = np.mean(scores) if selection_count > 0 else 0.5\n        \n        # UCB formula for exploration\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combined score for selection\n        combined_score = average_score + (epsilon * exploration_value)\n        action_scores.append(combined_score)\n\n    # Implement epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, 8)  # Explore: random selection\n    else:\n        action_index = np.argmax(action_scores)  # Exploit: select action with highest combined score\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 1.0  # Tunable exploration parameter\n    action_scores = []\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0.0\n        \n        # Calculate exploration term\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Dynamic weighting based on time to encourage early exploration\n        time_weight = (total_time_slots - current_time_slot) / total_time_slots\n        \n        # Combined score for action selection\n        combined_score = average_score + exploration_param * exploration_value * time_weight\n        action_scores.append(combined_score)\n    \n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Define exploration parameters\n    epsilon = 0.1\n    action_scores = []\n    \n    # Iterate over each action index\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if scores else 0.0\n        \n        # Calculate exploration term using UCB\n        exploration_value = (\n            np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1))\n            if selection_count > 0 else float('inf')\n        )\n        \n        # Combined score calculation\n        combined_score = average_score + exploration_value\n        \n        # Time decay factor\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        # Append the adjusted score for this action\n        action_scores.append(adjusted_score)\n    \n    # Select action with highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Constants\n    actions_count = 8\n    epsilon = 0.1  # Exploration factor\n    \n    # Initialize lists to store average scores and selection counts\n    action_scores = np.zeros(actions_count)\n    selection_counts = np.zeros(actions_count)\n    \n    # Calculate average scores and selection counts\n    for action_index in range(actions_count):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        action_scores[action_index] = np.mean(scores) if selection_counts[action_index] > 0 else 0\n    \n    # Compute exploration bonuses using UCB (Upper Confidence Bound)\n    exploration_values = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n    \n    # Calculate combined scores\n    combined_scores = action_scores + exploration_values\n    \n    # Implement epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, actions_count)  # Explore\n    else:\n        action_index = np.argmax(combined_scores)  # Exploit\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.1  # Exploration parameter\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0.0\n        \n        # Calculate exploration term using a modified UCB formula\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combined score for action selection\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters\n    epsilon = 0.1  # Exploration rate\n    action_count = 8  # Number of actions\n    action_scores = np.zeros(action_count)  # Array to hold scores\n    \n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score and avoid division by zero\n        average_score = np.mean(scores) if scores else 0\n        \n        # Handle exploration term\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Dynamic exploration-exploitation trade-off\n        combined_score = average_score + (epsilon * exploration_value)\n        \n        # Apply time decay factor to encourage earlier selections\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores[action_index] = adjusted_score\n    \n    # Select action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize parameters\n    exploration_factor = 0.1  # Exploration rate\n    action_scores = []\n\n    # Iterate through all actions\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration term using UCB\n        if selection_count > 0:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1))\n        else:\n            exploration_value = float('inf')  # Favor unselected actions\n\n        # Combine scores: UCB strategy\n        combined_score = average_score + exploration_factor * exploration_value\n        \n        # Adjust score with time decay factor\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n\n    # Select action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Exploration parameter\n    alpha = 1.0  # Weighting factor for exploration\n    beta = 1.0   # Weighting factor for exploitation\n\n    action_scores = []\n    action_counts = []\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        action_counts.append(selection_count)\n\n        # Handling case where no selections have been made for an action\n        average_score = np.mean(scores) if selection_count > 0 else 0\n\n        # Exploration value: UCB-like term\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n\n        # Calculate combined score using a balance of exploration and average score\n        combined_score = average_score + alpha * exploration_value\n        action_scores.append(combined_score)\n    \n    # Select action based on the highest combined score\n    action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -449.9999999999999,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    epsilon = 0.1  # Exploration parameter\n\n    action_scores = []\n    \n    # Calculate time-based weight\n    time_weight = (total_time_slots - current_time_slot) / total_time_slots\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n\n        # Calculate exploration value using UCB\n        exploration_value = (\n            np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1))\n            if selection_count > 0 else float('inf')\n        )\n        \n        # Combine average score with exploration term\n        combined_score = average_score + epsilon * exploration_value * time_weight\n        action_scores.append(combined_score)\n    \n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters for exploration\n    epsilon = 0.1\n    ucb_factor = 2\n    \n    action_scores = []\n    total_actions = 8\n\n    for action_index in range(total_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score, handling cases of no selections\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration value using UCB\n        if selection_count > 0:\n            exploration_value = np.sqrt(ucb_factor * np.log(total_selection_count) / selection_count)\n        else:\n            exploration_value = float('inf')  # Encourage exploration for unselected actions\n        \n        combined_score = average_score + exploration_value\n        action_scores.append(combined_score)\n        \n    # Epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, total_actions)  # Explore\n    else:\n        action_index = np.argmax(action_scores)  # Exploit\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters\n    epsilon = 0.1  # Exploration rate\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration term for UCB with smoothing\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combine scores\n        combined_score = average_score + (epsilon * exploration_value)\n        \n        # Adjust score with time decay\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n\n        action_scores.append(adjusted_score)\n    \n    # Select action with maximum adjusted score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_rate = 0.1  # Exploration parameter\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        average_score = np.mean(scores) if scores else 0\n        \n        # If no selection yet, explore this action fully\n        if selection_count == 0:\n            combined_score = float('inf')\n        else:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1))\n            combined_score = average_score + exploration_rate * exploration_value\n        \n        # Apply time decay factor\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    # Select action with highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_rate = 0.2  # Adaptive exploration rate\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score with protection against division by zero\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration term using Upper Confidence Bound (UCB) approach\n        exploration_value = (\n            np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) \n            if selection_count > 0 else float('inf')\n        )\n\n        # Combine average score and exploration term\n        combined_score = average_score + exploration_rate * exploration_value\n        \n        # Apply a time decay factor to the combined score\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    # Select action with the highest adjusted score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize parameters\n    num_actions = 8\n    epsilon = 0.1  # Exploration rate\n    action_scores = np.zeros(num_actions)\n    \n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        average_score = np.mean(scores) if scores else 0\n        \n        # UCB Exploration term\n        exploration_value = (np.sqrt(2 * np.log(total_selection_count + 1) / (selection_count + 1)) \n                             if selection_count > 0 else float('inf'))\n        \n        # Calculate combined score for action selection\n        combined_score = average_score + (epsilon * exploration_value)\n        \n        # Time decay factor\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores[action_index] = adjusted_score\n    \n    # Select action with the highest adjusted score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Define parameters for exploration\n    epsilon = 0.1  # Base exploration probability\n    tau = np.log(1 + current_time_slot)  # Decay factor for exploration\n\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score and handle division by zero\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration bonus using a form of Upper Confidence Bound\n        exploration_bonus = np.sqrt((2 * tau) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combine average score and exploration value using a weighted sum\n        combined_score = (1 - epsilon) * average_score + epsilon * exploration_bonus\n        action_scores.append(combined_score)\n    \n    # Select action with the highest combined score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration rate\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration term\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Adjust average score using a decay factor\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_average_score = average_score * time_decay_factor\n        \n        # Combined score using UCB-like strategy\n        combined_score = adjusted_average_score + (epsilon * exploration_value)\n        \n        action_scores.append(combined_score)\n    \n    # Select action with highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_weight = 0.1  # Exploration parameter\n    bias_towards_recent = 0.3  # Factor to bias towards recent selections\n    \n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score, handling division by zero\n        average_score = np.mean(scores) if scores else 0\n\n        # Calculate exploration term\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Adjust combined score for exploration\n        combined_score = average_score + exploration_weight * exploration_value\n        \n        # Apply time decay and bias towards recent actions\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * (1 - bias_towards_recent) + (bias_towards_recent * average_score) * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    # Select action with highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration rate\n    action_scores = []\n    \n    # Calculate the time decay factor once\n    time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score, handling division by zero\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration term based on remaining selection counts\n        exploration_term = (np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1))\n                            if selection_count > 0 else float('inf'))\n        \n        # Calculate total score with exploration and time decay\n        total_score = (average_score + (epsilon * exploration_term)) * time_decay_factor\n        \n        action_scores.append(total_score)\n    \n    # Select action with highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.5  # Increased exploration parameter for better exploration\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score with better handling for no selections\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration term\n        if selection_count > 0:\n            exploration_value = np.sqrt(np.log(total_selection_count + 1) / selection_count)\n        else:\n            exploration_value = float('inf')  # Max exploration for untried actions\n        \n        # Compute combined score\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    # Select action with the maximum combined score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 1.5  # Adjusted exploration parameter for better exploration-exploitation trade-off\n    n_actions = 8\n    action_scores = []\n    \n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration term using UCB with dynamic exploration\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Compute combined score using weighted average of exploration and exploitation\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    # Return action with the maximum combined score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    exploration_factor = 1.5  # Controls the balance between exploration and exploitation\n    action_scores = []\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score, handling division by zero\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration term\n        if selection_count > 0:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_count)\n        else:\n            exploration_value = float('inf')  # Encourage exploration for unselected actions\n        \n        # Calculate combined score for action selection\n        combined_score = average_score + (exploration_factor * exploration_value)\n        \n        # Add time decay factor\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    # Select action with highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Define exploration parameter and initialize necessary variables\n    exploration_factor = 0.1\n    n_actions = 8\n    action_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Compute average scores and selection counts for each action\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        selection_counts[action_index] = selection_count\n        action_scores[action_index] = np.mean(scores) if selection_count > 0 else 0\n\n    # Calculate UCB values for each action\n    ucb_values = action_scores + exploration_factor * np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n    \n    # Select action using a mixed strategy of UCB\n    action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration rate\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score, handling division by zero\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate confidence bounds for exploration\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Calculate combined score for action selection\n        combined_score = average_score + (epsilon * exploration_value)\n        \n        # Apply linear decay factor to adjust for time\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    # Select action with highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration rate\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score safely\n        average_score = np.mean(scores) if scores else 0.0\n        \n        # Calculate exploration term using the UCB formula\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combined score with exploration and exploitation\n        combined_score = average_score + (epsilon * exploration_value)\n\n        # Introduce a time-based decay factor\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    # Select action with the highest adjusted score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters\n    exploration_rate = 0.1  # Exploration factor\n    action_count = 8  # Number of actions\n    action_scores = np.zeros(action_count)\n    \n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Average score calculation\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # UCB calculation\n        if selection_count > 0:\n            exploration_value = np.sqrt(2 * np.log(total_selection_count) / selection_count)\n        else:\n            exploration_value = float('inf')  # Encourage exploration for unselected actions\n\n        # Combined score using UCB\n        action_scores[action_index] = average_score + exploration_value\n\n    # Epsilon-greedy exploration\n    if np.random.rand() < exploration_rate:\n        action_index = np.random.randint(0, action_count)  # Explore\n    else:\n        action_index = np.argmax(action_scores)  # Exploit the best action\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    exploration_factor = 0.1  # Exploration parameter\n    action_scores = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        selection_counts[action_index] = selection_count\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n\n        # Calculate UCB exploration value\n        if selection_count > 0:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count)) / selection_count)\n        else:\n            exploration_value = float('inf')  # Prioritize unchosen actions\n        \n        # Time decay weight\n        time_weight = (total_time_slots - current_time_slot) / total_time_slots\n        # Compute combined score\n        action_scores[action_index] = average_score + exploration_factor * exploration_value * time_weight\n\n    # Select action with the highest combined score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Define parameters\n    epsilon = 0.1  # Exploration rate\n    action_scores = []\n    \n    # Calculate time decay factor\n    time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score, handling division by zero\n        average_score = np.mean(scores) if scores else 0\n            \n        # Apply Upper Confidence Bound (UCB) for exploration\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Calculate combined score with time decay\n        combined_score = (average_score + (epsilon * exploration_value)) * time_decay_factor\n        \n        action_scores.append(combined_score)\n    \n    # Select action with highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    epsilon = 0.2  # Exploration parameter\n    action_scores = []\n    \n    # Calculate time decay factor to prioritize earlier actions\n    time_decay = (total_time_slots - current_time_slot) / total_time_slots\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n\n        # UCB exploration value calculation\n        if selection_count > 0:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count)) / selection_count)\n        else:\n            exploration_value = float('inf')  # Allocate exploration to actions not yet tried\n\n        # Calculate combined score: average score + exploration benefit\n        combined_score = average_score + epsilon * exploration_value * time_decay\n        action_scores.append(combined_score)\n    \n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Dynamic epsilon decreasing with more selections made\n    epsilon = max(0.01, 1 - (total_selection_count / total_time_slots))\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score for the action\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Upper Confidence Bound (UCB) calculation\n        if selection_count > 0:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count)) / selection_count)\n        else:\n            exploration_value = float('inf')  # Encourage exploration if action has never been selected\n        \n        # Combined score\n        combined_score = average_score + exploration_value\n        action_scores.append(combined_score)\n    \n    # Implement epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, 8)  # Explore: random selection\n    else:\n        action_index = np.argmax(action_scores)  # Exploit: choose best combined score\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration rate\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score, handling division by zero\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration term using Upper Confidence Bound (UCB)\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combine average score and exploration value\n        combined_score = average_score + exploration_value\n        \n        # Time decay factor for adjusting score based on the current time session\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    # Select action with the highest adjusted score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration rate\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score, handling division by zero\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration term only if selection_count > 0, else consider as infinite for exploration\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combined score with exploration\n        combined_score = average_score + (epsilon * exploration_value)\n        \n        # Time decay factor\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    # Select action with highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.2  # Increased exploration parameter for better exploration\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0.0\n        \n        # Implement UCB-based exploration term\n        exploration_value = np.sqrt(2 * np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combined score for action selection\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.5  # Adjusted exploration parameter\n    action_scores = []\n    \n    # Calculate current epsilon for exploration\n    epsilon = 1.0 - (current_time_slot / total_time_slots)\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration term (UCB)\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Compute combined score\n        combined_score = average_score + exploration_param * exploration_value\n        \n        # Apply epsilon-greedy logic for exploration\n        if np.random.rand() < epsilon:\n            combined_score += exploration_param  # Boost exploration scores\n        \n        action_scores.append(combined_score)\n    \n    # Return action with the maximum combined score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.9999999999999,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    beta = 1.0  # Exploration parameter adjusting balance between exploration and exploitation\n    action_scores = []\n    \n    # Prioritize exploration based on remaining time slots\n    time_weight = (total_time_slots - current_time_slot) / total_time_slots\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration value (UCB-like)\n        exploration_value = (np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) \n                             if selection_count > 0 else float('inf'))\n        \n        # Combined score with weighted exploration\n        combined_score = average_score + beta * exploration_value * time_weight\n        action_scores.append(combined_score)\n    \n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_factor = 0.1  # Exploration factor\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Average score calculation\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # UCB exploration value\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combined score for selection\n        combined_score = average_score + exploration_factor * exploration_value\n        action_scores.append(combined_score)\n    \n    # Select action based on combined scores\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_factor = 0.1  # Exploration rate\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score safely\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate UCB exploration term\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Calculate combined score for action selection\n        combined_score = average_score + exploration_factor * exploration_value\n        \n        # Adjust for time decay\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    # Select action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters for exploration-exploitation balance\n    epsilon = 0.1  # Exploration factor\n    exploration_bonus = 1.5  # Multiplier for exploration rewards\n\n    action_scores = []\n    action_selection_counts = [len(scores) for scores in score_set.values()]\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = action_selection_counts[action_index]\n\n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Compute exploration value using the UCB formula\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1e-5))\n        \n        # Combined score considering exploration\n        combined_score = average_score + exploration_bonus * exploration_value\n        action_scores.append(combined_score)\n\n    # Implement epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, 8)  # Explore: random selection\n    else:\n        action_index = np.argmax(action_scores)  # Exploit: choose best combined score\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 1.5  # Exploration parameter for UCB\n    time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score, handling division by zero\n        average_score = np.mean(scores) if scores else 0\n        \n        # UCB calculation: exploration term\n        if selection_count > 0:\n            exploration_value = exploration_param * np.sqrt(np.log(total_selection_count + 1) / selection_count)\n        else:\n            exploration_value = float('inf')\n        \n        # Calculate adjusted score\n        adjusted_score = (average_score + exploration_value) * time_decay_factor\n        action_scores.append(adjusted_score)\n    \n    # Select action with highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    action_scores = np.zeros(n_actions)\n    \n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score with safety for no selections\n        average_score = np.mean(scores) if scores else 0\n        \n        # Decaying exploration term: we will use UCB with a twist\n        exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Dynamic epsilon based on the current time slot\n        epsilon = 1.0 * (1 - (current_time_slot / total_time_slots))\n        \n        # Combined score based on exploitation and exploration\n        action_scores[action_index] = (1 - epsilon) * average_score + epsilon * exploration_bonus\n    \n    # Select the action with the highest combined score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Constants\n    epsilon = 0.1  # Exploration rate\n\n    # Initialize action scores and counts\n    action_scores = np.zeros(8)\n    action_counts = np.zeros(8)\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Update counts of selections\n        action_counts[action_index] = selection_count\n        \n        # Compute average score and handle division by zero\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate UCB exploration term\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Calculate combined score\n        combined_score = average_score + (epsilon * exploration_value)\n        \n        # Apply time decay factor to encourage earlier action selection\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores[action_index] = adjusted_score\n    \n    # Select the action with the highest adjusted score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Exploration factor settings\n    epsilon = 0.1  # Probability of exploration\n    exploration_weight = 1.5  # Weight factor for exploration\n    \n    action_scores = []\n    action_counts = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        action_counts.append(selection_count)\n        \n        # Handling average score calculation and zero-count cases\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # UCB calculation for exploration\n        if selection_count > 0:\n            confidence_interval = np.sqrt((2 * np.log(total_selection_count)) / selection_count)\n        else:\n            confidence_interval = float('inf')\n        \n        # Combined score with exploration bias\n        combined_score = average_score + exploration_weight * confidence_interval\n        action_scores.append(combined_score)\n    \n    # Epsilon-greedy strategy for exploration vs exploitation\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, 8)  # Explore: random selection\n    else:\n        action_index = np.argmax(action_scores)  # Exploit: choose best combined score\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Setting parameters for exploration and decay\n    epsilon = 0.1  # Exploration rate\n    action_scores = []\n    \n    # Iterate through each action to calculate its score\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration term based on remaining time slots\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Calculate combined score for action selection\n        combined_score = average_score + (epsilon * exploration_value)\n        \n        # Apply time decay factor\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    # Select action index with highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Dynamically adjusting exploration factor based on time slot\n    exploration_factor = 1 - (current_time_slot / total_time_slots)\n    action_scores = []\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Upper Confidence Bound calculation\n        if selection_count > 0:\n            ucb_value = np.sqrt((2 * np.log(total_selection_count)) / selection_count)\n        else:\n            ucb_value = float('inf')  # Encouraging exploration for unselected actions\n        \n        combined_score = average_score + exploration_factor * ucb_value\n        action_scores.append(combined_score)\n    \n    action_index = np.argmax(action_scores)  # Exploit: choose best combined score\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Define parameters for the exploration-exploitation trade-off\n    exploration_rate = 0.1  # Chance of exploring an untested action\n    ucb_constant = 2  # UCB exploration constant\n    \n    action_scores = np.zeros(8)\n    selection_counts = np.zeros(8)\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        # Handling case where no scores are available for an action\n        average_score = np.mean(scores) if selection_counts[action_index] > 0 else 0\n        \n        if selection_counts[action_index] > 0:\n            ucb_value = ucb_constant * np.sqrt(np.log(total_selection_count) / selection_counts[action_index])\n        else:\n            ucb_value = float('inf')  # Ensure exploration of untried actions\n        \n        action_scores[action_index] = average_score + ucb_value\n\n    # Epsilon-greedy strategy for action selection\n    if np.random.rand() < exploration_rate:\n        action_index = np.random.randint(0, 8)  # Explore\n    else:\n        action_index = np.argmax(action_scores)  # Exploit\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters for exploration and exploitation\n    exploration_factor = 0.1\n    \n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score for the action, handling cases where it hasn't been selected\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration value using UCB\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combined score using UCB approach\n        combined_score = average_score + exploration_factor * exploration_value\n        action_scores.append(combined_score)\n    \n    # Select action based on the highest combined score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters for exploration and exploitation\n    epsilon = 0.1  # Exploration rate\n    time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n    \n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration term using log and count-based uncertainty\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n\n        # Combine average score and exploration value\n        combined_score = average_score + (epsilon * exploration_value)\n        \n        # Adjust score for current time slot\n        adjusted_score = combined_score * time_decay_factor\n        action_scores.append(adjusted_score)\n\n    # Select action with maximum adjusted score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration rate\n    num_actions = 8\n    action_scores = []\n\n    # Calculate scores for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score, ensuring not to divide by zero\n        average_score = np.mean(scores) if selection_count > 0 else 0\n\n        # Exploration term, guarding against division by zero\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n\n        # Combined score calculation\n        combined_score = average_score + (epsilon * exploration_value)\n\n        # Time decay factor to emphasize earlier selections\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n\n        action_scores.append(adjusted_score)\n\n    # Select action with the highest adjusted score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Hyperparameters\n    epsilon = 0.1  # Exploration rate\n    kappa = 1.5    # Exploration parameter for UCB\n\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score, handling cases of zero selections\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration term based on UCB\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combined score using UCB approach\n        ucb_score = average_score + (kappa * exploration_value)\n        \n        # Adjust score based on current time slot\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = ucb_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    # Select action with highest adjusted score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize parameters\n    action_count = 8\n    epsilon = 0.1  # Exploration rate\n    action_scores = []\n    \n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration term, with safety against division by zero\n        if selection_count == 0:\n            exploration_value = float('inf')\n        else:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1))\n        \n        # UCB-like score computation\n        combined_score = average_score + (epsilon * exploration_value)\n        \n        # Adjust score based on time decay\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    # Return action index with the maximum score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    alpha = 0.1  # Exploration factor\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score, handling no scores\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate the exploration term with logarithmic scaling\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Apply time decay to the exploration value as time progresses\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        \n        # Compute the combined score using the calculated factors\n        adjusted_score = average_score + (alpha * exploration_value * time_decay_factor)\n        action_scores.append(adjusted_score)\n    \n    # Select the action with the highest adjusted score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 1.0  # Exploration adjustment\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration term\n        exploration_value = (np.sqrt(np.log(total_selection_count + 1)) / (selection_count + 1e-5)) if selection_count > 0 else float('inf')\n\n        # Combine scores with exploration\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    # Get action with the maximum score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters for UCB\n    exploration_weight = 2  # Weight for exploration\n    \n    # Initialize scores and selection counts\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate the exploration value using UCB\n        if selection_count > 0:\n            exploration_value = np.sqrt((exploration_weight * np.log(total_selection_count)) / selection_count)\n        else:\n            exploration_value = float('inf')  # Encourage exploration for unselected actions\n        \n        # Combined score\n        combined_score = average_score + exploration_value\n        action_scores.append(combined_score)\n    \n    # Select action based on the highest combined score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Adjust exploration factor dynamically based on the time slots\n    exploration_factor = max(1, (total_time_slots - current_time_slot) / total_time_slots)\n    action_scores = []\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Handling average score calculation\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Upper Confidence Bound (UCB) calculation\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        combined_score = average_score + exploration_factor * exploration_value\n        action_scores.append(combined_score)\n    \n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters for exploration and exploitation\n    exploration_param = 0.1\n    epsilon_decay = max(0.1, 1 - (current_time_slot / total_time_slots))\n    \n    # Track scores and selection counts\n    action_scores = []\n    total_actions = 8  # Fixed number of actions (0-7)\n    \n    for action_index in range(total_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        average_score = np.mean(scores) if scores else 0\n        \n        # UCB computation\n        if selection_count > 0:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count)) / selection_count)\n        else:\n            exploration_value = float('inf')  # Never selected actions should explore\n        \n        # Combined score using adjusted exploitation and exploration\n        combined_score = (1 - epsilon_decay) * average_score + epsilon_decay * exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    # Select action with the highest combined score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(range(len(score_set)))\n    \n    # Calculate average scores and selection counts for each action\n    action_avg_scores = np.array([np.mean(score_set.get(action_index, [])) for action_index in action_indices])\n    selection_counts = np.array([len(score_set.get(action_index, [])) for action_index in action_indices])\n\n    # Handle cases where actions have not been selected\n    selection_counts_with_epsilon = selection_counts + 1  # Add 1 to avoid division by zero\n    adjusted_total_count = total_selection_count + len(action_indices)  # Adjusted total with the number of actions\n\n    # UCB exploration term\n    exploration_values = np.sqrt((2 * np.log(adjusted_total_count)) / selection_counts_with_epsilon)\n\n    # Combine scores with exploration\n    combined_scores = action_avg_scores + exploration_values\n\n    # Select action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters\n    exploration_factor = 0.1  # Epsilon for the exploration-exploitation trade-off\n    exploration_bonus = 3.0    # Exploration multiplier for uncertainty\n    \n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Handle case where no selections have been made for an action\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Exploration value calculation to handle uncertainty\n        exploration_value = (exploration_bonus * np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1))\n                             if selection_count > 0 else float('inf'))\n        \n        # Combined score for the current action\n        combined_score = average_score + exploration_factor * exploration_value\n        action_scores.append(combined_score)\n    \n    # Select action using the epsilon-greedy strategy\n    if np.random.rand() < exploration_factor:\n        action_index = np.random.randint(0, 8)  # Explore: random selection\n    else:\n        action_index = np.argmax(action_scores)  # Exploit: choose best combined score\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.5  # Parameter to control exploration\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration term using UCB\n        if selection_count > 0:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_count)\n        else:\n            exploration_value = float('inf')  # Prioritize untried actions\n        \n        # Compute combined score\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    # Return action with the maximum score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(range(8)) \n    action_avg_scores = np.zeros(len(action_indices))\n    selection_counts = np.zeros(len(action_indices))\n\n    # Calculate average scores and selection counts\n    for action_index in action_indices:\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        action_avg_scores[action_index] = np.mean(scores) if scores else 0\n\n    # Calculate UCB exploration values\n    exploration_values = np.zeros(len(action_indices))\n    for action_index in action_indices:\n        if selection_counts[action_index] > 0:\n            exploration_values[action_index] = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts[action_index] + 1))\n        else:\n            exploration_values[action_index] = float('inf')  # Allow exploration for completely unselected actions\n\n    # Calculate combined scores\n    combined_scores = action_avg_scores + exploration_values\n\n    # Select and return the action index with the highest score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.5  # Adjust exploration weight as needed.\n    action_scores = []\n    total_actions = len(score_set)\n    \n    for action_index in range(total_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Epsilon parameter that decreases over time for exploitation.\n        epsilon = 0.1 * (total_time_slots - current_time_slot) / total_time_slots\n        \n        # Upper Confidence Bound (UCB) exploration term.\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combined score with dynamic exploration and exploitation.\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n\n    # Select the action with the highest score.\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters for exploring and exploiting\n    exploration_factor = 0.1\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Handling case where no selections have been made for an action\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Compute exploration value\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combined score\n        combined_score = average_score + exploration_factor * exploration_value\n        action_scores.append(combined_score)\n    \n    # Select action based on combined scores\n    action_index = np.argmax(action_scores)\n    \n    # Ensure exploration occasionally\n    if np.random.rand() < exploration_factor:\n        action_index = np.random.randint(0, 8)\n\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialization of parameters\n    num_actions = 8\n    exploration_factor = 0.1  # Exploration factor for UCB\n    action_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_counts[action_index] > 0 else 0\n        action_scores[action_index] = average_score\n\n    # Calculating UCB for each action\n    for action_index in range(num_actions):\n        if selection_counts[action_index] > 0:\n            ucb_value = exploration_factor * np.sqrt(np.log(total_selection_count) / selection_counts[action_index])\n        else:\n            ucb_value = float('inf')  # Encourage exploration for unselected actions\n        \n        action_scores[action_index] += ucb_value\n\n    # Select action with the highest score\n    action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Constants\n    epsilon = 0.1  # Exploration rate\n    alpha = 1.0   # Confidence term scaling factor\n\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if scores else 0\n        \n        # Exploration term based on selection counts\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combined score using Upper Confidence Bound (UCB)\n        ucb_score = average_score + (epsilon * exploration_value)\n\n        # Adjust for time decay\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = ucb_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    # Select action with highest score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters for exploration and exploitation\n    exploration_param = 1.0  # Strength of exploration\n    action_scores = []\n    total_actions = len(score_set)\n    \n    # Calculate average scores and exploration values for each action\n    for action_index in range(total_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Epsilon decreasing over time to favor exploitation later in the slots\n        epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n        \n        # Calculate exploration term using UCB\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combine average score and exploration value\n        combined_score = (1 - epsilon) * average_score + epsilon * exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    # Select action with the highest combined score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_factor = 1.0  # Adjust for exploration versus exploitation balance\n    action_scores = []\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Upper Confidence Bound (UCB) calculation\n        if selection_count > 0:\n            exploration_value = exploration_factor * np.sqrt(np.log(total_selection_count) / selection_count)\n        else:\n            exploration_value = float('inf')  # Encourage exploration for unselected actions\n\n        # Combined UCB score\n        combined_score = average_score + exploration_value\n        action_scores.append(combined_score)\n    \n    # Select action based on the calculated scores\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Exploration rate and confidence factor for UCB\n    epsilon = 0.1\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score, handling division by zero\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration term based on remaining time slots (UCB)\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combine average score with exploration value\n        combined_score = average_score + exploration_value\n        \n        # Apply time decay factor\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    # Select action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 1  # Adjusted exploration parameter.\n    action_scores = []\n    total_actions = len(score_set)\n    \n    for action_index in range(total_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Dynamic epsilon decreasing over time\n        epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n        \n        # UCB Exploration Value\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combined score calculation\n        combined_score = (1 - epsilon) * average_score + epsilon * exploration_param * exploration_value\n        action_scores.append(combined_score)\n\n    # Select the action with the highest score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters for exploration-exploitation balance\n    alpha = 1.0  # Weight for exploration\n    beta = 0.5   # Weight for time decay\n    \n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration bonus\n        exploration_bonus = np.sqrt(alpha * np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n\n        # Include time decay based on the current time slot\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_exploration = exploration_bonus * time_decay_factor\n        \n        # Combine average score with exploration\n        total_score = average_score + adjusted_exploration\n        \n        action_scores.append(total_score)\n    \n    # Select action with highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize parameters for the selection strategy\n    epsilon = 0.1  # Exploration rate\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score, handling edge case\n        average_score = np.mean(scores) if scores else 0\n        \n        # Exploration term based on UCB\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n\n        # Combined score calculation\n        combined_score = average_score + exploration_value\n\n        # Apply time decay factor\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n\n        action_scores.append(adjusted_score)\n    \n    # Select the action with the highest adjusted score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.1  # exploration coefficient\n    action_scores = []\n    epsilon = max(0, 1 - (current_time_slot / total_time_slots))  # Epsilon decreases over time\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration factor using UCB\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n\n        # Combine average score with exploration factor\n        combined_score = (1 - epsilon) * average_score + epsilon * exploration_value\n        action_scores.append(combined_score)\n    \n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Hyperparameters\n    exploration_param = 0.5  # Exploration parameter\n    epsilon = 0.1  # Epsilon for epsilon-greedy strategy\n    \n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration term\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Compute combined score using UCB method\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n\n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon and total_selection_count < total_time_slots:\n        action_index = np.random.choice([i for i in range(8) if len(score_set.get(i, [])) == 0]) if any(len(score_set.get(i, [])) == 0 for i in range(8)) else np.random.randint(0, 8)\n    else:\n        action_index = np.argmax(action_scores)\n        \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.5  # Increased exploration parameter\n    action_scores = []\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration term using UCB\n        if selection_count > 0:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_count)\n        else:\n            exploration_value = float('inf')  # Infinite exploration for untried actions\n        \n        # Compute combined score\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    # Return action with the maximum score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = len(score_set)  # Assume there are 8 actions (indices 0-7)\n    action_scores = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        # Compute average score\n        average_score = np.mean(scores) if selection_counts[action_index] > 0 else 0\n        \n        # Explore using UCB-like approach or epsilon-greedy\n        if selection_counts[action_index] > 0:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts[action_index] + 1))\n        else:\n            exploration_value = float('inf')  # Encourage exploration of unselected actions\n            \n        # Calculate combined score\n        action_scores[action_index] = average_score + exploration_value\n\n    # Epsilon decreases over time: exploration will diminish\n    epsilon = 1.0 / (current_time_slot + 1)\n    \n    # Blend exploration/exploitation\n    for action_index in range(action_count):\n        action_scores[action_index] = (1 - epsilon) * action_scores[action_index] + epsilon * np.random.rand()\n    \n    # Select the action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 1.0\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Average score and handle division by zero\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration term with Bayesian UCB\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combined score\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    # Return action with the maximum score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    epsilon = 0.1  # Exploration probability\n    action_scores = []\n    \n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration value (UCB)\n        if selection_count > 0:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count)) / selection_count)\n        else:\n            exploration_value = float('inf')  # Infinite for unexplored actions\n        \n        # Calculate combined score maximizing exploitation and exploration\n        combined_score = average_score + epsilon * exploration_value\n        action_scores.append(combined_score)\n    \n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Exploration rate\n    epsilon = 0.1\n    \n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score, handling edge cases\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration term using UCB\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combined score for action selection\n        combined_score = average_score + (epsilon * exploration_value) \n        \n        # Time decay factor\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    # Select action with highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_factor = 1.5  # Exploration constant\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score, defaulting to a small value if no selections have been made\n        average_score = np.mean(scores) if selection_count > 0 else 0.01\n        \n        # Upper Confidence Bound calculation\n        exploration_value = np.sqrt((exploration_factor * np.log(total_selection_count + 1)) / (selection_count + 1))\n        \n        # Combined UCB score\n        combined_score = average_score + exploration_value\n        action_scores.append(combined_score)\n    \n    action_index = np.argmax(action_scores)  # Choose the best action based on combined scores\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Hyperparameters\n    epsilon = 0.1  # Exploration factor\n    confidence_level = 1.96  # 95% confidence interval\n    action_count = 8  # Number of actions\n\n    action_scores = []\n    \n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Compute average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration value using Upper Confidence Bound (UCB)\n        if selection_count > 0:\n            exploration_value = confidence_level * np.sqrt(np.log(total_selection_count) / selection_count)\n        else:\n            exploration_value = float('inf')  # High value for unselected actions\n\n        # Combined score\n        combined_score = average_score + exploration_value\n        action_scores.append(combined_score)\n    \n    # Epsilon-greedy with exploration\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)  # Explore: random selection\n    else:\n        action_index = np.argmax(action_scores)  # Exploit: choose action with max score\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters\n    epsilon = 0.1  # Exploration rate\n    action_count = 8\n    action_scores = np.zeros(action_count)  # Initialize scores array\n    \n    # Calculate scores\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score safely\n        average_score = np.mean(scores) if scores else 0\n        \n        # Exploration component\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combined score calculation\n        adjusted_score = average_score + epsilon * exploration_value\n        \n        # Time decay factor\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        action_scores[action_index] = adjusted_score * time_decay_factor\n    \n    # Select action with highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize parameters\n    ucb_exploration_factor = 1.5\n    action_scores = []\n    n_actions = 8\n    \n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate UCB term\n        if selection_count == 0:\n            ucb_value = float('inf')  # Encourage exploration if the action has never been selected\n        else:\n            ucb_value = ucb_exploration_factor * np.sqrt(np.log(total_selection_count) / selection_count)\n        \n        # Combined score to encourage exploration and exploitation\n        combined_score = average_score + ucb_value\n        action_scores.append(combined_score)\n    \n    # Select action with the highest combined score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 1.0  # Exploration parameter\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration value using Upper Confidence Bound\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1e-5)) if selection_count > 0 else float('inf')\n        \n        # Compute combined score\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    # Return action with the maximum score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    beta = 0.1  # Exploration rate\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score, handling division by zero\n        average_score = np.mean(scores) if scores else 0.0\n        \n        # Calculate exploration term using UCB\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n\n        # Composite score considering the average and exploration\n        combined_score = average_score + beta * exploration_value\n\n        # Time decay factor\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    # Select action with the highest adjusted score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters\n    exploration_param = 1.0  # Exploration parameter for balance\n    epsilon = 0.1  # Epsilon for epsilon-greedy approach\n\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate the average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Exploration term based on UCB method\n        if selection_count > 0:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_count)\n        else:\n            exploration_value = float('inf')  # Encourage exploration for untried actions\n        \n        # Combine scores with some randomness\n        combined_score = average_score + exploration_param * exploration_value\n        \n        # Incorporate epsilon-greedy exploration\n        if np.random.rand() < epsilon:\n            combined_score += np.random.uniform(0, 1)  # Add a random score to encourage exploration\n        \n        action_scores.append(combined_score)\n    \n    # Select action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_factor = 1.5  # Factor to control exploration weight\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # UCB strategy for exploration allocation\n        if selection_count > 0:\n            confidence_bound = exploration_factor * np.sqrt(np.log(total_selection_count) / selection_count)\n        else:\n            confidence_bound = float('inf')  # Encourage exploration for unselected actions\n        \n        combined_score = average_score + confidence_bound\n        action_scores.append(combined_score)\n    \n    action_index = np.argmax(action_scores)  # Select action with the highest score\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameter for exploration\n    exploration_factor = 1.5  # Tunable parameter for exploration\n    \n    # Initialize arrays to hold average scores and selection counts\n    action_values = []\n    action_counts = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Average score calculation\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # UCB calculation\n        if selection_count > 0:\n            ucb_value = average_score + exploration_factor * np.sqrt(np.log(total_selection_count) / selection_count)\n        else:\n            ucb_value = float('inf')  # If never selected, prioritize exploring this action\n        \n        action_values.append(ucb_value)\n        action_counts.append(selection_count)\n    \n    # Select the action with the highest UCB value\n    action_index = np.argmax(action_values)\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration rate\n    action_scores = []\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score, handling division by zero\n        average_score = np.mean(scores) if scores else 0\n\n        # Calculate exploration term based on remaining time slots\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n\n        # Combined score with exploration term\n        combined_score = average_score + (epsilon * exploration_value)\n\n        # Time decay factor to prioritize earlier actions\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n\n    # Select action with highest adjusted score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Define exploration parameter and exploration rate\n    exploration_param = 0.5   # Increased exploration parameter\n    epsilon = 0.1              # Epsilon for epsilon-greedy strategy\n    \n    # Store action average scores and selection counts\n    action_scores = np.zeros(8)\n    selection_counts = np.zeros(8)\n\n    # Calculate average scores and counts\n    for action_index, scores in score_set.items():\n        selection_count = len(scores)\n        selection_counts[action_index] = selection_count\n        \n        # Calculate average score\n        if selection_count > 0:\n            action_scores[action_index] = np.mean(scores)\n        \n    # Calculate exploration bonuses\n    exploration_values = np.zeros(8)\n    for action_index in range(8):\n        if selection_counts[action_index] > 0:\n            exploration_values[action_index] = np.sqrt(np.log(total_selection_count + 1) / selection_counts[action_index])\n        else:\n            exploration_values[action_index] = float('inf')  # Untried actions have infinite exploration value\n    \n    # Combine scores with exploration terms\n    combined_scores = action_scores + exploration_param * exploration_values\n\n    # Epsilon-greedy to explore with a small probability\n    if np.random.rand() < epsilon:\n        # Choose a random action\n        action_index = np.random.choice(8)\n    else:\n        # Select the action with the maximum combined score\n        action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.2  # Exploration weight, can be tuned based on preferences\n    action_scores = []\n    epsilon = 0.1  # Epsilon for epsilon-greedy strategy\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0.0\n\n        # Calculate exploration term based on UCB\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n\n        # Combined score for action selection using UCB\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n\n    # Epsilon-greedy selection for exploration vs exploitation\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(range(8))  # Explore - random choice\n    else:\n        action_index = np.argmax(action_scores)  # Exploit - choose best option\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize parameters\n    n_actions = 8\n    exploration_weight = 0.1  # Exploration rate\n    action_scores = np.zeros(n_actions)\n    \n    # Calculate average scores and selection counts\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Compute average score\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate the exploration term (UCB)\n        if selection_count > 0:\n            exploration_term = np.sqrt((2 * np.log(total_selection_count)) / selection_count)\n        else:\n            exploration_term = float('inf')  # Prioritize actions that have never been selected\n\n        # Adjust score with exploration term\n        adjusted_score = average_score + (exploration_weight * exploration_term)\n        \n        # Apply time decay factor\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        action_scores[action_index] = adjusted_score * time_decay_factor\n    \n    # Select action with the highest adjusted score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters for exploration-exploitation strategy\n    beta = 1.0  # Exploration factor\n    \n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score, handling division by zero\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate the exploration term\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combine average score and exploration term with a weighting factor\n        combined_score = average_score + beta * exploration_value\n        \n        # Adjust score based on remaining time slots\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    # Select action with the highest adjusted score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters \n    exploration_factor = 0.05  # Smaller exploration factor for reduced randomness\n    \n    # Initialize action scores\n    action_scores = []\n    \n    # Calculate scores for each action\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate the average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate the exploration value using UCB\n        exploration_value = np.sqrt((2 * np.log(total_selection_count)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combine average score and exploration value\n        combined_score = average_score + exploration_factor * exploration_value\n        action_scores.append(combined_score)\n    \n    # Select action based on calculated scores\n    action_index = np.argmax(action_scores)  # Exploit: choose best combined score\n\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters\n    epsilon = 0.1  # Exploration factor\n    alpha = 0.5    # Learning rate for average score update\n    action_scores = []\n    \n    # Calculate average scores and selection counts\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score with a safeguard for division by zero\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration term based on UCB\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1) / (selection_count + 1))) if selection_count > 0 else float('inf')\n        \n        # Adjust combined score with time decay\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = average_score + (epsilon * exploration_value) * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n\n    # Select action with the highest adjusted score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    def ucb1(mean_score, selection_count, total_count):\n        \"\"\"Compute Upper Confidence Bound.\"\"\"\n        if selection_count == 0:\n            return float('inf')  # Encourage exploration for actions not selected yet\n        return mean_score + np.sqrt(2 * np.log(total_count) / selection_count)\n\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score\n        average_score = np.mean(scores) if scores else 0\n        \n        # Compute the UCB score\n        ucb_score = ucb1(average_score, selection_count, total_selection_count)\n        action_scores.append(ucb_score)\n\n    # Select action based on UCB scores\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Define a confidence level for exploration\n    epsilon = 0.1\n\n    # Initialize variables\n    action_scores = []\n    action_selection_counts = []\n    \n    # Calculate scores and counts for each action\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        action_selection_counts.append(selection_count)\n\n        # Compute average score\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration term using the Upper Confidence Bound (UCB) strategy\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combine average score with exploration term\n        combined_score = average_score + (epsilon * exploration_value)\n        \n        # Introduce a time decay factor\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    # Select action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 1.5  # Adjusted exploration parameter for a better exploration-exploitation balance\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration term\n        if selection_count > 0:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_count)\n        else:\n            exploration_value = float('inf')  # Encourage exploration for untried actions\n        \n        # Combine average score with exploration value\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    # Return action with the maximum combined score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters for epsilon-greedy strategy\n    exploration_param = 0.1  # Weight for exploration in scoring\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))  # Decrease epsilon over time\n    \n    action_scores = []\n    total_actions = len(score_set)\n\n    for action_index in range(total_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Exploration term using Upper Confidence Bound (UCB)\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combined score with exploration and exploitation\n        combined_score = (1 - epsilon) * average_score + epsilon * exploration_param * exploration_value\n        \n        action_scores.append(combined_score)\n\n    # Select the action with the highest score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 1.0  # Exploration parameter adjustment\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration term\n        exploration_value = (\n            np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1))\n            if selection_count > 0 else float('inf')\n        )\n        \n        # Combined score using UCB\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    # Return action with the maximum score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters for exploration/exploitation strategy\n    epsilon = 0.1  # Exploration rate\n    alpha = 1.0    # Exploration weight for UCB\n    action_count = len(score_set)  # Total number of actions (should be 8)\n\n    action_scores = np.zeros(action_count)\n    \n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration term based on UCB\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combine average score and exploration value\n        combined_score = average_score + alpha * exploration_value\n        \n        # Time decay factor\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores[action_index] = adjusted_score\n    \n    # Select action with highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration probability\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration value\n        if selection_count > 0:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_count)\n        else:\n            exploration_value = float('inf')  # Encourage exploration for untried actions\n        \n        # Combine average score and exploration value for final score\n        combined_score = average_score + exploration_value\n        \n        action_scores.append(combined_score)\n    \n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        # Explore: randomly select an action\n        action_index = np.random.choice(range(8))\n    else:\n        # Exploit: select action with the maximum score\n        action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    exploration_param = 0.1  # Exploration parameter\n    action_scores = []\n    \n    # Calculate remaining time weight\n    time_weight = (total_time_slots - current_time_slot) / total_time_slots\n    \n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration value using UCB\n        if selection_count > 0:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count)) / selection_count)\n        else:\n            exploration_value = float('inf')  # Prioritize unselected actions\n        \n        # Apply a balance of exploitation and exploration\n        combined_score = average_score + exploration_param * exploration_value * time_weight\n        action_scores.append(combined_score)\n    \n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize parameters\n    epsilon = 0.1  # Exploration rate\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score, handling division by zero\n        average_score = np.mean(scores) if scores else 0.0\n        \n        # Calculate exploration term, avoiding division by zero\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n\n        # Combine average score with exploration value\n        combined_score = average_score + (epsilon * exploration_value)\n        \n        # Adjust score based on the remaining time slots to promote timely actions\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    # Select action with the highest adjusted score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialization\n    action_count = 8\n    exploration_factor = 1.0  # Adjusted exploration factor\n    epsilon = 0.1  # Epsilon for exploration\n    action_scores = []\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n\n        # Upper Confidence Bound (UCB) for exploration\n        if selection_count > 0:\n            exploration_value = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1))\n        else:\n            exploration_value = float('inf')  # Encourage exploration for non-selected actions\n\n        # Combined UCB score\n        ucb_score = average_score + exploration_value\n        action_scores.append(ucb_score)\n\n    # Implement epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)  # Explore: random selection\n    else:\n        action_index = np.argmax(action_scores)  # Exploit: choose action with highest UCB score\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = np.arange(len(score_set))\n    \n    # Calculate average scores\n    action_avg_scores = np.array([np.mean(score_set.get(action_index, [0])) for action_index in action_indices])\n    \n    # Calculate selection counts\n    selection_counts = np.array([len(score_set.get(action_index, [])) for action_index in action_indices])\n    \n    # Exploration factor based on UCB with a cost for unselected actions\n    with np.errstate(divide='ignore', invalid='ignore'):\n        exploration_values = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1))\n\n    # Adjust for actions that have never been selected - assigning a high exploration value\n    exploration_values[selection_counts == 0] = float('inf')\n\n    # Combine scores with a dynamic balance between exploration and exploitation\n    total_time_percent = current_time_slot / total_time_slots\n    epsilon = 0.1 * (1 - total_time_percent)  # Decay epsilon over time\n    combined_scores = (1 - epsilon) * action_avg_scores + epsilon * exploration_values\n\n    # Select the action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # PARAMETERS\n    exploration_param = 0.1  # Exploration parameter\n    action_count = 8  # Number of actions\n    average_scores = np.zeros(action_count)  # To store average scores\n    selection_counts = np.zeros(action_count)  # To store selection counts\n    combined_scores = np.zeros(action_count)  # To store combined scores\n\n    # Calculate average scores and selection counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        # Calculate the average score\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Adjust selection counts for exploration score calculation\n    adjusted_counts = selection_counts + 1  # To avoid division by zero\n    total_selections_adjusted = total_selection_count + 1\n    \n    # Calculate combined scores for exploration and exploitation\n    for action_index in range(action_count):\n        exploration_value = np.sqrt(np.log(total_selections_adjusted) / adjusted_counts[action_index])\n        combined_scores[action_index] = average_scores[action_index] + exploration_param * exploration_value\n\n    # Select the action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(range(8))\n    \n    # Initialize average scores and selection counts\n    action_avg_scores = np.zeros(len(action_indices))\n    selection_counts = np.zeros(len(action_indices))\n    \n    # Calculate average scores and counts\n    for action_index in action_indices:\n        scores = score_set.get(action_index, [])\n        action_avg_scores[action_index] = np.mean(scores) if scores else 0\n        selection_counts[action_index] = len(scores)\n    \n    # Calculate UCB exploration factor\n    exploration_values = np.zeros(len(action_indices))\n    for action_index in action_indices:\n        if selection_counts[action_index] > 0:\n            exploration_values[action_index] = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts[action_index] + 1))\n        else:\n            exploration_values[action_index] = float('inf')  # Encourage exploration of untested actions\n\n    # Weight factor for balancing exploration and exploitation\n    remaining_time_slots = total_time_slots - current_time_slot\n    exploration_weight = min(1, remaining_time_slots / total_time_slots)\n\n    # Combine average scores with exploration using the weighted approach\n    combined_scores = (1 - exploration_weight) * action_avg_scores + exploration_weight * exploration_values\n    \n    # Select the action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(range(len(score_set)))\n    \n    # Calculate average scores and selection counts\n    action_avg_scores = np.zeros(len(action_indices))\n    selection_counts = np.zeros(len(action_indices))\n    \n    for action_index in action_indices:\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        action_avg_scores[action_index] = np.mean(scores) if scores else 0\n\n    # Dynamically adjust exploration factor\n    exploration_factor = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n\n    # Weighted score: 70% exploitation, 30% exploration\n    combined_scores = 0.7 * action_avg_scores + 0.3 * exploration_factor\n    \n    # Handle all actions that have never been selected\n    if total_selection_count < len(action_indices):\n        combined_scores += (total_time_slots - current_time_slot) * (1 - selection_counts)\n\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.5  # Exploration parameter adjustment\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration term\n        if selection_count > 0:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_count)\n        else:\n            exploration_value = float('inf')  # Favor unexplored actions\n            \n        # Compute combined score using UCB method\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    # Return action with the maximum score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.1  # Exploration parameter\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_counts[action_index] > 0 else 0.0\n        \n        # Calculate exploration term\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_counts[action_index] + 1)) if selection_counts[action_index] > 0 else float('inf')\n        \n        # Combined score for action selection\n        action_scores[action_index] = average_score + exploration_param * exploration_value\n        \n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon_start = 0.1\n    epsilon_decay = 0.99\n    min_epsilon = 0.01\n    exploration_factor = np.sqrt(2)\n    \n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score, handling division by zero\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration term\n        exploration_value = (exploration_factor * np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Calculate combined score for action selection\n        combined_score = average_score + exploration_value\n        \n        # Adjust score based on time decay\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    # Implementing epsilon-greedy with decay\n    epsilon = max(min_epsilon, epsilon_start * (epsilon_decay ** current_time_slot))\n    \n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, 8)  # Explore\n    else:\n        action_index = np.argmax(action_scores)  # Exploit\n\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.1  # Exploration parameter for balancing exploration and exploitation.\n    total_actions = len(score_set)\n    action_scores = []\n\n    for action_index in range(total_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate epsilon that decreases over time to encourage more exploitation.\n        epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n        \n        # Improved exploration term using Upper Confidence Bound (UCB).\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Calculate combined score.\n        combined_score = (1 - epsilon) * average_score + epsilon * exploration_param * exploration_value\n        action_scores.append(combined_score)\n\n    # Select the action with the highest score.\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters\n    epsilon = 0.1  # Exploration rate\n\n    # Initialize action scores\n    action_scores = []\n\n    for action_index in range(8):\n        # Retrieve scores for the action\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score, handling division by zero\n        average_score = np.mean(scores) if scores else 0\n        \n        # Exploration term (UCB style)\n        if selection_count > 0:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1))\n        else:\n            exploration_value = float('inf')  # prior exploration for unselected actions\n\n        # Combine average score and exploration term\n        combined_score = average_score + (epsilon * exploration_value)\n\n        # Time decay factor for the remaining time slots\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n\n    # Select the action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration rate\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score, handling division by zero\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration term using UCB\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combined score\n        combined_score = average_score + epsilon * exploration_value\n        \n        # Incorporate time decay\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    # Select action with highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Configure parameters\n    exploration_factor = 1.5  # Factor to scale exploration term\n\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score, handling empty selection case\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration term (Upper Confidence Bound)\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combine average score with exploration value\n        combined_score = average_score + exploration_factor * exploration_value\n        \n        # Adjust score based on time decay factor\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    # Select action with the highest adjusted score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.1  # Exploration parameter\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0.0\n        \n        # Calculate exploration term\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combined score for action selection\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    # Select action with the highest combined score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Hyperparameters\n    epsilon = 0.1  # Exploration rate\n    rho = 1.5  # Weight for exploration factor\n    \n    action_scores = []\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score, handling division by zero\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration term using UCB formula\n        if selection_count > 0:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_count)\n        else:\n            exploration_value = float('inf')\n\n        # Combine average score with exploration term\n        combined_score = average_score + (epsilon * exploration_value * rho)\n\n        # Adjust score based on remaining time slots\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    # Select action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Hyperparameters\n    exploration_weight = 0.1  # Weight for exploration\n    optimism_bonus = 1.0  # Bonus for untried actions\n    \n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score, handling division by zero\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration term\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combined score with optimism adjustment\n        combined_score = average_score + (exploration_weight * exploration_value)\n        \n        # Apply time decay factor\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        # Add optimism bonus for actions not yet tried\n        if selection_count == 0:\n            adjusted_score += optimism_bonus\n\n        action_scores.append(adjusted_score)\n    \n    # Select action with the highest adjusted score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Hyperparameters\n    epsilon = 0.1  # Exploration rate\n    exploration_weight = 1.5  # Weight applied to exploration term\n    time_decay_weight = 0.5  # Weight for time decay factor\n\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score, handling division by zero\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration term based on remaining time slots\n        if selection_count > 0:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1))\n        else:\n            exploration_value = float('inf')  # Encourage exploration for untried actions\n        \n        # Calculate combined score for action selection\n        combined_score = average_score + (exploration_weight * exploration_value)\n        \n        # Add context based on current time slot with decay\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * (1 + time_decay_weight * time_decay_factor)\n\n        action_scores.append(adjusted_score)\n    \n    # Select action with highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Constants\n    exploration_factor = 1.0  # Adjusts the exploration weight\n    delta = 0.1  # Small positive value to handle cases with zero scores\n    \n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score with a minimum delta to avoid division by zero\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # UCB calculation\n        exploration_value = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / (selection_count + delta))\n        \n        # Combined UCB score\n        combined_score = average_score + exploration_value\n        action_scores.append(combined_score)\n    \n    # Select the action with the highest combined score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 1.0\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate dynamic exploration term\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1e-5))\n        \n        # Combine scores with exploration\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    # Select action with maximum score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_factor = 1.0  # Exploration factor for UCB\n    action_count = len(score_set)\n    action_scores = []\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate the average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate the exploration value\n        exploration_value = np.sqrt((exploration_factor * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Calculate combined score using UCB formula\n        combined_score = average_score + exploration_value\n        action_scores.append(combined_score)\n    \n    # Select action based on the highest combined score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -449.9999999999999,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.1  # Exploration coefficient\n    action_scores = []\n    n_actions = 8\n    \n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration value\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combine scores\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Define the exploration parameter\n    exploration_param = 1.5  # Increased exploration for balancing early exploration\n    \n    action_scores = []\n    epsilon = 0.1  # Epsilon for exploration\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration term\n        if selection_count > 0:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_count)\n        else:\n            exploration_value = float('inf')  # Encourage exploration for untried actions\n        \n        # Time decay function to shift towards exploitation over time\n        time_weight = 1 - (current_time_slot / total_time_slots)\n        \n        # Combined score with time adjustment\n        combined_score = (average_score * time_weight) + (exploration_param * exploration_value * (1 - time_weight))\n        \n        # Incorporate epsilon-greedy exploration\n        if np.random.rand() < epsilon:\n            action_scores.append(float('inf'))  # Assign a high value to explore\n        else:\n            action_scores.append(combined_score)\n    \n    # Return action with the maximum score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Exploration rate\n    epsilon = 0.1  \n    # Calculate total actions\n    num_actions = len(score_set)\n    action_scores = []\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score, handle division by zero\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration term based on selection counts\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Calculate combined score for action selection\n        combined_score = average_score + (epsilon * exploration_value)\n        \n        # Apply a decay factor to reduce impact of scores as time progresses\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    # Select action with highest adjusted score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Hyperparameters\n    epsilon = 0.1  # Exploration rate\n    confidence_level = 1.5  # UCB exploration factor\n    \n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration term using Upper Confidence Bound (UCB)\n        exploration_value = (confidence_level * np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1))) if selection_count > 0 else float('inf')\n        \n        # Calculate combined dynamic score\n        combined_score = average_score + exploration_value\n        \n        # Adjust score with time decay\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    # Select action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Exploration rate\n    epsilon = 0.1  \n    action_scores = []\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score, handling division by zero\n        average_score = np.mean(scores) if scores else 0\n        \n        # Use maximum constant exploration to ensure exploration of all actions\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Calculate combined score with exploration adjustment\n        combined_score = average_score + exploration_value\n        \n        # Incorporate time decay factor for relevance\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    # Select action with highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Adjust exploration factor based on the time slot\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score or handle case where no selections were made\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # UCB exploration term; minimizes the zero selection case while balancing exploration\n        exploration_value = (np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1))\n                             if selection_count > 0 else np.inf)\n        \n        # Combine average score with exploration term adjusted by exploration_factor\n        combined_score = average_score + exploration_factor * exploration_value\n        action_scores.append(combined_score)\n    \n    # Select the action with the highest combined score for exploitation\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.1  # Exploration parameter for balancing exploration and exploitation\n    action_scores = []\n    total_actions = len(score_set)\n\n    for action_index in range(total_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Epsilon that decreases over time, promoting exploitation as time progresses\n        epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n        \n        # UCB-based exploration term\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combine exploration and exploitation using a weighted approach\n        combined_score = (1 - epsilon) * average_score + epsilon * exploration_param * exploration_value\n        action_scores.append(combined_score)\n\n    # Select the action with the highest combined score\n    action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration rate\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score, handling the case when there are no prior selections\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration term, add a small offset to avoid division by zero\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combine average score and exploration term\n        combined_score = average_score + epsilon * exploration_value\n        \n        # Apply time decay factor\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    # Select action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 1  # Exploration parameter, can be adjusted.\n    action_scores = []\n    total_actions = len(score_set)\n\n    for action_index in range(total_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Epsilon decay strategy for exploration\n        epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n        \n        # Apply an improved UCB formula\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n\n        # Calculate combined score: weighted average of exploration and exploitation\n        combined_score = (1 - epsilon) * average_score + epsilon * exploration_param * exploration_value\n        action_scores.append(combined_score)\n\n    # Select the action with the highest score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_factor = 0.5  # Controls the exploration level\n    action_scores = []\n    \n    # Calculate current time weight\n    time_weight = current_time_slot / total_time_slots if total_time_slots > 0 else 0\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Average score for the action\n        average_score = np.mean(scores) if selection_count > 0 else 0\n\n        # Exploration term with time consideration\n        if selection_count > 0:\n            exploration_value = np.sqrt(np.log(total_selection_count) / selection_count)\n        else:\n            exploration_value = float('inf')\n\n        # Confidence in the average score adjusted by time weight\n        combined_score = average_score + exploration_factor * exploration_value * (1 - time_weight)\n        action_scores.append(combined_score)\n\n    # Adaptive exploration-exploitation decision\n    probabilities = np.exp(action_scores - np.max(action_scores))  # Softmax to promote diversity\n    probabilities /= np.sum(probabilities)  # Normalize to get probabilities\n    action_index = np.random.choice(range(8), p=probabilities)  # Select action based on probabilities\n\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Define constants\n    alpha = 0.1  # Weighting factor for score contribution\n    beta = 0.9   # Weighting factor for exploration component\n    epsilon = 0.1 # Exploration rate \n\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score, ensuring no division by zero\n        average_score = np.mean(scores) if scores else 0\n        \n        # Exploration term based on selection counts\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combined score taking into account the exploration\n        combined_score = average_score + (beta * exploration_value)\n        \n        # Time decay factor to prioritize more recent selections\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        # Scale down the scores for more competitive choice\n        action_scores.append(adjusted_score)\n    \n    # Select the action with the highest adjusted score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration rate\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration term with smoothing\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Calculate the combined score\n        combined_score = average_score + (epsilon * exploration_value)\n\n        # Apply time decay factor\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    # Select action with highest adjusted score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Dynamic epsilon value that decays over time\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01)\n\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate the average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration value\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n\n        # Combined score\n        combined_score = average_score + epsilon * exploration_value\n        action_scores.append(combined_score)\n\n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, 8)\n    else:\n        action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Define exploration coefficient\n    epsilon = 0.1  \n    action_scores = []\n    \n    # Iterate over each action\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration value\n        exploration_value = (np.sqrt(2 * np.log(total_selection_count + 1) / (selection_count + 1))\n                             if selection_count > 0 else float('inf'))\n        \n        # Compute combined score\n        combined_score = average_score + (epsilon * exploration_value)\n        \n        # Apply time decay\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    # Select the action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.1\n    action_scores = []\n    total_actions = len(score_set)\n\n    for action_index in range(total_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Dynamic epsilon decreasing based on time slot, ensuring a minimum exploration rate\n        epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.05)\n        \n        # UCB exploration value, avoiding division by zero\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Calculating combined score with balanced exploration and exploitation\n        adjusted_score = (1 - epsilon) * average_score + epsilon * exploration_param * exploration_value\n        action_scores.append(adjusted_score)\n\n    # Select the action with the maximum score\n    action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Exploration parameter\n    exploration_param = 1.5\n    # Epsilon for epsilon-greedy approach\n    epsilon = 0.1\n    \n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration term with a dynamic adjustment\n        if selection_count > 0:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_count)\n        else:\n            exploration_value = float('inf')  # Favor untried actions\n        \n        # Combined score for UCB\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    # Epsilon-greedy decision-making\n    if np.random.rand() < epsilon:\n        # Select a random action to explore\n        action_index = np.random.randint(0, 8)\n    else:\n        # Select the action with the maximum combined score\n        action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration rate\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score, handling division by zero\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration term using Upper Confidence Bound (UCB)\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combined score\n        combined_score = average_score + exploration_value\n        \n        # Time decay factor\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    # Select action with highest adjusted score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameter for exploration-exploitation balance\n    exploration_factor = 1.5\n    \n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration value\n        exploration_value = (exploration_factor * np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1))) if selection_count > 0 else float('inf')\n        \n        # Combined score\n        combined_score = average_score + exploration_value\n        action_scores.append(combined_score)\n    \n    # Select action based on highest score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_factor = 0.1\n    action_scores = []\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Calculate average score\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration term\n        exploration_value = (np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) \n                             if selection_count > 0 else float('inf'))\n        \n        # Combine average score with exploration\n        combined_score = average_score + exploration_factor * exploration_value\n        \n        # Apply time decay for current selection\n        time_decay = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay\n        \n        action_scores.append(adjusted_score)\n    \n    # Select action with highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration rate\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score, handling division by zero\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration term based on remaining time slots and selection count\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Adjust combined score\n        combined_score = average_score + (epsilon * exploration_value)\n        \n        # Time decay factor\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    # Softmax for better exploration\n    action_probs = np.exp(action_scores) / np.sum(np.exp(action_scores))\n    \n    # Select action based on weighted probabilities\n    action_index = np.random.choice(range(8), p=action_probs)\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    average_scores = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n    \n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        average_scores[action_index] = np.mean(scores) if scores else 0\n        selection_counts[action_index] = len(scores)\n    \n    # Calculate exploration factor with a dynamic adjustment\n    exploration_values = np.zeros(action_count)\n    for action_index in range(action_count):\n        if selection_counts[action_index] > 0:\n            exploration_values[action_index] = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[action_index])\n        else:\n            exploration_values[action_index] = float('inf')  # Encourage exploration for untried actions\n    \n    # Calculate combined scores with a variable exploration parameter \n    combined_scores = average_scores + exploration_values\n    \n    # Ensure that we encourage exploration even in later time slots\n    time_factor = (total_time_slots - current_time_slot) / total_time_slots\n    combined_scores += (1 - time_factor) * exploration_values\n    \n    # Select action with the maximum combined score\n    action_index = np.argmax(combined_scores)\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Define parameters\n    epsilon = 0.1  # Exploration rate\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n\n    # Calculate scores for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score, handling division by zero\n        average_score = np.mean(scores) if selection_count > 0 else 0\n\n        # Calculate exploration term\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combine exploration and exploitation\n        combined_score = average_score + (epsilon * exploration_value)\n\n        # Adjust based on time remaining\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        action_scores[action_index] = combined_score * time_decay_factor\n\n    # Select action with highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.5  # Adjust exploration parameter\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration term\n        exploration_value = (\n            np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1e-5)) \n            if selection_count > 0 \n            else float('inf')\n        )\n        \n        # Compute combined score\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    # Select the action with the maximum combined score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration rate\n    action_scores = []\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score, handling division by zero\n        average_score = np.mean(scores) if scores else 0\n\n        # If the action has been selected before, calculate exploration term\n        if selection_count > 0:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1))\n        else:\n            exploration_value = float('inf')  # Prioritize exploring unselected actions\n\n        # Calculate combined score for action selection\n        combined_score = average_score + (epsilon * exploration_value)\n        \n        # Apply time decay factor\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    # Select the action with highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    alpha = 1.0  # Exploration weight\n    beta = 0.5   # Weight for time decay influence\n    action_scores = []\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score, handling division by zero\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration term based on total selections and current action selections\n        exploration_value = np.sqrt(alpha * np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Calculate time decay factor\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        time_adjusted_exploitation = average_score * time_decay_factor\n        \n        # Combined score using exploration-exploitation balance\n        combined_score = time_adjusted_exploitation + exploration_value\n        action_scores.append(combined_score)\n\n    # Select action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters\n    epsilon = 0.1  # Exploration rate, for balancing exploration and exploitation\n    action_count = 8  # Total number of actions\n    \n    # Prepare to calculate scores for each action\n    action_scores = []\n    \n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate the average score safely\n        average_score = np.mean(scores) if scores else 0\n        \n        # Compute exploration term for UCB\n        if selection_count > 0:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count)) / selection_count)\n        else:\n            exploration_value = float('inf')  # Encourage selection of untried actions\n            \n        # Calculate the combined score\n        combined_score = average_score + exploration_value\n        \n        # Apply time decay factor\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    # Select the action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 1.0  # Adjusted exploration factor\n    action_scores = []\n    total_actions = len(score_set)\n    \n    # Calculate the expected score and exploration value for each action\n    for action_index in range(total_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Epsilon should decrease over time to promote exploitation\n        epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n        \n        # Compute exploration term using UCB\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Calculate combined score with adjusted exploration/exploitation balance\n        combined_score = (1 - epsilon) * average_score + (epsilon * exploration_param * exploration_value)\n        action_scores.append(combined_score)\n\n    # Determine the action index with the highest score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    epsilon = 0.1  # Exploration parameter\n    action_scores = []\n    \n    # Calculate the time decay factor\n    time_decay = (total_time_slots - current_time_slot) / total_time_slots\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate the exploration value using Upper Confidence Bound approach\n        if selection_count > 0:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count)) / selection_count)\n        else:\n            exploration_value = float('inf')  # Allocate exploration for untried actions\n        \n        # Combined score calculation with normalized exploration weight\n        combined_score = average_score + epsilon * exploration_value * time_decay\n        action_scores.append(combined_score)\n    \n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Hyperparameters\n    epsilon = 0.1  # Exploration rate\n    optimism_constant = 2  # For UCB exploration\n    \n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score, handling cases with no selections\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration value (UCB adjustment)\n        exploration_value = (\n            np.sqrt((optimism_constant * np.log(total_selection_count + 1)) / (selection_count + 1))\n            if selection_count > 0 else float('inf')\n        )\n        \n        # Combine average score and exploration value\n        combined_score = average_score + exploration_value\n        \n        # Apply time decay to prioritize earlier selections\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    # Select action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.1  # Exploration parameter\n    action_scores = []\n    total_actions = len(score_set)\n    \n    for action_index in range(total_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Epsilon decay strategy for exploration\n        epsilon = np.clip(1 - (current_time_slot / total_time_slots), 0.1, 1)\n\n        # UCB exploration term\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n\n        # Combined score\n        combined_score = (1 - epsilon) * average_score + epsilon * exploration_param * exploration_value\n        action_scores.append(combined_score)\n    \n    # Select the action with the highest combined score\n    action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.5  # Adjust exploration weight\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration term\n        exploration_value = (np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1))) if selection_count > 0 else float('inf')\n        \n        # Calculate total time left for exploration\n        time_left = total_time_slots - current_time_slot\n        \n        # Compute combined score with consideration of exploration and time left\n        combined_score = average_score + exploration_param * exploration_value * (1 + time_left / total_time_slots)\n        action_scores.append(combined_score)\n\n    # Return action with the maximum score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    action_avg_scores = np.zeros(num_actions)\n    exploration_values = np.zeros(num_actions)\n    \n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        action_avg_scores[action_index] = np.mean(scores) if scores else 0\n        \n        # Calculate exploration values using UCB\n        if selection_count > 0:\n            exploration_values[action_index] = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1))\n        else:\n            exploration_values[action_index] = float('inf')  # Explore unselected actions\n            \n    # Combine exploration and exploitation\n    combined_scores = action_avg_scores + exploration_values\n    \n    # Select the action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores and action counts\n    action_scores = []\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / (np.array([len(scores) for scores in score_set.values()]) + 1))\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        \n        # Calculate average score and handle division by zero\n        average_score = np.mean(scores) if scores else 0\n        \n        # Combine average score and exploration bonus\n        combined_score = average_score + exploration_bonus[action_index]\n        action_scores.append(combined_score)\n\n    # Select action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.9999999999999,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters\n    epsilon = 0.1  # Exploration rate\n    \n    # Initialize action scores\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score, avoiding division by zero\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration term using UCB\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combined score\n        combined_score = average_score + exploration_value\n        \n        # Applying time decay factor\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    # Select action with highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration rate\n    n_actions = 8\n    action_scores = []\n    \n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration term using UCB\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combine scores for action selection\n        combined_score = average_score + (epsilon * exploration_value)\n        \n        # Apply time decay factor to adjust score\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    # Select action with highest adjusted score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    beta = 1.0  # Exploration parameter\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score, handling division by zero\n        average_score = np.mean(scores) if scores else 0\n        \n        # Calculate exploration term\n        exploration_value = np.sqrt((beta * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Calculate adjusted score that weighs exploration and exploitation\n        if total_selection_count > 0:\n            adjusted_score = average_score + exploration_value\n        else:\n            adjusted_score = 1.0  # Initially try all actions\n        \n        # Apply time decay factor\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score *= time_decay_factor\n        \n        action_scores.append(adjusted_score)\n    \n    # Select action with highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    total_actions = len(score_set)\n    action_scores = np.zeros(total_actions)\n    \n    for action_index in range(total_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Apply UCB to balance exploration and exploitation\n        if selection_count == 0:\n            exploration_value = float('inf')  # Prioritize unselected actions\n        else:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count)) / selection_count)\n        \n        action_scores[action_index] = average_score + exploration_value\n    \n    # Select the action with the highest adjusted score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 1.0 / (current_time_slot + 1)  # Decaying exploration rate\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score and handle division by zero\n        average_score = np.mean(scores) if scores else 0\n        \n        # Exploration term using UCB method\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combined score considering both average score and exploration terms\n        combined_score = (1 - epsilon) * average_score + (epsilon * exploration_value)\n        action_scores.append(combined_score)\n    \n    # Select action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Epsilon for epsilon-greedy exploration\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration factor\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Compute combined score using UCB\n        combined_score = average_score + exploration_value\n        \n        action_scores.append(combined_score)\n    \n    # Epsilon-greedy strategy for exploration\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, 8)  # Randomly select an action for exploration\n    else:\n        action_index = np.argmax(action_scores)  # Select best action based on combined scores\n    \n    return action_index",
          "objective": -449.99999999999994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters\n    exploration_factor = 0.1\n    \n    # Calculate average scores and selection counts\n    average_scores = []\n    selection_counts = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        average_score = np.mean(scores) if selection_count > 0 else 0\n        average_scores.append(average_score)\n        selection_counts.append(selection_count)\n    \n    # Calculate Upper Confidence Bound for each action\n    ucb_values = []\n    \n    for i in range(8):\n        if selection_counts[i] > 0:\n            ucb_value = average_scores[i] + exploration_factor * np.sqrt(np.log(total_selection_count) / selection_counts[i])\n        else:\n            ucb_value = float('inf')  # Encourages exploration of under-selected actions\n        ucb_values.append(ucb_value)\n    \n    # Select action using UCB strategy\n    action_index = np.argmax(ucb_values)\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    epsilon = 1.0 / (current_time_slot + 1)  # Decay exploration factor\n    action_scores = []\n\n    # Calculate average scores and selection counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        average_score = np.mean(scores) if scores else 0\n        \n        # UCB exploration term\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combine exploration and exploitation\n        combined_score = (1 - epsilon) * average_score + epsilon * exploration_value\n        action_scores.append(combined_score)\n\n    # Adjust scores for exploration at the start\n    if total_selection_count < action_count:\n        action_scores = [score + np.random.rand() * 0.1 for score in action_scores]\n\n    # Select action with the highest combined score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    epsilon = 0.1  # Exploration parameter\n    action_scores = []\n\n    time_weight = (total_time_slots - current_time_slot) / total_time_slots\n    \n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        average_score = np.mean(scores) if selection_count > 0 else 0\n\n        # Calculate exploration value using a safer approach\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Calculate combined score with exploration weight\n        combined_score = average_score + epsilon * exploration_value * time_weight\n        action_scores.append(combined_score)\n    \n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_param = 0.2  # Increased exploration parameter\n    action_scores = []\n  \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score with handling for empty scores\n        average_score = np.mean(scores) if selection_count > 0 else 0.0\n        \n        # Calculate exploration term with a smoothing factor to avoid division by zero\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1e-5))\n        \n        # Combined score for action selection using UCB\n        combined_score = average_score + exploration_param * exploration_value\n        action_scores.append(combined_score)\n        \n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Adaptive parameters\n    epsilon = 1 / (current_time_slot + 1)  # Epsilon decreasing with time\n    action_indices = list(range(len(score_set)))\n    \n    # Calculate average scores and selection counts\n    action_avg_scores = np.array([np.mean(score_set.get(i, [0])) for i in action_indices])\n    selection_counts = np.array([len(score_set.get(i, [])) for i in action_indices])\n    \n    # UCB exploration factor\n    exploration_values = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1))\n    \n    # Combine average scores with exploration\n    combined_scores = (1 - epsilon) * action_avg_scores + epsilon * exploration_values\n    \n    # Select the action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -450.0,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Epsilon that decreases over time for exploration-exploitation balance.\n    base_epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n    \n    # Calculate action scores\n    action_scores = []\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        if selection_count > 0:\n            # Calculate exploration term (UCB)\n            exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1))\n        else:\n            # Assign a high exploration value for untested actions\n            exploration_value = float('inf')\n        \n        # Combine exploitation and exploration\n        combined_score = average_score + base_epsilon * exploration_value\n        action_scores.append(combined_score)\n\n    # Select action with the highest score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(range(len(score_set)))\n    \n    # Calculate the average scores and selection counts for each action\n    action_avg_scores = np.zeros(len(action_indices))\n    action_selection_counts = np.zeros(len(action_indices))\n\n    for action_index in action_indices:\n        scores = score_set.get(action_index, [])\n        action_selection_counts[action_index] = len(scores)\n        action_avg_scores[action_index] = np.mean(scores) if scores else 0\n\n    # Epsilon value for exploration-exploitation\n    epsilon = 1 / (current_time_slot + 1)  # Decaying exploration\n    \n    # Combination of exploration bonus for less selected actions and exploitation of average scores\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / (action_selection_counts + 1))\n    \n    # Avoid division by zero\n    exploration_bonus[action_selection_counts == 0] = float('inf')\n    \n    # Use weighted approach with dynamic epsilon\n    combined_scores = (1 - epsilon) * action_avg_scores + epsilon * exploration_bonus\n    \n    # Select the action with the highest combined score\n    action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": -450.0,
          "other_inf": null
     }
]