[
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_scores = np.zeros(action_count)\n    action_selections = np.zeros(action_count)\n\n    # Calculate average scores and selection counts for each action\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_selections[action_index] = len(scores)\n        \n        total_score = np.sum(scores)\n        average_score = total_score / action_selections[action_index] if action_selections[action_index] > 0 else 0\n        \n        # Explore-exploit parameters\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (action_selections[action_index] + 1)) if action_selections[action_index] > 0 else 1\n        time_adjustment = (1 - current_time_slot / total_time_slots) if total_time_slots > 0 else 1\n        \n        # Score calculation combining exploration and average score\n        action_scores[action_index] = average_score + (0.1 * exploration_value * time_adjustment)\n\n    # Epsilon-greedy aspect for exploration\n    epsilon = 0.1\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(action_count)  # Random action for exploration\n    else:\n        action_index = np.argmax(action_scores)  # Best action based on calculated scores\n\n    return action_index",
          "objective": -449.9988928723379,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    epsilon = 0.1  # Probability of exploration\n    alpha = 0.5    # Time decay parameter\n\n    # Calculate average scores and adjusted scores\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)  # Number of times the action has been selected\n        avg_score = np.mean(scores) if n > 0 else 0.0\n        \n        # Normalize score based on total selections\n        normalized_score = (avg_score * n / total_selection_count) if total_selection_count > 0 else avg_score\n        \n        # Use UCB for exploration\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else np.inf\n        \n        # Time decay factor\n        time_factor = (total_time_slots - current_time_slot) / total_time_slots\n        action_scores[action_index] = normalized_score + (exploration_bonus * time_factor ** alpha)\n\n    # Epsilon-greedy strategy for action selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -449.9984461938054,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    epsilon = 0.1  # Exploration probability\n    exploration_weight = 0.75  # Increased weight for better exploration\n    alpha = 0.5  # Time decay factor\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)  # Number of times the action has been selected\n        avg_score = np.mean(scores) if n > 0 else 0.0\n        \n        # Normalize score based on selection count\n        normalized_score = avg_score * (n / total_selection_count) if total_selection_count > 0 else avg_score\n        \n        # Exploration bonus with a confidence interval\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else np.inf\n        \n        # Time factor to account for aging of scores\n        time_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = normalized_score + (exploration_weight * exploration_bonus * (time_factor ** alpha))\n        \n        action_scores[action_index] = adjusted_score\n\n    # Epsilon-greedy strategy for action selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -449.99814904506,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    epsilon = 0.1  # Exploration probability\n    exploration_weight = 0.5  # Weight for exploration term\n    alpha = 0.4  # Time decay factor\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)  # Number of times the action has been selected\n        avg_score = np.mean(scores) if n > 0 else 0.0\n        \n        # Normalizing score to avoid bias in early selections\n        normalized_score = avg_score * (n / total_selection_count) if total_selection_count > 0 else avg_score\n        \n        # Exploration bonus with a confidence interval\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else np.inf\n        \n        # Time decay factor\n        time_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = normalized_score + (exploration_weight * exploration_bonus * (time_factor ** alpha))\n        \n        action_scores[action_index] = adjusted_score\n\n    # Epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -449.99814538834266,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    average_scores = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n\n    # Gather selection counts and average scores for each action\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        # Calculate average score\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n        else:\n            average_scores[action_index] = 0  # Action has never been selected\n\n    # Calculate exploration bonuses and adjusted scores\n    scores_with_exploration = np.zeros(action_count)\n    for action_index in range(action_count):\n        if total_selection_count > 0:\n            normalized_score = average_scores[action_index] * (selection_counts[action_index] / total_selection_count)\n        else:\n            normalized_score = average_scores[action_index]\n        \n        # UCB exploration value\n        if selection_counts[action_index] > 0:\n            exploration_value = np.sqrt(np.log(total_selection_count) / selection_counts[action_index])\n        else:\n            exploration_value = np.inf  # Force exploration of unselected actions\n        \n        # Time-dependent factor\n        time_factor = 1 - (current_time_slot / total_time_slots)\n        \n        # Total score with exploration\n        scores_with_exploration[action_index] = normalized_score + exploration_value * time_factor\n\n    # Select the action with the highest score\n    action_index = np.argmax(scores_with_exploration)\n    return action_index",
          "objective": -449.99713047490025,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    epsilon = 0.1  # Probability of exploration\n    exploration_weight = 1.0  # Weight for exploration term\n    time_decay_factor = 0.5  # Weight for recent time slots\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)  # Number of times the action has been selected\n        avg_score = np.mean(scores) if n > 0 else 0.0\n        \n        # Normalized average score\n        normalized_score = avg_score / (n + 1) if total_selection_count > 0 else avg_score\n        \n        # Exploration bonus using a variant of the Upper Confidence Bound (UCB)\n        exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / (n + 1)) if n > 0 else np.inf\n\n        # Dynamic time adjustment factor\n        time_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = normalized_score + (exploration_weight * exploration_bonus * (time_factor ** time_decay_factor))\n        \n        action_scores[action_index] = adjusted_score\n\n    # Epsilon-greedy strategy for action selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -449.9968561824966,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_scores = np.zeros(action_count)\n    action_selections = np.zeros(action_count)\n\n    # Calculate selection counts and average scores for each action\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_selections[action_index] = len(scores)\n        \n        # Calculate mean and avoid division by zero\n        average_score = np.mean(scores) if action_selections[action_index] > 0 else 0\n        \n        # Calculate normalized score to factor in overall selection count\n        if total_selection_count > 0:\n            normalized_score = average_score * (action_selections[action_index] / total_selection_count)\n        else:\n            normalized_score = average_score\n        \n        # Calculate exploration bonus using UCB strategy\n        if action_selections[action_index] > 0:\n            exploration_value = (np.log(total_selection_count) / action_selections[action_index]) ** 0.5\n        else:\n            exploration_value = np.inf  # Promote fully unexplored actions\n        \n        # Incorporate a time-dependent factor for exploration\n        time_factor = 1 - (current_time_slot / total_time_slots)\n        \n        # Update action scores\n        action_scores[action_index] = normalized_score + exploration_value * time_factor\n\n    # Select the action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.9965283222602,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_scores = np.zeros(action_count)\n    action_selections = np.zeros(action_count)\n    \n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_selections[action_index] = len(scores)\n\n        average_score = np.mean(scores) if action_selections[action_index] > 0 else 0\n        \n        normalized_score = (average_score * action_selections[action_index] / total_selection_count\n                            if total_selection_count > 0 else average_score)\n        \n        # Exploration term using UCB\n        exploration_value = (np.sqrt(np.log(total_selection_count + 1) / (action_selections[action_index] + 1))\n                             * 0.1 * (1 - current_time_slot / total_time_slots)\n                             if action_selections[action_index] > 0 else 0)\n\n        # Adding a decay factor for exploration\n        decay_factor = np.exp(-current_time_slot / total_time_slots)\n        \n        action_scores[action_index] = (normalized_score + exploration_value) * decay_factor\n\n    # Select the action with the highest score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -449.9964493263588,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01)  # Epsilon decay\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n\n        # Calculate average score\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Normalize score and handle division by zero\n        normalized_score = avg_score * (n / total_selection_count) if total_selection_count > 0 and n > 0 else avg_score\n\n        # Calculate exploration factor\n        exploration_factor = epsilon * np.sqrt((np.log(total_selection_count + 1) / (n + 1))) if n > 0 else epsilon\n\n        # Bayesian approach with Beta distribution for confidence interval\n        beta_param_a = 1 + np.sum(scores)  # successes\n        beta_param_b = 1 + n - np.sum(scores)  # failures\n        bayesian_estimate = np.random.beta(beta_param_a, beta_param_b)\n\n        # Combine UCB and Bayesian estimate for the final action score\n        combined_value = normalized_score + exploration_factor + bayesian_estimate\n\n        action_scores.append(combined_value)\n\n    # Select the action with the maximum combined value\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99557099903166,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    epsilon = 0.1  # Exploration probability\n    exploration_weight = 1.0  # Weighing exploration more heavily\n    alpha = 0.5  # Time decay factor\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)  # Number of times the action has been selected\n        avg_score = np.mean(scores) if n > 0 else 0.0\n        \n        # Normalize score and adjust with total_selection_count\n        normalized_score = avg_score * (n / total_selection_count) if total_selection_count > 0 else avg_score\n        \n        # Exploration bonus with respect to selection count\n        exploration_bonus = np.sqrt((np.log(total_selection_count + 1) if total_selection_count > 0 else np.log(1)) / (n + 1)) if n > 0 else np.inf\n        \n        # Time factor based on the remaining slots\n        time_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = normalized_score + (exploration_weight * exploration_bonus * (time_factor ** alpha))\n        \n        action_scores[action_index] = adjusted_score\n\n    # Epsilon-greedy strategy for balance between exploration and exploitation\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -449.9952472700963,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n\n    # Calculate decay exploration factor (epsilon)\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01)  # Decay exploration rate\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)  # Number of times the action has been selected\n        \n        # Calculate average score\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Normalize score to account for total selections\n        normalized_score = (avg_score * n / total_selection_count) if total_selection_count > 0 and n > 0 else avg_score\n\n        # Exploration value to incentivize less selected actions\n        exploration_value = (epsilon * np.sqrt(np.log(total_selection_count + 1) / (n + 1))) if n > 0 else epsilon\n        \n        # Calculate Upper Confidence Bound value\n        ucb_value = normalized_score + exploration_value\n        \n        action_scores.append(ucb_value)\n\n    # Select the action with the maximum UCB value\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99521104146163,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    epsilon = 0.05  # Reduced exploration probability\n    alpha = 0.3     # Adjusted weight parameter for the time factor\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)  # Number of times the action has been selected\n        avg_score = np.mean(scores) if n > 0 else 0.0\n        \n        # Normalize score based on total selections\n        normalized_score = avg_score * (n / total_selection_count) if total_selection_count > 0 else avg_score\n        \n        # Exploration bonus based on UCB\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else np.inf\n        \n        # Time decaying factor\n        time_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = normalized_score + (exploration_bonus * time_factor ** alpha)\n        \n        action_scores[action_index] = adjusted_score\n\n    # Epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -449.9948775760064,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    epsilon = 0.1  # Exploration probability\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)  # Number of times the action has been selected\n        avg_score = np.mean(scores) if n > 0 else 0.0\n        \n        # Normalize score based on total selections\n        normalized_score = avg_score * (n / total_selection_count) if total_selection_count > 0 else avg_score\n        \n        # UCB exploration bonus\n        exploration_bonus = np.sqrt((np.log(total_selection_count + 1) / (n + 1))) if n > 0 else np.inf\n        \n        # Time decay factor\n        time_factor = np.power((total_time_slots - current_time_slot) / total_time_slots, 2)  # Squared decay rate\n        \n        # Adjusted score calculation\n        adjusted_score = normalized_score + exploration_bonus * time_factor\n        action_scores.append(adjusted_score)\n\n    # Epsilon-greedy strategy for action selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(8)\n    else:\n        action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -449.99474468518815,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = np.zeros(8)\n\n    # Constants for exploration and decay\n    epsilon = 0.1  # Probability of exploration\n    alpha = 0.5    # Weight for time decay\n    \n    # Loop through each action to calculate the adjusted scores\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)  # Number of times the action has been selected\n        avg_score = np.mean(scores) if n > 0 else 0.0\n        \n        # Normalize the average score considering total selections\n        normalized_score = avg_score * (n / total_selection_count) if total_selection_count > 0 else avg_score\n        \n        # Calculating the exploration bonus using UCB\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else np.inf\n        \n        # Applying time decay factor\n        time_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = normalized_score + (exploration_bonus * (time_factor ** alpha))\n        \n        action_scores[action_index] = adjusted_score\n\n    # Epsilon-greedy decision-making\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(8)  # Random exploration\n    else:\n        action_index = np.argmax(action_scores)  # Best-performing action\n\n    return action_index",
          "objective": -449.9941698659704,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = np.zeros(8)\n    epsilon = 0.1  # Exploration probability\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)  # Number of times the action has been selected\n        avg_score = np.mean(scores) if n > 0 else 0.0\n        \n        # Normalize score based on total selections\n        normalized_score = avg_score * (n / total_selection_count) if total_selection_count > 0 else avg_score\n        \n        # Exploration bonus based on UCB\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else np.inf\n        \n        # Decay exploration bonus based on current time\n        time_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = normalized_score + exploration_bonus * time_factor\n        \n        action_scores[action_index] = adjusted_score\n\n    # Epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(8)\n    else:\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -449.99340032810994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    epsilon = 0.1  # Exploration probability\n    max_actions = 8\n    \n    for action_index in range(max_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)  # Number of times the action has been selected\n        avg_score = np.mean(scores) if n > 0 else 0.0\n        \n        # Normalize score based on total selections\n        normalized_score = avg_score * (n / total_selection_count) if total_selection_count > 0 else avg_score\n        \n        # Exploration bonus based on UCB\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else np.inf\n        \n        # Decay exploration based on current time\n        time_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = normalized_score + exploration_bonus * time_factor\n        \n        action_scores.append(adjusted_score)\n\n    # Epsilon-greedy approach for exploration\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(max_actions)\n    else:\n        action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -449.99306356624356,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    epsilon = 0.1  # Exploration probability\n\n    # Calculate scores and selection counts\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)  # Number of times the action has been selected\n        avg_score = np.mean(scores) if n > 0 else 0.0\n        \n        # Handle scoring and selection count safely\n        normalized_score = avg_score * (n / total_selection_count) if total_selection_count > 0 else avg_score\n        \n        # UCB exploration bonus\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else np.inf\n        \n        # Temporal decay factor\n        time_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = normalized_score + exploration_bonus * time_factor\n        \n        action_scores.append(adjusted_score)\n\n    # Epsilon-greedy strategy for action selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(8)\n    else:\n        action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -449.9930587949281,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    epsilon = 1.0 / (current_time_slot + 1)  # Diminishing exploration factor\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        \n        avg_score = np.mean(scores) if n > 0 else 0.0\n        normalized_selection_count = total_selection_count if total_selection_count > 0 else 1\n        action_value = avg_score * (n / normalized_selection_count) if n > 0 else 0.0\n        exploration_value = epsilon * np.sqrt(np.log(normalized_selection_count + 1) / (n + 1)) if n > 0 else epsilon\n        \n        combined_score = action_value + exploration_value\n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.9927175707325,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    epsilon = 0.1  # Exploration probability\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)  # Number of times the action has been selected\n        \n        avg_score = np.mean(scores) if n > 0 else 0.0\n        normalized_score = avg_score * (n / total_selection_count) if total_selection_count > 0 else avg_score\n        \n        # Exploration bonus based on UCB\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else np.inf\n        \n        # Time decay factor\n        time_factor = (total_time_slots - current_time_slot) / total_time_slots\n        \n        # Combined adjusted score\n        adjusted_score = normalized_score + exploration_bonus * time_factor\n        \n        action_scores.append(adjusted_score)\n\n    # Epsilon-greedy decision making\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(8)\n    else:\n        action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -449.9921472134128,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01)  # Decay exploration\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        \n        avg_score = np.mean(scores) if n > 0 else 0\n        normalized_score = avg_score * (n / total_selection_count) if total_selection_count > 0 and n > 0 else avg_score\n        \n        # Exploration value to incentivize less selected actions\n        exploration_value = (epsilon * np.sqrt(np.log(total_selection_count + 1) / (n + 1))) if n > 0 else epsilon\n        \n        # Calculate UCB value\n        ucb_value = normalized_score + exploration_value\n        \n        action_scores.append(ucb_value)\n    \n    # Select the action with the maximum UCB value\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.9918256003892,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01)\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        \n        if n > 0:\n            avg_score = np.mean(scores)\n            normalized_score = avg_score * (n / total_selection_count)\n            uncertainty = epsilon / np.sqrt(n)\n        else:\n            avg_score = 0.0\n            normalized_score = 0.0\n            uncertainty = epsilon\n        \n        # Calculate combined score\n        combined_score = normalized_score + uncertainty\n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.9909165601049,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    actions_count = len(score_set)\n\n    # Calculate epsilon decay for exploration\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01)\n\n    for action_index in range(actions_count):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n\n        if n > 0:\n            avg_score = np.mean(scores)\n            # Calculate normalized score based on selection frequency\n            normalized_score = avg_score * (n / total_selection_count)\n            uncertainty = epsilon / np.sqrt(n)\n        else:\n            avg_score = 0.0\n            normalized_score = 0.0\n            uncertainty = epsilon  # Fully explore if not selected\n        \n        # Calculate UCB score\n        combined_score = normalized_score + uncertainty\n        action_scores.append(combined_score)\n\n    # Select action with the highest combined score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.99047500114193,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = 1.0  # Base exploration factor\n    decay_rate = 0.99  # Decay rate for exploration over time\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Normalize average score\n        normalized_score = avg_score / (total_selection_count + 1) if total_selection_count > 0 else 0\n        \n        # Calculate exploration term with decaying exploration factor\n        exploration_value = exploration_factor * (decay_rate ** current_time_slot)\n        action_score = normalized_score + exploration_value\n\n        action_scores.append(action_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.9892215411691,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Calculate exploration based on historical selection count\n        selection_count = n if n > 0 else 1  # Avoid division by zero\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / selection_count)\n        \n        # Weighted score: balance exploitation and exploration\n        combined_score = avg_score + exploration_value\n        \n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.9853214286312,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    min_epsilon = 0.01\n    max_epsilon = 0.5\n    epsilon_decay = max(0, (1 - current_time_slot / total_time_slots))\n    epsilon = max(min_epsilon, max_epsilon * epsilon_decay)\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0.0\n        normalized_score = avg_score / (n if n > 0 else 1)\n\n        exploration_value = epsilon * np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else epsilon\n        ucb_value = normalized_score + exploration_value\n\n        action_scores.append(ucb_value)\n\n    action_scores = np.array(action_scores) + np.random.rand(8) * (epsilon / 8)\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.98464069871153,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Normalizing count for exploration\n        selection_count = max(n, 1)  # Ensure at least one selection\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / selection_count)\n        \n        # Time decay factor to prioritize recent performance\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        normalized_avg_score = avg_score * time_decay_factor\n        \n        # Combined score with a balance of exploration and exploitation\n        combined_score = normalized_avg_score + exploration_value\n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.9829963967631,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    epsilon = 0.1  # Exploration probability\n    total_actions = 8\n\n    for action_index in range(total_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Normalize by total selections while avoiding division by zero\n        normalized_exploitation = avg_score * (n / (total_selection_count + 1)) if total_selection_count > 0 else 0\n        \n        # Exploration term based on the number of selections\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else 1\n        \n        # Combined score using a weighted approach\n        combined_score = normalized_exploitation + exploration_value\n        \n        # Adding time factor to promote actions early in the sequence\n        time_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_factor\n        \n        action_scores.append(adjusted_score)\n\n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(total_actions)  # Random action for exploration\n    else:\n        action_index = np.argmax(action_scores)  # Best action based on adjusted scores\n\n    return action_index",
          "objective": -449.9813203541065,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = 0.1  # Base exploration rate\n    decay_rate = 0.99  # Decay rate for epsilon over time\n    min_epsilon = 0.01  # Minimum exploration rate\n    \n    # Calculate epsilon for exploration based on current time slot\n    epsilon = max(min_epsilon, exploration_factor * (decay_rate ** current_time_slot))\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Normalized score to account for total selections\n        normalized_score = avg_score / (total_selection_count + 1) if total_selection_count > 0 else 0\n        \n        # Calculate exploration term\n        exploration_value = epsilon / (n + 1) if n > 0 else epsilon\n        \n        # Upper Confidence Bound (UCB) strategy\n        ucb_value = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else float('inf')\n        \n        # Combined score: balancing exploitation and exploration with UCB\n        combined_score = normalized_score + exploration_value + ucb_value\n        \n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.97893148320105,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = 0.1  # Initial exploration factor\n    decay_rate = 0.99  # Decay rate for exploration over time\n\n    # Calculate dynamic exploration based on time\n    dynamic_exploration = exploration_factor * ((total_time_slots - current_time_slot) / total_time_slots)\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n\n        # Normalized number of selections to avoid division by zero\n        selection_count = n / total_selection_count if total_selection_count > 0 else 0\n\n        # Upper Confidence Bound calculation\n        ucb_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (n + 1)) if n > 0 else np.inf\n        \n        # Combined score: favoring exploration of lesser-selected actions\n        combined_score = avg_score + dynamic_exploration + ucb_value * (1 - selection_count)\n        \n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.97467741218475,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    action_mean_scores = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate mean scores and counts for each action\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        action_mean_scores[action_index] = np.mean(scores) if scores else 0\n\n    # Ensure we avoid division by zero for actions never selected\n    total_actions_selected = np.clip(action_counts, 1, None)  # Avoid zero counts\n    \n    # Calculate exploration bonus using Upper Confidence Bound (UCB)\n    exploration_bonus = np.sqrt(2 * np.log(total_selection_count + 1) / total_actions_selected)\n    \n    # Combine scores using a weighted sum of mean scores and exploration bonus\n    combined_scores = action_mean_scores + exploration_bonus\n\n    # Select the action with the highest combined score\n    action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": -449.96935186757486,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        \n        # Calculate average score (be cautious of division by zero)\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Exploration bonus using a modified UCB formula\n        exploration_bonus = (np.sqrt(np.log(total_selection_count + 1) / (n + 1)) \n                             if n > 0 else float('inf'))\n        \n        # Temporal decay factor to encourage early exploration\n        time_decay = (1 - (current_time_slot / total_time_slots) if total_time_slots > 0 else 0)\n        \n        # Combined score: balancing exploitation and exploration\n        action_scores[action_index] = avg_score + time_decay * exploration_bonus\n\n    # Select the action with the highest combined score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -449.96520125548466,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    epsilon = 0.1  # Exploration probability\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        \n        if n > 0:\n            avg_score = np.mean(scores)\n        else:\n            avg_score = 0\n        \n        # Avoid division by zero for selection count\n        selection_count = n if n > 0 else 1\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_count)\n\n        # Normalizing average score based on total selection count\n        normalized_score = avg_score / (total_selection_count + 1) if total_selection_count > 0 else avg_score\n        \n        # Dynamic adjustment of exploration considering the time slot\n        exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n        \n        # Combine exploration and exploitation\n        combined_score = normalized_score + exploration_factor * exploration_value\n        \n        action_scores.append(combined_score)\n\n    # Implement \u03b5-greedy strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(range(8))\n    else:\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -449.9629843095321,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_exploitation = []\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        \n        avg_score = np.mean(scores) if n > 0 else 0\n        exploration_value = (1 / (n + 1)) * exploration_weight if total_selection_count > 0 else 1.0\n        \n        combined_score = avg_score + exploration_value\n        action_exploitation.append(combined_score)\n    \n    action_index = np.argmax(action_exploitation)\n    \n    return action_index",
          "objective": -449.9585793244485,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_exploitation = []\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        \n        avg_score = np.mean(scores) if n > 0 else 0\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else 1.0\n        \n        combined_score = avg_score + exploration_factor * exploration_value\n        action_exploitation.append(combined_score)\n    \n    action_index = np.argmax(action_exploitation)\n    \n    return action_index",
          "objective": -449.9412643947601,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_exploitation = []\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        \n        if n > 0:\n            avg_score = np.mean(scores)\n        else:\n            avg_score = 0\n        \n        exploration_value = (1 / (n + 1)) * exploration_factor if total_selection_count > 0 else 1.0\n        combined_score = avg_score + exploration_value\n        \n        action_exploitation.append(combined_score)\n    \n    action_index = np.argmax(action_exploitation)\n    \n    return action_index",
          "objective": -449.9302162373734,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = 1.0  # Base exploration factor\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01)  # Decaying exploration\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        \n        # Calculate average score; handle zero selections\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Normalize the average score using total_selection_count\n        normalized_score = avg_score * (n / total_selection_count) if total_selection_count > 0 else avg_score\n        \n        # Calculate exploration value, ensuring no division by zero\n        exploration_value = epsilon / (n + 1) if n > 0 else epsilon\n        \n        # Total UCB value\n        ucb_value = normalized_score + exploration_value\n        \n        action_scores.append(ucb_value)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.91089172243363,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    epsilon = 0.1  # Exploration factor\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Calculate selection count for exploration\n        selection_count = n if n > 0 else 1  # Avoid division by zero\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / selection_count)\n\n        # Dynamic weighting based on time\n        time_weight = (total_time_slots - current_time_slot) / total_time_slots\n        \n        # Combined score\n        combined_score = avg_score * time_weight + exploration_value * (1 - time_weight)\n        \n        action_scores.append(combined_score)\n\n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, 8)  # Explore\n    else:\n        action_index = np.argmax(action_scores)  # Exploit\n\n    return action_index",
          "objective": -449.9064168816226,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_scores = np.zeros(action_count)\n    action_selections = np.zeros(action_count)\n\n    # Calculate selection counts and average scores\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_selections[action_index] = len(scores)\n\n        # Compute average score, handling cases where action has not been selected\n        average_score = np.mean(scores) if action_selections[action_index] > 0 else 0\n        \n        # Explore favorably for less selected actions using UCB\n        exploration_value = (\n            np.sqrt(np.log(total_selection_count + 1) / (action_selections[action_index] + 1))\n            if action_selections[action_index] > 0 else np.inf\n        )\n\n        # Adjust exploration based on current time slot and time slots left\n        time_decay_factor = (1 - current_time_slot / total_time_slots) if total_time_slots > 0 else 1\n        \n        # Combine normalized score with exploration term\n        action_scores[action_index] = average_score + (0.1 * exploration_value * time_decay_factor)\n\n    # Select action with the highest score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -449.90007601011786,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_exploitation = []\n    action_counts = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_count = len(scores)\n        action_counts.append(action_count)\n        \n        if action_count > 0:\n            avg_score = np.mean(scores)\n        else:\n            avg_score = 0\n        \n        action_exploitation.append(avg_score)\n    \n    if total_selection_count == 0 or current_time_slot == 0:\n        return np.random.choice(range(8))  # Random selection for first time slots\n\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n    exploration_scores = [(1 / (count + 1)) * exploration_factor if count > 0 else exploration_factor \n                          for count in action_counts]\n\n    combined_scores = np.array(action_exploitation) + np.array(exploration_scores)\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -449.8815096358546,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    epsilon = 0.1  # Exploration probability\n    exploration_weight = 0.5  # Weight for exploration term\n    alpha = 0.4  # Time decay factor\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)  # Number of times the action has been selected\n        avg_score = np.mean(scores) if n > 0 else 0.0\n        \n        # Avoid division by zero\n        normalized_score = avg_score * (n / total_selection_count) if total_selection_count > 0 else avg_score\n        \n        # Exploration bonus adjusted with confidence interval\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else np.inf\n        \n        # Time decay factor\n        time_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = normalized_score + (exploration_weight * exploration_bonus * (time_factor ** alpha))\n        \n        action_scores[action_index] = adjusted_score\n\n    # Epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -449.8705174474542,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8  # Fixed number of actions, indexed from 0 to 7\n    action_exploitation = np.zeros(action_count)\n    action_selection_count = np.zeros(action_count)\n\n    for action_index, scores in score_set.items():\n        action_selection_count[action_index] = len(scores)\n        if scores:\n            action_exploitation[action_index] = np.mean(scores)\n\n    # Handle edge case for actions never selected\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n    exploration_bonus = np.array([\n        exploration_factor / (count + 1) for count in action_selection_count\n    ])\n\n    # If total_selection_count is 0, prioritize exploration\n    if total_selection_count == 0:\n        return np.random.choice(range(action_count))\n\n    combined_scores = action_exploitation + exploration_bonus\n    action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": -449.85720722753064,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_weight = 1.0  # Weight given to exploration\n    exploitation_weight = 1.0  # Weight given to exploitation\n    exploration_factor = 0.1  # Base exploration rate\n    decay_rate = 0.99  # Decay rate for epsilon over time\n    min_epsilon = 0.01  # Minimum exploration rate\n    \n    # Calculate epsilon for exploration based on current time slot\n    epsilon = max(min_epsilon, exploration_factor * (decay_rate ** current_time_slot))\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        normalized_score = avg_score / (total_selection_count + 1) if total_selection_count > 0 else 0\n        \n        # Exploration term\n        exploration_value = epsilon * (exploration_weight / (n + 1)) if n > 0 else epsilon * exploration_weight\n        \n        # Combined score: balance exploitation and exploration\n        combined_score = (exploitation_weight * normalized_score) + exploration_value\n        \n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.82064627324957,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_scores = np.zeros(action_count)\n    action_selections = np.zeros(action_count)\n    \n    # Calculate average scores and selection counts for each action\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_selections[action_index] = len(scores)\n        \n        # Calculate mean score, defaulting to 0 if no selections have been made\n        average_score = np.mean(scores) if action_selections[action_index] > 0 else 0\n        \n        # Normalize average score\n        normalized_score = average_score * (action_selections[action_index] / total_selection_count) if total_selection_count > 0 else average_score\n        \n        # Calculate exploration value using a modified UCB approach\n        if action_selections[action_index] > 0:\n            exploration_value = np.sqrt(np.log(total_selection_count) / action_selections[action_index])\n        else:\n            exploration_value = np.inf  # Encourage selection of unexplored actions\n        \n        # Factor in time-dependent adjustment for exploration\n        time_factor = (total_time_slots - current_time_slot) / total_time_slots\n        \n        # Combine scores for final action scoring\n        action_scores[action_index] = normalized_score + exploration_value * time_factor\n\n    # Determine the action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.80295944014074,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon = 0.1  # Exploration probability\n    decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n    \n    action_scores = np.zeros(num_actions)\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n\n        # Calculate average score\n        avg_score = np.mean(scores) if n > 0 else 0.0\n        \n        # Normalize the score based on total selections\n        normalized_score = avg_score * (n / total_selection_count) if total_selection_count > 0 else avg_score\n        \n        # UCB exploration bonus\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else np.inf\n        \n        # Combine scores with temporal decay\n        adjusted_score = normalized_score + exploration_bonus * decay_factor\n        action_scores[action_index] = adjusted_score\n    \n    # Epsilon-greedy action selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -449.66942110107783,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_scores = np.zeros(action_count)\n    action_selections = np.zeros(action_count)\n\n    # Calculate selection counts and average scores for each action\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_selections[action_index] = len(scores)\n        \n        if action_selections[action_index] > 0:\n            average_score = np.mean(scores)\n        else:\n            average_score = 0\n        \n        normalized_score = average_score * (action_selections[action_index] / total_selection_count) if total_selection_count > 0 else average_score\n        \n        # UCB with exploration factor\n        exploration_value = (np.log(total_selection_count + 1) / (action_selections[action_index] + 1)) ** 0.5 if action_selections[action_index] > 0 else np.inf\n        \n        # Incorporate a time-dependent decay factor for exploration\n        time_factor = (total_time_slots - current_time_slot) / total_time_slots\n        \n        # Final score calculation\n        action_scores[action_index] = normalized_score + exploration_value * time_factor\n\n    # Select and return the action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.6563444635571,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    epsilon = 0.1  # Exploration probability\n    total_actions = 8\n\n    for action_index in range(total_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)  # Number of times the action has been selected\n        avg_score = np.mean(scores) if n > 0 else 0.0\n        \n        # Normalize score based on total selections to ensure comparisons across actions\n        normalized_score = avg_score * (n / total_selection_count) if total_selection_count > 0 else avg_score\n        \n        # Exploration bonus using UCB\n        exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / (n + 1)) if n > 0 else np.inf\n        \n        # Time-decayed exploration\n        time_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = normalized_score + exploration_bonus * time_factor\n\n        action_scores.append(adjusted_score)\n\n    # Epsilon-greedy approach for exploration\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, total_actions)\n    else:\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -449.63155420317764,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    epsilon = 0.1  # Exploration factor\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Calculate the adjusted exploration factor\n        selection_count = n if n > 0 else 1  # Avoid division by zero\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_count)\n        \n        # Weighted score: balance exploitation and exploration\n        combined_score = avg_score + exploration_value\n        \n        action_scores.append(combined_score)\n\n    # Epsilon-greedy strategy for exploration\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(range(8))\n    else:\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -449.61899229455213,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    epsilon = 0.1  # Exploration probability\n\n    # Calculate scores and selection counts\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)  # Number of times the action has been selected\n        avg_score = np.mean(scores) if n > 0 else 0.0\n        \n        # Handle normalized score to favor less selected actions\n        normalized_score = avg_score if total_selection_count == 0 else avg_score * (n / total_selection_count)\n        \n        # UCB exploration bonus\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else np.inf\n        \n        # Time-based weighting\n        time_weight = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = normalized_score + exploration_bonus * time_weight\n\n        action_scores.append(adjusted_score)\n\n    # Epsilon-greedy strategy for action selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(8)\n    else:\n        action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -449.58834735647054,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    epsilon = 0.1  # Exploration probability\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)  # Number of times the action has been selected\n        avg_score = np.mean(scores) if n > 0 else 0.0\n        \n        # Normalize score based on total selections with a min count to prevent division by zero\n        normalized_score = avg_score * (n / total_selection_count) if total_selection_count > 0 else avg_score\n        \n        # Exploration bonus using modified UCB\n        exploration_bonus = (np.sqrt(np.log(total_selection_count + 1) / (n + 1)) \n                             if n > 0 else np.inf)\n        \n        # Decay time factor to prioritize early selections\n        time_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = normalized_score + exploration_bonus * time_factor\n        \n        action_scores.append(adjusted_score)\n\n    # Epsilon-greedy approach for exploration\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(8)\n    else:\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -449.56715550354056,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01)  # Decay exploration over time\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        \n        # Calculate average score and handle the case of zero selections\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Normalize score based on total selection count to handle bias due to lower selections\n        normalized_score = avg_score * (n / total_selection_count) if total_selection_count > 0 else avg_score\n        \n        # Exploration value to encourage trying less selected actions\n        exploration_value = epsilon * np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else epsilon\n        \n        # Calculate upper confidence bound value\n        ucb_value = normalized_score + exploration_value\n        \n        action_scores.append(ucb_value)\n\n    # Select the action with the maximum UCB value\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.5475673350219,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01)  # Decay exploration\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        \n        # Calculate average score\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Normalize score and handle division by zero\n        normalized_score = avg_score * (n / total_selection_count) if total_selection_count > 0 and n > 0 else avg_score\n        \n        # Exploration value to incentivize less selected actions\n        exploration_value = epsilon * np.sqrt((np.log(total_selection_count + 1) / (n + 1))) if n > 0 else epsilon\n        \n        # Calculate Upper Confidence Bound value\n        ucb_value = normalized_score + exploration_value\n        \n        action_scores.append(ucb_value)\n\n    # Select the action with the maximum UCB value\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.52672011924136,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    epsilon_decay = 1.0  # Initial exploration factor\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01)\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        \n        avg_score = np.mean(scores) if n > 0 else 0.0\n        normalized_score = avg_score * (n / total_selection_count) if total_selection_count > 0 else avg_score\n        \n        # Exploration term using dynamic epsilon\n        exploration_value = epsilon / (n + 1) if n > 0 else epsilon\n        \n        # Upper Confidence Bound (UCB)\n        ucb_value = normalized_score + exploration_value\n        \n        action_scores.append(ucb_value)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.4124170300765,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_scores = np.zeros(action_count)\n    action_selections = np.zeros(action_count)\n\n    # Calculate average scores and selection counts for each action\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_selections[action_index] = len(scores)\n        \n        total_score = np.sum(scores)\n        average_score = total_score / action_selections[action_index] if action_selections[action_index] > 0 else 0\n        \n        # UCB parameters\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (action_selections[action_index] + 1)) if action_selections[action_index] > 0 else np.inf\n        action_scores[action_index] = average_score + exploration_value\n\n    # Epsilon-greedy aspect for exploration\n    epsilon = 0.1\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(action_count)  # Random action for exploration\n    else:\n        action_index = np.argmax(action_scores)  # Best action based on calculated scores\n\n    return action_index",
          "objective": -449.35455221954913,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = 0.1  # Exploration factor\n    total_actions = len(score_set)\n\n    for action_index in range(total_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Calculate exploration term: UCB based\n        exploration_value = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else np.inf\n        \n        # Combined score: Balancing exploitation and exploration\n        combined_score = avg_score + exploration_value\n        \n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.25980085823863,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n\n        # Calculate average score with safeguard against zero division\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Epsilon-greedy exploration factor\n        epsilon = 0.1\n        exploration_factor = np.random.rand() < epsilon\n        \n        if exploration_factor:\n            # Random selection for exploration\n            action_scores[action_index] = np.random.rand()\n        else:\n            # Combined score: weighted average and exploration bonus\n            exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / (n + 1)) if n > 0 else float('inf')\n            time_decay = (1 - (current_time_slot / total_time_slots)) if total_time_slots > 0 else 0\n            \n            # Use a weighted score for the action\n            action_scores[action_index] = avg_score + time_decay * exploration_bonus\n\n    # Select the action with the highest score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -449.25495988004667,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    action_mean_scores = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate mean scores and counts for each action\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        if action_counts[action_index] > 0:\n            action_mean_scores[action_index] = np.mean(scores)\n\n    # Calculate exploration parameter\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5)) if total_selection_count > 0 else np.ones(num_actions)\n\n    # Combine scores\n    combined_scores = action_mean_scores + exploration_bonus\n\n    # Select action with the highest combined score\n    action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": -449.20114124735034,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    epsilon = 0.1  # Probability of exploration\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)  # Number of times the action has been selected\n        avg_score = np.mean(scores) if n > 0 else 0.0\n        \n        # Calculate normalized score\n        normalized_score = avg_score / (n + 1) if total_selection_count > 0 else avg_score\n        \n        # Confidence interval for exploration\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else np.inf\n        \n        # Dynamic time adjustment factor\n        time_factor = (total_time_slots - current_time_slot) / total_time_slots\n        \n        # Combined scoring metric\n        action_scores[action_index] = normalized_score + exploration_bonus * time_factor\n\n    # Epsilon-greedy strategy for action selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -449.1520112556055,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01)\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        \n        avg_score = np.mean(scores) if n > 0 else 0.0\n        normalized_score = avg_score / total_selection_count if total_selection_count > 0 else avg_score\n        \n        exploration_value = epsilon / (n + 1) if n > 0 else epsilon\n        upper_confidence_bound = normalized_score + exploration_value\n        \n        # Dynamic time factor for exploration\n        time_weight = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = upper_confidence_bound * time_weight\n        \n        action_scores.append(adjusted_score)\n\n    # Adding exploration component with uniform random noise\n    exploration_noise = np.random.rand(8) * (epsilon / 8)\n    action_scores = np.array(action_scores) + exploration_noise\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.1509777400197,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = 0.1\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0.0\n        \n        # Normalized score based on total selections\n        normalized_score = avg_score * (n / total_selection_count) if total_selection_count > 0 else avg_score\n        \n        # Exploration bonus using the UCB approach\n        exploration_bonus = (exploration_factor * np.sqrt(np.log(total_selection_count + 1) / (n + 1))) if n > 0 else np.inf\n        \n        # Decay factor for exploration based on time\n        time_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = normalized_score + exploration_bonus * time_factor\n        \n        action_scores.append(adjusted_score)\n\n    # Adding a random exploration component for better exploration\n    action_scores = np.array(action_scores) + np.random.rand(8) * exploration_factor\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.1258638661966,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_probability = 0.1  # Epsilon for exploration\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)  # Number of times the action has been selected\n        avg_score = np.mean(scores) if n > 0 else 0.0\n        \n        # Normalize score with respect to total selections\n        normalized_score = (avg_score * n / total_selection_count) if total_selection_count > 0 else avg_score\n        \n        # UCB exploration bonus\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else np.inf\n        \n        # Dynamic decay factor for exploration\n        time_decay = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = normalized_score + exploration_bonus * time_decay\n        \n        action_scores.append(adjusted_score)\n\n    # Epsilon-greedy for exploration vs exploitation\n    if np.random.rand() < exploration_probability:\n        action_index = np.random.randint(8)\n    else:\n        action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -449.0953573661246,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_scores = np.zeros(action_count)\n    action_selections = np.zeros(action_count)\n\n    # Calculate selection counts for each action\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_selections[action_index] = len(scores)\n\n        average_score = np.mean(scores) if action_selections[action_index] > 0 else 0\n        normalized_score = average_score * (action_selections[action_index] / total_selection_count) if total_selection_count > 0 else average_score\n        \n        # Calculate exploration value\n        exploration_value = (\n            (np.log(total_selection_count + 1) / (action_selections[action_index] + 1)) ** 0.5\n            * np.max(np.array([0.1 * (1 - current_time_slot / total_time_slots), 0.01]))\n            if action_selections[action_index] > 0 else 0\n        )\n        \n        action_scores[action_index] = normalized_score + exploration_value\n\n    # Select the action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -449.09129547435356,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration probability\n    action_scores = []\n    action_count = len(score_set)\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n\n        # Normalize exploration term \n        normalized_exploitation = avg_score * (n / total_selection_count) if total_selection_count > 0 else 0\n        \n        # Epsilon-greedy strategy\n        if np.random.rand() < epsilon:\n            action_scores.append(np.random.rand())  # Explore: random score\n        else:\n            # Calculate exploration term using UCB\n            exploration_value = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else np.sqrt(np.log(total_selection_count + 1))\n            # Incorporate time decay effect\n            time_decay = 1 - (current_time_slot / total_time_slots)\n            combined_score = normalized_exploitation * time_decay + exploration_value\n            action_scores.append(combined_score)\n\n    # Select action with highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -448.6412999790353,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    delta = 0.1  # Exploration factor\n    total_actions = 8\n\n    for action_index in range(total_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n\n        # Normalized exploitation score\n        normalized_exploitation = avg_score * (n / (total_selection_count + 1)) if total_selection_count > 0 else 0\n\n        # Exploration component based on the square root of count\n        exploration_value = delta * np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else delta * np.sqrt(np.log(total_selection_count + 1))\n\n        # Time sensitivity adjustment\n        time_weight = (current_time_slot + 1) / total_time_slots\n\n        # Combined score with time sensitivity\n        combined_score = (normalized_exploitation + exploration_value) * time_weight\n        action_scores.append(combined_score)\n\n    # Select action with the highest combined score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -448.37618807558,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon = 0.1  # Exploration probability\n    decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n    \n    action_scores = np.zeros(num_actions)\n    n_selections = np.zeros(num_actions)\n    \n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        n_selections[action_index] = n\n        avg_score = np.mean(scores) if n > 0 else 0.0\n        \n        # Normalized average score\n        normalized_score = avg_score * (n / total_selection_count) if total_selection_count > 0 else avg_score\n        \n        # UCB exploration bonus\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else np.inf\n        \n        # Combined score with decay\n        action_scores[action_index] = normalized_score + exploration_bonus * decay_factor\n\n    # Epsilon-greedy action selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -448.36688402825087,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_scores = np.zeros(action_count)\n    action_selections = np.zeros(action_count)\n\n    # Calculate average scores and selection counts for each action\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_count = len(scores)  # Count of selections\n        \n        # Calculate cumulative score for the action\n        total_score = np.sum(scores)\n        \n        # Calculate average score if selections are made\n        average_score = total_score / action_count if action_count > 0 else 0\n        \n        # Adjust exploration factor by number of selections\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (action_count + 1)) if action_count > 0 else 1\n        \n        # Time-sensitive adjustment to encourage earlier exploration\n        time_adjustment = 1 - (current_time_slot / total_time_slots) if total_time_slots > 0 else 1\n        \n        # Calculate total score for the action\n        action_scores[action_index] = average_score + (0.1 * exploration_value * time_adjustment)\n\n    # Epsilon-greedy strategy for exploration vs exploitation\n    epsilon = 0.1\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(action_count)  # Random action for exploration\n    else:\n        action_index = np.argmax(action_scores)  # Best action based on calculated scores\n\n    return action_index",
          "objective": -448.36504972476257,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01)  # Decay exploration\n    \n    # Iterate over each action to compute its score\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        \n        # Calculate average score, normalized by total selections\n        avg_score = np.mean(scores) if n > 0 else 0\n        normalized_score = avg_score * (n / total_selection_count) if total_selection_count > 0 and n > 0 else avg_score\n        \n        # Exploration term\n        exploration_value = epsilon * np.sqrt((np.log(total_selection_count + 1) / (n + 1))) if n > 0 else epsilon\n        \n        # Upper Confidence Bound calculation\n        ucb_value = normalized_score + exploration_value\n        \n        action_scores.append(ucb_value)\n\n    # Select the action with the maximum UCB value\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -448.3062519343302,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = 1.0  # Exploration weight\n    exploration_coefficient = 2.0  # Adjust the degree of exploration vs exploitation\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        \n        avg_score = np.mean(scores) if n > 0 else 0\n        exploration_value = (exploration_factor * np.sqrt(np.log(total_selection_count + 1) / (n + 1))) if n > 0 else exploration_factor * np.sqrt(np.log(total_selection_count + 1))\n        \n        combined_score = avg_score + exploration_value\n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -448.2165120443951,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    decay_factor = 0.9  # Weight for recent scores\n    exploration_factor = 1.0 - (current_time_slot / total_time_slots)  # Decay exploration\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n\n        # Average score calculation\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Temporal adjustment for decay\n        weighted_score = avg_score * (decay_factor ** (total_selection_count - n)) if total_selection_count > 0 else 0\n\n        # Exploration value\n        exploration_value = exploration_factor / (n + 1) if n > 0 else exploration_factor\n        \n        action_scores.append(weighted_score + exploration_value)\n    \n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -448.2030927179096,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = 0.1  # Base exploration factor\n    total_actions = 8\n\n    for action_index in range(total_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Normalized score\n        normalized_score = avg_score / (total_selection_count + 1e-5)  # Prevent division by zero\n        \n        # Exploration term: more exploration early, decreasing over time\n        exploration_value = exploration_factor / (n + 1) if n > 0 else exploration_factor\n        dynamic_exploration = (1 - current_time_slot / total_time_slots) * exploration_value\n        \n        # Combined score using Upper Confidence Bound (UCB) approach\n        upper_confidence_bound = normalized_score + dynamic_exploration\n        \n        action_scores.append(upper_confidence_bound)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -448.14151131984784,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n\n        avg_score = np.mean(scores) if n > 0 else 0\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) * exploration_factor if total_selection_count > 0 else 1.0\n        combined_score = avg_score + exploration_value\n\n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -448.1411702729095,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01)  # Epsilon decay\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        \n        # Calculate average score\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Handle total_selection_count to avoid division by zero\n        normalized_score = (avg_score * n / total_selection_count) if total_selection_count > 0 and n > 0 else avg_score\n\n        # Calculate exploration term using UCB\n        exploration_term = epsilon * np.sqrt((np.log(total_selection_count + 1) / (n + 1))) if n > 0 else epsilon\n        \n        # Bayesian estimate for confidence interval\n        beta_param_a = 1 + np.sum(scores)\n        beta_param_b = 1 + n - np.sum(scores)\n        bayesian_estimate = np.random.beta(beta_param_a, beta_param_b)\n\n        # Combine scores\n        combined_value = normalized_score + exploration_term + bayesian_estimate\n        action_scores.append(combined_value)\n\n    # Select the action with the maximum combined value\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -448.1318253014767,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    epsilon = 0.1  # Exploration probability\n\n    # Calculate average scores and selection counts\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)  # Number of times the action has been selected\n        avg_score = np.mean(scores) if n > 0 else 0.0\n        \n        # Normalize score based on total selections\n        normalized_score = avg_score * (n / total_selection_count) if total_selection_count > 0 else avg_score\n        \n        # UCB exploration bonus\n        exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / (n + 1)) if n > 0 else np.inf\n        \n        # Time decay factor\n        time_factor = (total_time_slots - current_time_slot) / total_time_slots\n        \n        # Adjusted score calculation\n        adjusted_score = normalized_score + exploration_bonus * time_factor\n        action_scores.append(adjusted_score)\n\n    # Epsilon-greedy strategy for action selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(8)\n    else:\n        action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -448.0780968897203,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    \n    # Exploration factor dynamic adjustment\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01)\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Normalized selection count\n        normalized_count = n / (total_selection_count + 1e-5)  # Avoid division by zero\n        exploration_value = epsilon * (1 - normalized_count)  # Adjust exploration based on utilization\n        \n        # Combined score: balancing exploitation and exploration\n        combined_score = avg_score + exploration_value\n        \n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -448.01882742857555,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_param = 1.0  # Exploration factor\n\n    # Dynamic epsilon based on time slot\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01)\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        \n        avg_score = np.mean(scores) if n > 0 else 0.0\n        normalized_score = avg_score * (n / total_selection_count) if total_selection_count > 0 else avg_score\n        \n        # Exploration term using dynamic epsilon\n        exploration_value = epsilon / (n + 1) if n > 0 else epsilon\n        \n        # Upper Confidence Bound (UCB) with adaptive exploration\n        ucb_value = normalized_score + exploration_param * exploration_value\n        \n        action_scores.append(ucb_value)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -447.9484767792105,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    \n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))  # Decay exploration factor\n    explore_prob = np.random.rand()\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Calculate exploration value\n        selection_count = n if n > 0 else 1  # Avoid division by zero\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / selection_count)\n\n        # Weighted score for exploration-exploitation\n        combined_score = avg_score + exploration_value\n\n        # Apply epsilon-greedy strategy\n        if explore_prob < epsilon:\n            action_scores.append(np.random.rand())  # Explore: random score\n        else:\n            action_scores.append(combined_score)  # Exploit: combined score\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -447.91404372480804,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = 1.0  # Exploration weight\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n\n        # Calculate average score and handle zero division\n        avg_score = np.mean(scores) if n > 0 else 0\n\n        # Adjust exploration based on the number of times the action has been selected\n        exploration_value = (exploration_factor * np.sqrt(np.log(total_selection_count + 1) / (n + 1))) if n > 0 else exploration_factor * np.sqrt(np.log(total_selection_count + 1))\n\n        # Combine scores for selection\n        combined_score = avg_score + exploration_value\n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -447.82662886022706,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = 0.1  # Base exploration rate\n    decay_rate = 0.99  # Decay rate for epsilon over time\n    min_epsilon = 0.01  # Minimum exploration rate\n\n    # Calculate the exploration rate (epsilon)\n    epsilon = max(min_epsilon, exploration_factor * (decay_rate ** current_time_slot))\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Normalize by total selections\n        normalized_score = avg_score / (total_selection_count + 1) if total_selection_count > 0 else 0\n        \n        # Exploration term based on action selection count\n        exploration_value = epsilon / (n + 1) if n > 0 else epsilon\n        \n        # Combine scores for action selection\n        combined_score = normalized_score + exploration_value\n        \n        action_scores.append(combined_score)\n\n    # Select action with the highest combined score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -447.6848673246216,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    epsilon = 0.1  # Exploration factor\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        mean_score = np.mean(scores) if n > 0 else 0\n\n        # Normalized selection count with Laplace smoothing\n        selection_count = n if n > 0 else 0\n        adjusted_count = selection_count + 1  # Avoid division by zero\n        exploration_factor = np.sqrt(np.log(total_selection_count + 1) / adjusted_count)\n\n        # Combining mean score and exploration factor\n        combined_score = mean_score + exploration_factor\n\n        action_scores.append(combined_score)\n\n    # Epsilon-greedy strategy for exploration\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, 8)\n    else:\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -447.64490997785947,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    decay_factor = (total_time_slots - current_time_slot) / total_time_slots  # Decay for exploration\n    epsilon = max(0.1 * decay_factor, 0.01)  # Minimum exploration value\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        \n        # Calculate average score; handle zero selections\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Normalize the average score using the total_selection_count\n        normalized_score = (avg_score * (n / total_selection_count)) if total_selection_count > 0 else avg_score\n        \n        # Exploration term with prevention of division by zero\n        exploration_value = epsilon / (n + 1) if n > 0 else epsilon\n        \n        # UCB calculation\n        ucb_value = normalized_score + exploration_value\n        \n        action_scores.append(ucb_value)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -447.61939269352666,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    \n    # Calculate exploration factor based on time slot\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01)  \n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n\n        # Calculate average score and handle cases with no selections\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Normalize average score\n        normalized_score = avg_score * (n / total_selection_count) if total_selection_count > 0 and n > 0 else avg_score\n\n        # Exploration adjustment to balance exploration and exploitation\n        exploration_factor = epsilon * np.sqrt((np.log(total_selection_count + 1) / (n + 1))) if n > 0 else epsilon\n        \n        # Calculate Bayesian estimate using Beta distribution\n        successes = np.sum(scores)  # total score as the count of \"successes\"\n        failures = n - successes  # number of trials minus successes\n        beta_param_a = 1 + successes\n        beta_param_b = 1 + failures\n        bayesian_estimate = np.random.beta(beta_param_a, beta_param_b)\n\n        # Combine normalized score, exploration factor, and Bayesian estimate\n        combined_value = normalized_score + exploration_factor + bayesian_estimate\n        \n        # Store calculated score for current action\n        action_scores.append(combined_value)\n\n    # Select the action with the highest combined score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -447.59795615446666,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01)\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        normalized_score = average_score * (selection_count / total_selection_count) if total_selection_count > 0 else average_score\n        exploration_value = epsilon * np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else epsilon\n        \n        # UCB calculation\n        ucb_value = normalized_score + exploration_value\n        action_scores.append(ucb_value)\n\n    # Select the action with the maximum UCB value\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -447.53923266758466,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01)  # Decay exploration\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n\n        # Calculate the average score for the action if it has been selected before\n        avg_score = np.mean(scores) if n > 0 else 0\n\n        # Normalize average score based on total selection count\n        normalized_score = avg_score * (n / total_selection_count) if total_selection_count > 0 and n > 0 else avg_score\n\n        # Calculate exploration value using a variant of UCB\n        if n > 0:\n            exploration_value = epsilon * np.sqrt(np.log(total_selection_count + 1) / (n + 1))\n        else:\n            exploration_value = epsilon  # Bonus for unselected actions\n\n        # UCB value calculation\n        ucb_value = normalized_score + exploration_value\n\n        # Append calculated UCB value\n        action_scores.append(ucb_value)\n\n    # Select the action with the highest UCB value\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -447.5079453027128,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01)\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        \n        avg_score = np.mean(scores) if n > 0 else 0\n        normalized_score = avg_score * (n / total_selection_count) if total_selection_count > 0 else avg_score\n        \n        exploration_value = epsilon / (n + 1) if n > 0 else epsilon\n        ucb_value = normalized_score + exploration_value\n        \n        action_scores.append(ucb_value)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -447.49984292328253,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_scores = np.zeros(action_count)\n    \n    # Epsilon for epsilon-greedy strategy\n    epsilon = 0.1\n\n    # Calculate selection counts and average scores for each action\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_selections = len(scores)\n\n        # Calculate average score with safety against division by zero\n        average_score = np.mean(scores) if action_selections > 0 else 0\n        \n        # Normalized score\n        normalized_score = average_score * (action_selections / total_selection_count) if total_selection_count > 0 else average_score\n        \n        # Calculate exploration bonus using UCB strategy\n        exploration_value = np.sqrt((np.log(total_selection_count + 1) / (action_selections + 1))) if action_selections > 0 else np.inf\n        \n        # Incorporate a time decay factor\n        time_factor = (total_time_slots - current_time_slot) / total_time_slots\n        \n        # Combine scores\n        action_scores[action_index] = normalized_score + exploration_value * time_factor\n        \n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(action_count)\n    else:\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -447.49314751796527,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_scores = np.zeros(action_count)\n    action_selections = np.zeros(action_count)\n\n    # Calculate selection counts and average scores for each action\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_selections[action_index] = len(scores)\n        \n        average_score = np.mean(scores) if action_selections[action_index] > 0 else 0\n        \n        # Normalize average score considering total selections\n        normalized_score = average_score * (action_selections[action_index] / total_selection_count) if total_selection_count > 0 else average_score\n\n        # Implementing UCB with exploration term\n        if action_selections[action_index] > 0:\n            exploration_value = np.sqrt(np.log(total_selection_count) / action_selections[action_index])\n        else:\n            exploration_value = np.inf  # Fully unexplored action\n        \n        # Time factor for balancing exploration\n        time_factor = 1 - (current_time_slot / total_time_slots)\n        \n        # Calculate final score for the action\n        action_scores[action_index] = normalized_score + exploration_value * time_factor * 0.5\n\n    # Select action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -447.39039066192976,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon = 0.1  # Exploration probability\n    alpha = 0.5    # Weight parameter for the time factor\n\n    action_scores = np.zeros(num_actions)\n    time_weight = (total_time_slots - current_time_slot) / total_time_slots\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        total_score = sum(scores)\n\n        # Average score calculation\n        avg_score = total_score / n if n > 0 else 0.0\n        \n        # Normalized score\n        normalized_score = avg_score * (n / total_selection_count) if total_selection_count > 0 else avg_score\n\n        # Exploration bonus using UCB\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else np.inf\n        \n        # Adjusted score with the time decay factor\n        adjusted_score = normalized_score + (exploration_bonus * (time_weight ** alpha))\n        \n        action_scores[action_index] = adjusted_score\n\n    # Epsilon-greedy strategy for exploration\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -447.3782907402582,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_scores = np.zeros(action_count)\n    action_selections = np.zeros(action_count)\n    \n    # Calculate average scores and counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_selections[action_index] = len(scores)\n        \n        # Calculate average score\n        total_score = np.sum(scores) if scores else 0\n        average_score = total_score / action_selections[action_index] if action_selections[action_index] > 0 else 0\n        \n        # UCB with exploration factor\n        if action_selections[action_index] > 0:\n            exploration_value = np.sqrt(np.log(total_selection_count + 1) / action_selections[action_index])\n        else:\n            exploration_value = float('inf')  # Prioritize unexplored actions\n        \n        # Scale exploration based on time slot\n        time_weight = 1 - (current_time_slot / total_time_slots) if total_time_slots > 0 else 1\n        \n        # Calculate final action score\n        action_scores[action_index] = average_score + (0.1 * exploration_value * time_weight)\n\n    # Select action with the highest score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -447.3388464386222,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = 1.0  # Base exploration weight\n    decay_factor = 0.95  # Decay factor for exploration over time\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        \n        avg_score = np.mean(scores) if n > 0 else 0\n        exploration_value = (exploration_factor * (np.sqrt(np.log(total_selection_count + 1) / (n + 1))) \n                             if n > 0 else exploration_factor * np.sqrt(np.log(total_selection_count + 1)))\n        \n        # Decay exploration based on current time slot\n        exploration_value *= (decay_factor ** (current_time_slot / total_time_slots))\n        \n        combined_score = avg_score + exploration_value\n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -447.1653964354311,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n\n        avg_score = np.mean(scores) if n > 0 else 0\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) * exploration_factor if total_selection_count > 0 else 1.0\n        \n        action_scores[action_index] = avg_score + exploration_value\n\n    # Apply epsilon-greedy strategy for exploration\n    epsilon = 0.1\n    if np.random.random() < epsilon:\n        action_index = np.random.choice(num_actions)\n    else:\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -447.15400700087366,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = 1.0\n    base_exploration = np.sqrt(np.log(total_selection_count + 1))\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n\n        avg_score = np.mean(scores) if n > 0 else 0\n        exploration_value = (exploration_factor * base_exploration / (n + 1)) if n > 0 else exploration_factor * base_exploration\n\n        combined_score = avg_score + exploration_value\n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -447.137248316561,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        \n        avg_score = np.mean(scores) if n > 0 else 0\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) * exploration_factor if total_selection_count > 0 else 1.0\n        action_scores[action_index] = avg_score + exploration_value\n\n    # Implementing Upper Confidence Bound (UCB)\n    action_index = np.argmax(action_scores)\n\n    # Epsilon-greedy strategy for exploration\n    epsilon = 0.1\n    if np.random.random() < epsilon:\n        action_index = np.random.choice(num_actions)\n\n    return action_index",
          "objective": -447.1321218735678,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_scores = np.zeros(action_count)\n    action_selections = np.zeros(action_count)\n\n    # Calculate selection counts and average scores for each action\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_selections[action_index] = len(scores)\n        \n        # Calculate the average score, accounting for no scores\n        average_score = np.mean(scores) if action_selections[action_index] > 0 else 0\n        \n        # Normalize the average score by total selection count\n        normalized_score = average_score * (action_selections[action_index] / total_selection_count) if total_selection_count > 0 else average_score\n        \n        # Calculate exploration term using UCB-like strategy\n        # We add a small constant to prevent division by zero when an action hasn't been selected\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (action_selections[action_index] + 1)) if action_selections[action_index] > 0 else np.inf\n        \n        # A time decay factor to shift focus from exploration to exploitation\n        time_factor = 1 - (current_time_slot / total_time_slots)\n        \n        # Update action scores\n        action_scores[action_index] = normalized_score + exploration_value * time_factor\n\n    # Select the action with the highest score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -447.0361240208731,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = 0.1 * (1 - current_time_slot / total_time_slots) + 0.01\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n        normalized_score = avg_score * (n / total_selection_count) if total_selection_count > 0 and n > 0 else avg_score\n\n        exploration_value = (\n            exploration_factor * np.sqrt(np.log(total_selection_count + 1) / (n + 1)) \n            if n > 0 else exploration_factor\n        )\n        \n        ucb_value = normalized_score + exploration_value\n        action_scores.append(ucb_value)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -446.90292116611295,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_scores = np.zeros(action_count)\n    action_selections = np.zeros(action_count)\n    \n    # Calculate selection counts and average scores for each action\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_selections[action_index] = len(scores)\n\n        # Calculate mean and avoid division by zero\n        average_score = np.mean(scores) if action_selections[action_index] > 0 else 0\n        \n        # Calculate normalized score to factor in overall selection count\n        normalized_score = average_score * (action_selections[action_index] / total_selection_count) if total_selection_count > 0 else average_score\n        \n        # Calculate exploration bonus using UCB strategy\n        if action_selections[action_index] > 0:\n            exploration_value = (np.log(total_selection_count) / action_selections[action_index]) ** 0.5\n        else:\n            exploration_value = np.inf  # Promote fully unexplored actions\n        \n        # Temporal consideration with decay for exploration\n        time_factor = (1 - (current_time_slot / total_time_slots)) ** 2  # Non-linear decay\n        \n        # Update action scores\n        action_scores[action_index] = normalized_score + exploration_value * time_factor\n\n    # Select the action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -446.7507968014288,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01)  # Decaying epsilon factor\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Use UCB for balancing exploration and exploitation\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (n + 1)) if n > 0 else np.inf\n        \n        # Combined score: Incorporating both exploration and exploitation\n        combined_score = avg_score + exploration_value * epsilon\n        \n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -446.73509307136885,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    average_scores = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n\n    # Calculate average scores and selection counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        # Calculate average score for each action\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n        else:\n            average_scores[action_index] = 0  # No score available if action has never been selected\n\n    # Adjust scores with exploration and time factor\n    scores_with_exploration = np.zeros(action_count)\n    for action_index in range(action_count):\n        # Normalize average scores based on total selections\n        normalized_score = average_scores[action_index]\n        \n        # Calculate exploration bonus using UCB\n        if selection_counts[action_index] > 0:\n            exploration_value = np.sqrt(np.log(total_selection_count) / selection_counts[action_index])\n        else:\n            exploration_value = np.inf  # Encourage selection of untried actions\n        \n        # Time-dependent factor to reduce exploration over time\n        time_factor = 1 - (current_time_slot / total_time_slots)\n        \n        # Combine normalized score with exploration value\n        scores_with_exploration[action_index] = normalized_score + exploration_value * time_factor\n\n    # Select action with the highest adjusted score\n    action_index = np.argmax(scores_with_exploration)\n    return action_index",
          "objective": -446.44914212783686,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01)\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n\n        # Calculate average score\n        avg_score = np.mean(scores) if n > 0 else 0\n\n        # Normalize score and handle division by zero\n        normalized_score = avg_score * (n / total_selection_count) if total_selection_count > 0 else avg_score\n\n        # Calculate exploration factor using UCB\n        exploration_factor = epsilon * np.sqrt((np.log(total_selection_count + 1) / (n + 1))) if n > 0 else epsilon\n\n        # Use Thompson Sampling with Beta distribution\n        successes = np.sum(scores)\n        failures = n - successes\n        alpha = 1 + successes\n        beta = 1 + failures\n        bayesian_estimate = np.random.beta(alpha, beta)\n\n        # Calculate combined action score\n        combined_value = normalized_score + exploration_factor + bayesian_estimate\n        action_scores.append(combined_value)\n\n    # Select the action with the maximum combined value\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -446.1684647553996,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon = 0.1  # Exploration probability\n    decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n\n    action_scores = []\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n\n        # Calculate average score\n        avg_score = np.mean(scores) if n > 0 else 0.0\n        \n        # Normalized score\n        normalized_score = avg_score * (n / total_selection_count) if total_selection_count > 0 else avg_score\n        \n        # UCB exploration bonus using adjusted n\n        exploration_bonus = np.sqrt((np.log(total_selection_count + 1) / (n + 1))) if n > 0 else np.inf\n        \n        # Combine scores with temporal decay\n        adjusted_score = normalized_score + exploration_bonus * decay_factor\n\n        action_scores.append(adjusted_score)\n\n    # Epsilon-greedy action selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -446.1661451489407,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_scores = np.zeros(action_count)\n    action_selections = np.zeros(action_count)\n\n    # Calculate scores and counts for each action\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_selections[action_index] = len(scores)\n\n        # Calculate average score for the action\n        total_score = np.sum(scores)\n        average_score = total_score / action_selections[action_index] if action_selections[action_index] > 0 else 0\n        \n        # Use a modified UCB approach for exploration\n        exploration_value = (np.sqrt(np.log(total_selection_count + 1) / (action_selections[action_index] + 1))\n                             if action_selections[action_index] > 0 else 1)  # Prevent division by zero\n        \n        # Scale exploration by time factor\n        time_adjustment = (1 - current_time_slot / total_time_slots) if total_time_slots > 0 else 1\n\n        # Combine average score and exploration value\n        action_scores[action_index] = average_score + (0.1 * exploration_value * time_adjustment)\n\n    # Select the action with the highest score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -446.16255524667446,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    epsilon = 0.1  # Exploration probability\n    decay_rate = 0.7  # Time decay influence\n\n    # Calculate average scores and adjusted scores\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)  # Number of times the action has been selected\n        avg_score = np.mean(scores) if n > 0 else 0.0\n\n        # Normalize score based on total selections\n        normalized_score = avg_score * (n / total_selection_count) if total_selection_count > 0 else avg_score\n\n        # UCB exploration bonus\n        exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1) / (n + 1))) if n > 0 else np.inf\n        \n        # Time decay factor\n        time_factor = (total_time_slots - current_time_slot) / total_time_slots\n        decay_factor = time_factor ** decay_rate\n        \n        # Combining scores\n        action_scores[action_index] = normalized_score + (exploration_bonus * decay_factor)\n\n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -446.0717527932714,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    n_actions_selected = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        n_actions_selected[action_index] = n\n\n        average_score = np.mean(scores) if n > 0 else 0\n        \n        if total_selection_count > 0:\n            exploration_factor = np.sqrt(np.log(total_selection_count) / (n + 1)) if n > 0 else float('inf')\n        else:\n            exploration_factor = float('inf')\n\n        progress_factor = (total_time_slots - current_time_slot) / total_time_slots if total_time_slots > 0 else 0\n\n        action_scores[action_index] = average_score + exploration_factor * progress_factor\n\n    # Epsilon-greedy approach for action selection\n    epsilon = 0.1\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(num_actions)  # Explore: choose randomly\n    else:\n        action_index = np.argmax(action_scores)  # Exploit: choose the best estimated action\n\n    return action_index",
          "objective": -445.97961010641035,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    average_scores = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n\n    # Calculate average scores and selection counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Avoid division by zero for total_selection_count\n    if total_selection_count == 0:\n        return np.random.randint(action_count)  # Random selection if no actions have been selected\n\n    # Calculate adjusted scores using UCB and exploration factor\n    scores_with_exploration = np.zeros(action_count)\n    for action_index in range(action_count):\n        normalized_score = average_scores[action_index] * (selection_counts[action_index] / total_selection_count)\n        \n        if selection_counts[action_index] > 0:\n            exploration_value = np.sqrt(np.log(total_selection_count) / selection_counts[action_index])\n        else:\n            exploration_value = np.inf  # Unselected actions have infinite exploration value\n        \n        # Time-dependent factor to encourage early exploration\n        time_factor = (1 - (current_time_slot / total_time_slots)) + 0.1  # Adding a small constant to avoid zero\n\n        scores_with_exploration[action_index] = normalized_score + exploration_value * time_factor\n\n    # Select the action with the highest score\n    action_index = np.argmax(scores_with_exploration)\n    return action_index",
          "objective": -445.88877291411654,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    epsilon = 0.1  # Epsilon for exploration\n    tau = 0.05  # Temperature for softmax selection\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)  # Number of times the action has been selected\n        \n        # Calculate normalized average score\n        avg_score = np.mean(scores) if n > 0 else 0.0\n        normalized_score = avg_score * (n / total_selection_count) if total_selection_count > 0 else avg_score\n\n        # Exploration bonus using Upper Confidence Bound\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else np.inf\n\n        # Decay exploration influence over time\n        time_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = normalized_score + exploration_bonus * time_factor\n\n        action_scores.append(adjusted_score)\n\n    # Softmax selection based on adjusted scores for better exploration\n    exp_scores = np.exp(np.array(action_scores) / tau)\n    probabilities = exp_scores / np.sum(exp_scores)\n    action_index = np.random.choice(range(8), p=probabilities)\n\n    return action_index",
          "objective": -445.8276088067297,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n    epsilon = 0.1  # Exploration rate for epsilon-greedy strategy\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n\n        avg_score = np.mean(scores) if n > 0 else 0\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) * exploration_factor if total_selection_count > 0 else 1.0\n        \n        # Epsilon-greedy approach\n        if np.random.rand() < epsilon:\n            combined_score = np.random.uniform(0, 1)  # Explore by randomly selecting a value\n        else:\n            combined_score = avg_score + exploration_value\n\n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -445.2617635996059,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    epsilon = 0.1  # Exploration probability\n    num_actions = 8\n    \n    # Pre-calculate constants for normalization and time decay\n    time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)  # Number of times the action has been selected\n        avg_score = np.mean(scores) if n > 0 else 0.0\n        \n        # Normalize score based on total selections\n        normalized_score = avg_score * (n / total_selection_count) if total_selection_count > 0 else avg_score\n        \n        # Upper Confidence Bound calculation\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else np.inf\n        \n        # Adjust the exploration bonus with time decay\n        adjusted_score = normalized_score + (exploration_bonus * time_decay_factor)\n        \n        action_scores.append(adjusted_score)\n\n    # Epsilon-greedy approach for exploration\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -445.19019316797255,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_scores = np.zeros(action_count)\n    action_selections = np.zeros(action_count)\n\n    # Calculate average scores and selection counts for each action\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_selections[action_index] = len(scores)\n        \n        total_score = np.sum(scores)\n        average_score = total_score / action_selections[action_index] if action_selections[action_index] > 0 else 0\n\n        # Calculate exploration value using Upper Confidence Bound (UCB)\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (action_selections[action_index] + 1)) if action_selections[action_index] > 0 else float('inf')\n        time_adjustment = (1 - current_time_slot / total_time_slots) if total_time_slots > 0 else 1\n        \n        # Score calculation combining exploration and average score\n        action_scores[action_index] = average_score + exploration_value * time_adjustment\n\n    # Epsilon-greedy approach for exploration\n    epsilon = 0.1\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(action_count)  # Random action for exploration\n    else:\n        action_index = np.argmax(action_scores)  # Best action based on calculated scores\n\n    return action_index",
          "objective": -445.0905673053285,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_exploitation = []\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        \n        avg_score = np.mean(scores) if n > 0 else 0\n        exploration_value = np.log(total_selection_count + 1) / (n + 1) if n > 0 else 1.0\n        \n        combined_score = avg_score + exploration_weight * exploration_value\n        action_exploitation.append(combined_score)\n    \n    action_index = np.argmax(action_exploitation)\n    \n    return action_index",
          "objective": -444.76249408422404,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    epsilon = 0.1  # Exploration factor\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        if selection_count > 0:\n            avg_score = np.mean(scores)\n        else:\n            avg_score = 0\n\n        # Explore with epsilon-greedy strategy\n        if np.random.rand() < epsilon:\n            action_scores.append(np.random.rand())  # Explore\n        else:\n            exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n            combined_score = avg_score + exploration_value\n            action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -444.7024320551366,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    # Calculate exploration factor based on the current time slot\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01)\n    \n    # Iterate through each action (0 to 7)\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        \n        # Calculate average score. Default to 0 if there are no scores.\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Normalize score; prevent division by zero\n        normalized_score = (avg_score * n / total_selection_count) if total_selection_count > 0 and n > 0 else avg_score\n        \n        # Calculate exploration value\n        if n > 0:\n            exploration_value = epsilon * np.sqrt(np.log(total_selection_count + 1) / (n + 1))\n        else:\n            exploration_value = epsilon  # Encourage exploration if an action hasn't been selected\n        \n        # Calculate the Upper Confidence Bound value\n        ucb_value = normalized_score + exploration_value\n        action_scores.append(ucb_value)\n\n    # Select the action with the maximum UCB value\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -444.4583063415046,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = 0.1  # Control the level of exploration\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        \n        avg_score = np.mean(scores) if n > 0 else 0\n        selection_count = n if n > 0 else 1  # Avoid division by zero\n        \n        # Exploration component\n        exploration_value = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / selection_count)\n        \n        # Update combined score for better exploitation-exploration balance\n        combined_score = avg_score + exploration_value\n        \n        # Temporal decay to give more weight to recent time slots\n        time_weight = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_weight\n        \n        action_scores.append(adjusted_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -444.32155791874584,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = 0.1\n    decay_rate = 0.95\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Normalizing the score\n        normalized_score = avg_score / (total_selection_count + 1e-5)  # Avoid division by zero\n        \n        # UCB combined with exploration\n        exploration_value = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / (n + 1e-5)) if n > 0 else exploration_factor\n        combined_score = normalized_score + exploration_value\n        \n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -444.14918573916054,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = np.zeros(8)\n    epsilon = 0.05  # Exploration probability (lowering for more exploitation)\n    exploration_constant = 2.0  # Constant for exploration bonus\n\n    # Iterate through each action index to compute scores\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)  # Number of times the action has been selected\n        avg_score = np.mean(scores) if n > 0 else 0.0\n\n        # Use the number of selections to normalize the score\n        normalized_score = avg_score * (n / total_selection_count) if total_selection_count > 0 else avg_score\n\n        # Exploration bonus based on UCB\n        exploration_bonus = exploration_constant * np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else np.inf\n\n        # Time decaying factor\n        time_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = normalized_score + (exploration_bonus * time_factor)\n\n        action_scores[action_index] = adjusted_score\n\n    # Epsilon-greedy strategy for action selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, 8)  # Explore\n    else:\n        action_index = np.argmax(action_scores)  # Exploit\n\n    return action_index",
          "objective": -443.78334886834057,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_scores = np.zeros(action_count)\n    action_selections = np.zeros(action_count)\n\n    # Calculate scores and counts for each action\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_selections[action_index] = len(scores)\n\n        # Calculate average score for the action\n        total_score = np.sum(scores)\n        average_score = total_score / action_selections[action_index] if action_selections[action_index] > 0 else 0\n\n        # Calculate exploration factor\n        exploration_factor = np.sqrt(np.log(total_selection_count + 1) / (action_selections[action_index] + 1) \n                                      if action_selections[action_index] > 0 else 1)\n\n        # Time-dependent scaling for exploration\n        time_factor = (1 - current_time_slot / total_time_slots) if total_time_slots > 0 else 1\n\n        # Combine average score with the exploration factor using a weighted sum\n        action_scores[action_index] = average_score + (0.1 * exploration_factor * time_factor)\n\n    # Select the action with the highest score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -443.53243938222835,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01)\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Normalized score calculation\n        normalized_score = (average_score * selection_count / total_selection_count) if total_selection_count > 0 else average_score\n        \n        # Exploration term\n        exploration_value = epsilon * np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else epsilon\n        \n        # UCB calculation\n        ucb_value = normalized_score + exploration_value\n        action_scores.append(ucb_value)\n\n    # Select the action with the maximum UCB value\n    return action_index",
          "objective": -443.1537420549478,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_scores = np.zeros(action_count)\n    action_selections = np.zeros(action_count)\n\n    # Calculate average scores and selection counts for each action\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_selections[action_index] = len(scores)\n        \n        total_score = np.sum(scores)\n        average_score = total_score / action_selections[action_index] if action_selections[action_index] > 0 else 0\n        \n        # Calculate exploration value using UCB approach\n        if action_selections[action_index] > 0:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count)) / action_selections[action_index])\n        else:\n            exploration_value = float('inf')  # Encourage exploration of unselected actions\n        \n        # Time-sensitive adjustment\n        time_adjustment = (1 - current_time_slot / total_time_slots) if total_time_slots > 0 else 1\n        \n        # Combine average score with exploration and time adjustment\n        action_scores[action_index] = average_score + (0.1 * exploration_value * time_adjustment)\n\n    # Epsilon-greedy strategy for additional exploration\n    epsilon = 0.1\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(action_count)  # Randomly select action for exploration\n    else:\n        action_index = np.argmax(action_scores)  # Select action with highest calculated score\n\n    return action_index",
          "objective": -443.1229548474306,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_scores = np.zeros(action_count)\n    \n    # Calculate average scores and selection counts for each action\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_selections = len(scores)\n        \n        average_score = np.mean(scores) if action_selections > 0 else 0\n        \n        # Calculate normalized score\n        normalized_score = average_score * (action_selections / total_selection_count) if total_selection_count > 0 else average_score\n        \n        # Calculate exploration bonus\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_selections + 1)) if action_selections > 0 else np.inf\n        \n        # Incorporate time-decay factor to encourage exploration over time\n        time_decay = (total_time_slots - current_time_slot) / total_time_slots\n        \n        # Update action scores combining normalized score, exploration bonus, and time decay\n        action_scores[action_index] = normalized_score + exploration_bonus * time_decay\n    \n    # Select the action with the highest score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -443.0959619665664,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Calculate effective selection count to avoid zero division\n        selection_count = n if n > 0 else 1\n        \n        # Exploration term: UCB-inspired (adjusting for a logarithmic factor)\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1) / selection_count))\n        \n        # Time decay factor to encourage exploration in earlier slots\n        time_decay = 1 - (current_time_slot / total_time_slots)\n        \n        # Combined score balancing exploitation, exploration, and time sensitivity\n        combined_score = avg_score + (exploration_value * time_decay)\n        \n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -442.84148145590785,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    average_scores = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n\n    # Compute average scores and selection counts for each action\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n        else:\n            average_scores[action_index] = 0\n\n    # Calculate exploration bonuses and adjusted scores using UCB\n    scores_with_exploration = np.zeros(action_count)\n    for action_index in range(action_count):\n        exploitation_score = average_scores[action_index] * (selection_counts[action_index] / (total_selection_count + 1e-5))\n        \n        if selection_counts[action_index] > 0:\n            exploration_value = np.sqrt(np.log(total_selection_count + 1) / selection_counts[action_index])\n        else:\n            exploration_value = np.inf\n        \n        # Time-dependent adjustment factor\n        time_factor = 1 - (current_time_slot / total_time_slots)\n        \n        # Total score computation\n        scores_with_exploration[action_index] = exploitation_score + (exploration_value * time_factor)\n\n    # Select the action with the highest score\n    action_index = np.argmax(scores_with_exploration)\n    return action_index",
          "objective": -442.4777858008419,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = 0.1  # Exploration factor\n    exploitation_weight = 0.9  # Weight of exploitation\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Normalize the selection count to avoid division by zero\n        normalized_selection_count = n / (total_selection_count + 1e-5)\n        \n        # Exploration term\n        exploration_value = exploration_factor * (1 - normalized_selection_count)\n        \n        # Combined score: balancing exploitation and exploration\n        combined_score = exploitation_weight * avg_score + exploration_value\n        \n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -442.3455183862605,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    epsilon_decay = 1.0  # Initial exploration factor\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01)\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n\n        avg_score = np.mean(scores) if n > 0 else 0.0\n        normalized_score = avg_score * (n / total_selection_count) if total_selection_count > 0 else avg_score\n        \n        exploration_value = epsilon / (n + 1) if n > 0 else epsilon\n        ucb_value = normalized_score + exploration_value\n\n        # Incorporate a decay factor to encourage exploration earlier in the process\n        time_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_ucb = ucb_value * time_factor\n        \n        action_scores.append(adjusted_ucb)\n\n    # Adding a small uniform random exploration component\n    action_scores = np.array(action_scores) + np.random.rand(8) * (epsilon / 8)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -442.31676883150936,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    epsilon = 0.1  # Exploration probability\n    exploration_weight = 1.0  # Weight for exploration\n    alpha = 0.5  # Time decay factor\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)  # Number of times the action has been selected\n        avg_score = np.mean(scores) if n > 0 else 0.0\n        \n        # Avoid division by zero in normalized score\n        normalized_score = avg_score * (n / total_selection_count) if total_selection_count > 0 else avg_score\n        \n        # Exploration bonus with respect to selection count\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else np.inf\n        \n        # Time decay factor\n        time_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = normalized_score + (exploration_weight * exploration_bonus * (time_factor ** alpha))\n        \n        action_scores[action_index] = adjusted_score\n\n    # Epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -442.29280178965564,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_exploitation = []\n    action_counts = [len(scores) for scores in score_set.values()]\n\n    for action_index in range(8):\n        if action_counts[action_index] > 0:\n            avg_score = np.mean(score_set[action_index])\n        else:\n            avg_score = 0\n        \n        action_exploitation.append(avg_score)\n    \n    if total_selection_count == 0:\n        return np.random.choice(range(8))  # Randomly select an action if no selections have been made yet\n\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n    exploration_scores = [(1 / (count + 1)) * exploration_factor for count in action_counts]\n\n    combined_scores = np.array(action_exploitation) + np.array(exploration_scores)\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -442.2871670245546,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n\n        avg_score = np.mean(scores) if n > 0 else 0\n        exploration_weight = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else float('inf')\n\n        time_adjustment = 1 - (current_time_slot / total_time_slots) if total_time_slots > 0 else 0\n        adjusted_exploration = time_adjustment * exploration_weight\n\n        action_scores[action_index] = avg_score + adjusted_exploration\n\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -442.0957779217065,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        \n        # Calculate average score safely\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Compute exploration weight based on selection frequency\n        exploration_weight = 1 / (n + 1) if n > 0 else 1\n        \n        # Temporal exploration factor that decreases over time\n        temporal_exploration_factor = 1 - (current_time_slot / total_time_slots) if total_time_slots > 0 else 0\n        \n        # Upper Confidence Bound (UCB) component\n        ucb_value = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else float('inf')\n        \n        # Combined score: average score plus exploration adjustment\n        action_scores[action_index] = avg_score + temporal_exploration_factor * ucb_value * exploration_weight\n\n    # Choose action with the highest score\n    action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -441.9533973881536,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n\n        # Calculate average score and handle division by zero\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Use exploration parameter: handle zero selections gracefully\n        exploration_weight = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else float('inf')\n\n        # Temporal adjustment factor\n        time_adjustment = 1 - (current_time_slot / total_time_slots) if total_time_slots > 0 else 0\n        \n        # Combined score with exploration\n        action_scores[action_index] = avg_score + time_adjustment * exploration_weight\n\n    # Return the index of the action with the highest score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -441.949225451644,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon = 0.1  # Exploration probability\n\n    # Calculate scores and selection counts\n    action_scores = []\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        num_times_selected = len(scores)\n        \n        # Calculate average score\n        avg_score = np.mean(scores) if num_times_selected > 0 else 0.0\n        \n        # Temporal decay factor\n        decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        \n        # Normalize the score based on total selections\n        normalized_score = avg_score * (num_times_selected / total_selection_count) if total_selection_count > 0 else avg_score\n        \n        # UCB exploration bonus\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (num_times_selected + 1)) if num_times_selected > 0 else np.inf\n        \n        # Adjusted score\n        adjusted_score = normalized_score + exploration_bonus * decay_factor\n        \n        action_scores.append(adjusted_score)\n\n    # Epsilon-greedy action selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -441.84499377797346,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_weight = 0.5  # Parameter to tune exploration vs exploitation\n    epsilon = 0.1  # Epsilon for epsilon-greedy approach\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Calculate exploration term\n        exploration_term = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if total_selection_count > 0 else 1.0\n        \n        # Compute combined score\n        combined_score = (exploration_weight * exploration_term) + ((1 - exploration_weight) * avg_score)\n        action_scores.append(combined_score)\n\n    # Epsilon-greedy exploration\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(range(8))\n    else:\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -441.74708007981286,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon = 0.1  # Exploration probability\n    alpha = 0.5    # Weight parameter for the time factor\n    action_scores = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)  # Number of times the action has been selected\n        \n        avg_score = np.mean(scores) if n > 0 else 0.0\n        normalized_score = avg_score * (n / total_selection_count) if total_selection_count > 0 else avg_score\n        \n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else np.inf\n        \n        time_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = normalized_score + (exploration_bonus * (time_factor ** alpha))\n        \n        action_scores[action_index] = adjusted_score\n\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -441.73291475782054,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    epsilon = 0.1  # Exploration rate\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Epsilon-greedy approach for exploration\n        exploration_value = (epsilon / (n + 1)) if n > 0 else epsilon\n        \n        combined_score = avg_score + exploration_value\n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    \n    # If all actions have never been selected, select randomly\n    if total_selection_count == 0:\n        action_index = np.random.randint(0, 8)\n\n    return action_index",
          "objective": -441.7306217693666,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = 0.1\n    epsilon = 0.1  # Epsilon for exploration\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0.0\n        \n        # Normalized score based on action selections\n        normalized_score = avg_score * (n / total_selection_count) if total_selection_count > 0 else avg_score\n        \n        # Exploration bonus using UCB approach\n        exploration_bonus = (exploration_factor * np.sqrt(np.log(total_selection_count + 1) / (n + 1))) if n > 0 else np.inf\n        \n        # Decay the exploration bonus based on the time left\n        time_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = normalized_score + exploration_bonus * time_factor\n        \n        action_scores.append(adjusted_score)\n\n    # Epsilon-greedy exploration mechanism\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, 8)  # Random action selected\n    else:\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -441.68478289137346,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_weight = 0.1  # Exploration factor\n    total_actions = 8\n    \n    for action_index in range(total_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        \n        # Average score calculation\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Confidence bound for UCB\n        if n > 0:\n            confidence_bound = np.sqrt((2 * np.log(total_selection_count + 1)) / n)\n        else:\n            confidence_bound = float('inf')  # Infinite confidence for unexplored actions\n        \n        # Combined score: UCB + exploration weight\n        combined_score = avg_score + confidence_bound * exploration_weight\n        \n        action_scores.append(combined_score)\n    \n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -441.6308185625747,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    epsilon = 0.1  # Exploration probability\n    exploration_weight = 1.5  # Increased weight for exploration\n    decay_factor = 0.9  # Weightage decay factor for time\n\n    # Calculate the average scores and adjusted action scores\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)  # Number of times the action has been selected\n        avg_score = np.mean(scores) if n > 0 else 0.0\n        \n        # Normalize score based on selection count\n        normalized_score = avg_score * (n / total_selection_count) if total_selection_count > 0 else avg_score\n\n        # Exploration bonus calculation\n        exploration_bonus = np.sqrt((np.log(total_selection_count + 1) / (n + 1))) if n > 0 else np.inf\n        \n        # Adjust score for time decay\n        time_factor = (total_time_slots - current_time_slot) / total_time_slots\n        decay_adjusted_score = normalized_score * (decay_factor ** (total_time_slots - current_time_slot))\n        \n        # Final adjusted score\n        action_scores[action_index] = decay_adjusted_score + (exploration_weight * exploration_bonus)\n\n    # Epsilon-greedy strategy for action selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -441.5475527527289,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    epsilon = 0.1  # Exploration probability\n    beta = 0.5     # Weight factor for time decay\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)  # Number of selections for this action\n        avg_score = np.mean(scores) if n > 0 else 0.0\n\n        # Normalize score\n        normalized_score = avg_score * (n / total_selection_count) if total_selection_count > 0 else avg_score\n        \n        # Exploration component using UCB\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else np.inf\n        \n        # Time-weighted factor to prioritize near-term actions\n        time_weight = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = normalized_score + (exploration_bonus * (time_weight ** beta))\n        \n        action_scores[action_index] = adjusted_score\n\n    # Epsilon-greedy action selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -441.11047131670585,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = 0.1  # Exploration factor\n    total_time_slots = max(total_time_slots, 1)  # Prevent division by zero\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n\n        # Normalization for total selections\n        normalized_exploitation = avg_score * (n / (total_selection_count + 1)) if total_selection_count > 0 else 0\n\n        # Exploration component\n        exploration_value = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else exploration_factor * np.sqrt(np.log(total_selection_count + 1))\n\n        # Temporal adjustment\n        time_weight = (current_time_slot + 1) / total_time_slots\n\n        # Combined score with a time factor\n        combined_score = (normalized_exploitation + exploration_value) * time_weight\n        action_scores.append(combined_score)\n\n    # Select action with the highest combined score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -439.77947318810914,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    num_actions = 8\n    epsilon = 0.1  # Exploration probability\n    decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        \n        # Calculate average score\n        avg_score = np.mean(scores) if n > 0 else 0.0\n        \n        # Normalize the score based on total selections\n        normalized_score = avg_score * (n / total_selection_count) if total_selection_count > 0 else avg_score\n        \n        # UCB exploration bonus\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else np.inf\n        \n        # Combine scores with temporal decay\n        adjusted_score = normalized_score + exploration_bonus * decay_factor\n        \n        action_scores.append(adjusted_score)\n\n    # Epsilon-greedy action selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -439.6460100259922,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots + 0.01  # added small constant to avoid division by zero\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        \n        avg_score = np.mean(scores) if n > 0 else 0\n        if total_selection_count > 0:\n            exploration_value = np.sqrt(np.log(total_selection_count) / (n + 1)) * exploration_factor\n        else:\n            exploration_value = 1.0\n            \n        combined_score = avg_score + exploration_value\n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -439.2992259444235,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_scores = np.zeros(action_count)\n    action_selections = np.zeros(action_count)\n    \n    # Calculate selection counts and average scores\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_selections[action_index] = len(scores)\n\n        average_score = np.mean(scores) if action_selections[action_index] > 0 else 0\n        normalized_score = (average_score * action_selections[action_index] / total_selection_count\n                            if total_selection_count > 0 else average_score)\n\n        # Calculate exploration value using Upper Confidence Bound\n        exploration_value = (np.sqrt(np.log(total_selection_count + 1) / (action_selections[action_index] + 1))\n                             * 0.1 * (1 - current_time_slot / total_time_slots)\n                             if action_selections[action_index] > 0 else 0)\n\n        action_scores[action_index] = normalized_score + exploration_value\n    \n    # Select the action with the highest score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -439.297756152274,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_exploitation = []\n    \n    # Calculate exploration factor based on remaining time slots\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n\n        avg_score = np.mean(scores) if n > 0 else 0\n        # If action has never been selected, consider a bias for exploration\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else np.inf\n        \n        # Combine exploitation and exploration values\n        combined_score = avg_score + exploration_factor * exploration_value\n        action_exploitation.append(combined_score)\n\n    # Select the action with the highest combined score\n    action_index = np.argmax(action_exploitation)\n    \n    return action_index",
          "objective": -439.04870128079,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    action_mean_scores = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n    \n    # Calculate mean scores and counts for each action\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        action_mean_scores[action_index] = np.mean(scores) if scores else 0\n\n    # Avoid division by zero\n    total_actions_selected = np.clip(action_counts, 1, None)\n    \n    # Calculate exploration bonus using UCB\n    exploration_bonus = np.sqrt(2 * np.log(total_selection_count + 1) / total_actions_selected)\n    \n    # Calculate decay factor based on the current time slot\n    time_decay_factor = 1 - (current_time_slot / total_time_slots)\n    \n    # Combine scores considering time decay to encourage timely performance\n    combined_scores = (action_mean_scores + exploration_bonus) * time_decay_factor\n\n    # Select the action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -439.0483905621504,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01)\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Normalize average score by total selections\n        normalized_score = average_score * (selection_count / total_selection_count) if total_selection_count > 0 else average_score\n        \n        # Calculation for exploration value\n        exploration_value = (\n            epsilon * np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) \n            if selection_count > 0 else epsilon\n        )\n        \n        # UCB calculation\n        ucb_value = normalized_score + exploration_value\n        action_scores.append(ucb_value)\n\n    # Select the action with the maximum UCB value\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -438.8642021746123,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon = 0.1  # Exploration probability\n    decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n    \n    action_scores = []\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        \n        # Calculate average score\n        avg_score = np.mean(scores) if n > 0 else 0.0\n        \n        # Normalize the score based on total selections\n        normalized_score = avg_score * (n / total_selection_count) if total_selection_count > 0 else avg_score\n        \n        # UCB exploration bonus\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else np.inf\n        \n        # Combine scores with temporal decay\n        adjusted_score = normalized_score + exploration_bonus * decay_factor\n        action_scores.append(adjusted_score)\n\n    # Epsilon-greedy action selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -438.85013035522496,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8  # Fixed number of actions\n    action_scores = np.zeros(action_count)\n    action_selection_count = np.zeros(action_count)\n\n    for action_index, scores in score_set.items():\n        action_selection_count[action_index] = len(scores)\n        if scores:\n            action_scores[action_index] = np.mean(scores)\n    \n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n    min_selection_count = 1e-5  # To avoid division by zero\n    exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / (action_selection_count + min_selection_count))\n\n    combined_scores = action_scores + exploration_bonus\n    action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": -438.7888743043917,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Use exploration factor based on selection counts\n        selection_count = n if n > 0 else 1  # Avoid division by zero\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_count)\n        \n        # Time decay factor to promote timely selection\n        time_decay = (total_time_slots - current_time_slot) / total_time_slots\n        \n        # Weighted score: balance exploitation and exploration with time decay\n        combined_score = avg_score + exploration_value * time_decay\n        \n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -438.548941704062,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = 0.1  # Base exploration rate\n    decay_rate = 0.99  # Decay rate for epsilon over time\n    min_epsilon = 0.01  # Minimum exploration rate\n    \n    # Calculate epsilon for exploration based on current time slot\n    epsilon = max(min_epsilon, exploration_factor * (decay_rate ** current_time_slot))\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Normalized score to account for total selections\n        normalized_score = avg_score / (total_selection_count + 1) if total_selection_count > 0 else 0\n        \n        # Calculate exploration term\n        exploration_value = epsilon / (n + 1) if n > 0 else epsilon\n        \n        # Combined score: balancing exploitation and exploration\n        combined_score = normalized_score + exploration_value\n        \n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -438.4765760666902,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_scores = np.zeros(action_count)\n    action_selections = np.zeros(action_count)\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_selections[action_index] = len(scores)\n        \n        total_score = np.sum(scores)\n        average_score = total_score / action_selections[action_index] if action_selections[action_index] > 0 else 0\n\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (action_selections[action_index] + 1)) if action_selections[action_index] > 0 else float('inf')\n        time_adjustment = (1 - current_time_slot / total_time_slots) if total_time_slots > 0 else 1\n        \n        action_scores[action_index] = average_score + (0.1 * exploration_value * time_adjustment)\n\n    epsilon = 0.1\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(action_count)\n    else:\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -438.4369176947628,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    action_scores = np.zeros(n_actions)\n    \n    exploration_weight = 1.0\n    exploitation_weight = 1.0\n    exploration_factor = 0.1\n    decay_rate = 0.99\n    min_epsilon = 0.01\n    \n    epsilon = max(min_epsilon, exploration_factor * (decay_rate ** current_time_slot))\n    \n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        n_selected = len(scores)\n        avg_score = np.mean(scores) if n_selected > 0 else 0\n        \n        normalized_score = avg_score / (total_selection_count + 1) if total_selection_count > 0 else 0\n        \n        exploration_value = exploration_weight * epsilon / (n_selected + 1) if n_selected > 0 else epsilon * exploration_weight\n        \n        combined_score = (exploitation_weight * normalized_score) + exploration_value\n        action_scores[action_index] = combined_score\n    \n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -438.0707361591237,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Calculate exploration term with a decay based on time slot\n        exploration_count = (total_selection_count + 1) / (n + 1) if n > 0 else (total_selection_count + 1)\n        time_factor = (total_time_slots - current_time_slot) / total_time_slots\n        \n        # Balance exploration and exploitation\n        combined_score = avg_score * (1 - time_factor) + exploration_count * time_factor\n        \n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -437.98597731073994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_scores = np.zeros(action_count)\n    action_selections = np.zeros(action_count)\n\n    # Calculate average scores and count of selections for each action\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_selections[action_index] = len(scores)\n        \n        total_score = np.sum(scores)\n        # Calculate average score, avoiding division by zero\n        average_score = total_score / action_selections[action_index] if action_selections[action_index] > 0 else 0\n        \n        # Calculate exploration factor using UCB approach\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (action_selections[action_index] + 1)) if action_selections[action_index] > 0 else np.inf\n        \n        # Time adjustment to favor actions earlier in the time frame\n        time_adjustment = (1 - current_time_slot / total_time_slots) if total_time_slots > 0 else 1\n        \n        # Score calculation combining average score and exploration bonus\n        action_scores[action_index] = average_score + (exploration_value * time_adjustment)\n\n    # Epsilon-greedy exploration strategy\n    epsilon = 0.1\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(action_count)  # Choose random action\n    else:\n        action_index = np.argmax(action_scores)  # Select action with highest score\n\n    return action_index",
          "objective": -437.9151304028336,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    total_actions = 8\n    delta = 0.1  # Exploration factor\n    epsilon = 0.1  # Exploration rate for epsilon-greedy\n\n    action_scores = []\n    \n    for action_index in range(total_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Calculate normalized exploitation score\n        normalized_exploitation = avg_score * (n / (total_selection_count + 1)) if total_selection_count > 0 else 0\n        \n        # Exploration value using UCB\n        exploration_value = delta * np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else delta * np.sqrt(np.log(total_selection_count + 1))\n        \n        # Time sensitivity adjustment\n        time_weight = (current_time_slot + 1) / total_time_slots\n        \n        # Combined score\n        combined_score = (normalized_exploitation + exploration_value) * time_weight\n        action_scores.append(combined_score)\n\n    # Epsilon-greedy strategy for exploration\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(total_actions)\n    else:\n        action_index = np.argmax(action_scores)\n        \n    return action_index",
          "objective": -437.78842263399565,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = len(score_set)\n    action_exploitation = np.zeros(action_count)\n    action_selection_count = np.zeros(action_count)\n\n    for action_index in range(action_count):\n        scores = score_set[action_index]\n        if scores:\n            avg_score = np.mean(scores)\n            action_exploitation[action_index] = avg_score\n            action_selection_count[action_index] = len(scores)\n\n    if total_selection_count == 0:\n        return np.random.randint(action_count)  # Random action on first run\n\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n    exploration_scores = np.zeros(action_count)\n\n    for action_index in range(action_count):\n        if action_selection_count[action_index] > 0:\n            exploration_scores[action_index] = (\n                exploration_factor / (action_selection_count[action_index] + 1)\n            )\n        else:\n            exploration_scores[action_index] = exploration_factor\n\n    combined_scores = action_exploitation + exploration_scores\n    action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": -436.62615956060114,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    num_actions = 8\n    epsilon = 0.1  # Exploration probability\n    time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)  # Number of times the action has been selected\n        \n        # Average score calculation\n        avg_score = np.mean(scores) if n > 0 else 0.0\n        \n        # Normalized score based on total selections\n        normalized_score = avg_score * (n / total_selection_count) if total_selection_count > 0 else avg_score\n        \n        # Exploration bonus based on UCB with time decay \n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else np.inf\n        \n        # Combine scores\n        adjusted_score = normalized_score + (exploration_bonus * time_decay_factor)\n        action_scores.append(adjusted_score)\n\n    # Epsilon-greedy approach for action selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -435.6950805018884,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    epsilon = 0.1  # Probability of exploration\n    beta = 0.1     # Exploration factor\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)  # Number of times the action has been selected\n        avg_score = np.mean(scores) if n > 0 else 0.0\n        \n        # Calculate normalized score to consider selection frequency\n        normalized_score = (avg_score * n / total_selection_count) if total_selection_count > 0 else avg_score\n        \n        # Exploration bonus using a simple decay function\n        exploration_bonus = beta * np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else np.inf\n        \n        # Time decay factor impacting future rewards\n        time_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = normalized_score + (exploration_bonus * time_factor)\n\n        action_scores[action_index] = adjusted_score\n\n    # Epsilon-greedy action selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -435.46109476756345,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration factor\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        selection_count = n if n > 0 else 1  # Avoid division by zero\n        exploration_value = np.sqrt(np.log(total_selection_count) / selection_count) if total_selection_count > 0 else 0\n        \n        # Combine average score with exploration term\n        combined_score = avg_score + exploration_value\n        \n        # Epsilon-greedy exploration\n        if np.random.rand() < epsilon:\n            combined_score += np.random.uniform(0, 1)  # Add random value for exploration\n\n        action_scores.append(combined_score)\n    \n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -435.4580582589432,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = np.zeros(8)\n    epsilon = 0.1  # Exploration probability\n    alpha = 0.5    # Weight parameter for the time factor\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)  # Number of times the action has been selected\n        avg_score = np.mean(scores) if n > 0 else 0.0\n        \n        # Normalize score based on total selections\n        normalized_score = avg_score * (n / total_selection_count) if total_selection_count > 0 else avg_score\n        \n        # Exploration bonus based on UCB\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else np.inf\n        \n        # Time decaying factor\n        time_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = normalized_score + (exploration_bonus * time_factor ** alpha)\n        \n        action_scores[action_index] = adjusted_score\n\n    # Epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(8)\n    else:\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -434.9439040975022,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = 0.1  # Exploration factor\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Normalized selection count for UCB\n        selection_count = total_selection_count if total_selection_count > 0 else 1\n        exploration_term = np.sqrt(np.log(selection_count) / (n + 1)) if n > 0 else 1\n        \n        # Combined score using UCB\n        combined_score = avg_score + exploration_factor * exploration_term\n        \n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -434.93969070127804,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01)\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score with proper handling of selection count\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Normalize the score based on total selections\n        normalized_score = average_score * (selection_count / total_selection_count) if total_selection_count > 0 else average_score\n        \n        # Exploration term for UCB\n        exploration_value = epsilon * np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else epsilon\n        \n        # Calculate UCB value\n        ucb_value = normalized_score + exploration_value\n        action_scores.append(ucb_value)\n\n    # Select the action with the maximum UCB value\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -434.0415441823894,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n\n        # Calculate average score, handle division by zero\n        avg_score = np.mean(scores) if n > 0 else 0\n\n        # Exploration term with epsilon-greedy strategy\n        epsilon = 0.1  # Exploration rate\n        exploration_weight = epsilon / (n + 1) if n > 0 else float('inf')\n\n        # Temporal adjustment factor\n        time_adjustment = 1 - (current_time_slot / total_time_slots) if total_time_slots > 0 else 0\n        \n        # Final combined score\n        action_scores[action_index] = avg_score + (time_adjustment * exploration_weight)\n    \n    # Select action with the highest score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -433.66438304282246,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_weight = 1.0 / (current_time_slot + 1)  # Diminishing exploration factor\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        \n        avg_score = np.mean(scores) if n > 0 else 0.0\n        normalized_selection_count = total_selection_count if total_selection_count > 0 else 1\n        \n        # Calculating the exploitation term\n        action_value = avg_score * (n / normalized_selection_count) if n > 0 else 0.0\n        \n        # Calculating the exploration term using UCB\n        exploration_value = exploration_weight * np.sqrt(np.log(normalized_selection_count + 1) / (n + 1)) if n > 0 else exploration_weight\n        \n        combined_score = action_value + exploration_value\n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -432.7134670247622,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    epsilon = 0.1  # Exploration factor\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Calculate exploration term with epsilon-greedy strategy\n        exploration_value = (epsilon / (n + 1)) if n > 0 else epsilon\n        \n        # Combined score: balancing exploitation and exploration\n        combined_score = avg_score + exploration_value\n        \n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -432.379254964203,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_scores = np.zeros(action_count)\n    \n    # Initialize parameters for exploration-exploitation balance\n    epsilon = 0.1  # Epsilon value for exploration\n    exploration_factor = np.sqrt(np.log(total_selection_count + 1) / (np.array([len(scores) for scores in score_set.values()]) + 1))\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_count_for_action = len(scores)\n        \n        # Calculate the average score and normalize\n        average_score = np.mean(scores) if action_count_for_action > 0 else 0\n        normalized_score = average_score * (action_count_for_action / total_selection_count) if total_selection_count > 0 else average_score\n        \n        # Temporal context scaling\n        temporal_adjustment = 1 - (current_time_slot / total_time_slots)\n        adjusted_exploration_value = exploration_factor[action_index] * (0.1 * temporal_adjustment + 0.01)\n        \n        # Combined scoring\n        action_scores[action_index] = normalized_score + adjusted_exploration_value\n    \n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:  # Explore with a chance of epsilon\n        action_index = np.random.randint(action_count)\n    else:  # Exploit the best known option\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -432.2317255352851,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    delta = 0.1  # Exploration factor\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n\n        # Adjust for total selections to prevent division by zero and normalize scores\n        normalized_exploitation = avg_score * (n / (total_selection_count + 1)) if total_selection_count > 0 else 0\n\n        # Exploration component\n        exploration_value = delta * np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else delta * np.sqrt(np.log(total_selection_count + 1))\n\n        # Combined score\n        combined_score = normalized_exploitation + exploration_value\n        action_scores.append(combined_score)\n\n    # Select action with highest combined score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -432.13466054249574,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    epsilon = 0.1  # Exploration probability\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)  # Number of times the action has been selected\n        avg_score = np.mean(scores) if n > 0 else 0.0\n        \n        # Normalize score based on total selections\n        normalized_score = avg_score * (n / total_selection_count) if total_selection_count > 0 else avg_score\n        \n        # UCB exploration bonus\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else np.inf\n        \n        # Time decay factor\n        time_factor = (total_time_slots - current_time_slot) / total_time_slots\n        \n        # Adjusted score calculation\n        adjusted_score = normalized_score + exploration_bonus * time_factor\n        action_scores.append(adjusted_score)\n\n    # Epsilon-greedy strategy for action selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(8)\n    else:\n        action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -431.6352925238973,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Enhanced exploration factor based on time slots\n        time_based_factor = (total_time_slots - current_time_slot) / total_time_slots\n        \n        # Calculate exploration based on historical selection count\n        selection_count = n if n > 0 else 1  # Avoid division by zero\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / selection_count)\n        \n        # Weighted score: balance exploitation, exploration, and time consideration\n        combined_score = avg_score + exploration_value * time_based_factor\n        \n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -431.4149285826572,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    \n    # Calculate average scores and counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n        action_scores[action_index] = avg_score\n    \n    # Normalization factor to balance exploration\n    exploration_factor = 1.0 / (1 + (total_selection_count / (total_time_slots * 2)))\n    \n    # Upper Confidence Bound calculation\n    for action_index in range(num_actions):\n        n = len(score_set.get(action_index, []))\n        confidence_term = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else 1.0\n        action_scores[action_index] += exploration_factor * confidence_term\n\n    # Epsilon-greedy strategy for exploration\n    epsilon = 0.1\n    if np.random.random() < epsilon:\n        action_index = np.random.choice(num_actions)\n    else:\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -430.8613893727783,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    total_actions = 8\n    action_scores = np.zeros(total_actions)\n    epsilon = 0.1  # Exploration probability\n    \n    # Calculate average scores and selection counts\n    for action_index in range(total_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Normalized exploitation score\n        normalized_exploitation = avg_score * (n / (total_selection_count + 1)) if total_selection_count > 0 else 0\n        \n        # Exploration using UCB formula\n        exploration_value = np.sqrt((np.log(total_selection_count + 1) / (n + 1))) if n > 0 else np.inf\n        \n        # Combined score\n        action_scores[action_index] = normalized_exploitation + exploration_value\n        \n        # Time decay factor\n        time_decay = (total_time_slots - current_time_slot) / total_time_slots\n        action_scores[action_index] *= time_decay\n    \n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(total_actions)  # Random action for exploration\n    else:\n        action_index = np.argmax(action_scores)  # Best action based on adjusted scores\n    \n    return action_index",
          "objective": -430.4080886805173,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01)\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        \n        # Calculate average score\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Normalize score\n        normalized_score = avg_score * (n / total_selection_count) if total_selection_count > 0 and n > 0 else avg_score\n        \n        # Exploration term\n        exploration_value = epsilon * np.sqrt((np.log(total_selection_count + 1) / (n + 1))) if n > 0 else epsilon\n        \n        # Compute UCB\n        ucb_value = normalized_score + exploration_value\n        \n        action_scores.append(ucb_value)\n\n    # Select the action with the maximum UCB value\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -430.3845204456194,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    # Decay factor to prioritize recent scores\n    decay_factor = 0.9  # Adjust this value as needed\n\n    # Calculate the exploration probability\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01)\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        \n        # Calculate average score with a decay on historical scores\n        if n > 0:\n            weighted_scores = [scores[i] * (decay_factor ** (n - i - 1)) for i in range(n)]\n            avg_score = np.sum(weighted_scores) / np.sum([decay_factor ** (n - i - 1) for i in range(n)])\n        else:\n            avg_score = 0\n        \n        # Normalize score based on total selection count\n        normalized_score = avg_score * (n / total_selection_count) if total_selection_count > 0 else avg_score\n        \n        # Exploration term with adjusted epsilon\n        exploration_value = epsilon * np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else epsilon\n        \n        # Upper Confidence Bound calculation\n        ucb_value = normalized_score + exploration_value\n        \n        action_scores.append(ucb_value)\n\n    # Select the action with the maximum UCB value\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -430.319911635666,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    action_mean_scores = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n    \n    # Calculate mean scores and counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_mean_scores[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Compute exploration factor (UCB variant)\n    exploration_bonus = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        if action_counts[action_index] > 0:\n            exploration_bonus[action_index] = np.sqrt((2 * np.log(total_selection_count)) / action_counts[action_index])\n        else:\n            exploration_bonus[action_index] = np.inf  # Encourage underexplored actions in the first rounds\n\n    # Combine scores using Upper Confidence Bound (UCB)\n    combined_scores = action_mean_scores + exploration_bonus\n\n    # Select the action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -429.94063089390085,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon = 0.1  # Exploration probability\n    decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n\n    action_scores = np.zeros(num_actions)\n    counts = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        \n        # Count of selections for the action\n        counts[action_index] = n\n        \n        # Calculate average score or fallback to zero\n        avg_score = np.mean(scores) if n > 0 else 0.0\n        \n        # Normalize the average score based on total selections\n        normalized_score = avg_score * (n / total_selection_count) if total_selection_count > 0 else avg_score\n        \n        # UCB exploration bonus or an infinitely high value if unselected\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else np.inf\n        \n        # Combining scores with temporal decay\n        action_scores[action_index] = normalized_score + exploration_bonus * decay_factor\n\n    # Epsilon-greedy action selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -429.6277253484949,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = 0.1  # Exploration factor\n    total_actions = len(score_set)  # Number of actions (expected to be 8)\n\n    for action_index in range(total_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Avoid division by zero and calculate ucb\n        if total_selection_count > 0:\n            exploration_value = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / (n + 1))\n        else:\n            exploration_value = exploration_factor\n\n        # Combined score: exploitation + exploration\n        combined_score = avg_score + exploration_value\n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -429.16754290537824,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = 0.1  # Epsilon for exploration\n    decay_rate = 0.95  # Decay rate for epsilon\n    epsilon = max(exploration_factor * (decay_rate ** current_time_slot), 0.01)  # Ensure a minimum level of exploration\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        \n        # Calculate average score and handle zero selection cases\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Normalize average score\n        normalized_score = avg_score / (total_selection_count + 1) if total_selection_count > 0 else 0\n        \n        # Upper Confidence Bound (UCB) component\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else float('inf')\n        \n        # Final action score\n        action_score = normalized_score + exploration_value\n        \n        action_scores.append(action_score)\n\n    # Epsilon-greedy selection\n    if np.random.random() < epsilon:\n        action_index = np.random.choice(range(8))\n    else:\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -428.7982671574117,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n\n        avg_score = np.mean(scores) if n > 0 else 0\n        exploration_weight = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else float('inf')\n        \n        time_adjustment = 1 - (current_time_slot / total_time_slots) if total_time_slots > 0 else 0\n        \n        # Dynamic adjustment combining exploitation and exploration\n        action_scores[action_index] = avg_score + time_adjustment * exploration_weight\n\n    # Select the action with the highest calculated score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -428.3160348107056,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_factor = 0.1  # Factor for exploration\n    action_scores = []\n    action_count = len(score_set)\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n\n        # Normalize exploitation term based on total selection count\n        if total_selection_count > 0:\n            normalized_exploitation = avg_score * (n / total_selection_count)\n        else:\n            normalized_exploitation = 0\n        \n        # Exploration term with UCB approach\n        exploration_value = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else exploration_factor * np.sqrt(np.log(total_selection_count + 1))\n        \n        # Consider time decay in scores (optional improvement)\n        time_decay = 1 - (current_time_slot / total_time_slots)\n        adjusted_score = normalized_exploitation * time_decay\n        \n        # Combined score\n        combined_score = adjusted_score + exploration_value\n        action_scores.append(combined_score)\n\n    # Select action with highest combined score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -427.76252227800717,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    epsilon = 0.1  # Exploration probability\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)  # Number of times the action has been selected\n        avg_score = np.mean(scores) if n > 0 else 0.0\n        \n        # Normalize average score regarding total selections\n        normalized_score = avg_score * (n / total_selection_count) if total_selection_count > 0 else 0.0\n        \n        # Exploration bonus using UCB formula\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else np.inf\n        \n        # Adjusting exploration factor based on time remaining\n        time_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = normalized_score + exploration_bonus * time_factor\n        \n        action_scores.append(adjusted_score)\n\n    # Epsilon-greedy decision\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(8)\n    else:\n        action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -427.5680395900509,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = 0.1  # Exploration factor\n    action_count = len(score_set)\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Calculate UCB exploration term\n        if n > 0:\n            exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / n)\n        else:\n            exploration_value = float('inf')  # Explore if action has never been selected\n            \n        # Combined score: UCB-based approach\n        combined_score = avg_score + exploration_factor * exploration_value\n        \n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -427.3515146332706,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    epsilon = 0.1  # Exploration probability\n    delta = 0.5    # Time decay parameter\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)  # Number of times the action has been selected\n        avg_score = np.mean(scores) if n > 0 else 0.0\n        \n        # Adjusted average score considering selection count\n        adjusted_score = avg_score * (n / total_selection_count) if total_selection_count > 0 else avg_score\n        \n        # Exploration term using Upper Confidence Bound\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else np.inf\n        \n        # Time weighting\n        time_weight = (total_time_slots - current_time_slot) / total_time_slots\n        action_scores[action_index] = adjusted_score + (exploration_bonus * (time_weight ** delta))\n\n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -424.38226104187464,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n\n        # Calculate the average score\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Variance calculation to assess certainty\n        variance = np.var(scores) if n > 1 else 0\n        exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / (n + 1)) if n > 0 else float('inf')\n        \n        # Temporal decay to prioritize earlier actions\n        time_adjustment = 1 - (current_time_slot / total_time_slots) if total_time_slots > 0 else 0\n\n        # Combined score with exploration and uncertainty\n        action_scores[action_index] = avg_score + exploration_bonus + time_adjustment * (1.0 - variance)\n    \n    # Return the index of the action with the highest score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -424.18550786284806,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    decay_factor = 0.9  # Decay factor for temporal relevance\n    epsilon = 0.1  # Exploration factor\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        \n        # Calculate the average score, applying decay\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Apply temporal decay to the average score based on the current time slot\n        time_decay = decay_factor ** (total_time_slots - current_time_slot)\n        adjusted_score = avg_score * time_decay\n        \n        # Calculate exploration term using a modified epsilon-greedy strategy\n        exploration_value = (epsilon / (n + 1)) if n > 0 else epsilon\n        \n        # Calculate combined score\n        combined_score = adjusted_score + exploration_value\n        \n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -422.59934117852856,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    total_actions = 8\n    action_scores = []\n    epsilon = 0.1  # Exploration probability\n\n    for action_index in range(total_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Calculate normalized exploitation\n        normalized_exploitation = avg_score * (n / (total_selection_count + 1)) if total_selection_count > 0 else 0\n        \n        # Upper Confidence Bound (UCB) calculation for exploration\n        exploration_term = np.sqrt((2 * np.log(total_selection_count + 1)) / (n + 1)) if n > 0 else 1\n        \n        # Combine normalized exploitation and exploration term\n        combined_score = normalized_exploitation + exploration_term\n        \n        # Temporal influence with a decay factor\n        time_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_factor\n        \n        action_scores.append(adjusted_score)\n\n    # Epsilon-greedy algorithm for action selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(total_actions)\n    else:\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -422.0797200909522,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)  # Number of times the action has been selected\n        \n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Calculate exploration factor using UCB approach\n        selection_count = n if n > 0 else 1  # Avoid division by zero\n        exploration_value = np.sqrt(2 * np.log(total_selection_count + 1) / selection_count)\n        \n        # Weighted score: balance exploitation (avg_score) and exploration (exploration_value)\n        combined_score = avg_score + exploration_value\n        \n        action_scores.append(combined_score)\n\n    # Select action with the highest combined score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -419.429177065042,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = 1.0  # Base exploration factor\n    epsilon_decay = 1.0 - (current_time_slot / total_time_slots)\n    epsilon = max(0.1 * epsilon_decay, 0.01)  # Decay exploration over time\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        \n        # Calculate normalized score\n        avg_score = np.mean(scores) if n > 0 else 0\n        normalized_score = avg_score * (n / total_selection_count) if total_selection_count > 0 else avg_score\n        \n        # Exploration term\n        exploration_value = epsilon * (1 / (n + 1)) if n > 0 else epsilon\n        \n        # Combine normalized score and exploration\n        ucb_value = normalized_score + exploration_value\n        \n        action_scores.append(ucb_value)\n\n    action_index = np.argmax(action_scores)  # Choose action with highest UCB value\n    return action_index",
          "objective": -418.921303060726,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = 0.1  # Exploration probability\n    decay_rate = 0.9           # Decay factor for exploration\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)  # Number of times the action has been selected\n        avg_score = np.mean(scores) if n > 0 else 0.0\n        \n        # Normalize score based on total selections\n        normalized_score = avg_score * (n / total_selection_count) if total_selection_count > 0 else avg_score\n        \n        # Exploration bonus based on UCB\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else np.inf\n        \n        # Dynamic exploration decay\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_exploration = exploration_factor * (decay_rate ** (total_time_slots - current_time_slot))\n        \n        # Final score calculation\n        adjusted_score = normalized_score + exploration_bonus * time_decay_factor * adjusted_exploration\n        action_scores.append(adjusted_score)\n\n    # Epsilon-greedy approach for balancing exploration and exploitation\n    if np.random.rand() < exploration_factor:\n        action_index = np.random.randint(8)\n    else:\n        action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -417.55793136654773,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    epsilon = 0.1  # Epsilon for exploration probability\n\n    # Function to calculate the average score and adjusted score\n    def calculate_scores(action_index):\n        scores = score_set.get(action_index, [])\n        n = len(scores)  # Number of times the action has been selected\n        avg_score = np.mean(scores) if n > 0 else 0.0\n        \n        # Normalize the average score based on selections\n        normalized_score = avg_score * (n / total_selection_count) if total_selection_count > 0 else avg_score\n\n        # Exploration bonus based on UCB\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else np.inf\n        \n        # Time factor for decay of exploration bonus\n        time_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = normalized_score + (exploration_bonus * time_factor)\n        \n        return adjusted_score\n\n    # Compute scores for all actions\n    for action_index in range(8):\n        action_scores.append(calculate_scores(action_index))\n\n    # Epsilon-greedy exploration strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(8)\n    else:\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -417.2132398714311,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    n_actions_selected = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        n_actions_selected[action_index] = n\n\n        average_score = np.mean(scores) if n > 0 else 0\n        \n        if total_selection_count > 0:\n            exploration_factor = np.sqrt(np.log(total_selection_count) / (n + 1)) if n > 0 else float('inf')\n        else:\n            exploration_factor = float('inf')\n\n        progress_factor = (total_time_slots - current_time_slot) / total_time_slots if total_time_slots > 0 else 0\n\n        action_scores[action_index] = average_score + exploration_factor * progress_factor\n\n    action_probs = np.exp(action_scores - np.max(action_scores))\n    action_probs /= np.sum(action_probs)\n\n    action_index = np.random.choice(num_actions, p=action_probs)\n\n    return action_index",
          "objective": -416.7418184685332,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    epsilon = 0.05  # Lower exploration probability to encourage exploitation\n    exploration_factor = 1.5  # Scale exploration bonus\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)  # Number of times the action has been selected\n        avg_score = np.mean(scores) if n > 0 else 0.0\n        \n        # Compute normalized score\n        normalized_score = avg_score * (n / total_selection_count) if total_selection_count > 0 else avg_score\n        \n        # UCB exploration bonus\n        exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else np.inf\n        \n        # Time decay factor (encourages earlier actions)\n        time_factor = (total_time_slots - current_time_slot) / total_time_slots\n        \n        # Adjusted score calculation\n        adjusted_score = normalized_score + exploration_bonus * time_factor\n        action_scores.append(adjusted_score)\n\n    # Epsilon-greedy strategy with improved exploration\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(8)\n    else:\n        action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -416.5544804802017,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n\n        # Calculate average score safely\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Dynamic exploration factor based on time\n        exploration_factor = (1 - (current_time_slot / total_time_slots)) if total_time_slots > 0 else 0\n        \n        # UCB calculation (handling division by zero)\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else float('inf')\n\n        # Combined score: average score plus adjusted exploration value\n        action_scores[action_index] = avg_score + exploration_factor * exploration_value\n\n    # Selecting the action index with the highest evaluated score\n    action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -415.83600375947003,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = 0.1  # Exploration term factor\n    epsilon_decay = 0.5  # Decay factor for exploration over time\n    min_epsilon = 0.01\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        \n        avg_score = np.mean(scores) if n > 0 else 0.0\n        normalized_score = avg_score * (n / total_selection_count) if total_selection_count > 0 else avg_score\n        \n        # Adaptive exploration term\n        epsilon = max(min_epsilon, exploration_factor * (1 - current_time_slot / total_time_slots))\n        exploration_value = epsilon / (n + 1) if n > 0 else epsilon\n\n        # Upper Confidence Bound (UCB)\n        ucb_value = normalized_score + exploration_value\n        \n        action_scores.append(ucb_value)\n    \n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -415.3955147378834,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8  # Since action indices range from 0 to 7\n    action_exploitation = np.zeros(num_actions)\n    action_selection_count = np.zeros(num_actions)\n\n    # Calculate average scores for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        action_selection_count[action_index] = selection_count\n        if selection_count > 0:\n            action_exploitation[action_index] = np.mean(scores)\n        else:\n            action_exploitation[action_index] = 0\n\n    # Calculate exploration scores, encouraging exploration of less-selected actions\n    beta = 1.0  # Exploration coefficient\n    exploration_scores = beta * (1 / (action_selection_count + 1)) * ((total_time_slots - current_time_slot + 1) / total_time_slots)\n\n    # Combine scores using the UCB method\n    ucb_scores = action_exploitation + exploration_scores\n\n    # Select the action with the highest UCB score\n    action_index = np.argmax(ucb_scores)\n\n    return action_index",
          "objective": -413.69275587545815,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    action_count = 8  # Total number of actions\n\n    # Calculate decay exploration factor (epsilon)\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01)\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        n = len(scores)  # Number of times the action has been selected\n\n        # Calculate average score for the action\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Normalize average score based on total selections\n        normalized_score = avg_score / (total_selection_count + 1) if total_selection_count > 0 else avg_score\n        \n        # Exploration incentive for less selected actions\n        exploration_value = (epsilon * np.sqrt(np.log(total_selection_count + 1) / (n + 1))) if n > 0 else epsilon\n        \n        # Upper Confidence Bound calculation\n        ucb_value = normalized_score + exploration_value\n        \n        action_scores.append(ucb_value)\n\n    # Select the action with the maximum UCB value\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -407.4692117238165,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01)  # Decaying exploration\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        \n        avg_score = np.mean(scores) if n > 0 else 0\n        normalized_exploitation = avg_score * (n / (total_selection_count + 1)) if total_selection_count > 0 else 0\n        \n        # UCB formula\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else np.sqrt(np.log(total_selection_count + 1))\n        \n        # Combined score using UCB\n        combined_score = normalized_exploitation + epsilon * exploration_value\n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -407.20721349706133,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n\n        average_score = np.mean(scores) if n > 0 else 0\n        \n        if total_selection_count > 0:\n            exploration_factor = np.sqrt(np.log(total_selection_count) / (n + 1)) if n > 0 else float('inf')\n        else:\n            exploration_factor = float('inf')\n\n        progress_factor = (total_time_slots - current_time_slot) / total_time_slots if total_time_slots > 0 else 0\n        \n        action_scores[action_index] = average_score + exploration_factor * progress_factor\n\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -406.05904609502676,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = []\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n\n        if n > 0:\n            avg_score = np.mean(scores)\n            std_dev = np.std(scores)\n        else:\n            avg_score = 0\n            std_dev = 0\n\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else exploration_factor\n        \n        combined_score = avg_score + exploration_value * std_dev\n        \n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -405.8058533316339,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n    action_exploitation = []\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        \n        avg_score = np.mean(scores) if n > 0 else 0\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else float('inf')\n        \n        combined_score = avg_score + exploration_factor * exploration_value\n        action_exploitation.append(combined_score)\n\n    action_index = np.argmax(action_exploitation)\n    \n    return action_index",
          "objective": -405.3336877688044,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    epsilon = max(0.1, min(1.0, 1 - (current_time_slot / total_time_slots)))  # Decaying exploration rate\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Calculate exploration bonus\n        selection_count = n if n > 0 else 1\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_count)\n        \n        # Epsilon-greedy strategy: slightly randomize exploration\n        if np.random.rand() < epsilon:\n            combined_score = avg_score + exploration_value * np.random.rand()\n        else:\n            combined_score = avg_score + exploration_value\n        \n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -405.03684791561767,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = 0.1  # Exploration probability\n    time_decay_coeff = 0.5  # Coefficient to balance time decay\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)  # Number of times the action has been selected\n        avg_score = np.mean(scores) if n > 0 else 0.0\n        \n        # Normalize average score\n        normalized_score = avg_score * (n / total_selection_count) if total_selection_count > 0 else avg_score\n        \n        # Exploration bonus using UCB\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else np.inf\n        \n        # Time factor to adjust exploration over time\n        time_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = normalized_score + (exploration_bonus * time_factor)\n        \n        action_scores.append(adjusted_score)\n\n    # Epsilon-greedy approach for exploration\n    if np.random.rand() < exploration_factor:\n        action_index = np.random.randint(8)\n    else:\n        action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -403.9521688732108,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = 1.0  # This can be adjusted to increase or decrease exploration\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01)  # Epsilon decay\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        \n        # Calculate average score with handling for division by zero\n        avg_score = np.mean(scores) if n > 0 else 0\n        normalized_score = avg_score * (n / total_selection_count) if total_selection_count > 0 else avg_score\n        \n        # Exploration term: avoid zero division by adding a small constant (1e-5)\n        exploration_value = epsilon / (n + 1e-5) if n > 0 else epsilon\n        \n        # Upper Confidence Bound (UCB)\n        ucb_value = normalized_score + exploration_value\n        \n        action_scores.append(ucb_value)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -403.7821304353142,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        \n        # Calculate average score while handling possible division by zero\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Exploration factor to encourage trying out less-selected actions\n        exploration_factor = 1.0  # Can be adjusted to control exploration level\n        exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else float('inf')\n        \n        # Time decay factor to prioritize more recent actions\n        time_decay = np.clip(1 - (current_time_slot / total_time_slots), 0, 1) if total_time_slots > 0 else 0\n        \n        # Combine average score with exploration and time decay\n        action_scores[action_index] = avg_score + time_decay * exploration_bonus\n    \n    # Select the action with the highest combined score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -402.4026712756147,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_scores = np.zeros(action_count)\n    action_selections = np.zeros(action_count)\n\n    # Calculate average scores and selection counts for each action\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_selections[action_index] = len(scores) \n        \n        # Handle zero selection cases\n        total_score = np.sum(scores)\n        average_score = total_score / action_selections[action_index] if action_selections[action_index] > 0 else 0\n        \n        # Calculate exploration value using UCB formula\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (action_selections[action_index] + 1)) if action_selections[action_index] > 0 else np.inf\n        \n        # Time decay effect\n        time_adjustment = (1 - current_time_slot / total_time_slots) if total_time_slots > 0 else 1\n        \n        # Combined score\n        action_scores[action_index] = average_score + (0.1 * exploration_value * time_adjustment)\n\n    # Epsilon-greedy selection strategy\n    epsilon = 0.1\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(action_count)  # Explore\n    else:\n        action_index = np.argmax(action_scores)  # Exploit\n\n    return action_index",
          "objective": -401.48137448052324,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = []\n    epsilon = 0.1  # Exploration probability\n    \n    # Pre-calculate constants for normalization\n    time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)  # Number of times the action has been selected\n        avg_score = np.mean(scores) if n > 0 else 0.0\n        \n        # Normalize score based on total selections\n        normalized_score = avg_score * (n / total_selection_count) if total_selection_count > 0 else avg_score\n        \n        # Upper Confidence Bound calculation\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else np.inf\n        \n        # Combine scores with time decay and exploration\n        adjusted_score = normalized_score + (exploration_bonus * time_decay_factor)\n        action_scores.append(adjusted_score)\n\n    # Epsilon-greedy exploration\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -400.14621237092706,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = 0.5  # Adjusted exploration factor\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n\n        # Normalization factor to avoid division by zero\n        selection_count = n if n > 0 else 1\n\n        # Calculating Upper Confidence Bound (UCB)\n        exploration_value = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / selection_count)\n        \n        # Combined score: UCB balances the average score with exploration\n        combined_score = avg_score + exploration_value\n        \n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -396.63624639192363,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = 1.0  # Decaying exploration factor based on time\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01)  # Decay exploration over time\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        \n        # Normalize average score based on total selection count\n        avg_score = np.mean(scores) if n > 0 else 0\n        normalized_score = avg_score * (n / total_selection_count) if total_selection_count > 0 else avg_score\n        \n        # Exploration term\n        exploration_value = epsilon / (n + 1) if n > 0 else epsilon\n        \n        # Upper Confidence Bound (UCB) approach\n        ucb_value = exploration_value + normalized_score\n        \n        action_scores.append(ucb_value)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -395.1928062758745,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    num_actions = 8\n    epsilon = 0.1  # Exploration factor\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        \n        # Compute average score handling the case when the action has not been selected\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Calculate the exploration term\n        exploration_value = epsilon / (n + 1) if n > 0 else epsilon\n        \n        # Time decay factor\n        time_decay = 1 - (current_time_slot / total_time_slots)\n        adjusted_avg_score = avg_score * time_decay\n        \n        # Upper Confidence Bound (UCB) implementation\n        if n > 0:\n            ucb_score = adjusted_avg_score + exploration_value * np.sqrt(np.log(total_selection_count) / n)\n        else:\n            ucb_score = exploration_value\n        \n        action_scores.append(ucb_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -393.5548513720707,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n\n        avg_score = np.mean(scores) if n > 0 else 0\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) * exploration_factor if total_selection_count > 0 else 1.0\n        combined_score = (avg_score * (n + 1) + exploration_value) / (n + 2)  # Weighted average to balance exploration and exploitation\n\n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -392.4265235478042,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n\n        # Calculate average score\n        avg_score = np.mean(scores) if n > 0 else 0\n\n        # Exploration term (UCB approach)\n        exploration_weight = np.sqrt(2 * np.log(total_selection_count + 1) / (n + 1)) if n > 0 else float('inf')\n\n        # Temporal influence\n        time_factor = (1 - (current_time_slot / total_time_slots)) if total_time_slots > 0 else 0\n\n        # Combined score\n        action_scores[action_index] = avg_score + time_factor * exploration_weight\n\n    action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -391.25603340905906,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    epsilon = 0.1  # Exploration probability\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)  # Number of times the action has been selected\n        avg_score = np.mean(scores) if n > 0 else 0.0\n        \n        # Normalize score based on total selections\n        normalized_score = avg_score * (n / total_selection_count) if total_selection_count > 0 else avg_score\n        \n        # Exploration bonus based on UCB\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else np.inf\n        \n        # Decay exploration based on current time\n        time_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = normalized_score + exploration_bonus * time_factor\n        \n        action_scores.append(adjusted_score)\n\n    # Epsilon-greedy approach for exploration\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(8)\n    else:\n        action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -390.95188082653124,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    action_mean_scores = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n    exploration_bonus = np.zeros(num_actions)\n\n    # Calculate mean scores and counts for each action\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        if action_counts[action_index] > 0:\n            action_mean_scores[action_index] = np.mean(scores)\n        else:\n            action_mean_scores[action_index] = 0\n\n    # Calculate exploration bonus to encourage less frequently selected actions\n    for action_index in range(num_actions):\n        if total_selection_count > 0:\n            exploration_bonus[action_index] = (1 / (action_counts[action_index] + 1)) * (\n                (total_time_slots - current_time_slot + 1) / total_time_slots)\n        else:\n            exploration_bonus[action_index] = 1  # Full exploration in the first round\n\n    # Combine scores using a weighted sum of mean scores and exploration bonus\n    alpha = 0.75  # Weight towards exploitation\n    combined_scores = alpha * action_mean_scores + (1 - alpha) * exploration_bonus\n\n    # Select the action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -390.4440702203319,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = 0.1  # Small exploration parameter\n    total_actions = 8\n\n    for action_index in range(total_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # UCB calculation for balancing exploration and exploitation\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else float('inf')\n        \n        # Combined score incorporating average score and exploration\n        combined_score = avg_score + exploration_factor * exploration_value\n        \n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -387.9828021823197,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    actions_count = len(score_set)\n\n    # Dynamic epsilon for exploratory behavior\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01)\n    \n    for action_index in range(actions_count):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n\n        if n > 0:\n            avg_score = np.mean(scores)\n            normalized_score = avg_score * (n / total_selection_count)\n            uncertainty = epsilon / np.sqrt(n)\n        else:\n            avg_score = 0.0\n            normalized_score = 0.0\n            uncertainty = epsilon  # Fully explore if not selected\n\n        # Combined score considering normalized average and uncertainty\n        combined_score = normalized_score + uncertainty\n        action_scores.append(combined_score)\n\n    # Select action with the highest combined score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -386.9582768084719,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    base_exploration_rate = 0.1\n    decay_rate = 0.99\n    min_exploration_rate = 0.01\n    time_decay_factor = 0.5  # Factor to weight recent performance more\n    \n    # Calculate exploration rate based on time slot\n    exploration_rate = max(min_exploration_rate, base_exploration_rate * (decay_rate ** current_time_slot))\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Normalize the average score based on total selections\n        normalized_score = (avg_score / (total_selection_count + 1)) if total_selection_count > 0 else 0\n\n        # Time decay adjustment to favor recent scores\n        time_weighted_score = normalized_score * (1 - time_decay_factor) + (time_decay_factor * avg_score)\n        \n        # Exploration term\n        exploration_value = exploration_rate * (1 / (n + 1)) if n > 0 else exploration_rate\n        \n        # Combined score\n        combined_score = time_weighted_score + exploration_value\n\n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -384.69188845054896,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_weight = 1.0\n    decay_factor = 0.99  # Factor to reduce exploration over time\n\n    # Dynamic exploration based on time slot\n    exploration_factor = exploration_weight * (1 - (current_time_slot / total_time_slots))\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        \n        avg_score = np.mean(scores) if n > 0 else 0\n        normalized_avg_score = avg_score / (total_selection_count + 1) if total_selection_count > 0 else 0\n        \n        exploration_value = (exploration_factor * np.sqrt(np.log(total_selection_count + 1) / (n + 1))) if n > 0 else exploration_factor * np.sqrt(np.log(total_selection_count + 1))\n        \n        combined_score = normalized_avg_score + exploration_value\n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -377.1272137050594,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        \n        avg_score = np.mean(scores) if n > 0 else 0\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) * exploration_factor if n > 0 else 1.0\n        \n        # Incorporate a decay for the exploration value based on time\n        time_decay = (total_time_slots - current_time_slot) / total_time_slots\n        combined_score = avg_score + exploration_value * time_decay\n\n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -377.0858613360059,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = 1.0  # Maximum exploration factor\n    min_epsilon = 0.01  # Minimum exploration rate\n    decay_rate = 0.99  # Decay rate for exploration\n    \n    # Calculate epsilon for exploration based on current time slot\n    epsilon = max(min_epsilon, exploration_factor * (decay_rate ** current_time_slot))\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Normalized score considering total selections\n        normalized_score = avg_score / (total_selection_count + 1) if total_selection_count > 0 else 0\n        \n        # Exploration term adjusted based on how many times the action was selected\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else epsilon\n        \n        # Combined score: favoring higher average scores while allowing exploration\n        combined_score = normalized_score + exploration_value + epsilon\n        \n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -374.4898429924421,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    \n    exploration_factor = 0.1  # Exploration constant, can be tuned\n    epsilon = 0.1  # Epsilon for epsilon-greedy\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        \n        avg_score = np.mean(scores) if n > 0 else 0\n        selection_count = n if n > 0 else 1  # Avoid division by zero\n        \n        # Exploration value using the logarithm of selection count\n        exploration_value = np.sqrt((np.log(total_selection_count + 1) / selection_count)) * exploration_factor\n        \n        # Dynamic decay based on current time slot\n        time_decay = (total_time_slots - current_time_slot) / total_time_slots\n        \n        # Combined score balancing exploration and time relevance\n        combined_score = (avg_score * time_decay) + exploration_value\n        \n        action_scores.append(combined_score)\n    \n    # Epsilon-greedy decision-making for exploration vs exploitation\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, 8)  # Explore\n    else:\n        action_index = np.argmax(action_scores)  # Exploit\n        \n    return action_index",
          "objective": -371.8231494608515,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    epsilon_decay = 1.0  # Initial exploration factor\n    min_epsilon = 0.01\n    epsilon = max(min_epsilon, 1.0 - (current_time_slot / total_time_slots))\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        \n        avg_score = np.mean(scores) if n > 0 else 0.0\n        normalized_score = avg_score * (n / total_selection_count) if total_selection_count > 0 else avg_score\n\n        exploration_value = epsilon / (n + 1) if n > 0 else epsilon\n        ucb_value = normalized_score + exploration_value\n        \n        action_scores.append(ucb_value)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -370.04104708584646,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_weight = 1.5  # Weight for exploration\n    epsilon = 0.1  # Exploration factor\n    confidence_level = 1.0  # For UCB\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Normalize avg score by the total selection count\n        normalized_score = avg_score / (total_selection_count + 1) if total_selection_count > 0 else 0\n        \n        # Calculate exploration term to encourage less explored actions\n        exploration_value = exploration_weight * (epsilon / (n + 1)) if n > 0 else exploration_weight * epsilon\n        \n        # Upper Confidence Bound (UCB)\n        ucb_value = (\n            confidence_level * np.sqrt(np.log(total_selection_count + 1) / (n + 1))\n            if n > 0 else float('inf')\n        )\n        \n        combined_score = normalized_score + exploration_value + ucb_value\n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -366.0039386332762,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_factor = 1.5  # Exploration weight\n    epsilon = 0.1  # Epsilon value for exploration\n    \n    action_scores = []\n    actions = range(8)\n    \n    for action_index in actions:\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Calculate exploration value\n        selection_count = n if n > 0 else 1  # Avoid division by zero\n        exploration_value = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / selection_count)\n        \n        # Epsilon-greedy approach\n        exploitation_value = (1 - epsilon) * avg_score\n        exploration_value = epsilon * exploration_value\n        \n        # Combined score\n        combined_score = exploitation_value + exploration_value\n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -365.6556422583402,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Use a base exploration factor\n        epsilon = 0.1  # exploration rate\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (n + 1)) if n > 0 else float('inf')\n        \n        # Factor in time decay based on current time slot\n        time_decay = 1 - (current_time_slot / total_time_slots) if total_time_slots > 0 else 0\n        \n        # Calculate combined score: exploitation + exploration\n        action_scores[action_index] = avg_score + time_decay * (epsilon * exploration_value)\n\n    # Use softmax for action selection to allow for exploration of high-scoring actions\n    exp_scores = np.exp(action_scores - np.max(action_scores))\n    probabilities = exp_scores / np.sum(exp_scores)\n    \n    # Sample an action based on the computed probabilities\n    action_index = np.random.choice(num_actions, p=probabilities)\n    \n    return action_index",
          "objective": -363.24007908293106,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    exploration_factor = 0.1  # epsilon for exploration\n    decay_rate = 1 - (current_time_slot / total_time_slots) if total_time_slots > 0 else 0\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n\n        # Calculate average score, safeguard against division by zero\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Explore less selected actions; add exploration factor\n        action_weight = exploration_factor * (1 / (n + 1)) if n > 0 else 1  # Prioritize unselected actions\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else float('inf')\n        \n        # Combined score with decay and exploration\n        action_scores[action_index] = avg_score + decay_rate * (exploration_bonus + action_weight)\n\n    # Select action with the highest score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -362.4080297849347,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_exploitation = []\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n    epsilon = 0.1  # Epsilon for epsilon-greedy strategy\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        \n        # Calculate average score, handling division by zero\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Compute exploration value with a safeguard against division by zero\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else np.sqrt(np.log(total_selection_count + 1))\n        \n        # Combine exploitation and exploration with a weight\n        combined_score = avg_score + exploration_factor * exploration_value\n        \n        action_exploitation.append(combined_score)\n\n    # Epsilon-greedy strategy to encourage exploration\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(range(8))\n    else:\n        action_index = np.argmax(action_exploitation)\n    \n    return action_index",
          "objective": -361.5003863048642,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    total_actions = 8\n    \n    # Parameters for exploration-exploitation balance\n    exploration_weight = 1.0\n    exploitation_weight = 1.0\n    epsilon = max(0.01, 1.0 - (current_time_slot / total_time_slots))  # Dynamic epsilon decay\n\n    for action_index in range(total_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        \n        # Calculate average score\n        avg_score = np.mean(scores) if n > 0 else 0.0\n        \n        # Normalize scores based on total selections\n        normalized_score = avg_score / (total_selection_count + 1) if total_selection_count > 0 else 0.0\n        \n        # UCB component to balance exploration and exploitation\n        exploration_value = exploration_weight * np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else epsilon * exploration_weight\n        \n        # Combined score\n        combined_score = (exploitation_weight * normalized_score) + exploration_value\n        \n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -360.99207699530325,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        \n        # Calculate average score and handle division by zero\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Calculate exploration factor using softmax\n        exploration_factor = np.log(total_selection_count + 1) / (n + 1) if n > 0 else float('inf')\n        \n        # Temporal decay factor to encourage exploration\n        time_decay = (total_time_slots - current_time_slot) / total_time_slots if total_time_slots > 0 else 0\n        \n        # Calculate total score for the action\n        total_score = avg_score + time_decay * exploration_factor\n        \n        action_scores[action_index] = total_score\n\n    # Select the action with the highest total score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -359.1680079975432,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_weight = 0.5  # Balance between exploitation and exploration\n    action_count = len(score_set)\n    action_scores = np.zeros(action_count)\n    \n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Normalize the average score by total selection count\n        normalized_exploitation = (avg_score * n) / (total_selection_count + 1) if total_selection_count > 0 else 0\n        \n        # Epsilon greedy exploration\n        exploration_value = exploration_weight * np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else exploration_weight\n        \n        # Combine scores\n        combined_score = normalized_exploitation + exploration_value\n        action_scores[action_index] = combined_score\n\n    # Select action with the highest combined score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -358.11998446039104,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_scores = np.zeros(action_count)\n    action_selections = np.zeros(action_count)\n\n    # Calculate total scores and select counts for each action\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_selections[action_index] = len(scores)\n        \n        total_score = np.sum(scores)\n        average_score = total_score / action_selections[action_index] if action_selections[action_index] > 0 else 0\n        \n        # Compute UCB for exploration\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (action_selections[action_index] + 1)) if action_selections[action_index] > 0 else np.inf\n        \n        # Combine average score and exploration value\n        action_scores[action_index] = average_score + exploration_value\n\n    # Epsilon-greedy aspect for additional exploration\n    epsilon = 0.1\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(action_count)\n    else:\n        action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -356.8131956372684,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    total_actions = len(score_set)\n\n    # Epsilon for exploration, could be tuned or made dynamic based on current_time_slot\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n\n    for action_index in range(total_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Calculate exploration term\n        selection_count = n if n > 0 else 1  # Avoid division by zero\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_count)\n        \n        # Combined score using UCB approach\n        combined_score = (1 - epsilon) * avg_score + epsilon * exploration_value\n        \n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -353.71389031095237,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_exploitation = []\n    action_selection_counts = [len(scores) for scores in score_set.values()]\n    \n    for action_index in range(8):\n        scores = score_set[action_index]\n        avg_score = np.mean(scores) if scores else 0\n        action_exploitation.append(avg_score)\n\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n    \n    combined_scores = []\n    for action_index in range(8):\n        if action_selection_counts[action_index] == 0:\n            # If the action has never been selected, give a high exploration score\n            exploration_score = float('inf')\n        else:\n            exploration_score = (1 / action_selection_counts[action_index]) * exploration_factor\n            \n        combined_scores.append(action_exploitation[action_index] + exploration_score)\n\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -352.6183197893274,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = max(0.1, (total_time_slots - current_time_slot) / total_time_slots)\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n\n        avg_score = np.mean(scores) if n > 0 else 0.0\n        exploration_value = (np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if total_selection_count > 0 else 1.0)\n        \n        combined_score = avg_score + (exploration_factor * exploration_value)\n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -351.85392035848145,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    n_actions = 8\n    \n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Use a small constant to avoid division by zero\n        selection_count = n if n > 0 else 1\n        \n        # Exploration term: Square root of the logarithm of total selections divided by historical selections\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / selection_count)\n        \n        # Dynamic adjustment factor based on current time slot\n        time_factor = (current_time_slot + 1) / total_time_slots\n        \n        # Combined score with time factor adjustment\n        combined_score = avg_score + exploration_value * time_factor\n        \n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -351.5326022745801,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        \n        # Calculate average score with a safeguard against division by zero\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Calculate confidence interval using UCB\n        exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / (n + 1)) if n > 0 else float('inf')\n        \n        # Temporal decay factor to encourage exploration\n        time_decay = (1 - (current_time_slot / total_time_slots)) if total_time_slots > 0 else 0\n        \n        # Combined score\n        action_scores[action_index] = avg_score + time_decay * exploration_bonus\n\n    # Select the action with the highest combined score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -350.8437902155081,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_probability = 0.1  # Epsilon for epsilon-greedy strategy\n    action_count = 8  # Number of actions\n\n    # Loop through all actions to calculate scores\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        n = len(scores)  # Number of times the action has been selected\n        avg_score = np.mean(scores) if n > 0 else 0.0\n        \n        # Normalize score using total selections\n        normalized_score = avg_score * (n / total_selection_count) if total_selection_count > 0 else avg_score\n        \n        # Exploration bonus using UCB formula\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else np.inf\n        \n        # Incorporate time decay to exploit earlier in time\n        time_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = normalized_score + exploration_bonus * time_factor\n        \n        action_scores.append(adjusted_score)\n\n    # Epsilon-greedy approach implementation\n    if np.random.rand() < exploration_probability:\n        action_index = np.random.randint(action_count)\n    else:\n        action_index = np.argmax(action_scores)  # Select action with maximum adjusted score\n    \n    return action_index",
          "objective": -350.4186824762759,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    action_values = np.zeros(n_actions)\n    exploration_values = np.zeros(n_actions)\n    \n    # Calculate average score and exploration values\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        \n        # Calculate average score (handling cases with no scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Exploration term: (1 / (n + 1)) * exploration_factor if action was selected before\n        exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n        exploration_value = (1 / (n + 1)) * exploration_factor if total_selection_count > 0 else 1.0\n        \n        # Combine average score and exploration value\n        action_values[action_index] = avg_score\n        exploration_values[action_index] = exploration_value\n        \n    combined_scores = action_values + exploration_values\n    \n    # Select the action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -348.6775215174637,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    total_actions = 8\n    \n    for action_index in range(total_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Adjust exploration based on the current time slot relative to total time slots\n        exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n        \n        # Handle division for exploration value\n        selection_count = n if n > 0 else 1  # Avoid division by zero\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / selection_count) * exploration_factor\n        \n        # Weighted score: prioritize scores but consider exploration\n        combined_score = avg_score + exploration_value\n        \n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -347.77683827163906,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_weight = 2  # Weight for exploring underrepresented actions\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n\n        # UCB exploration term\n        exploration_value = (exploration_weight * np.sqrt(np.log(total_selection_count + 1) / (n + 1))) if n > 0 else float('inf')\n\n        # Combined score: balancing exploitation and exploration\n        combined_score = avg_score + exploration_value\n        \n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -347.3096148409146,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = 1.5  # Exploration boost factor\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Calculate exploration using a modified version of UCB\n        selection_count = n if n > 0 else 1  # Ensure no division by zero\n        exploration_value = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / selection_count)\n        \n        # Combined score: balancing exploration and exploitation\n        combined_score = avg_score + exploration_value\n        \n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -340.42978469521995,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_parameter = 1.0  # Exploration coefficient\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Calculate selection count handling zero cases\n        selection_count = n if n > 0 else 1  # Avoid division by zero\n        exploration_value = exploration_parameter * np.sqrt(np.log(total_selection_count + 1) / selection_count)\n        \n        # Time-based decay factor to prioritize recent actions\n        time_decay = (current_time_slot / total_time_slots) if total_time_slots > 0 else 1\n        \n        # Weighted score: balance exploitation and exploration with temporal perspective\n        combined_score = avg_score * time_decay + exploration_value\n        \n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -330.7314800607092,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    action_mean_scores = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n    exploration_bonus = np.zeros(num_actions)\n\n    # Calculate mean scores and counts for each action\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        action_mean_scores[action_index] = np.mean(scores) if scores else 0\n\n    # Calculate exploration bonus\n    for action_index in range(num_actions):\n        if action_counts[action_index] > 0:\n            exploration_bonus[action_index] = (1 / (action_counts[action_index] + 1)) * \\\n                                              ((total_time_slots - current_time_slot + 1) / total_time_slots)\n        else:\n            exploration_bonus[action_index] = 1  # Prefer exploration for unselected actions\n\n    # Combine scores using Upper Confidence Bound (UCB) approach\n    ucb_scores = action_mean_scores + exploration_bonus\n\n    # Select the action with the highest UCB score\n    action_index = np.argmax(ucb_scores)\n    \n    return action_index",
          "objective": -324.09319262172323,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    action_indices = list(range(8))\n    \n    for action_index in action_indices:\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Handling selection count and using a small constant to avoid division by zero\n        selection_count = n if n > 0 else 1\n        \n        # Calculate an exploration bonus\n        exploration_value = np.sqrt((np.log(total_selection_count + 1) / selection_count)) if total_selection_count > 0 else 1\n        \n        # Temporal factor to encourage exploration when close to the end of the time slots\n        temporal_factor = (total_time_slots - current_time_slot) / total_time_slots\n        \n        # Combined score with exploration, tuning with temporal factor\n        combined_score = avg_score + exploration_value * temporal_factor\n        \n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -322.6852818588069,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8  # Actions are indexed from 0 to 7\n    action_exploitation = np.zeros(action_count)\n    action_selection_count = np.zeros(action_count)\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        if scores:\n            avg_score = np.mean(scores)\n            action_exploitation[action_index] = avg_score\n            action_selection_count[action_index] = len(scores)\n\n    if total_selection_count == 0:\n        return np.random.randint(action_count)  # Random action on first run\n\n    # Epsilon greedy exploration with a decreasing epsilon over time\n    epsilon = 1 - (current_time_slot / total_time_slots)\n    exploration_scores = np.zeros(action_count)\n\n    for action_index in range(action_count):\n        if action_selection_count[action_index] == 0:\n            exploration_scores[action_index] = 1  # Max exploration for unselected actions\n        else:\n            exploration_scores[action_index] = np.sqrt(np.log(total_selection_count) / action_selection_count[action_index])\n\n    combined_scores = action_exploitation + exploration_scores\n    action_index = np.argmax(combined_scores)\n    \n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(np.flatnonzero(combined_scores == combined_scores.max()))\n    \n    return action_index",
          "objective": -317.8458743794355,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        \n        # Calculate average score\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Handle exploration: UCB strategy\n        exploration_weight = np.sqrt((2 * np.log(total_selection_count + 1)) / (n + 1)) if n > 0 else float('inf')\n        \n        # Time decay factor\n        if total_time_slots > 0:\n            time_factor = (total_time_slots - current_time_slot) / total_time_slots\n        else:\n            time_factor = 1\n        \n        # Combined score with exploration adjustment and time factor\n        action_scores[action_index] = avg_score + time_factor * exploration_weight\n\n    # Select the action with the highest score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -317.54706892376225,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n\n        avg_score = np.mean(scores) if n > 0 else 0\n        exploration_value = (1 / (n + 1)) * exploration_factor if total_selection_count > 0 else 1.0\n        combined_score = avg_score + exploration_value\n\n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -313.49170106034796,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    n_actions_selected = np.array([len(score_set.get(action_index, [])) for action_index in range(num_actions)])\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        n = n_actions_selected[action_index]\n\n        average_score = np.mean(scores) if n > 0 else 0\n        \n        # UCB-like exploration term\n        exploration_factor = np.sqrt((2 * np.log(total_selection_count + 1)) / (n + 1)) if n > 0 else float('inf')\n        \n        # Temporal factor\n        progress_factor = (total_time_slots - current_time_slot) / total_time_slots if total_time_slots > 0 else 0\n\n        # Combined score\n        action_scores[action_index] = average_score + exploration_factor * progress_factor\n\n    # Softmax function for action selection\n    action_probs = np.exp(action_scores - np.max(action_scores))  \n    action_probs /= np.sum(action_probs)\n\n    # Randomly selecting an action based on the calculated probabilities\n    action_index = np.random.choice(num_actions, p=action_probs)\n\n    return action_index",
          "objective": -313.36145063376983,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))  # Decaying exploration factor\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        \n        avg_score = np.mean(scores) if n > 0 else 0\n        normalized_score = avg_score / total_selection_count if total_selection_count > 0 else 0\n        exploration = np.sqrt((2 * np.log(total_selection_count + 1)) / (n + 1)) if n > 0 else 1\n        \n        # Combining normalized score with exploration term\n        combined_score = (1 - epsilon) * normalized_score + epsilon * exploration\n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -312.2214467670948,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n\n        # Calculate average score safely\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Adjust the exploration factor to promote exploration in the early time slots\n        exploration_factor = 1 - (current_time_slot / total_time_slots) if total_time_slots > 0 else 0\n        \n        # Calculate the exploration value with a small constant to avoid division by zero\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else float('inf')\n        \n        # Combined score: average score plus exploration value\n        action_scores[action_index] = avg_score + exploration_factor * exploration_value\n\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -311.7671920963208,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n\n        # Calculate average score safely\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Exploration factor decreases as we progress in time slots\n        exploration_factor = 1 - (current_time_slot / total_time_slots) if total_time_slots > 0 else 0\n        \n        # UCB with an epsilon contribution based on the number of selections\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else float('inf')\n\n        # Combined score: average score plus adjusted exploration value\n        action_scores[action_index] = avg_score + exploration_factor * exploration_value\n\n    # Choose action with the highest score\n    action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -309.2607753046018,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score\n        avg_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration term\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1e-6))\n        \n        # Combine average score and exploration value\n        combined_score = avg_score + exploration_value\n        \n        # Consider a decay factor based on time slot to encourage timely decisions\n        time_factor = 1 - (current_time_slot / total_time_slots)  # Encourages earlier selections\n        combined_score *= time_factor\n        \n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -308.21919304385506,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n\n        # Manage division by zero and calculate exploration factor\n        selection_count = n if n > 0 else 1\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_count)\n\n        # Upper Confidence Bound calculation\n        combined_score = avg_score + exploration_value\n\n        action_scores.append(combined_score)\n\n    # Select the action with the highest combined score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -304.33635489185383,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = 0.1  # Exploring factor for epsilon-greedy\n    n_actions = 8\n    \n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Calculate exploration term based on the number of selections\n        exploration_value = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else exploration_factor * np.sqrt(np.log(total_selection_count + 1))\n        \n        # UCB score combining exploration and exploitation\n        combined_score = avg_score + exploration_value\n        \n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -302.03774466288365,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    \"\"\"\n    Selects the most suitable action index based on historical scores and selection counts.\n    \n    Parameters:\n    score_set (dict): A dictionary where keys are action indices (0-7) and values are lists of historical scores.\n    total_selection_count (int): The total number of actions selected across all time slots.\n    current_time_slot (int): The index of the current time slot.\n    total_time_slots (int): The total number of time slots.\n\n    Returns:\n    int: The index of the selected action (0-7).\n    \"\"\"\n    action_scores = []\n    n_actions = 8\n    \n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Calculate exploration value with a Bayesian approach\n        selection_count = n if n > 0 else 1  # Avoid division by zero\n        exploration_value = np.sqrt(2 * np.log(total_selection_count + 1) / selection_count)\n\n        # Add a decay factor based on current time slot to encourage timely exploration\n        decay_factor = 1 - (current_time_slot / total_time_slots)\n        \n        # Calculate a weighted score with exploration and decay\n        combined_score = (avg_score * decay_factor) + exploration_value\n        \n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -296.61895176465043,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    epsilon = 0.1  # Exploration factor\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        avg_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration bonus\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1e-5))\n        \n        # Temporal component to favor actions that may impact near future time slots\n        temporal_factor = 1 - (current_time_slot / total_time_slots)\n\n        # Weighted combined score\n        combined_score = (1 - epsilon) * avg_score + epsilon * exploration_value * temporal_factor\n        \n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -291.64953579614416,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = 0.1  # Exploration factor\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Calculation of the exploration bonus\n        if n > 0:\n            # Using UCB for exploration\n            exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count) / n)\n        else:\n            exploration_bonus = float('inf')  # Allow exploration of untried actions\n        \n        combined_score = avg_score + exploration_bonus\n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -279.2469671742389,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration rate\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n\n        # Effective selection count\n        selection_count = n if n > 0 else 1\n        \n        # UCB component\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1) / selection_count))\n\n        # Time decay factor\n        time_decay = 1 - (current_time_slot / total_time_slots)\n        \n        # Combined score\n        combined_score = avg_score + (exploration_value * time_decay)\n\n        action_scores.append(combined_score)\n\n    # Epsilon-greedy approach for exploration\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(range(8))\n    else:\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -276.0238539564944,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Using softmax to allow for exploration while giving preference to higher scores\n        if total_selection_count > 0:\n            exploration_value = np.sqrt(np.log(total_selection_count) / (n + 1e-7))  # Small epsilon to avoid division by zero\n            combined_score = avg_score + exploration_value\n        else:\n            combined_score = avg_score  # If no selections have been made yet\n\n        action_scores.append(combined_score)\n\n    # Implementing softmax normalization to balance exploration and exploitation\n    exp_scores = np.exp(action_scores - np.max(action_scores))  # Subtract max for numerical stability\n    probabilities = exp_scores / np.sum(exp_scores)\n    \n    # Using numpy's ability to randomly select based on probabilities\n    action_index = np.random.choice(range(8), p=probabilities)\n    \n    return action_index",
          "objective": -270.90710380069027,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_exploitation = []\n    action_count = len(score_set)\n    \n    for action_index in range(action_count):\n        scores = score_set[action_index]\n        avg_score = np.mean(scores) if scores else 0\n        action_exploitation.append(avg_score)\n    \n    if total_selection_count == 0:\n        return np.random.choice(range(action_count))  # Random action selection on the first run\n\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n    exploration_bonus = np.array([\n        exploration_factor / (len(score_set[action_index]) + 1) if len(score_set[action_index]) > 0 else exploration_factor\n        for action_index in range(action_count)\n    ])\n\n    combined_scores = np.array(action_exploitation) + exploration_bonus\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -268.09510228159996,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    n_actions_selected = np.zeros(num_actions)\n\n    # Calculate scores for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        n_actions_selected[action_index] = n\n        \n        # Average score calculation\n        average_score = np.mean(scores) if n > 0 else 0\n        \n        # Exploration factor adjusted with UCB\n        exploration_factor = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else float('inf')\n        \n        # Time decay factor\n        progress_factor = (total_time_slots - current_time_slot) / total_time_slots if total_time_slots > 0 else 0\n        \n        action_scores[action_index] = average_score + exploration_factor * progress_factor\n\n    # Stability in softmax to avoid overflow\n    action_probs = np.exp(action_scores - np.max(action_scores))\n    action_probs /= np.sum(action_probs)\n\n    # Select action based on calculated probabilities\n    action_index = np.random.choice(num_actions, p=action_probs)\n\n    return action_index",
          "objective": -267.3225325823248,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n\n        # Calculate average score safely\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Handle exploration factor dynamically depending on the progress of time slots\n        exploration_factor = 1 - (current_time_slot / total_time_slots) if total_time_slots > 0 else 0\n        \n        # Explore less-selected actions more, using UCB-style exploration\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else float('inf')\n        \n        # Combined score: a simple linear combination of average score and exploration value\n        action_scores[action_index] = avg_score + exploration_factor * exploration_value\n\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -264.53109962682663,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_exploitation = []\n    action_count = len(score_set)\n    \n    for action_index in range(action_count):\n        scores = score_set[action_index]\n        avg_score = np.mean(scores) if scores else 0\n        action_exploitation.append(avg_score)\n\n    if total_selection_count == 0:\n        return np.random.choice(range(action_count))  # Random action selection on first run\n    \n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n    \n    # Use a modified UCB approach\n    exploration_scores = [\n        (1 / (len(score_set[action_index]) + 1)) * exploration_factor if len(score_set[action_index]) > 0 else exploration_factor\n        for action_index in range(action_count)\n    ]\n\n    combined_scores = np.array(action_exploitation) + np.array(exploration_scores)\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -257.5234624725983,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = 1.0  # Initial exploration factor\n    decay_rate = 0.99  # Decay rate for exploration factor\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Adjust exploration factor based on total selection count\n        exploration_value = exploration_factor * (1 / (n + 1)) if n > 0 else exploration_factor\n        \n        # Normalize average score considering total selections\n        normalized_avg_score = avg_score / (total_selection_count + 1) if total_selection_count > 0 else 0\n        \n        # Combined score: balance of exploitation and exploration\n        combined_score = normalized_avg_score + exploration_value\n        \n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n\n    # Update exploration factor for future selections\n    exploration_factor *= decay_rate\n    \n    return action_index",
          "objective": -239.4914904379419,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Initialize arrays for mean scores and counts\n    action_mean_scores = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate mean scores and counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_mean_scores[action_index] = np.mean(scores) if scores else 0.0\n\n    # Epsilon decay for exploration\n    epsilon = 0.1 * (1 - (current_time_slot / total_time_slots))\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(num_actions)  # Explore\n    else:\n        # UCB strategy for exploitation\n        exploration_bonus = np.sqrt(2 * np.log(total_selection_count + 1) / (action_counts + 1e-5))\n        combined_scores = action_mean_scores + exploration_bonus\n        action_index = np.argmax(combined_scores)  # Exploit\n\n    return action_index",
          "objective": -237.84229832094258,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Initialize arrays to hold mean scores and counts\n    action_mean_scores = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate mean scores and counts for each action\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        action_mean_scores[action_index] = np.mean(scores) if scores else 0\n\n    # Calculate exploration factor using epsilon-greedy method\n    epsilon = 0.1\n    if np.random.rand() < epsilon:\n        # Explore: select a random action\n        action_index = np.random.choice(num_actions)\n        return action_index\n\n    # Calculate bonus for actions that have been selected fewer times\n    exploration_bonus = np.sqrt(2 * np.log(total_selection_count + 1) / (action_counts + 1e-5))  # Small value to avoid division by zero\n\n    # Combine scores for selection decision\n    combined_scores = action_mean_scores + exploration_bonus\n    action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": -218.28829827898522,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n\n        # Calculate average score (or 0 if no selections)\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Confidence bound calculation\n        exploration_weight = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else float('inf')\n\n        # Temporal decay factor\n        time_decay = (1 - current_time_slot / total_time_slots) if total_time_slots > 0 else 0\n        \n        # Weighted score combining exploitation and exploration\n        action_scores[action_index] = avg_score + time_decay * exploration_weight\n\n    # Return the action with the highest score\n    return action_index",
          "objective": -208.21478061400254,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    epsilon = 0.1  # Exploration probability\n    action_count = len(score_set)  # Fixed number of actions (0 to 7)\n\n    # Calculate scores and selection counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        n = len(scores)  # Number of times the action has been selected\n        avg_score = np.mean(scores) if n > 0 else 0.0\n        \n        # Normalized score calculation\n        normalized_score = (avg_score * n / total_selection_count) if total_selection_count > 0 else avg_score\n        \n        # UCB exploration bonus\n        exploration_bonus = np.sqrt((np.log(total_selection_count + 1) / (n + 1))) if n > 0 else np.inf\n        \n        # Temporal decay factor to consider the time relevance\n        time_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = normalized_score + exploration_bonus * time_factor\n        \n        action_scores.append(adjusted_score)\n\n    # Epsilon-greedy strategy for action selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(action_count)\n    else:\n        action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -207.75270843211047,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n\n    # Allocate weights for exploration and exploitation\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        \n        # Calculate average score and handle zero selections\n        avg_score = np.mean(scores) if n > 0 else 0\n\n        # Exploration term using UCB\n        exploration_weight = np.sqrt((2 * np.log(total_selection_count + 1)) / (n + 1)) if n > 0 else float('inf')\n        \n        # Time-based decay factor\n        time_adjustment = 1 - (current_time_slot / total_time_slots) if total_time_slots > 0 else 0\n        \n        # Combine scores\n        action_scores[action_index] = avg_score + exploration_weight + time_adjustment\n\n    # Return the index of the action with the highest score\n    return action_index",
          "objective": -190.39894539799002,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        \n        # Calculate average score\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Exploration factor (logarithmic scaling for exploration)\n        exploration_factor = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else float('inf')\n        \n        # Time decay applied to the exploration factor\n        time_decay = (1 - (current_time_slot / total_time_slots) if total_time_slots > 0 else 1)\n        \n        # Combined score incorporating average score, exploration, and time decay\n        action_scores[action_index] = avg_score + time_decay * exploration_factor\n        \n    # Select action with highest combined score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -172.62086368727864,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    c = 2  # Exploration factor for UCB\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Adjusted exploration term using UCB\n        exploration_value = (c * np.sqrt(np.log(total_selection_count + 1) / (n + 1))) if n > 0 else float('inf')\n        \n        # Combined score: UCB formulation\n        combined_score = avg_score + exploration_value\n        \n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -170.0150586935128,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = len(score_set)\n    action_exploitation = np.zeros(action_count)\n    action_selection_count = np.zeros(action_count)\n\n    # Calculate average scores and selection counts\n    for action_index in range(action_count):\n        scores = score_set[action_index]\n        if scores:\n            action_exploitation[action_index] = np.mean(scores)\n            action_selection_count[action_index] = len(scores)\n\n    # Initial random action on the first run\n    if total_selection_count == 0:\n        return np.random.randint(action_count)\n\n    # Epsilon greedy approach for exploration\n    epsilon = 0.1  # Exploration factor\n    if np.random.rand() < epsilon:\n        return np.random.randint(action_count)\n\n    # Calculate effective scores for exploitation\n    effective_scores = action_exploitation.copy()\n\n    # Adding exploration to the effective scores\n    for action_index in range(action_count):\n        if action_selection_count[action_index] > 0:\n            exploration_bonus = (np.log(total_selection_count) / (action_selection_count[action_index] + 1))\n            effective_scores[action_index] += exploration_bonus\n        else:\n            # Give a higher score to actions never selected\n            effective_scores[action_index] += np.log(total_selection_count + 1)\n\n    # Select the action with the maximum effective score\n    action_index = np.argmax(effective_scores)\n\n    return action_index",
          "objective": -168.2892859098721,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    n_actions = 8\n    \n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if scores else 0\n        \n        selection_count = max(n, 1)  # Ensure no division by zero\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_count)\n\n        # Dynamic scaling of exploration factor based on the current time slot\n        time_factor = (total_time_slots - current_time_slot) / total_time_slots\n        combined_score = avg_score + exploration_value * time_factor\n        \n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -166.04756747427206,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = 0.1  # Exploration rate\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Normalized selection count to adjust for exploration\n        normalized_count = n / total_selection_count if total_selection_count > 0 else 0\n        \n        # Upper Confidence Bound (UCB) approach for exploration-exploitation\n        if n > 0:\n            ucb_value = avg_score + np.sqrt((exploration_factor * np.log(total_selection_count)) / n)\n        else:\n            ucb_value = exploration_factor * np.log(total_selection_count)  # Explore new actions\n\n        action_scores.append(ucb_value)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -160.42490476234434,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    epsilon = 0.1  # Epsilon for epsilon-greedy strategy\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Calculate exploration value using UCB\n        selection_count = n if n > 0 else 1  # Avoid division by zero\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_count)\n        \n        # Combined score with balanced exploration and exploitation\n        combined_score = avg_score + exploration_value\n        \n        # Apply epsilon-greedy strategy for exploration\n        if np.random.rand() < epsilon:\n            action_scores.append(np.random.rand())  # Random score for exploration\n        else:\n            action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -138.7385883296347,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    min_epsilon = 0.01\n    initial_epsilon = 1.0\n    epsilon = max(min_epsilon, initial_epsilon * (1 - current_time_slot / total_time_slots))\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        \n        avg_score = np.mean(scores) if n > 0 else 0.0\n        normalized_score = avg_score / (1 + total_selection_count) if total_selection_count > 0 else avg_score\n        \n        exploration_value = (epsilon / (n + 1)) if n > 0 else epsilon\n        ucb_value = normalized_score + exploration_value\n        \n        time_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_ucb = ucb_value * time_factor\n        \n        action_scores.append(adjusted_ucb)\n\n    # Adding a small uniform random exploration component\n    action_scores = np.array(action_scores) + np.random.rand(8) * (epsilon / 8)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -137.7177790564611,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = 1.0  # Exploration weight\n    epsilon = 0.1  # Epsilon for exploration\n    exploration_bonus = epsilon / (total_selection_count + 1)  # Exploration term\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Normalized average score\n        normalized_avg_score = avg_score * (n / (total_selection_count + 1)) if total_selection_count > 0 else avg_score\n        \n        # Calculate confidence interval using exploration factor\n        if n > 0:\n            confidence_interval = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / n)\n        else:\n            confidence_interval = float('inf')  # Encourage exploration if action never taken\n        \n        # Combined score balancing exploration and exploitation\n        combined_score = normalized_avg_score + confidence_interval + exploration_bonus\n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -133.47849433321846,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration probability\n    action_scores = []\n    action_count = len(score_set)\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n\n        # Normalize exploitation term\n        normalized_exploitation = avg_score * (n / total_selection_count) if total_selection_count > 0 else 0\n        \n        # Exploration term with UCB approach\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else 0\n        \n        # Adjusting for epsilon-greedy exploration\n        exploration_term = (epsilon / action_count) + (1 - epsilon) * exploration_value\n        \n        # Time decay component\n        time_decay = (total_time_slots - current_time_slot) / total_time_slots if total_time_slots > 0 else 0\n        adjusted_score = normalized_exploitation * time_decay\n\n        # Combined score\n        combined_score = adjusted_score + exploration_term\n        action_scores.append(combined_score)\n\n    # Select action with highest combined score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -125.57943630761417,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        \n        # Calculate average score\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Calculate exploration value\n        selection_count = n if n > 0 else 1  # Avoid division by zero\n        exploration_value = np.sqrt(2 * np.log(total_selection_count + 1) / selection_count)\n        \n        # Combine exploitation and exploration\n        action_scores[action_index] = avg_score + exploration_value\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -110.0266027815494,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n\n        average_score = np.mean(scores) if n > 0 else 0\n        exploration_factor = np.sqrt((2 * np.log(total_selection_count + 1)) / (n + 1)) if n > 0 else float('inf')\n\n        # Adjust exploration based on time dynamics\n        time_weight = (total_time_slots - current_time_slot) / total_time_slots if total_time_slots > 0 else 0\n        adjusted_exploration = time_weight * exploration_factor\n\n        action_scores[action_index] = average_score + adjusted_exploration\n\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -56.73854787060685,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)  # Number of actions (should be 8)\n    \n    # Initialize average scores and action counts\n    avg_scores = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n    \n    # Calculate average scores and counts for each action\n    for action_index in range(num_actions):\n        scores = score_set[action_index]\n        action_counts[action_index] = len(scores)\n        avg_scores[action_index] = np.mean(scores) if len(scores) > 0 else 0.0\n\n    # Epsilon for exploration factor\n    epsilon = 0.1  # 10% exploration\n    exploration_scores = np.zeros(num_actions)\n\n    # Calculate exploration scores\n    for action_index in range(num_actions):\n        if total_selection_count > 0:\n            exploration_scores[action_index] = (total_time_slots - current_time_slot) / (1 + action_counts[action_index])\n    \n    # Combine exploitation and exploration\n    combined_scores = (1 - epsilon) * avg_scores + epsilon * exploration_scores\n    \n    # Select action with highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -48.74516202805995,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        \n        avg_score = np.mean(scores) if n > 0 else 0\n        exploration_value = (\n            np.sqrt(np.log(total_selection_count + 1) / (n + 1)) * exploration_factor\n            if total_selection_count > 0 else 1.0\n        )\n        \n        action_scores[action_index] = avg_score + exploration_value\n\n    epsilon = 0.1\n    if np.random.random() < epsilon:\n        return np.random.choice(num_actions)\n    \n    return action_index",
          "objective": -24.882272682312305,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    action_mean_scores = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate mean scores and counts for each action\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        action_mean_scores[action_index] = np.mean(scores) if scores else 0\n\n    # Avoid division by zero for actions never selected\n    total_actions_selected = np.clip(action_counts, 1, None)\n\n    # Calculate exploration factor using softmax to balance exploration and exploitation\n    temperature = 1.0\n    exploration_bonus = np.log(total_selection_count + 1) / total_actions_selected\n    scores_with_exploration = action_mean_scores + exploration_bonus\n\n    # Softmax to get probabilities\n    exp_scores = np.exp(scores_with_exploration / temperature)\n    action_probabilities = exp_scores / np.sum(exp_scores)\n\n    # Select action based on probabilities\n    action_index = np.random.choice(range(num_actions), p=action_probabilities)\n\n    return action_index",
          "objective": -24.5140944383524,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n\n        avg_score = np.mean(scores) if n > 0 else 0\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) * exploration_factor if total_selection_count > 0 else 1.0\n        \n        action_scores[action_index] = avg_score + exploration_value\n\n    epsilon = 0.1\n    if np.random.random() < epsilon:\n        action_index = np.random.choice(num_actions)\n    else:\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": -21.85452993383052,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Epsilon-greedy strategy for exploration\n        exploration_value = (1 / (n + 1)) * exploration_factor if total_selection_count > 0 else 1.0\n        combined_score = avg_score + exploration_value\n\n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": -10.969903824496896,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = 1.0\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))  # Decreasing epsilon over time\n\n    for action_index in range(8):\n        scores = np.array(score_set.get(action_index, []))\n        n = len(scores)\n        \n        avg_score = np.mean(scores) if n > 0 else 0\n        normalized_score = avg_score / max(total_selection_count, 1)  # Prevent division by zero\n        exploration_value = (exploration_factor * np.sqrt(np.log(total_selection_count + 1) / (n + 1))) if n > 0 else exploration_factor * np.sqrt(np.log(total_selection_count + 1))\n        \n        # Incorporate epsilon-greedy decision making\n        if np.random.rand() < epsilon:\n            action_scores.append(exploration_value)  # Favor exploration\n        else:\n            action_scores.append(normalized_score + exploration_value)  # Favor exploitation\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 33.83506114114874,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    \"\"\"\n    Select an action index based on historical scores and the exploration-exploitation strategy.\n\n    Parameters:\n    - score_set (dict): A dictionary where keys are action indices (0 to 7) and values are lists of historical scores.\n    - total_selection_count (int): Total number of times all actions have been selected.\n    - current_time_slot (int): The current time slot (not used in scoring but can inform strategy).\n    - total_time_slots (int): Total number of time slots available.\n\n    Returns:\n    - action_index (int): The index of the selected action (between 0 and 7).\n    \"\"\"\n    \n    action_scores = []\n    \n    for action_index in range(8):  # There are 8 actions indexed from 0 to 7\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0  # Average score for this action\n        \n        # Use selection_count as adjusted to avoid the division by zero \n        selection_count = n if n > 0 else 1  \n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / selection_count)  # UCB exploration\n        \n        # Consider a time-based influence factor\n        time_factor = (current_time_slot / total_time_slots) if total_time_slots > 0 else 1\n        \n        # Combined score with a balance of exploitation and exploration\n        combined_score = avg_score + exploration_value * time_factor\n        \n        action_scores.append(combined_score)\n    \n    action_index = np.argmax(action_scores)  # Select the action with the highest score\n    return action_index",
          "objective": 39.200844962914005,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n\n        if n > 0:\n            avg_score = np.mean(scores)\n        else:\n            avg_score = 0\n        \n        # Implementing a more adaptive exploration strategy using the UCB approach\n        exploration_value = np.sqrt(2 * np.log(total_selection_count + 1) / (n + 1)) if n > 0 else float('inf')\n        combined_score = avg_score + exploration_factor * exploration_value\n\n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": 66.1657600864321,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_values = []\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        \n        avg_score = np.mean(scores) if n > 0 else 0\n        exploration_value = (1 / (n + 1)) * exploration_factor if total_selection_count > 0 else 1.0\n        \n        combined_score = avg_score + exploration_value\n        action_values.append(combined_score)\n    \n    action_index = np.random.choice(np.flatnonzero(action_values == np.max(action_values)))  # Break ties randomly\n    return action_index",
          "objective": 74.09409138266915,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize lists for storing average scores and exploration values\n    avg_scores = []\n    exploration_factors = []\n\n    # Calculate exploration factor based on time slots\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        \n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # If there are no selections for this action, set exploration factor higher\n        exploration_value = (exploration_factor / (n + 1)) if total_selection_count > 0 else 1.0\n        \n        # Compute combined score using a weighted strategy\n        combined_score = avg_score + exploration_value\n        \n        avg_scores.append(avg_score)\n        exploration_factors.append(exploration_value)\n\n    # Select the action index with the highest combined score\n    action_index = np.argmax(avg_scores + exploration_factors)\n    \n    return action_index",
          "objective": 75.77174387482103,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n\n        avg_score = np.mean(scores) if n > 0 else 0\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) * exploration_weight if total_selection_count > 0 else 1.0\n\n        combined_score = avg_score + exploration_value\n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": 179.33659724308188,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    action_exploitation = np.zeros(num_actions)\n    action_selection_count = np.array([len(scores) for scores in score_set.values()])\n\n    # Calculate average scores for each action and handle divisions by zero\n    for action_index in range(num_actions):\n        scores = score_set[action_index]\n        if len(scores) > 0:\n            action_exploitation[action_index] = np.mean(scores)\n        else:\n            action_exploitation[action_index] = 0  # No score means exploitation score is 0\n\n    # Implement Upper Confidence Bound (UCB) for exploration\n    exploration_scores = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        if action_selection_count[action_index] > 0:\n            exploration_scores[action_index] = np.sqrt((2 * np.log(total_selection_count)) / action_selection_count[action_index])\n        else:\n            exploration_scores[action_index] = float('inf')  # max exploration for unselected actions\n\n    # Combine scores for decision-making\n    combined_scores = action_exploitation + exploration_scores\n\n    # Select the action with the highest combined score\n    action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": 189.98205262309978,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        action_counts[action_index] = n\n\n        # Calculate the average score safely\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Exploration-exploitation trade-off\n        exploration_scale = 1 - (current_time_slot / total_time_slots) if total_time_slots > 0 else 0\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else float('inf')\n        \n        # Combined score with extra smoothing for more exploratory behavior on fewer selections\n        action_scores[action_index] = avg_score + exploration_scale * exploration_value\n    \n    # Handle the case where all actions have zero historical scores\n    if np.all(action_counts == 0):\n        return np.random.randint(num_actions)\n\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": 206.72278988185155,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_weight = 1.0  # Weight for exploration\n    epsilon = 0.1  # Epsilon for epsilon-greedy strategy\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        \n        avg_score = np.mean(scores) if n > 0 else 0\n        exploration_value = exploration_weight * np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else exploration_weight * np.sqrt(np.log(total_selection_count + 1))\n        \n        if np.random.rand() < epsilon:\n            combined_score = exploration_value  # Prioritize exploration\n        else:\n            combined_score = avg_score + exploration_value  # Balance with exploitation\n        \n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 224.40073712586684,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon_initial = 0.1\n    epsilon_decay = 0.99\n    epsilon_min = 0.01\n    total_actions = 8\n\n    # Calculate epsilon based on decaying strategy\n    epsilon = max(epsilon_initial * (epsilon_decay ** (total_selection_count // 10)), epsilon_min)\n\n    action_scores = []\n    \n    for action_index in range(total_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        normalized_score = avg_score / (total_selection_count + 1) if total_selection_count > 0 else 0\n        \n        # Confidence bound for exploration\n        exploration_term = np.sqrt((2 * np.log(total_selection_count + 1)) / (n + 1)) if n > 0 else np.inf\n        \n        # Combined score\n        combined_score = normalized_score + exploration_term\n        \n        # Time factor for time-sensitive optimization\n        time_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_score = combined_score * time_factor\n        \n        action_scores.append(adjusted_score)\n\n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(total_actions)  # Random action for exploration\n    else:\n        action_index = np.argmax(action_scores)  # Best action based on adjusted scores\n\n    return action_index",
          "objective": 233.49538550576813,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = len(score_set)\n    action_exploitation = np.zeros(action_count)\n    \n    # Calculate average scores for exploitation\n    for action_index in range(action_count):\n        scores = score_set[action_index]\n        if scores:  # Calculate average if there are scores\n            action_exploitation[action_index] = np.mean(scores)\n    \n    # Exploration factor to encourage exploration in later time slots\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n    \n    # Selecting actions based on counts to handle division by zero case\n    action_counts = np.array([len(score_set[action_index]) for action_index in range(action_count)])\n    exploration_scores = np.where(action_counts > 0, 1 / action_counts, np.inf) * exploration_factor\n    \n    # Combined score: Exploitation with exploration\n    combined_scores = action_exploitation + exploration_scores\n    \n    # Select the action with the maximum combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": 250.01365215931548,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    n_actions_selected = np.zeros(num_actions)\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        n_actions_selected[action_index] = n\n        \n        average_score = np.mean(scores) if scores else 0\n        \n        # UCB calculation\n        exploration_factor = np.sqrt(2 * np.log(total_selection_count + 1) / (n + 1)) if n > 0 else float('inf')\n        \n        # Temporal decay factor\n        progress_factor = (total_time_slots - current_time_slot) / total_time_slots if total_time_slots > 0 else 0\n\n        # Combine average score with exploration and progress factors\n        action_scores[action_index] = average_score + exploration_factor * progress_factor\n\n    # Improved softmax for selection based on adjusted scores\n    adjusted_scores = action_scores - np.max(action_scores)\n    action_probs = np.exp(adjusted_scores)\n    action_probs /= np.sum(action_probs)\n\n    # Select action based on calculated probabilities\n    action_index = np.random.choice(num_actions, p=action_probs)\n\n    return action_index",
          "objective": 357.46427029401764,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_values = []\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        \n        avg_score = np.mean(scores) if n > 0 else 0\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else 1.0\n        \n        combined_score = avg_score + exploration_factor * exploration_value\n        action_values.append((combined_score, action_index))\n    \n    action_index = max(action_values, key=lambda x: x[0])[1]\n    \n    return action_index",
          "objective": 459.05540596831406,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    n_actions_selected = np.zeros(num_actions)\n\n    # Calculate average scores and the number of times each action has been selected\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        n_actions_selected[action_index] = n\n\n        average_score = np.mean(scores) if n > 0 else 0\n        action_scores[action_index] = average_score\n\n    # Exploration factor using UCB (Upper Confidence Bound)\n    for action_index in range(num_actions):\n        if n_actions_selected[action_index] > 0:\n            exploration_factor = np.sqrt(2 * np.log(total_selection_count) / n_actions_selected[action_index])\n        else:\n            exploration_factor = float('inf')\n        \n        action_scores[action_index] += exploration_factor\n\n    # Time decay factor\n    if total_time_slots > 0:\n        time_factor = (total_time_slots - current_time_slot) / total_time_slots\n        action_scores *= time_factor\n\n    # Softmax for selection probabilities\n    action_probs = np.exp(action_scores - np.max(action_scores))  # Numerical stability\n    action_probs /= np.sum(action_probs)\n\n    action_index = np.random.choice(num_actions, p=action_probs)\n\n    return action_index",
          "objective": 468.43440220785726,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = []\n    beta = 1.0  # Exploration parameter\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Calculate the exploration component\n        selection_count = n if n > 0 else 1  # Avoid division by zero\n        exploration_value = beta * np.sqrt(np.log(total_selection_count + 1) / selection_count)\n        \n        # Calculate decay factor based on the time slot\n        decay_factor = (total_time_slots - current_time_slot) / total_time_slots if total_time_slots > 0 else 1\n\n        # Weighted score: balance exploitation and exploration with decay factor\n        combined_score = avg_score + exploration_value * decay_factor\n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 497.0125458026008,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_exploits = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        if scores:\n            action_exploits[action_index] = np.mean(scores)\n\n    relative_time_left = (total_time_slots - current_time_slot) / total_time_slots\n    exploration_bonuses = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n\n    combined_scores = action_exploits + relative_time_left * exploration_bonuses\n    \n    if total_selection_count == 0:\n        return np.random.choice(range(action_count))\n\n    action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": 527.7453996199015,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = 0.1  # Exploration factor\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n\n        # If no selections have been made for this action, give it a base high exploration score\n        exploration_value = exploration_factor / (n + 1) if n > 0 else exploration_factor\n\n        # Use Upper Confidence Bound (UCB) approach to balance exploitation and exploration\n        ucb_value = exploration_value + avg_score\n        \n        action_scores.append(ucb_value)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 600.3303578071861,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    epsilon = 0.1  # Exploration factor\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        \n        # Calculate average score\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Calculate exploration value with a small adjustment to avoid division by zero\n        selection_count = n if n > 0 else 1  \n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / selection_count)\n        \n        # Adding a time-dependent factor: recent time slots may have more influence\n        time_decay = 1 - (current_time_slot / total_time_slots) if total_time_slots > 0 else 1\n        \n        # Combined score\n        combined_score = (avg_score * time_decay) + exploration_value\n        \n        action_scores.append(combined_score)\n\n    # Epsilon-greedy approach for exploration\n    if np.random.random() < epsilon:\n        action_index = np.random.choice(range(8))\n    else:\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": 680.1214363761351,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = 0.1  # Base exploration probability\n    confidence_constant = 1.0  # UCB confidence scale\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Calculating the UCB value\n        exploration = confidence_constant * np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else float('inf')\n        \n        # Combined score using UCB\n        combined_score = avg_score + exploration\n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 745.8919578759967,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = 1.0 / (1.0 + current_time_slot / total_time_slots)  # Decaying exploration\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # UCB calculation to balance exploration and exploitation\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (n + 1)) if n > 0 else float('inf')\n        \n        combined_score = avg_score + exploration_factor * exploration_value\n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 924.3589992406351,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    n_actions_selected = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        n_actions_selected[action_index] = n\n        \n        average_score = np.mean(scores) if n > 0 else 0\n        \n        exploration_factor = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else float('inf')\n        progress_factor = (total_time_slots - current_time_slot) / total_time_slots if total_time_slots > 0 else 0\n\n        action_scores[action_index] = average_score + exploration_factor * progress_factor\n\n    action_probs = np.exp(action_scores - np.max(action_scores))  # Stability in softmax\n    action_probs /= np.sum(action_probs)\n\n    action_index = np.random.choice(num_actions, p=action_probs)\n\n    return action_index",
          "objective": 934.104203027382,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        avg_score = np.mean(scores) if selection_count > 0 else 0\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1))  # Avoid division by zero\n        \n        # Time-sensitive adjustment: Decay the influence of older scores\n        time_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_avg_score = avg_score * time_factor\n        \n        # Combined score considering exploration and adjusted average score\n        combined_score = adjusted_avg_score + exploration_value\n        \n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 1075.6921317426943,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8  # Fixed number of actions\n    action_exploitation = np.zeros(action_count)\n    action_selection_count = np.zeros(action_count)\n\n    for action_index, scores in score_set.items():\n        action_selection_count[action_index] = len(scores)\n        if scores:\n            action_exploitation[action_index] = np.mean(scores)\n\n    # Handling cases with no selections\n    if total_selection_count == 0:\n        return np.random.choice(range(action_count))\n    \n    # Exploration bonus based on selection counts\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_selection_count + 1e-5))\n    \n    # Combined score calculation with exploration\n    combined_scores = action_exploitation + exploration_bonus\n    action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": 1481.329441964479,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize lists to hold average scores and selection counts\n    action_exploitation = []\n    action_counts = np.array([len(scores) for scores in score_set.values()])  # Number of times each action has been selected\n    \n    # Calculate average scores for each action\n    for action_index in range(8):\n        if action_counts[action_index] > 0:\n            avg_score = np.mean(score_set[action_index])\n        else:\n            avg_score = 0\n        action_exploitation.append(avg_score)\n    \n    # Avoid division by zero in exploration calculations\n    adjusted_action_counts = action_counts + 1  # Add 1 to avoid division by zero\n    \n    # Exploration factor based on total time slots and current time slot\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n    exploration_scores = (1 / adjusted_action_counts) * exploration_factor  # UCB-like approach\n    \n    # Combine exploration and exploitation scores\n    combined_scores = np.array(action_exploitation) + exploration_scores\n    action_index = np.argmax(combined_scores)  # Select action with highest combined score\n    \n    return action_index",
          "objective": 1592.694715973807,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_exploitation = []\n    action_selection_counts = [len(scores) for scores in score_set.values()]\n    \n    # Calculate average scores for each action\n    for action_index in range(8):\n        scores = score_set[action_index]\n        avg_score = np.mean(scores) if scores else 0\n        action_exploitation.append(avg_score)\n\n    # Formula for exploration factor\n    exploration_scores = []\n    for action_index in range(8):\n        selection_count = action_selection_counts[action_index]\n        if selection_count > 0:\n            exploration_score = np.sqrt(np.log(total_selection_count) / selection_count)\n        else:\n            exploration_score = float('inf')  # Prioritize unselected actions by giving them a very high exploration score\n        \n        exploration_scores.append(exploration_score)\n\n    # Calculate combined score using a balanced approach\n    total_time_weight = (total_time_slots - current_time_slot) / total_time_slots\n    combined_scores = [\n        (0.7 * action_exploitation[i] + 0.3 * exploration_scores[i] * total_time_weight)\n        for i in range(8)\n    ]\n    \n    # Select the action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": 1749.860585012747,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    \n    n_actions_selected = np.array([len(score_set.get(action_index, [])) for action_index in range(num_actions)])\n    average_scores = np.array([np.mean(score_set.get(action_index, [])) if n > 0 else 0 for action_index, n in enumerate(n_actions_selected)])\n    \n    # Base exploration factor: beta sets how much weight is placed on exploration\n    beta = 0.1\n    exploration_factor = beta * np.sqrt(np.log(total_selection_count + 1) / (n_actions_selected + 1))\n    \n    # Progress factor\n    progress_factor = (total_time_slots - current_time_slot) / total_time_slots if total_time_slots > 0 else 0\n\n    # Combined score with exploration balancing\n    action_scores = average_scores + exploration_factor * progress_factor\n    \n    # Softmax function for action selection\n    action_probs = np.exp(action_scores - np.max(action_scores))\n    action_probs /= np.sum(action_probs)\n\n    # Randomly selecting an action based on the calculated probabilities\n    action_index = np.random.choice(num_actions, p=action_probs)\n\n    return action_index",
          "objective": 1814.0148661426779,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_exploitation = []\n    for action_index in range(8):\n        scores = score_set[action_index]\n        if len(scores) > 0:\n            avg_score = np.mean(scores)\n        else:\n            avg_score = 0\n        \n        action_exploitation.append(avg_score)\n    \n    if total_selection_count == 0:\n        return np.random.choice(range(8))  # Randomly select an action if no selections have been made yet\n    \n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n    exploration_scores = [(1 / (len(score_set[action_index]) + 1)) * exploration_factor for action_index in range(8)]\n    \n    combined_scores = np.array(action_exploitation) + np.array(exploration_scores)\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": 1854.0271004541373,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    n_actions_selected = np.array([len(score_set.get(action_index, [])) for action_index in range(num_actions)])\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        n = n_actions_selected[action_index]\n\n        average_score = np.mean(scores) if n > 0 else 0\n        \n        # UCB-like exploration term\n        exploration_factor = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else float('inf')\n        \n        # Temporal factor\n        progress_factor = (total_time_slots - current_time_slot) / total_time_slots if total_time_slots > 0 else 0\n\n        # Combined score\n        action_scores[action_index] = average_score + exploration_factor * progress_factor\n\n    # Softmax function for action selection\n    action_probs = np.exp(action_scores - np.max(action_scores))  \n    action_probs /= np.sum(action_probs)\n\n    # Randomly selecting an action based on the calculated probabilities\n    action_index = np.random.choice(num_actions, p=action_probs)\n\n    return action_index",
          "objective": 1872.435046494907,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = len(score_set)\n    action_exploitation = np.zeros(action_count)\n    exploration_scores = np.zeros(action_count)\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        # Calculate average score; handle division by zero using np.nanmean to ignore NaNs\n        action_exploitation[action_index] = np.nanmean(scores) if scores else 0\n\n    # Handle cases for exploration based on each action's selection count\n    for action_index in range(action_count):\n        selection_count = len(score_set.get(action_index, []))\n        if selection_count > 0:\n            exploration_scores[action_index] = (1 / (selection_count + 1)) * ((total_time_slots - current_time_slot) / total_time_slots)\n        else:\n            exploration_scores[action_index] = (total_time_slots - current_time_slot) / total_time_slots\n    \n    # Combined scores\n    combined_scores = action_exploitation + exploration_scores\n    action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": 1924.888121433611,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_weight = 1.0 / (total_selection_count + 1) if total_selection_count > 0 else 1.0\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n\n        avg_score = np.mean(scores) if n > 0 else 0\n        exploration_value = exploration_weight * exploration_factor * (1 / (n + 1)) if n > 0 else exploration_factor\n        \n        combined_score = avg_score + exploration_value\n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": 2132.460013074948,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    n_actions_selected = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        n_actions_selected[action_index] = n\n        \n        average_score = np.mean(scores) if n > 0 else 0\n        \n        # UCB-based exploration term\n        exploration_factor = np.sqrt((2 * np.log(total_selection_count + 1)) / (n + 1)) if n > 0 else float('inf')\n        \n        # Time-based decay factor\n        progress_factor = (total_time_slots - current_time_slot) / total_time_slots if total_time_slots > 0 else 0\n        \n        # Calculate combined score\n        action_scores[action_index] = average_score + exploration_factor * progress_factor\n\n    # Softmax for selection\n    action_probs = np.exp(action_scores - np.max(action_scores))\n    action_probs /= np.sum(action_probs)\n\n    action_index = np.random.choice(num_actions, p=action_probs)\n\n    return action_index",
          "objective": 2174.9269234555422,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = len(score_set)\n    action_exploitation = np.zeros(action_count)\n    exploration_bonus = np.zeros(action_count)\n    \n    for action_index in range(action_count):\n        scores = score_set[action_index]\n        if scores:\n            action_exploitation[action_index] = np.mean(scores)\n        else:\n            action_exploitation[action_index] = 0\n        \n        # Using total_selection_count to avoid division by zero for exploration bonus\n        action_counts = len(scores)\n        exploration_bonus[action_index] = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1)) if action_counts > 0 else float('inf')\n    \n    combined_scores = action_exploitation + exploration_bonus\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": 2259.4110734922688,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    \n    # Initialize average scores and counts\n    avg_scores = np.zeros(num_actions)\n    counts = np.zeros(num_actions)\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        counts[action_index] = len(scores)\n        if counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n\n    # Calculate exploration bonus, using a small epsilon value for exploration\n    epsilon = 0.1\n    exploration_bonus = epsilon * (1 / (counts + 1e-5))  # Avoid division by zero\n    \n    # Time-aware exploration factor: encourage exploration of actions as time progresses\n    time_factor = (total_time_slots - current_time_slot) / total_time_slots\n    exploration_scores = exploration_bonus * time_factor\n\n    # Combine exploitation and exploration values\n    combined_scores = avg_scores + exploration_scores\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": 2791.995395896457,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n    exploration_weight = 1.0 / (total_selection_count + 1)\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n\n        # Upper Confidence Bound (UCB) for exploration\n        exploration_value = np.sqrt((np.log(total_selection_count + 1) / (n + 1))) * exploration_weight if n > 0 else 1.0\n        combined_score = avg_score + exploration_value * exploration_factor\n\n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 3186.6061255821282,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = 0.1  # Base exploratory factor\n    decay_rate = 0.95  # Time decay factor\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        \n        # Calculate average score\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Time-adjusted effective scores\n        decay_factor = decay_rate ** (total_time_slots - current_time_slot)  # Reduce influence of older scores\n        effective_score = avg_score * decay_factor\n        \n        # Exploration term for UCB\n        exploration_value = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else exploration_factor\n        \n        # Combined score\n        combined_score = effective_score + exploration_value\n        \n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 3197.549197702901,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    action_exploitation = np.zeros(num_actions)\n    exploration_scores = np.zeros(num_actions)\n    \n    # Calculate average scores for each action and handle divisions by zero\n    for action_index in range(num_actions):\n        scores = score_set[action_index]\n        selection_count = len(scores)\n        if selection_count > 0:\n            action_exploitation[action_index] = np.mean(scores)\n        else:\n            action_exploitation[action_index] = 0  # No score means exploitation score is 0\n\n    # Calculate dynamic exploration scores, reducing exploration as total selections increase\n    for action_index in range(num_actions):\n        selection_count = len(score_set[action_index])\n        if total_selection_count > 0:\n            exploration_scores[action_index] = (1 / (selection_count + 1)) * ((total_time_slots - current_time_slot + 1) / total_time_slots)\n        else:\n            exploration_scores[action_index] = 1  # Max exploration if no selections have been made\n\n    # Combine scores using a weighted approach for better exploration-exploitation balance\n    alpha = 0.7  # Weight factor for balancing exploitation and exploration\n    combined_scores = alpha * action_exploitation + (1 - alpha) * exploration_scores\n\n    # Select the action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": 3357.6249205525883,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n    epsilon = 0.1  # Exploration parameter for epsilon-greedy strategy\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n\n        # Calculate average score with safety against division by zero\n        avg_score = np.mean(scores) if n > 0 else 0.0\n        \n        # Calculate exploration value\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) * exploration_factor if total_selection_count > 0 else 1.0\n        \n        # Combine avg score with exploration value\n        combined_score = avg_score + exploration_value\n        \n        # Use epsilon-greedy to encourage exploration\n        if np.random.rand() < epsilon:\n            action_scores.append(np.random.rand())  # Random score to explore\n        else:\n            action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": 3499.871267556281,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    action_mean_scores = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate mean scores and counts for each action\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        action_mean_scores[action_index] = np.mean(scores) if scores else 0\n\n    # Prevent division by zero using a small epsilon\n    epsilon = 1e-6\n    total_actions_selected = np.clip(action_counts, 1, None)  # Avoid zero counts\n    \n    # Calculate UCB\n    exploration_bonus = np.sqrt(2 * np.log(total_selection_count + 1) / (total_actions_selected + epsilon))\n    \n    # Weighted combined scores with exponential decay for time\n    decay_factor = np.exp(-current_time_slot / total_time_slots)\n    combined_scores = action_mean_scores + exploration_bonus * decay_factor\n\n    # Select the action with the highest combined score\n    action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": 3504.486999724959,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_exploits = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n    \n    for action_index, scores in score_set.items():\n        if scores:\n            action_exploits[action_index] = np.mean(scores)\n            action_counts[action_index] = len(scores)\n    \n    # Handle the case when no actions have been selected yet\n    if total_selection_count == 0:\n        return np.random.choice(range(action_count))\n    \n    exploration_bonuses = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n    \n    # Relative time remaining\n    time_weight = (total_time_slots - current_time_slot) / total_time_slots\n    \n    # Combine exploitation and exploration components\n    combined_scores = action_exploits + time_weight * exploration_bonuses\n    \n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": 3655.449772094855,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Initialize arrays for mean scores and counts\n    action_mean_scores = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate mean scores and counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_mean_scores[action_index] = np.mean(scores) if scores else 0.0\n\n    # Apply epsilon-greedy exploration strategy\n    epsilon = 0.1 * (1 - (current_time_slot / total_time_slots))  # Decreases epsilon over time\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(num_actions)  # Explore\n    else:\n        # UCB based exploitation\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n        combined_scores = action_mean_scores + exploration_bonus\n        action_index = np.argmax(combined_scores)  # Exploit\n\n    return action_index",
          "objective": 3868.147768898192,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n    epsilon = 0.1  # Exploration probability\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n\n        avg_score = np.mean(scores) if n > 0 else 0\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if total_selection_count > 0 else 1.0\n        combined_score = avg_score + exploration_value * exploration_factor\n\n        # Epsilon-greedy exploration\n        if np.random.rand() < epsilon:\n            action_scores.append(np.random.rand())  # Random score for exploration\n        else:\n            action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": 4358.83277641151,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    epsilon = 0.1  # Exploration probability\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        \n        # Calculate average score\n        avg_score = np.mean(scores) if n > 0 else 0.0\n        \n        # Avoid division by zero in exploration calculation\n        selection_count = n if n > 0 else 1\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / selection_count)\n        \n        # Use the epsilon-greedy approach\n        if np.random.rand() < epsilon:\n            action_scores.append(np.random.rand())  # Choose randomly for exploration\n        else:\n            # Combine exploitation and exploration\n            combined_score = avg_score + exploration_value\n            action_scores.append(combined_score)\n\n    # Select the action with the highest score\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 4455.621423196389,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    epsilon = 0.1  # Exploration factor\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Calculate the exploration term\n        selection_count = n if n > 0 else 1\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / selection_count)\n\n        # Epsilon-greedy strategy: explore with probability epsilon\n        if np.random.rand() < epsilon:\n            action_scores.append(np.random.rand())  # Random score for exploration\n        else:\n            # Weighted score: combine exploitation and exploration\n            combined_score = avg_score + exploration_value\n            action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 4550.822656930396,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_weight = 1.0  # Weight for exploration\n    total_actions = len(score_set)\n\n    for action_index in range(total_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Calculate exploration term: higher weight for less explored actions\n        if total_selection_count > 0:\n            exploration_value = exploration_weight * np.sqrt(np.log(total_selection_count) / (n + 1)) if n > 0 else float('inf')\n        else:\n            exploration_value = float('inf')\n        \n        # Combined score: balancing exploitation and exploration\n        combined_score = avg_score + exploration_value\n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 4846.367603989675,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    action_mean_scores = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate mean scores and counts for each action\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        action_mean_scores[action_index] = np.mean(scores) if scores else 0\n\n    # Handle counts to avoid division by zero for actions never selected\n    total_actions_selected = np.clip(action_counts, 1, None)  \n\n    # Calculate exploration bonus using Upper Confidence Bound (UCB)\n    exploration_bonus = np.sqrt(2 * np.log(total_selection_count + 1) / total_actions_selected)\n\n    # Combine scores with consideration for exploration and the time dimension\n    time_weight = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_mean_scores = action_mean_scores * time_weight\n    combined_scores = adjusted_mean_scores + exploration_bonus\n\n    # Select the action with the highest combined score\n    action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": 4869.258178435548,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_exploitation = []\n    action_selection_counts = [len(scores) for scores in score_set.values()]\n    \n    for action_index in range(8):\n        scores = score_set[action_index]\n        avg_score = np.mean(scores) if scores else 0\n        action_exploitation.append(avg_score)\n\n    # Epsilon for exploration\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n    epsilon = 0.1  # Exploration probability\n\n    combined_scores = []\n    for action_index in range(8):\n        selection_count = action_selection_counts[action_index]\n        exploitation_score = action_exploitation[action_index]\n        \n        # Exploration term\n        if selection_count > 0:\n            exploration_score = np.sqrt(np.log(total_selection_count) / selection_count)\n        else:\n            exploration_score = float('inf')  # High score for unselected actions\n            \n        combined_score = (1 - epsilon) * exploitation_score + epsilon * exploration_score\n        combined_scores.append(combined_score)\n\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": 5332.871574076907,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    action_exploitation = np.zeros(num_actions)\n    exploration_scores = np.zeros(num_actions)\n    \n    # Calculate average scores for exploitation\n    for action_index in range(num_actions):\n        scores = score_set[action_index]\n        if scores:\n            action_exploitation[action_index] = np.mean(scores)\n    \n    # Calculate exploration scores\n    for action_index in range(num_actions):\n        selection_count = len(score_set[action_index])\n        exploration_scores[action_index] = (1 / (selection_count + 1)) * ((total_time_slots - current_time_slot) / total_time_slots)\n    \n    # Combine scores\n    combined_scores = action_exploitation + exploration_scores\n    \n    # Select the action with the highest score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": 6418.499939130665,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    \"\"\"\n    Select the most suitable action based on historical scores and selection counts.\n\n    Parameters:\n    score_set (dict): A dictionary with action indices as keys and lists of scores as values.\n    total_selection_count (int): The total number of selections made so far.\n    current_time_slot (int): The current time slot for selection.\n    total_time_slots (int): The total number of time slots available.\n\n    Returns:\n    int: The index of the selected action (0 to 7).\n    \"\"\"\n    action_exploitation = []\n    exploration_factor = (total_time_slots - current_time_slot + 1) / total_time_slots\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n\n        avg_score = np.mean(scores) if n > 0 else 0\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else 1.0\n        \n        combined_score = avg_score + exploration_factor * exploration_value\n        action_exploitation.append(combined_score)\n\n    action_index = np.argmax(action_exploitation)\n\n    return action_index",
          "objective": 6534.0296573127025,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    n_actions_selected = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        n_actions_selected[action_index] = n\n\n        average_score = np.mean(scores) if n > 0 else 0\n        \n        if total_selection_count > 0:\n            exploration_factor = np.sqrt(np.log(total_selection_count) / (n + 1)) if n > 0 else float('inf')\n        else:\n            exploration_factor = float('inf')\n\n        progress_factor = (total_time_slots - current_time_slot) / total_time_slots if total_time_slots > 0 else 0\n\n        action_scores[action_index] = average_score + exploration_factor * progress_factor\n\n    # Incorporating a softmax function for selection probability to promote exploration\n    action_probs = np.exp(action_scores - np.max(action_scores))  # Stability in softmax\n    action_probs /= np.sum(action_probs)\n\n    action_index = np.random.choice(num_actions, p=action_probs)\n\n    return action_index",
          "objective": 6605.602861133517,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Initialize mean scores and counts\n    action_mean_scores = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate mean scores and counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_mean_scores[action_index] = np.mean(scores) if scores else 0.0\n\n    # Define epsilon; decreases over time and ensures exploration\n    epsilon = 0.1 * (1 - (current_time_slot / total_time_slots))\n    \n    # Select action based on epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(num_actions)  # Explore\n    else:\n        # UCB based exploitation\n        exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / (action_counts + 1e-5))\n        combined_scores = action_mean_scores + exploration_bonus\n        action_index = np.argmax(combined_scores)  # Exploit\n    \n    return action_index",
          "objective": 6807.294774797709,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    action_exploitation = np.zeros(num_actions)\n    action_selection_count = np.zeros(num_actions)\n    \n    # Calculate average scores for each action\n    for action_index in range(num_actions):\n        scores = score_set[action_index]\n        selection_count = len(scores)\n        action_selection_count[action_index] = selection_count\n        if selection_count > 0:\n            action_exploitation[action_index] = np.mean(scores)\n    \n    # Implementing Upper Confidence Bound (UCB)\n    total_selections = total_selection_count if total_selection_count > 0 else 1\n    exploration_bonus = np.sqrt(2 * np.log(total_selections) / (action_selection_count + 1e-5))  # avoid division by zero\n    combined_scores = action_exploitation + exploration_bonus\n\n    # Select the action with the highest combined score\n    action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": 7235.695436988503,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    exploration_weight = 1.5  # Exploration vs. exploitation balancing factor\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        \n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Calculate exploration based on historical selection count\n        selection_count = n if n > 0 else 1\n        exploration_value = exploration_weight * np.sqrt(np.log(total_selection_count + 1) / selection_count)\n        \n        # Add a time decay factor to encourage earlier actions\n        time_decay = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_avg_score = avg_score * time_decay\n        \n        # Combine scores\n        combined_score = adjusted_avg_score + exploration_value\n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 8421.156507296324,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    action_exploitation = np.zeros(num_actions)\n    exploration_scores = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set[action_index]\n        selection_count = len(scores)\n        if selection_count > 0:\n            action_exploitation[action_index] = np.mean(scores)\n        else:\n            action_exploitation[action_index] = 0  # No score means exploitation score is 0\n\n    # Calculate exploration based on UCB formula\n    for action_index in range(num_actions):\n        selection_count = len(score_set[action_index])\n        if selection_count > 0:\n            confidence_interval = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_count)\n        else:\n            confidence_interval = np.inf  # Max exploration if no selections have been made\n        \n        exploration_scores[action_index] = confidence_interval\n\n    # Combine scores using UCB approach for better exploration-exploitation balance\n    combined_scores = action_exploitation + exploration_scores\n\n    # Select the action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": 9686.747571303727,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = len(score_set)\n    total_scores = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    for action_index in range(action_count):\n        scores = score_set[action_index]\n        action_counts[action_index] = len(scores)\n        total_scores[action_index] = np.sum(scores)\n\n    average_scores = np.divide(total_scores, action_counts, out=np.zeros_like(total_scores), where=action_counts != 0)\n    \n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1))\n\n    combined_scores = average_scores + exploration_factor * exploration_bonus\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": 10217.215783799553,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_scores = np.zeros(num_actions)\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Exploration factor decreases as more time slots pass\n        exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n        \n        # Use a modified UCB to account for exploration\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (n + 1)) if n > 0 else float('inf')\n        \n        # Combine average score with exploration value\n        action_scores[action_index] = avg_score + exploration_factor * exploration_value\n\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": 10926.704305224745,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    avg_scores = np.array([np.mean(score_set[i]) if score_set[i] else 0 for i in action_indices])\n    selection_counts = np.array([len(score_set[i]) for i in action_indices])\n    \n    # Explore: Calculate exploration weights\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1))  # Avoid division by zero\n    \n    # Combine exploitation and exploration\n    combined_scores = avg_scores + exploration_bonus\n    \n    # Normalize with regard to remaining time slots (to prevent preferential bias towards later time slots)\n    time_factor = (total_time_slots - current_time_slot) / total_time_slots\n    final_scores = combined_scores * time_factor\n    \n    # Select action with the highest score\n    action_index = action_indices[np.argmax(final_scores)]\n    \n    return action_index",
          "objective": 12671.722230939054,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_exploits = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Calculate the mean scores and counts for each action\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        if scores:\n            action_exploits[action_index] = np.mean(scores)\n\n    # Handle the case where total_selection_count is zero for random selection at start\n    if total_selection_count == 0:\n        return np.random.choice(range(action_count))\n\n    # Compute exploration bonus using a modified UCB formula\n    exploration_bonuses = np.sqrt(2 * np.log(total_selection_count + 1) / (action_counts + 1e-5))\n\n    # Calculate combined scores emphasizing exploitation while still encouraging exploration\n    combined_scores = action_exploits + exploration_bonuses\n\n    # Select the action with the highest combined score\n    action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": 13399.339293267692,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Calculate exploration value\n        selection_count = n if n > 0 else 1  # Avoid division by zero\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_count)\n        \n        # Incorporate temporal dynamics for exploration\n        time_factor = 1 - (current_time_slot / total_time_slots)\n\n        # Weighted score: balance exploitation and exploration\n        combined_score = avg_score + exploration_value * time_factor\n        \n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 29364.831612939863,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        avg_score = np.mean(scores) if n > 0 else 0\n        \n        # Handling selection count\n        selection_count = n if n > 0 else 1  # Avoid division by zero\n        \n        # Explore more in the beginning and decrease over time\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_count)\n        \n        # Adjusting for diminishing exploration\n        exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n        \n        # Combined score: Diminishing exploration over time\n        combined_score = avg_score + exploration_weight * exploration_value\n        \n        action_scores.append(combined_score)\n\n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 31324.322983013637,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = len(score_set)\n    action_exploitation = np.zeros(action_count)\n    action_selection_count = np.zeros(action_count)\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        if scores:\n            action_exploitation[action_index] = np.mean(scores)\n            action_selection_count[action_index] = len(scores)\n\n    if total_selection_count == 0:\n        return np.random.randint(action_count)  # Random action on the first run\n\n    # Epsilon value for exploration\n    epsilon = 1.0 - (current_time_slot / total_time_slots)\n\n    # Calculate Upper Confidence Bound (UCB)\n    ucb_values = np.zeros(action_count)\n    \n    for action_index in range(action_count):\n        if action_selection_count[action_index] > 0:\n            ucb_values[action_index] = action_exploitation[action_index] + \\\n                np.sqrt((2 * np.log(total_selection_count)) / action_selection_count[action_index])\n        else:\n            # Encourage exploration of actions that have not been selected\n            ucb_values[action_index] = np.inf\n\n    # Select action using epsilon-greedy approach\n    if np.random.rand() < epsilon:\n        action_index = np.argmax(ucb_values)\n    else:\n        action_index = np.random.randint(action_count)\n\n    return action_index",
          "objective": 34021.864565656826,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize average scores and action counts\n    avg_scores = np.zeros(8)\n    action_counts = np.zeros(8)\n    \n    # Calculate average scores and action counts\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        if action_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n    \n    # If no selections have been made, select randomly\n    if total_selection_count == 0:\n        return np.random.choice(range(8))\n    \n    # Adjust exploration weight based on current time slot\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    \n    # Calculate UCB-based scores\n    ucb_scores = avg_scores + exploration_weight * np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1))\n    \n    # Select the action with the highest UCB score\n    action_index = np.argmax(ucb_scores)\n    \n    return action_index",
          "objective": 41935.64307044307,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_exploits = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        if scores:\n            action_exploits[action_index] = np.mean(scores)\n\n    # Calculate exploration factor: UCB approach\n    exploration_bonuses = np.sqrt((2 * np.log(total_selection_count + 1)) / (action_counts + 1e-5))\n\n    # Incorporating time decay factor\n    relative_time_left = (total_time_slots - current_time_slot) / total_time_slots\n    decay_factor = 1 - relative_time_left\n    combined_scores = action_exploits + decay_factor * exploration_bonuses\n\n    # Handle case where no action has been selected yet\n    if total_selection_count == 0:\n        return np.random.choice(range(action_count))\n\n    # Select the action with the highest combined score\n    action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": 78035.42455715883,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_rate = (total_time_slots - current_time_slot) / total_time_slots\n    action_exploitation = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        n = len(scores)\n        \n        avg_score = np.mean(scores) if n > 0 else 0\n        if total_selection_count > 0:\n            selection_frequency = n / total_selection_count\n            exploration_value = exploration_rate * (1 - selection_frequency)\n        else:\n            exploration_value = 1.0\n        \n        combined_score = avg_score + exploration_value\n        action_exploitation.append(combined_score)\n    \n    action_index = np.argmax(action_exploitation)\n    return action_index",
          "objective": 91156.95905094262,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_exploitation = np.zeros(action_count)\n    action_selection_count = np.zeros(action_count)\n    \n    for action_index, scores in score_set.items():\n        action_selection_count[action_index] = len(scores)\n        if scores:\n            action_exploitation[action_index] = np.mean(scores)\n    \n    # Prevent division by zero\n    action_selection_count = np.maximum(action_selection_count, 1)  # Avoid zero counts\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n\n    # Use UCB for balancing exploration/exploitation\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / action_selection_count)\n    \n    combined_scores = action_exploitation + exploration_factor * exploration_bonus\n    action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": 138521.17818570734,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration rate\n    num_actions = 8\n    \n    # Initialize average scores and counts\n    avg_scores = [0] * num_actions\n    selection_counts = [0] * num_actions\n    \n    # Calculate average scores and counts\n    for action_index, scores in score_set.items():\n        selection_counts[action_index] = len(scores)\n        avg_scores[action_index] = np.mean(scores) if scores else 0.0\n        \n    # Epsilon-greedy strategy for action selection\n    if np.random.rand() < epsilon:\n        # Explore: randomly select one of the actions\n        action_index = np.random.choice(num_actions)\n    else:\n        # Exploit: select the action with the highest average score\n        action_index = np.argmax(avg_scores)\n        \n        # If there's a tie or very close scores, prefer less selected actions\n        max_avg = avg_scores[action_index]\n        max_actions = [i for i in range(num_actions) if avg_scores[i] == max_avg]\n        if len(max_actions) > 1:\n            # Break tie by selecting the one with the least selection count\n            action_index = min(max_actions, key=lambda x: selection_counts[x])\n\n    return action_index",
          "objective": 729285.0127079173,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = len(score_set)\n\n    # Calculate average scores and selection counts\n    avg_scores = np.array([np.mean(scores) if scores else 0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n\n    # Avoid division by zero for selection counts\n    adjusted_counts = selection_counts + 1\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / adjusted_counts)\n\n    # Dynamic exploration factor based on time\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    exploration_scores = exploration_weight * np.ones(action_count)\n\n    # Combine scores for the final selection criteria\n    total_scores = avg_scores + exploration_bonus + exploration_scores\n\n    # Select the action with the maximum score\n    action_index = np.argmax(total_scores)\n\n    return action_index",
          "objective": 1427837.789300804,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    exploration_rate = 0.1  # Exploration rate for epsilon-greedy\n    \n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    # Calculate average scores and counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        avg_scores[action_index] = np.mean(scores) if scores else 0.0\n        \n    # Dynamically adjust exploration rate based on the time slot\n    adaptive_exploration_rate = (total_time_slots - current_time_slot) / total_time_slots\n    \n    # Epsilon-greedy strategy for action selection\n    if np.random.rand() < exploration_rate * adaptive_exploration_rate:\n        # Explore: randomly select one of the actions\n        action_index = np.random.choice(num_actions)\n    else:\n        # Exploit: select the action with the highest average score\n        action_index = np.argmax(avg_scores)\n        \n        # Break ties by selecting the least selected action\n        max_avg = avg_scores[action_index]\n        max_actions = [i for i in range(num_actions) if avg_scores[i] == max_avg]\n        if len(max_actions) > 1:\n            action_index = min(max_actions, key=lambda x: selection_counts[x])\n\n    return action_index",
          "objective": 1488584.7770267392,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    action_exploitation = []\n    \n    for action in actions:\n        scores = score_set[action]\n        avg_score = np.mean(scores) if scores else 0\n        action_exploitation.append(avg_score)\n\n    if total_selection_count == 0:\n        return np.random.choice(actions)  # Randomly select an action\n\n    action_counts = [len(score_set[action]) for action in actions]\n    exploration_bonus = np.sqrt(np.log(total_selection_count) / (np.array(action_counts) + 1))\n\n    combined_scores = np.array(action_exploitation) + exploration_bonus\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": 1636924.764192122,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    averages = np.zeros(num_actions)\n    exploration_bonus = np.zeros(num_actions)\n\n    for action in range(num_actions):\n        if total_selection_count > 0:\n            averages[action] = np.mean(score_set[action]) if score_set[action] else 0.0\n            exploration_bonus[action] = np.sqrt(np.log(total_selection_count) / (len(score_set[action]) + 1)) if score_set[action] else np.sqrt(np.log(total_selection_count))\n    \n    # Weighted scores based on averages and exploration\n    weighted_scores = averages + exploration_bonus\n    \n    # Selection strategy: choose action with maximum score\n    action_index = np.argmax(weighted_scores)\n    \n    return action_index",
          "objective": 6450261.189841424,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n\n    # Initialize average scores and counts\n    avg_scores = np.zeros(num_actions)\n    counts = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        counts[action_index] = len(scores)\n        if counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n\n    # Handle cases where no selections have been made\n    with np.errstate(divide='ignore', invalid='ignore'):\n        beta = np.sqrt(2 * np.log(total_selection_count + 1) / (counts + 1e-5))  # UCB exploration\n\n    # Explore less selected actions and apply a time-aware factor\n    time_factor = (total_time_slots - current_time_slot + 1) / total_time_slots  # +1 to avoid zero division for last time slot\n    exploration_values = beta * time_factor\n\n    # Combine average scores and exploration values\n    combined_scores = avg_scores + exploration_values\n    action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": 10680634.800425349,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8  # Total actions from 0 to 7\n    action_exploitation = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n    \n    # Calculate average scores and counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_exploitation[action_index] = np.mean(scores) if scores else 0\n\n    if total_selection_count == 0:\n        return np.random.choice(range(action_count))\n\n    # Calculate exploration bonus based on UCB\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / (action_counts + 1e-5))\n\n    # Dynamic exploration factor based on the time slots remaining\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n    combined_scores = action_exploitation + exploration_factor * exploration_bonus\n\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": 16456627.607251955,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = len(score_set)\n    action_exploitation = np.zeros(action_count)\n    action_selection_count = np.zeros(action_count)\n\n    for action_index in range(action_count):\n        scores = score_set[action_index]\n        if scores:\n            action_exploitation[action_index] = np.mean(scores)\n            action_selection_count[action_index] = len(scores)\n\n    if total_selection_count == 0:\n        return np.random.randint(action_count)  # Random action on first run\n\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n\n    # Using UCB strategy for balance between exploration and exploitation\n    ucb_scores = np.zeros(action_count)\n    for action_index in range(action_count):\n        if action_selection_count[action_index] > 0:\n            ucb_scores[action_index] = (\n                action_exploitation[action_index] + \n                np.sqrt(2 * np.log(total_selection_count) / action_selection_count[action_index])\n            )\n        else:\n            ucb_scores[action_index] = exploration_factor + np.inf  # Encourage exploration of unselected actions\n\n    action_index = np.argmax(ucb_scores)\n\n    return action_index",
          "objective": 29479620.46882192,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = len(score_set)\n    action_exploitation = np.zeros(action_count)\n    action_selection_counts = np.zeros(action_count)\n\n    for action_index in range(action_count):\n        scores = score_set[action_index]\n        if scores:\n            action_exploitation[action_index] = np.mean(scores)\n            action_selection_counts[action_index] = len(scores)\n\n    # Calculate exploration bonus\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / (action_selection_counts + 1e-5))\n    exploration_bonus[np.isnan(exploration_bonus)] = 0  # Handle division by zero\n\n    # Prioritize actions with higher average scores plus exploration bonus\n    total_scores = action_exploitation + exploration_bonus\n\n    if total_selection_count == 0:\n        return np.random.choice(range(action_count))  # Random selection if no prior selections\n\n    action_index = np.argmax(total_scores)\n    \n    return action_index",
          "objective": 55080902.15559907,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = len(score_set)\n    action_exploitation = np.zeros(action_count)\n    \n    for action_index in range(action_count):\n        scores = score_set[action_index]\n        if scores:\n            action_exploitation[action_index] = np.mean(scores)\n        else:\n            action_exploitation[action_index] = 0\n            \n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n    exploration_scores = np.zeros(action_count)\n    \n    for action_index in range(action_count):\n        selection_count = len(score_set[action_index])\n        exploration_scores[action_index] = (1 / (selection_count + 1)) * exploration_factor\n    \n    combined_scores = action_exploitation + exploration_scores\n    \n    # Apply epsilon-greedy exploration strategy\n    epsilon = 0.1  # 10% exploration chance\n    if np.random.rand() < epsilon and total_selection_count > 0:\n        return np.random.choice(range(action_count))\n    \n    return action_index",
          "objective": 86395885.05774194,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon = 0.1  # Exploration rate\n    \n    # Initialize average scores and counts\n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    # Calculate average scores and counts\n    for action_index, scores in score_set.items():\n        selection_counts[action_index] = len(scores)\n        avg_scores[action_index] = np.mean(scores) if scores else 0.0\n\n    # Normalize the average scores to address selection count imbalance\n    exploration_bonus = (1 / (selection_counts + 1e-5))  # Avoid division by zero\n    normalized_scores = avg_scores * exploration_bonus\n\n    # Epsilon-greedy strategy for action selection\n    if np.random.rand() < epsilon:\n        # Explore: randomly select one of the actions\n        action_index = np.random.choice(num_actions)\n    else:\n        # Exploit: select the action with the highest normalized score\n        action_index = np.argmax(normalized_scores)\n\n    return action_index",
          "objective": 98346747.74361567,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        selection_counts[action_index] = selection_count\n        \n        if selection_count > 0:\n            avg_scores[action_index] = np.mean(scores)\n\n    # Avoid division by zero\n    epsilon = 1e-10\n    exploration_scores = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + epsilon))\n\n    # Calculate combined scores for exploration and exploitation\n    combined_scores = avg_scores + exploration_scores\n    \n    # Adjust for exploration at the beginning of the time slots\n    exploration_adjustment = (total_time_slots - current_time_slot) / total_time_slots\n    combined_scores *= exploration_adjustment\n\n    # Select the action with the highest score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": 115693680.18274371,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = len(score_set)\n    action_exploitation = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        if scores:\n            action_exploitation[action_index] = np.mean(scores)\n            selection_counts[action_index] = len(scores)\n\n    if total_selection_count == 0:\n        return np.random.choice(action_count)  # Random selection if no actions have been taken\n    \n    # Calculate exploration bonus using UCB\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n    combined_scores = action_exploitation + exploration_bonus\n\n    action_index = np.argmax(combined_scores)\n    return action_index",
          "objective": 120931032.10443497,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    action_exploitation = np.zeros(num_actions)\n    action_selection_counts = np.zeros(num_actions)\n    \n    for action_index, scores in score_set.items():\n        action_selection_counts[action_index] = len(scores)\n        if action_selection_counts[action_index] > 0:\n            action_exploitation[action_index] = np.mean(scores)\n        else:\n            action_exploitation[action_index] = 0  # Default to 0 if no previous scores\n\n    # Calculate exploration based on selection counts and remaining time slots\n    exploration_scores = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        if total_selection_count > 0:\n            exploration_scores[action_index] = 1 / (action_selection_counts[action_index] + 1)\n        else:\n            exploration_scores[action_index] = 1  # Maximum exploration if no actions were selected\n        \n        # Adjust exploration based on the remaining time slots\n        exploration_scores[action_index] *= (total_time_slots - current_time_slot + 1) / total_time_slots\n\n    # Combine exploitation and exploration with softmax normalization for better stability\n    combined_scores = action_exploitation + exploration_scores\n    softmax_scores = np.exp(combined_scores - np.max(combined_scores))  # Stability in softmax\n    softmax_scores /= np.sum(softmax_scores)\n\n    # Select an action based on softmax probabilities\n    action_index = np.random.choice(num_actions, p=softmax_scores)\n    \n    return action_index",
          "objective": 128455908.0088616,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1\n    exploration = np.random.rand() < epsilon\n    \n    avg_scores = []\n    \n    for action_index in range(8):\n        scores = score_set[action_index]\n        if len(scores) > 0:\n            avg_score = np.mean(scores)\n        else:\n            avg_score = 0  # Handle case where action has never been selected\n        avg_scores.append(avg_score)\n    \n    if exploration:\n        action_index = np.random.choice(range(8))\n    else:\n        action_index = np.argmax(avg_scores)\n\n    return action_index",
          "objective": 140959715.0218288,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    \n    # Calculate average scores and selection counts\n    avg_scores = np.array([np.mean(score_set[i]) if score_set[i] else 0 for i in action_indices])\n    selection_counts = np.array([len(score_set[i]) for i in action_indices])\n    \n    # Calculate exploration weights using the UCB formula\n    exploration_bonus = np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1e-6))  # Adding a small value to avoid division by zero\n    \n    # Combine exploitation and exploration scores\n    combined_scores = avg_scores + exploration_bonus\n    \n    # Normalize scores based on remaining time slots\n    time_factor = (total_time_slots - current_time_slot) / total_time_slots\n    final_scores = combined_scores * time_factor\n    \n    # Select action with the highest final score\n    action_index = action_indices[np.argmax(final_scores)]\n    \n    return action_index",
          "objective": 252705860.6536679,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = len(score_set)\n    \n    avg_scores = np.array([\n        np.mean(scores) if scores else 0 for scores in score_set.values()\n    ])\n    \n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Use an epsilon-greedy strategy with adaptive exploration\n    epsilon = max(1 - (current_time_slot / total_time_slots), 0.1)  # Decaying epsilon\n    exploration_bonus = np.random.rand(action_count) < epsilon\n    \n    effective_counts = selection_counts + 1\n    confidence_intervals = np.sqrt(2 * np.log(total_selection_count + 1) / effective_counts)\n    \n    total_scores = avg_scores + confidence_intervals + exploration_bonus.astype(float)\n    \n    action_index = np.argmax(total_scores)\n    \n    return action_index",
          "objective": 281050088.8672647,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = len(score_set)\n    \n    avg_scores = np.array([\n        np.mean(scores) if scores else 0 for scores in score_set.values()\n    ])\n    \n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Avoid division by zero\n    effective_counts = selection_counts + 1\n    bonus = np.sqrt(np.log(total_selection_count + 1) / effective_counts)\n    \n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n    exploration_bonus = exploration_factor * np.ones(action_count)\n    \n    total_scores = avg_scores + bonus + exploration_bonus\n    \n    action_index = np.argmax(total_scores)\n    \n    return action_index",
          "objective": 312849093.52404314,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = len(score_set)\n    action_exploitation = []\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n    \n    # Calculate average scores and prepare exploration scores\n    for action_index in range(action_count):\n        scores = score_set[action_index]\n        avg_score = np.mean(scores) if scores else 0\n        action_exploitation.append(avg_score)\n\n    # Epsilon for exploration (adaptive)\n    epsilon = max(0.1, 0.5 * (total_time_slots - current_time_slot) / total_time_slots)\n\n    # Calculate combined scores for exploitation and exploration\n    combined_scores = []\n    for action_index in range(action_count):\n        avg_score = action_exploitation[action_index]\n        n_selections = len(score_set[action_index])\n        exploration_score = (1 / (n_selections + 1)) * exploration_factor if n_selections > 0 else exploration_factor\n        combined_score = avg_score + exploration_score * (1 - epsilon)\n        combined_scores.append(combined_score)\n\n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(range(action_count))\n    else:\n        action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": 320797361.5781255,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_scores = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_scores[action_index] = np.mean(scores) if scores else 0\n        selection_counts[action_index] = len(scores)\n\n    if total_selection_count == 0:\n        return np.random.choice(range(action_count))  # Randomly select an action initially\n\n    # Calculate UCB scores\n    ucb_scores = np.zeros(action_count)\n    for action_index in range(action_count):\n        if selection_counts[action_index] > 0:\n            ucb_scores[action_index] = (action_scores[action_index] +\n                                         np.sqrt(np.log(total_selection_count) / selection_counts[action_index]))\n        else:\n            ucb_scores[action_index] = float('inf')  # Prioritize unselected actions\n\n    # Selection of action based on UCB scores\n    action_index = np.argmax(ucb_scores)\n\n    return action_index",
          "objective": 340368548.35890126,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))  # Decaying exploration probability\n    exploration = np.random.rand() < epsilon\n    \n    if exploration:\n        action_index = np.random.randint(0, 8)  # Random action\n    else:\n        avg_scores = []\n        \n        for i in range(8):\n            if len(score_set[i]) > 0:\n                avg_score = np.mean(score_set[i])\n            else:\n                avg_score = 0  # Default to zero if the action has no scores\n            \n            avg_scores.append(avg_score)\n        \n        action_index = np.argmax(avg_scores)  # Select action with the highest average score\n        \n    return action_index",
          "objective": 484885344.4234661,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_exploitation = []\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        avg_score = np.mean(scores) if scores else 0\n        action_exploitation.append(avg_score)\n    \n    if total_selection_count == 0:\n        return np.random.choice(range(8))  # Randomly select an action if no selections have been made yet\n    \n    ucb_scores = []\n    for action_index in range(8):\n        n_selected = len(score_set.get(action_index, []))\n        if n_selected > 0:\n            ucb_value = (action_exploitation[action_index] +\n                         np.sqrt(np.log(total_selection_count) / n_selected))\n        else:\n            ucb_value = float('inf')  # Explore unselected actions\n        ucb_scores.append(ucb_value)\n    \n    action_index = np.argmax(ucb_scores)\n    \n    return action_index",
          "objective": 548535207.8564734,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    action_exploitation = np.zeros(num_actions)\n    action_selection_counts = np.zeros(num_actions)\n    \n    # Calculate average scores for exploitation and selection counts\n    for action_index in range(num_actions):\n        scores = score_set[action_index]\n        action_selection_counts[action_index] = len(scores)\n        if scores:\n            action_exploitation[action_index] = np.mean(scores)\n    \n    # Epsilon value for exploration (could be set based on application)\n    epsilon = 0.1\n    \n    # Calculate exploration bonus based on selection frequency\n    action_exploration = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        if total_selection_count > 0:\n            action_exploration[action_index] = (total_time_slots - current_time_slot) / (action_selection_counts[action_index] + 1)\n    \n    # Determine combined scores with exploration strategy\n    combined_scores = (1 - epsilon) * action_exploitation + epsilon * action_exploration\n    \n    # Select action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": 558648149.0951966,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    action_exploitation = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n    \n    # Compute average scores and selection counts\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        if action_counts[action_index] > 0:\n            action_exploitation[action_index] = np.mean(scores)\n    \n    # Calculate exploration scores using UCB\n    exploration_scores = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        if action_counts[action_index] > 0:\n            exploration_scores[action_index] = np.sqrt((2 * np.log(total_selection_count)) / action_counts[action_index])\n        else:\n            # Assign a high score to actions that have never been selected\n            exploration_scores[action_index] = float('inf')\n    \n    # Combine exploitation and exploration scores\n    combined_scores = action_exploitation + exploration_scores\n    \n    # Select action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": 570538502.533166,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    exploration_rate = 0.1  # Epsilon for exploration\n\n    # Initialize average scores and counts\n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and counts\n    for action_index, scores in score_set.items():\n        selection_counts[action_index] = len(scores)\n        avg_scores[action_index] = np.mean(scores) if scores else 0.0\n\n    # Upper Confidence Bound (UCB) calculation\n    ucb_values = np.zeros(num_actions)\n    for i in range(num_actions):\n        if selection_counts[i] == 0:\n            ucb_values[i] = float('inf')  # Assign infinite value to unselected actions\n        else:\n            confidence_interval = np.sqrt((2 * np.log(total_selection_count)) / selection_counts[i])\n            ucb_values[i] = avg_scores[i] + confidence_interval\n    \n    # Combine exploration and exploitation\n    if np.random.rand() < exploration_rate:\n        action_index = np.random.choice(num_actions)\n    else:\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 648690668.2078813,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon = 0.1  # Exploration rate\n    confidence_level = 1.96  # For 95% confidence intervals\n    \n    # Initialize average scores and counts\n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    # Calculate average scores and counts\n    for action_index, scores in score_set.items():\n        selection_counts[action_index] = len(scores)\n        avg_scores[action_index] = np.mean(scores) if scores else 0.0\n        \n    # Implement Upper Confidence Bound (UCB) strategy\n    if total_selection_count == 0:\n        # If no actions have been selected yet, explore all actions equally\n        action_index = np.random.choice(num_actions)\n    else:\n        ucb_values = np.zeros(num_actions)\n        for i in range(num_actions):\n            if selection_counts[i] > 0:\n                ucb_values[i] = avg_scores[i] + confidence_level * np.sqrt(np.log(total_selection_count) / selection_counts[i])\n            else:\n                # Assign a high value for unselected actions to encourage exploration\n                ucb_values[i] = np.inf\n        \n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 800642440.5112627,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        if len(scores) > 0:\n            average_score = np.mean(scores)\n        else:\n            average_score = 0\n            \n        exploration_bonus = 1 / (1 + len(scores))  # Attracts exploration for less selected actions\n        action_scores.append(average_score + exploration_bonus)\n\n    # Normalize scores by total selection count to prevent domination of frequently selected actions\n    normalized_scores = [(score / (1 + total_selection_count)) for score in action_scores]\n    \n    # Select action based on normalized scores\n    action_index = np.argmax(normalized_scores)\n    \n    return action_index",
          "objective": 1033918253.1730497,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration rate\n    num_actions = 8\n    \n    # Initialize average scores and counts\n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    # Calculate average scores and counts\n    for action_index, scores in score_set.items():\n        selection_counts[action_index] = len(scores)\n        avg_scores[action_index] = np.mean(scores) if scores else 0.0\n        \n    # Epsilon-greedy strategy for action selection\n    if np.random.rand() < epsilon:\n        # Explore: randomly select one of the actions\n        action_index = np.random.choice(num_actions)\n    else:\n        # Exploit: Upper Confidence Bound (UCB)\n        total_count = total_selection_count if total_selection_count > 0 else 1\n        ucb_values = avg_scores + np.sqrt((2 * np.log(total_count)) / (selection_counts + 1e-5))\n        \n        # Select action with the highest UCB value\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 1059916393.4430587,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = len(score_set)\n\n    # Calculate average scores and selection counts\n    avg_scores = np.array([np.mean(scores) if scores else 0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n\n    # Handle case where no selections have been made\n    if total_selection_count == 0:\n        return np.random.choice(range(action_count))  # Random action selection on first run\n    \n    # Compute exploration weight\n    exploration_weights = (1 + selection_counts) / (total_selection_count + 1)\n    exploration_scores = np.sqrt(np.log(total_selection_count + 1) / (1 + selection_counts))\n    \n    # Calculate combined scores\n    combined_scores = avg_scores / exploration_weights + exploration_scores\n    \n    # Select action based on maximized combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": 1075411187.1605823,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize variables to store scores and selection counts\n    action_scores = []\n    action_counts = []\n    \n    # Calculate scores and counts for each action\n    for action in range(8):\n        scores = score_set.get(action, [])\n        action_score = np.mean(scores) if scores else 0.0\n        action_count = len(scores)\n        \n        action_scores.append(action_score)\n        action_counts.append(action_count)\n\n    # Calculate UCB (Upper Confidence Bound) for each action\n    ucb_values = []\n    for action in range(8):\n        if action_counts[action] == 0:\n            ucb = float('inf')  # Encourage exploration for actions not yet selected\n        else:\n            ucb = action_scores[action] + np.sqrt(2 * np.log(total_selection_count) / action_counts[action])\n        \n        ucb_values.append(ucb)\n    \n    # Select the action with the highest UCB value\n    action_index = np.argmax(ucb_values)\n    \n    return action_index",
          "objective": 1616841992.7171366,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon = 0.1  # Exploration rate\n    \n    # Initialize average scores and counts\n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    # Collect average scores and counts for each action\n    for action_index, scores in score_set.items():\n        selection_counts[action_index] = len(scores)\n        avg_scores[action_index] = np.mean(scores) if scores else 0.0\n\n    # Calculate exploration bonus based on selection counts\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))  # UCB\n    normalized_scores = avg_scores + exploration_bonus\n\n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(num_actions)\n    else:\n        action_index = np.argmax(normalized_scores)\n\n    return action_index",
          "objective": 1761266849.1925714,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration factor\n    num_actions = 8  # We have indices from 0 to 7\n    \n    # Calculate average scores and selection counts\n    action_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    for action_index, scores in score_set.items():\n        if scores:\n            action_scores[action_index] = np.mean(scores)\n            selection_counts[action_index] = len(scores)\n    \n    # Apply epsilon-greedy strategy with exploration\n    if np.random.rand() < epsilon or total_selection_count == 0:\n        action_index = np.random.randint(num_actions)  # Explore random action\n    else:\n        # Calculate UCB\n        ucb_values = action_scores + np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n        action_index = np.argmax(ucb_values)  # Select action with highest UCB score\n    \n    return action_index",
          "objective": 1853580732.5939722,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon = 0.1  # Exploration rate\n    \n    # Initialize average scores and counts\n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    # Calculate average scores and counts\n    for action_index, scores in score_set.items():\n        selection_counts[action_index] = len(scores)\n        avg_scores[action_index] = np.mean(scores) if scores else 0.0\n\n    # Avoid division by zero for exploration bonus\n    selection_counts[selection_counts == 0] = 1e-5\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / selection_counts)\n    \n    # Compute scores for selection\n    normalized_scores = avg_scores + exploration_bonus\n\n    # Epsilon-greedy strategy for action selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(num_actions)\n    else:\n        action_index = np.argmax(normalized_scores)\n\n    return action_index",
          "objective": 1867585705.447839,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_weight = 0.1  # weight influencing exploration\n    average_scores = []\n    \n    for action_index in range(8):\n        if total_selection_count == 0 or action_index not in score_set:\n            average_scores.append(0.5)  # Arbitrary score for unselected actions\n        else:\n            avg_score = np.mean(score_set[action_index]) if score_set[action_index] else 0\n            exploration_factor = exploration_weight * (1 / (len(score_set[action_index]) + 1))\n            adjusted_score = avg_score + exploration_factor\n            average_scores.append(adjusted_score)\n    \n    probabilities = np.exp(average_scores) / np.sum(np.exp(average_scores))\n    action_index = np.random.choice(range(8), p=probabilities)\n    \n    return action_index",
          "objective": 2381199146.3498955,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_factor = 0.1\n    exploration = np.random.rand() < exploration_factor\n    \n    avg_scores = []\n    action_counts = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        if len(scores) > 0:\n            avg_score = np.mean(scores)\n        else:\n            avg_score = 0\n        \n        action_count = len(scores)\n        avg_scores.append(avg_score)\n        action_counts.append(action_count)\n    \n    if exploration:\n        action_index = np.random.choice(range(8))\n    else:\n        ucb_values = [\n            avg_scores[i] + np.sqrt((2 * np.log(total_selection_count + 1)) / (action_counts[i] + 1)) if action_counts[i] > 0 else float('inf')\n            for i in range(8)\n        ]\n        action_index = np.argmax(ucb_values)\n    \n    return action_index",
          "objective": 2736842071.482927,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_exploitation = []\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        avg_score = np.mean(scores) if scores else 0\n        action_exploitation.append(avg_score)\n\n    if total_selection_count == 0:\n        return np.random.choice(range(num_actions))  # Randomly select an action if no selections have been made yet\n\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n    exploration_scores = [(1 / (len(score_set.get(action_index, [])) + 1)) * exploration_factor for action_index in range(num_actions)]\n    \n    combined_scores = np.array(action_exploitation) + np.array(exploration_scores)\n\n    # Epsilon-greedy strategy\n    epsilon = 0.1  # Exploration probability\n    if np.random.rand() < epsilon:\n        return np.random.choice(range(num_actions))  # Random selection\n    else:\n        return action_index",
          "objective": 3225747657.220161,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))  # Decreasing epsilon over time\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        average_score = np.mean(scores) if scores else 0\n\n        action_scores.append(average_score)\n\n    # Determine exploration vs exploitation\n    if np.random.rand() < epsilon:\n        # Exploration: Select a random action\n        action_index = np.random.choice([i for i in range(8) if i not in score_set or len(score_set[i]) < total_selection_count / 8] or [np.random.randint(8)])\n    else:\n        # Exploitation: Select action with the highest average score\n        action_index = np.argmax(action_scores)\n\n    return action_index",
          "objective": 4718585928.667792,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = len(score_set)\n    \n    # Calculate average scores and selection counts\n    avg_scores = np.array([np.mean(scores) if scores else 0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Avoid division by zero for selection counts\n    adjusted_counts = selection_counts + 1\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / adjusted_counts)\n    \n    # Dynamic exploration factor based on time\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    exploration_scores = exploration_weight * np.ones(action_count)\n\n    # Combine scores for the final selection criteria using UCB-like strategy\n    total_scores = avg_scores + exploration_bonus + exploration_scores\n    \n    # Select the action with the maximum score\n    action_index = np.argmax(total_scores)\n\n    return action_index",
          "objective": 8586505030.396876,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_exploitation = []\n    action_counts = []\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_count = len(scores)\n        action_counts.append(action_count)\n        \n        avg_score = np.mean(scores) if action_count > 0 else 0\n        action_exploitation.append(avg_score)\n\n    if total_selection_count == 0 or current_time_slot == 0:\n        return np.random.choice(range(8))  # Random selection for the first time slots\n\n    # Epsilon-greedy factor\n    epsilon = 0.1\n    exploration_probability = np.random.rand()\n\n    if exploration_probability < epsilon:\n        return np.random.choice(range(8))  # Explore: random selection\n\n    # UCB calculation\n    total_scores = np.array(action_exploitation)\n    action_counts_np = np.array(action_counts)\n    ucb_values = total_scores + np.sqrt((2 * np.log(total_selection_count + 1)) / (action_counts_np + 1e-5))\n\n    action_index = np.argmax(ucb_values)\n    \n    return action_index",
          "objective": 8647060591.378532,
          "other_inf": null
     }
]