[
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n    \n    # Calculate historical means and counts for each action\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Recent performance evaluation focusing on the last few scores\n    recent_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        recent_scores = score_set.get(action_index, [])[-5:]  # Last 5 scores\n        recent_means[action_index] = np.mean(recent_scores) if recent_scores else 0\n\n    # Weighting parameters\n    recency_weight = np.clip(current_time_slot / (total_time_slots * 0.5), 0, 1)\n    historical_weight = 1 - recency_weight\n\n    # Combine historical and recent performance\n    combined_scores = (recency_weight * recent_means) + (historical_weight * action_means)\n\n    # Exploration bonus using improved UCB method\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n\n    # Final scores integration\n    final_scores = combined_scores + exploration_bonus\n\n    # Epsilon-greedy mechanism to promote exploration\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.05)\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(np.arange(action_count))\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -449.93713699522095,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Normalization of action scores\n    score_min = np.min(action_means) if action_counts.sum() > 0 else 0\n    score_range = np.max(action_means) - score_min if action_counts.sum() > 0 else 1\n    \n    normalized_scores = (action_means - score_min) / (score_range + 1e-5)\n    \n    # Focus on recent performance\n    recent_bonus = action_means * (1 + (current_time_slot / (total_time_slots + 1)))\n    \n    # Calculate exploration factor\n    exploration_factor = (np.log(total_selection_count + 1) / (action_counts + 1e-5)) if total_selection_count > 0 else np.ones(action_count)\n\n    # Composite score calculation\n    composite_scores = recent_bonus + exploration_factor\n\n    # Adaptive exploration probability\n    exploration_strength = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.05)\n    \n    # Stochastic selection mechanism\n    if np.random.rand() < exploration_strength:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(composite_scores)\n\n    return action_index",
          "objective": -449.9367789958304,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Calculate action means and counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if scores else 0\n\n    # Recent performance weighting\n    decay_factor = 1 + (current_time_slot / total_time_slots)\n    recent_weighted_means = action_means * decay_factor\n\n    # Exploration bonus based on selection frequency and variance\n    action_variances = np.array([\n        np.var(scores) if scores else 0 for scores in score_set.values()\n    ])\n    \n    exploration_bonus = (np.log(total_selection_count + 1) / (action_counts + 1e-5)) * (1 + action_variances)\n\n    # Composite score calculation\n    composite_scores = recent_weighted_means + exploration_bonus\n\n    # Dynamic exploration factor\n    exploration_strength = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.05)\n\n    # Epsilon-greedy selection\n    if np.random.rand() < exploration_strength:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(composite_scores)\n\n    return action_index",
          "objective": -449.9358061675116,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    recent_weight = 0.7\n    recent_scores = np.zeros(action_count)\n\n    for action_index in range(action_count):\n        recent_scores[action_index] = np.mean(score_set.get(action_index, [])[-5:]) if action_counts[action_index] > 0 else 0\n\n    combined_scores = (recent_weight * recent_scores) + ((1 - recent_weight) * action_means)\n\n    exploration_bonus = (np.log(total_selection_count + 1) / (action_counts + 1e-5)) * (1 - np.clip(current_time_slot / total_time_slots, 0, 1))\n    \n    final_scores = combined_scores + exploration_bonus\n\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n    \n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -449.9354445487542,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Calculate means and counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Recent performance consideration\n    recent_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        recent_scores = score_set.get(action_index, [])[-10:]  # Last 10 scores\n        recent_means[action_index] = np.mean(recent_scores) if recent_scores else 0\n\n    # Dynamic exploration-exploitation weight\n    exploration_constant = np.log(total_selection_count + 1) / (action_counts + 1e-5)\n    time_weight = (current_time_slot / total_time_slots) ** 2  # Encouraging exploring early\n    historical_weight = 1 - time_weight\n\n    # Composite score calculation\n    adjusted_scores = (time_weight * recent_means + historical_weight * action_means) + exploration_constant\n\n    # Probabilistic selection strategy with balanced exploration\n    exploration_factor = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n    score_adjusted = adjusted_scores + exploration_factor\n\n    # Epsilon-greedy exploration\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(score_adjusted)\n\n    return action_index",
          "objective": -449.934702496242,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n    \n    # Calculate action means and counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if scores else 0\n\n    # Emphasize recent performance with weights\n    recent_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        recent_scores = score_set.get(action_index, [])[-5:]\n        recent_means[action_index] = np.mean(recent_scores) if recent_scores else 0\n\n    # Calculate weights for historical and recent performances\n    recency_weight = min(1, current_time_slot / total_time_slots)\n    historical_weight = 1 - recency_weight\n\n    # Combine weighted scores\n    weighted_scores = (recency_weight * recent_means) + (historical_weight * action_means)\n\n    # Exploration bonus\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n\n    # Final scores for actions\n    final_scores = weighted_scores + exploration_bonus\n\n    # Epsilon-greedy strategy for exploration\n    epsilon = max(0.1, (1 - (current_time_slot / total_time_slots)))  \n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(np.arange(action_count))\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -449.9343816747005,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Calculate action means and counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if scores else 0\n\n    # Calculate recent performance means with a different weight\n    recent_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        recent_scores = score_set.get(action_index, [])[-5:]\n        recent_means[action_index] = np.mean(recent_scores) if recent_scores else 0\n\n    # Weighted scores combining historical and recent performance\n    beta_recent = 0.75  # Weight for recent performance\n    beta_historical = 1 - beta_recent\n    weighted_scores = (beta_recent * recent_means) + (beta_historical * action_means)\n\n    # Exploration bonus based on selection count\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n    \n    # Adjust final scores\n    final_scores = weighted_scores + exploration_bonus\n\n    # Epsilon-greedy strategy but with a dynamic epsilon\n    epsilon = max(0.1, (1 - (current_time_slot / total_time_slots) * 0.9))  \n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(np.arange(action_count))\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -449.9315018867688,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Calculate means and counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Recent mean calculation\n    recent_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        recent_scores = score_set.get(action_index, [])[-10:]\n        recent_means[action_index] = np.mean(recent_scores) if recent_scores else 0\n\n    # Dynamic weights\n    exploration_constant = np.log(total_selection_count + 1) / (action_counts + 1e-5)\n    recency_weight = current_time_slot / total_time_slots\n    historical_weight = 1 - recency_weight\n\n    # Composite score calculation\n    adjusted_scores = (recency_weight * recent_means + historical_weight * action_means) + exploration_constant\n\n    # Epsilon-greedy exploration\n    epsilon_decay = 0.1 * (1 - (current_time_slot / total_time_slots))\n    epsilon = max(epsilon_decay, 0.01)\n\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": -449.9259345399669,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Calculate means and counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Recent performance consideration\n    recent_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        recent_scores = score_set.get(action_index, [])[-10:]  # Last 10 scores\n        recent_means[action_index] = np.mean(recent_scores) if recent_scores else 0\n\n    # Dynamic exploration-exploitation weight\n    exploration_constant = np.log(total_selection_count + 1) / (action_counts + 1e-5)\n    time_weight = (current_time_slot / total_time_slots) ** 2  # Encouraging exploring early\n    historical_weight = 1 - time_weight\n\n    # Composite score calculation\n    adjusted_scores = (time_weight * recent_means + historical_weight * action_means) + exploration_constant\n\n    # Epsilon-greedy strategy for exploration\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n    \n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": -449.9255083982035,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    recent_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        recent_scores = score_set.get(action_index, [])[-10:]\n        recent_means[action_index] = np.mean(recent_scores) if recent_scores else 0\n\n    recency_weight = np.clip(current_time_slot / total_time_slots, 0.0, 1.0)\n    historical_weight = 1 - recency_weight\n\n    weighted_scores = recency_weight * recent_means + historical_weight * action_means\n    exploration_bonus = np.log(total_selection_count + 1) / (action_counts + 1e-5)\n\n    final_scores = weighted_scores + exploration_bonus\n\n    epsilon_decay = 0.1 * (1 - (current_time_slot / total_time_slots))\n    epsilon = max(epsilon_decay, 0.01)\n\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -449.91833456455134,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n    \n    # Calculate historical means and counts for each action\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Recent performance evaluation focusing on the last few scores\n    recent_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        recent_scores = score_set.get(action_index, [])[-5:]  # Last 5 scores\n        recent_means[action_index] = np.mean(recent_scores) if recent_scores else 0\n\n    # Combine historical and recent performance with a focus on recency\n    recency_weight = np.clip(current_time_slot / (total_time_slots * 0.5), 0, 1)\n    historical_weight = 1 - recency_weight\n    combined_scores = (recency_weight * recent_means) + (historical_weight * action_means)\n\n    # Exploration bonus using UCB method\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n    \n    # Final scores integration\n    final_scores = combined_scores + exploration_bonus\n\n    # Epsilon-greedy mechanism to encourage exploration\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.05)\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(np.arange(action_count))\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -449.9182734624449,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n    \n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if scores else 0\n\n    # Recent performance adjustment\n    recent_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        recent_scores = score_set.get(action_index, [])[-5:]\n        recent_means[action_index] = np.mean(recent_scores) if recent_scores else 0\n    \n    # Dynamic weights based on time slot\n    recency_weight = min(1, current_time_slot / (total_time_slots * 0.5))\n    historical_weight = 1 - recency_weight\n\n    # Weighted scores\n    weighted_scores = (recency_weight * recent_means) + (historical_weight * action_means)\n\n    # Exploration bonus\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n\n    # Final scores\n    final_scores = weighted_scores + exploration_bonus\n\n    # Epsilon-greedy selection\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.05)\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(np.arange(action_count))\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -449.91634865158437,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n    \n    # Calculate action means and counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if scores else 0\n\n    # Emphasize recent performance with weights\n    recent_scores = np.zeros(action_count)\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        recent_scores[action_index] = np.mean(scores[-5:]) if len(scores) > 5 else np.mean(scores) if scores else 0\n\n    # Normalize counts to avoid division by zero\n    normalized_counts = action_counts / (total_selection_count + 1e-5)\n\n    # Combine historical and recent performances\n    recent_weight = min(1.0, current_time_slot / total_time_slots)\n    historical_weight = 1 - recent_weight\n    combined_scores = (historical_weight * action_means) + (recent_weight * recent_scores)\n\n    # Exploration bonus\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1 + 1e-5) / (action_counts + 1e-5))\n\n    # Final scores\n    final_scores = combined_scores + exploration_bonus\n\n    # Epsilon-greedy strategy for exploration\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(np.arange(action_count))\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -449.9153800844529,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Calculate action means and counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Normalize scores to prevent bias from highly selected actions if count > 0\n    normalized_means = action_means / (action_counts + 1e-5)\n\n    # Recent performance evaluation (consider last 5 scores)\n    recent_scores = np.zeros(action_count)\n    for action_index in range(action_count):\n        total_scores = score_set.get(action_index, [])\n        recent_weights = total_scores[-5:] if len(total_scores) > 5 else total_scores\n        recent_scores[action_index] = np.mean(recent_weights) if recent_weights else 0\n\n    # Balance historical and recent scores\n    recent_weight = current_time_slot / total_time_slots\n    historical_weight = 1 - recent_weight\n    weighted_scores = (recent_weight * recent_scores) + (historical_weight * normalized_means)\n\n    # Exploration bonus\n    exploration_bonus = np.log(total_selection_count + 1) / (action_counts + 1e-5)\n\n    # Combine scores and exploration components\n    final_scores = weighted_scores + exploration_bonus\n\n    # Epsilon-greedy action selection\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -449.9137645422438,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Calculate means and counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Normalize means to account for action counts\n    normalized_means = action_means / (action_counts + 1)  # Adding 1 to avoid division by zero\n\n    # Calculate recent performance (last 5 scores)\n    recent_scores = np.array([\n        np.mean(score_set.get(action_index, [])[-5:]) if len(score_set.get(action_index, [])) > 0 else 0\n        for action_index in range(action_count)\n    ])\n\n    # Exploration bonus to encourage diversity\n    exploration_bonus = (np.log(total_selection_count + 1) + 1) / (action_counts + 1)\n\n    # Dynamic weighting between recent and historical performance\n    dynamic_weight = np.clip(current_time_slot / total_time_slots, 0.2, 0.8)\n    weighted_scores = (dynamic_weight * recent_scores) + ((1 - dynamic_weight) * normalized_means)\n\n    # Final scores with exploration component\n    final_scores = weighted_scores + exploration_bonus\n\n    # Epsilon for exploration-exploitation\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.05)\n    \n    # Select action index\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -449.9131357166484,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Recent performance focus using the last 3 scores\n    recent_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        recent_scores = score_set.get(action_index, [])[-3:]\n        recent_means[action_index] = np.mean(recent_scores) if recent_scores else 0\n\n    # Enhanced weighting\n    recency_weight = 1 - (current_time_slot / total_time_slots)\n    weighted_scores = recency_weight * action_means + (1 - recency_weight) * recent_means\n\n    # Exploration bonus\n    exploration_bonus = np.log(total_selection_count + 1) / (action_counts + 1e-5)\n    \n    final_scores = weighted_scores + exploration_bonus\n\n    # Epsilon-greedy policy for action selection\n    epsilon = 0.1 * (1 - (current_time_slot / total_time_slots))\n    epsilon = max(epsilon, 0.05)  # minimum exploration rate\n\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -449.9129886231535,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n    \n    # Calculate historical means and counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Calculate recent means (last 10 scores)\n    recent_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        recent_scores = score_set.get(action_index, [])[-10:]\n        recent_means[action_index] = np.mean(recent_scores) if recent_scores else 0\n\n    # Define weights for recency and historical data\n    recency_weight = min(1, current_time_slot / total_time_slots)\n    historical_weight = 1 - recency_weight\n\n    # Compute the adjusted scores\n    weighted_scores = (recency_weight * recent_means) + (historical_weight * action_means)\n\n    # Exploration bonus\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n    final_scores = weighted_scores + exploration_bonus\n\n    # Epsilon-greedy strategy for action selection\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n    \n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -449.9128099223177,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Calculate historical performance and counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Prevent division by zero and normalizing\n    normalized_means = action_means / (action_counts + 1e-5)\n\n    # Recent performance using the last few scores\n    recent_scores = np.array([\n        np.mean(score_set.get(action_index, [])[-10:]) if action_counts[action_index] > 0 else 0\n        for action_index in range(action_count)\n    ])\n\n    # Weighting based on time slot progression\n    recent_weight = current_time_slot / total_time_slots\n    historical_weight = 1 - recent_weight\n    weighted_scores = (recent_weight * recent_scores) + (historical_weight * normalized_means)\n\n    # Exploration bonus\n    exploration_bonus = (np.log(total_selection_count + 1 + 1e-5) + 1) / (action_counts + 1)\n\n    # Final scores combining weighted and exploration factors\n    final_scores = weighted_scores + exploration_bonus\n\n    # Epsilon for exploration-exploitation\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -449.9058875539584,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n    \n    # Calculate historical means and counts for each action\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Recent performance evaluation\n    recent_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        recent_scores = score_set.get(action_index, [])[-5:]  # Last 5 scores\n        recent_means[action_index] = np.mean(recent_scores) if recent_scores else 0\n\n    # Weight adjustment factors\n    recency_weight = min(1, current_time_slot / (total_time_slots * 0.5))\n    historical_weight = 1 - recency_weight\n\n    # Calculate combined scores\n    weighted_scores = (recency_weight * recent_means) + (historical_weight * action_means)\n\n    # Exploration bonus using UCB approach\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n    \n    # Final scores combining weighted scores and exploration bonus\n    final_scores = weighted_scores + exploration_bonus\n\n    # Probabilistic action selection: Epsilon-greedy strategy\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.05)\n    \n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(np.arange(action_count))\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -449.9046632035414,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Calculate means and counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Recent performance trends (last 5 scores)\n    recent_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        recent_scores = score_set.get(action_index, [])[-5:]\n        recent_means[action_index] = np.mean(recent_scores) if recent_scores else 0\n\n    # Weighting factors for historical and recent performance\n    recency_weight = min(1, current_time_slot / (total_time_slots * 0.5))\n    historical_weight = 1 - recency_weight\n\n    # Combined weighted scores\n    weighted_scores = (recency_weight * recent_means) + (historical_weight * action_means)\n\n    # Exploration bonus\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n\n    # Final scores\n    final_scores = weighted_scores + exploration_bonus\n\n    # Epsilon-greedy exploration strategy\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.05)\n    \n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(np.arange(action_count))\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -449.9000529302851,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Calculate historical means and counts for each action\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Recent performance evaluation focusing on the last few scores\n    recent_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        recent_scores = score_set.get(action_index, [])[-5:]  # Last 5 scores\n        recent_means[action_index] = np.mean(recent_scores) if recent_scores else 0\n\n    # Weighting parameters: emphasizing recent performance initially\n    recent_weight = np.clip(current_time_slot / total_time_slots, 0.5, 1.0)\n    historical_weight = 1 - recent_weight\n\n    # Combine historical and recent performance\n    combined_scores = (recent_weight * recent_means) + (historical_weight * action_means)\n\n    # UCB exploration bonus\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n\n    # Final scores integration\n    final_scores = combined_scores + exploration_bonus\n\n    # Epsilon-greedy mechanism for exploration\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.05)\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -449.89903257901614,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    recent_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        recent_scores = score_set.get(action_index, [])[-10:]\n        recent_means[action_index] = np.mean(recent_scores) if recent_scores else 0\n\n    exploration_bonus = np.log(total_selection_count + 1) / (action_counts + 1e-5)\n    \n    recency_weight = (current_time_slot / total_time_slots)**2\n    historical_weight = 1 - recency_weight\n\n    adjusted_scores = (recency_weight * recent_means + historical_weight * action_means + exploration_bonus)\n\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n    \n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": -449.89460287514476,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Calculate historical performance and counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Normalize performance metrics\n    normalized_performance = action_means / (action_counts + 1e-5)\n\n    # Updating the recent performance weight\n    recent_scores = np.array([\n        np.mean(score_set.get(action_index, [])[-10:]) if action_counts[action_index] > 0 else 0\n        for action_index in range(action_count)\n    ])\n    \n    recent_weight = min(1.0, current_time_slot / (total_time_slots + 1e-5))\n    combined_scores = recent_weight * recent_scores + (1 - recent_weight) * normalized_performance\n\n    # Adding exploration bonus\n    exploration_bonus = (np.log(total_selection_count + 1) + 1) / (action_counts + 1)\n    final_scores = combined_scores + exploration_bonus\n\n    # Epsilon for exploration-exploitation\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -449.88654282821466,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Calculate historical averages and counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Avoid division by zero when normalizing\n    normalized_means = np.where(action_counts > 0, action_means / (action_counts + 1e-5), 0)\n\n    # Recent score influence\n    recent_scores = np.array([\n        np.mean(score_set.get(action_index, [])[-3:]) if action_counts[action_index] > 0 else 0\n        for action_index in range(action_count)\n    ])\n\n    # Time-based weighting factors\n    recent_weight = 0.7 * (1 + current_time_slot / total_time_slots) / 2\n    historical_weight = 1 - recent_weight\n\n    # Combine recent and historical scores\n    weighted_scores = (recent_weight * recent_scores) + (historical_weight * normalized_means)\n\n    # Exploration factor for less frequently selected actions\n    exploration_factor = (np.log(total_selection_count + 1) / (action_counts + 1)) * (current_time_slot / total_time_slots)\n    final_scores = weighted_scores + exploration_factor\n\n    # Probabilistic action selection with epsilon for exploration\n    epsilon = 0.1 * (1 - current_time_slot / total_time_slots)\n    \n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -449.88454493254574,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Compute historical means and counts for each action\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Normalize action counts for weighted scores\n    normalized_counts = np.sqrt(np.maximum(action_counts, 1))  # Use sqrt to dampen the effect of counts\n\n    # Calculate scores with recency bias\n    recency_factor = min(1, current_time_slot / total_time_slots)\n    historical_factor = 1 - recency_factor\n\n    recent_scores = np.zeros(action_count)\n    for action_index in range(action_count):\n        if action_counts[action_index] > 0:\n            recent_mean = np.mean(score_set[action_index][-10:]) if len(score_set[action_index]) >= 10 else action_means[action_index]\n            recent_scores[action_index] = (recency_factor * recent_mean + historical_factor * action_means[action_index]) / normalized_counts[action_index]\n        else:\n            recent_scores[action_index] = action_means[action_index]  # Fall back to the historical mean, no count adjustment\n\n    # Exploration bonus using UCB\n    exploration_bonus = np.log(total_selection_count + 1) / (action_counts + 1e-5)\n\n    # Final scores\n    final_scores = recent_scores + exploration_bonus\n\n    # Stochastic selection\n    probabilities = np.exp(final_scores) / np.sum(np.exp(final_scores))\n    action_index = np.random.choice(action_count, p=probabilities)\n\n    return action_index",
          "objective": -449.8804029327344,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Calculate historical performance and counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Normalize means \n    normalized_means = np.divide(action_means, (action_counts + 1e-5))\n\n    # Recent performance consideration\n    recent_scores = np.array([\n        np.mean(score_set.get(action_index, [])[-10:]) if action_counts[action_index] > 0 else 0\n        for action_index in range(action_count)\n    ])\n\n    # Weighting scores based on time progression\n    recent_weight = current_time_slot / total_time_slots\n    weighted_scores = (recent_weight * recent_scores) + ((1 - recent_weight) * normalized_means)\n\n    # Exploration bonus calculation\n    exploration_bonus = np.log(total_selection_count + 1) / (action_counts + 1e-5)\n\n    # Combine scores\n    final_scores = weighted_scores + exploration_bonus\n\n    # Epsilon for exploration\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -449.8775262315627,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Calculate action means and counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Calculate recent performance (last 10 scores)\n    recent_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        recent_scores = score_set.get(action_index, [])[-10:]\n        recent_means[action_index] = np.mean(recent_scores) if recent_scores else 0\n\n    # Weightage for recent vs historical data\n    recency_weight = min(1, current_time_slot / total_time_slots)\n    historical_weight = 1 - recency_weight\n\n    # Combining recent and historical means\n    weighted_scores = (recency_weight * recent_means) + (historical_weight * action_means)\n\n    # Exploration bonus\n    exploration_bonus = np.log(total_selection_count + 1) / (action_counts + 1e-5)  # Avoid division by zero\n\n    # Total scores\n    final_scores = weighted_scores + exploration_bonus\n\n    # Epsilon-greedy strategy for exploration\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n    \n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -449.8768701742728,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Historical performance normalization\n    normalized_means = action_means / (action_counts + 1)  # Adding 1 to prevent division by zero\n\n    # Dynamic epsilon calculation\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n\n    # Recent performance weighting\n    recent_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        recent_scores = scores[-5:]  # Considering the last 5 scores for recent performance\n        recent_means[action_index] = np.mean(recent_scores) if recent_scores else 0\n\n    # Combining historical and recent scores\n    combined_scores = 0.7 * recent_means + 0.3 * normalized_means\n\n    # Exploration term\n    exploration_bonus = np.log(total_selection_count + 1) / (action_counts + 1)\n\n    # Final scores\n    final_scores = combined_scores + exploration_bonus\n\n    # Probabilistic selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -449.87668145490574,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.array([np.mean(scores) if scores else 0 for scores in score_set.values()])\n    action_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Calculate recent means (last 3 scores or fewer)\n    recent_means = np.array([\n        np.mean(scores[-3:]) if len(scores) > 0 else 0 for scores in score_set.values()\n    ])\n    \n    # Weighting system for recent and historical data\n    weight_recent = 0.7 if current_time_slot > 3 else 0.3\n    weight_historical = 1 - weight_recent\n\n    # Compute weighted scores\n    weighted_scores = (weight_recent * recent_means) + (weight_historical * action_means)\n\n    # Exploration bonus\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n    final_scores = weighted_scores + exploration_bonus\n    \n    # Epsilon-greedy for action selection\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -449.8754131903264,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n    \n    # Calculate means and counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Calculate recent means\n    recent_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        recent_scores = score_set.get(action_index, [])[-5:]\n        recent_means[action_index] = np.mean(recent_scores) if recent_scores else 0\n\n    # Weighting factors for historical and recent performance\n    recency_weight = min(1, current_time_slot / (total_time_slots * 0.5))\n    historical_weight = 1 - recency_weight\n\n    # Calculate combined weighted scores\n    weighted_scores = (recency_weight * recent_means) + (historical_weight * action_means)\n\n    # Exploration bonus based on selection history\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n    \n    # Final scores for selection\n    final_scores = weighted_scores + exploration_bonus\n\n    # Epsilon-greedy strategy implementation\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.05)\n    \n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(np.arange(action_count))\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -449.8750329236938,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Calculate historical performance and counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Recent performance emphasis\n    recent_scores = np.array([\n        np.mean(score_set.get(action_index, [])[-10:]) if action_counts[action_index] > 0 else 0\n        for action_index in range(action_count)\n    ])\n\n    # Dynamic weighting between recent and historical performances\n    recent_weight = current_time_slot / total_time_slots\n    effective_scores = (recent_weight * recent_scores) + ((1 - recent_weight) * action_means)\n\n    # Adding exploration bonus\n    exploration_bonus = (np.log(total_selection_count + 1) + 1) / (action_counts + 1e-5)\n\n    # Total score calculation\n    final_scores = effective_scores + exploration_bonus\n\n    # Probabilistic decision-making with exploration\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n    \n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -449.8748360338831,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Calculate action means and counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if scores else 0\n\n    # Emphasize recent performance with a more aggressive weighting on the last few scores\n    recent_scores = np.array([\n        np.mean(scores[-5:]) if len(scores) > 5 else np.mean(scores) if scores else 0\n        for scores in score_set.values()\n    ])\n\n    # Calculate normalized counts to avoid zero division\n    normalized_counts = np.maximum(action_counts, 1) / (total_selection_count + 1)\n\n    # Combine historical and recent performances using weights that change over time\n    recent_weight = current_time_slot / total_time_slots\n    historical_weight = 1 - recent_weight\n    combined_scores = (historical_weight * action_means) + (recent_weight * recent_scores)\n\n    # Exploration factor based on counts\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / np.maximum(action_counts, 1))\n\n    # Final scores\n    final_scores = combined_scores + exploration_bonus\n\n    # Epsilon-greedy strategy for exploration\n    epsilon = max(0.1, 1 - (current_time_slot / (total_time_slots + 1e-5)))\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(np.arange(action_count))\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -449.87437244019594,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Calculate weighted average scores\n    weighted_scores = action_means / (action_counts + 1)  # Avoid divide by zero\n\n    # Recent performance scores (consider the last 5 scores)\n    recent_scores = np.zeros(action_count)\n    for action_index in range(action_count):\n        total_scores = score_set.get(action_index, [])\n        recent_weights = total_scores[-5:] if len(total_scores) > 5 else total_scores\n        recent_scores[action_index] = np.mean(recent_weights) if recent_weights else 0\n\n    # Normalize the recent and historical scores\n    recent_weight = np.clip(current_time_slot / total_time_slots, 0, 1)\n    historical_weight = 1 - recent_weight\n    combined_scores = (recent_weight * recent_scores) + (historical_weight * weighted_scores)\n\n    # Exploration bonus\n    exploration_bonus = np.log(total_selection_count + 1) / (action_counts + 1)\n\n    # Final scores with exploration bonus\n    final_scores = combined_scores + exploration_bonus\n\n    # Epsilon-greedy strategy for exploration\n    exploration_probability = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n    if np.random.rand() < exploration_probability:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -449.8650156077526,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate means and counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Adjust means to avoid division by zero and reflect uncertainty\n    adjusted_means = np.divide(action_means, action_counts + 1, out=np.zeros_like(action_means), where=(action_counts > 0))\n\n    # Dynamic exploration-exploitation balance\n    exploration_weight = max(0, 1 - (current_time_slot / total_time_slots))\n    exploitation_weight = 1 - exploration_weight\n\n    # Recent performance emphasis\n    recent_scores = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        if action_counts[action_index] > 0:\n            recent_scores[action_index] = np.mean(score_set[action_index][-min(action_counts[action_index], 3):])  # Last 3 scores\n\n    combined_scores = (exploration_weight * adjusted_means) + (exploitation_weight * recent_scores)\n\n    # Action selection through epsilon-greedy strategy\n    if np.random.rand() < (0.2 * (1 - (current_time_slot / total_time_slots))):  # Epsilon decays over time\n        action_index = np.random.randint(0, num_actions)  # Explore\n    else:\n        action_index = np.argmax(combined_scores)  # Exploit\n\n    return action_index",
          "objective": -449.86314634329443,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.array([np.mean(scores) if scores else 0 for scores in score_set.values()])\n    action_counts = np.array([len(scores) for scores in score_set.values()])\n\n    # Calculate recent performance (last 5 scores or fewer)\n    recent_means = np.array([\n        np.mean(scores[-5:]) if len(scores) > 0 else 0 for scores in score_set.values()\n    ])\n    \n    # Weighting system \n    weight_recent = min(1, current_time_slot / total_time_slots)\n    weight_historical = 1 - weight_recent\n\n    # Compute weighted scores\n    weighted_scores = (weight_recent * recent_means) + (weight_historical * action_means)\n\n    # Exploration bonus using UCB approach\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / (action_counts + 1e-5))\n    final_scores = weighted_scores + exploration_bonus\n\n    # Epsilon-greedy strategy for action selection\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -449.859404934029,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Calculate historical performance, action means, and counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if scores else 0\n\n    # Handle zero counts with a minimum mean value for exploration\n    adjusted_means = np.where(action_counts > 0, action_means, 0.1)\n\n    # Recent score emphasis (last 5 scores weighted more heavily)\n    recent_scores = np.array([\n        np.mean(score_set.get(action_index, [])[-5:]) if action_counts[action_index] > 0 else 0\n        for action_index in range(action_count)\n    ])\n\n    # Weighting factors\n    recent_weight = 0.6 * (current_time_slot / total_time_slots) + 0.4\n    historical_weight = 1 - recent_weight\n    weighted_scores = (recent_weight * recent_scores) + (historical_weight * adjusted_means)\n\n    # Exploration bonus\n    exploration_bonus = np.log(total_selection_count + 1) / (action_counts + 1)\n\n    # Combined final scores\n    final_scores = weighted_scores + exploration_bonus\n\n    # Epsilon for exploration-exploitation\n    epsilon = 0.1 if current_time_slot < 0.1 * total_time_slots else 0.01\n\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -449.85592321169565,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if scores else 0.0\n\n    # Calculate recent means and give more weight to scores from the last 5 trials\n    recent_scores = {i: score_set.get(i, [])[-5:] for i in range(action_count)}\n    recent_means = np.array([np.mean(recent_scores[i]) if recent_scores[i] else 0.0 for i in range(action_count)])\n\n    # Define weighting factors\n    recency_weight = min(1.0, current_time_slot / (total_time_slots / 2))\n    historical_weight = 1.0 - recency_weight\n\n    # Calculate combined scores\n    weighted_scores = (recency_weight * recent_means) + (historical_weight * action_means)\n\n    # Exploration bonus\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n    final_scores = weighted_scores + exploration_bonus\n\n    # Dynamic epsilon-greedy strategy\n    epsilon = max(0.1 - (current_time_slot / total_time_slots) * 0.1, 0.01)\n    \n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -449.8545098636088,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate means and counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Normalize means to account for selection counts\n    adjusted_means = action_means / (action_counts + 1e-5)\n\n    # Dynamic exploration factor\n    epsilon = 0.1 * (1 - (current_time_slot / total_time_slots))\n\n    # Recent performance weighting\n    recent_weight = 0.7 * (current_time_slot / total_time_slots)\n    weighted_performance = adjusted_means * (1 + recent_weight)\n    \n    # Calculate scores for exploitation and random exploration\n    exploration_value = np.random.rand()\n    \n    if exploration_value < epsilon:\n        action_index = np.random.randint(0, num_actions)  # Explore\n    else:\n        # Calculate probabilities for selection based on performance\n        selection_probs = weighted_performance / weighted_performance.sum()\n        action_index = np.random.choice(num_actions, p=selection_probs)  # Exploit with softmax-like sampling\n\n    return action_index",
          "objective": -449.8530879476298,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n    \n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    recent_scores = np.zeros(action_count)\n    for action_index in range(action_count):\n        recent_scores_action = score_set.get(action_index, [])[-5:]\n        recent_scores[action_index] = np.mean(recent_scores_action) if recent_scores_action else 0\n\n    recency_weight = min(1, current_time_slot / (total_time_slots * 0.5))\n    historical_weight = 1 - recency_weight\n\n    weighted_scores = (recency_weight * recent_scores) + (historical_weight * action_means)\n    \n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n    \n    final_scores = weighted_scores + exploration_bonus\n\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.05)\n    \n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(np.arange(action_count))\n    else:\n        action_index = np.argmax(final_scores)\n    \n    return action_index",
          "objective": -449.8526323517496,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_means = np.zeros(8)\n    action_counts = np.zeros(8)\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Recent scores weigh more heavily in exploration\n    recent_weight = 0.5\n    if current_time_slot > 0:\n        recent_scores = np.array([score_set.get(i, [])[-1] for i in range(8) if len(score_set.get(i, [])) > 0])\n        recent_means = np.zeros(8)\n        for i in range(8):\n            if len(score_set.get(i, [])) > 0:\n                recent_means[i] = recent_scores[i]\n        action_means += recent_weight * (recent_means - action_means)\n\n    # Exploration factor increases as more actions are taken\n    epsilon = max(0.1 * (1 - (total_selection_count / (total_time_slots + 1))), 0.01)\n    \n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, 8)  # Explore\n    else:\n        action_index = np.argmax(action_means)  # Exploit\n\n    return action_index",
          "objective": -449.8429024865609,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Calculate action counts and means\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Dynamic performance metrics\n    adjusted_means = (action_means + 1e-5) / (action_counts + 1e-5)\n\n    # Recent performance trend\n    recent_scores = np.array([\n        np.mean(score_set.get(action_index, [])[-10:]) if action_counts[action_index] > 0 else 0\n        for action_index in range(action_count)\n    ])\n\n    # Weighting based on time slot progression\n    recent_weight = 1.0 / (1 + current_time_slot / total_time_slots)\n    performance_scores = recent_weight * recent_scores + (1 - recent_weight) * adjusted_means\n\n    # Exploration bonus using square root for more gradual influence\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) + 1) / (action_counts + 1)\n\n    # Combine performance scores and exploration factors\n    final_scores = performance_scores + exploration_bonus\n\n    # Epsilon for exploration\n    epsilon = 0.1 * (1 - (current_time_slot / total_time_slots)) + 0.01\n\n    # Action selection based on exploration-exploitation balance\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -449.84195054179,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Calculate action means and counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Determine weights based on time\n    recency_weight = min(1, current_time_slot / (total_time_slots * 0.5))\n    historical_weight = 1 - recency_weight\n\n    # Calculate combined scores\n    weighted_scores = (recency_weight * action_means) + (historical_weight * action_means)\n\n    # Exploration bonus based on count and performance\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n    \n    # Aggregate scores for decision making\n    final_scores = (weighted_scores + exploration_bonus) / (1 + action_counts)\n\n    # Epsilon-greedy strategy for exploration\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.05)\n    \n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(np.arange(action_count))\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -449.8402759196964,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Calculate means and counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Normalize action means for a better comparison\n    min_mean = np.min(action_means)\n    max_mean = np.max(action_means) if np.max(action_means) > 0 else 1  # Avoid division by zero\n\n    normalized_action_means = (action_means - min_mean) / (max_mean - min_mean) if max_mean > min_mean else np.zeros_like(action_means)\n\n    # Exploration factor, scaled by current time\n    exploration_factor = np.sqrt(current_time_slot / total_time_slots)\n    exploration_bonus = (1 + exploration_factor) * np.log(total_selection_count + 1) / (action_counts + 1e-5)\n\n    # Combine normalized means and exploration bonus\n    composite_scores = normalized_action_means + exploration_bonus\n\n    # Epsilon-greedy strategy with more gradual exploration reduction\n    epsilon_start = 0.15\n    epsilon_decay_rate = 0.1\n    epsilon = max(epsilon_start * (1 - current_time_slot / total_time_slots), 0.01)\n\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(action_count)\n    else:\n        action_index = np.argmax(composite_scores)\n\n    return action_index",
          "objective": -449.83607546621806,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if scores else 0\n\n    recent_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        recent_scores = score_set.get(action_index, [])[-10:]\n        recent_means[action_index] = np.mean(recent_scores) if recent_scores else 0\n\n    recency_weight = np.clip(current_time_slot / total_time_slots, 0.0, 1.0)\n    historical_weight = 1 - recency_weight\n\n    weighted_scores = (recency_weight * recent_means) + (historical_weight * action_means)\n    exploration_bonus = np.log(total_selection_count + 1) / (action_counts + 1e-5)\n\n    final_scores = weighted_scores + exploration_bonus\n\n    epsilon_decay = 0.1 * (1 - (current_time_slot / total_time_slots))\n    epsilon = max(epsilon_decay, 0.01)\n\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -449.8292364083257,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    recent_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        recent_scores = score_set.get(action_index, [])[-5:]\n        recent_means[action_index] = np.mean(recent_scores) if recent_scores else 0\n\n    recency_weight = min(1.0, current_time_slot / total_time_slots)\n    historical_weight = 1 - recency_weight\n\n    adjusted_scores = (historical_weight * action_means) + (recency_weight * recent_means)\n\n    exploration_bonus = np.log(1 + total_selection_count) / (action_counts + 1e-5)\n    \n    final_scores = adjusted_scores + exploration_bonus\n\n    epsilon_decay = 0.1 * (1 - (current_time_slot / total_time_slots))\n    epsilon = max(epsilon_decay, 0.01)\n\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -449.8250557182105,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_means = np.zeros(8)\n    action_counts = np.zeros(8)\n\n    # Performance analysis\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Dynamic epsilon calculation\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n    \n    # Calculating recent performance weight\n    decay_factor = 0.9\n    recent_weighted_means = np.zeros(8)\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        recent_scores = scores[-10:] if len(scores) > 10 else scores\n        recent_mean = np.mean(recent_scores) if recent_scores else 0\n        recent_weighted_means[action_index] = (0.7 * recent_mean + 0.3 * action_means[action_index])\n    \n    # Encouragement for underutilized actions\n    bonus_factor = np.log(total_selection_count + 1) / (action_counts + 1)\n    final_scores = recent_weighted_means * bonus_factor\n\n    # Exploration-exploitation decision\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, 8)  # Explore\n    else:\n        action_index = np.argmax(final_scores)  # Exploit\n    \n    return action_index",
          "objective": -449.82431142414777,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Recent performance focus with decay\n    decay_factor = (current_time_slot + 1) / (total_time_slots + 1)\n    recent_weighted_means = action_means * (1 + decay_factor)\n\n    # Variance as an exploration factor\n    action_variances = np.array([\n        np.var(scores) if action_counts[action_index] > 0 else 0 for action_index, scores in score_set.items()\n    ])\n    \n    exploration_bonus = (np.log(total_selection_count + 1) / (action_counts + 1e-5)) * (1 + action_variances)\n\n    # Composite score calculation\n    composite_scores = recent_weighted_means + exploration_bonus\n\n    # Adaptive exploration probability\n    exploration_strength = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.05)\n    \n    # Epsilon-greedy selection\n    if np.random.rand() < exploration_strength:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(composite_scores)\n\n    return action_index",
          "objective": -449.8215680365701,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Calculate action means and counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if scores else 0\n\n    # Recent performance adaptation using exponential weighting\n    weighted_means = action_means * (1 - (current_time_slot / total_time_slots))\n\n    # Exploration bonus with square root scaling\n    exploration_bonus = np.sqrt(total_selection_count + 1) / (action_counts + 1e-5)\n\n    # Composite scores calculation\n    composite_scores = weighted_means + exploration_bonus\n\n    # Dynamic exploration probability (more exploration at the beginning)\n    exploration_strength = 1 - (current_time_slot / total_time_slots)**2\n\n    # Epsilon-greedy selection approach\n    if np.random.rand() < exploration_strength:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(composite_scores)\n\n    return action_index",
          "objective": -449.8187173612817,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate action means and counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if scores else 0.0\n\n    # Recent performance emphasis\n    recent_scores = np.array([scores[-1] if scores else 0 for scores in score_set.values()])\n    recent_weight = 0.75  # Increased weight for recent performance\n    weighted_means = (1 - recent_weight) * action_means + recent_weight * recent_scores\n\n    # Adaptive exploration adjustment based on current time slot\n    epsilon = max(0.1, 1.0 - (current_time_slot / total_time_slots) * 0.9)\n\n    # Exploration bonus normalized by selection counts and overall selections\n    exploration_bonus = np.sqrt(action_counts + 1) / (total_selection_count + 1)\n    \n    # Combine performance with exploration\n    total_values = weighted_means + epsilon * exploration_bonus\n\n    # Safe normalization\n    total_values_normalized = total_values / np.sum(total_values) if np.sum(total_values) > 0 else np.zeros(num_actions)\n\n    # Action selection based on probabilities\n    action_index = np.random.choice(np.arange(num_actions), p=total_values_normalized)\n    \n    return action_index",
          "objective": -449.8007919996728,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n    \n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if scores else 0.0\n\n    recent_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        recent_scores = score_set.get(action_index, [])[-5:]\n        recent_means[action_index] = np.mean(recent_scores) if recent_scores else 0.0\n\n    recency_weight = min(1.0, current_time_slot / (total_time_slots / 2))\n    historical_weight = 1.0 - recency_weight\n\n    weighted_scores = (recency_weight * recent_means) + (historical_weight * action_means)\n    \n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n    final_scores = weighted_scores + exploration_bonus\n\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)    \n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -449.7896885417639,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Calculate recent performance: considers scores from the last 5 selections to emphasize adaptability\n    recent_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        recent_scores = score_set.get(action_index, [])[-5:]\n        recent_means[action_index] = np.mean(recent_scores) if recent_scores else 0\n\n    # Linear recency weight to balance historical and recent performance\n    recency_weight = min(1.0, current_time_slot / total_time_slots)\n    historical_weight = 1 - recency_weight\n    weighted_scores = (historical_weight * action_means) + (recency_weight * recent_means)\n\n    # Exploration bonus based on selection frequency\n    exploration_bonus = np.log(total_selection_count + 1) / (action_counts + 1e-5)\n\n    final_scores = weighted_scores + exploration_bonus\n\n    # Epsilon-greedy approach for exploration\n    epsilon_decay = 0.1 * (1 - (current_time_slot / total_time_slots))\n    epsilon = max(epsilon_decay, 0.01)\n\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -449.78437634191755,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    recent_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        recent_scores = score_set.get(action_index, [])[-5:]\n        recent_means[action_index] = np.mean(recent_scores) if recent_scores else 0\n\n    recent_weight = min(1, current_time_slot / (total_time_slots * 0.5))\n    historical_weight = 1 - recent_weight\n\n    weighted_scores = (recent_weight * recent_means) + (historical_weight * action_means)\n\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1))\n\n    final_scores = weighted_scores + exploration_bonus\n\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.05)\n\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(np.arange(action_count))\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -449.7786752115064,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate action means and counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if scores else 0\n\n    # Emphasize recent performance\n    recent_scores = np.array([scores[-1] if scores else 0 for scores in score_set.values()])\n    recent_weight = 0.5\n    weighted_means = (1 - recent_weight) * action_means + recent_weight * recent_scores\n\n    # Adaptive epsilon for exploration\n    epsilon = max(0.05, 1.0 - (current_time_slot / total_time_slots))\n    \n    # Exploration bonus based on action counts and total selections\n    exploration_bonus = np.sqrt(action_counts + 1) / (total_selection_count + 1)\n\n    # Combine weighted means and exploration\n    total_values = weighted_means + epsilon * exploration_bonus\n\n    # Normalize to create a probability distribution\n    total_values_normalized = total_values / total_values.sum()\n\n    # Select action based on probabilities\n    action_index = np.random.choice(np.arange(num_actions), p=total_values_normalized)\n    \n    return action_index",
          "objective": -449.77494742565847,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Compute action means and counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n    \n    # Recent means calculation with respect to the last 5 scores for sensitivity\n    recent_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        recent_scores = score_set.get(action_index, [])[-5:]  # last 5 scores\n        recent_means[action_index] = np.mean(recent_scores) if recent_scores else 0\n\n    # Dynamic weights between historical and recent means\n    recent_weight = min(1, (current_time_slot / total_time_slots) ** 2)  # Increasing weight as time goes\n    historical_weight = 1 - recent_weight\n    combined_scores = (recent_weight * recent_means) + (historical_weight * action_means)\n\n    # Exploration bonus formulation\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))  # Slightly adapted exploration\n\n    # Final scores\n    final_scores = combined_scores + exploration_bonus\n\n    # Probabilistic selection framework\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -449.77419032721724,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Calculate means and counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Recent mean calculation (considering last 5 scores)\n    recent_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        recent_scores = score_set.get(action_index, [])[-5:]\n        recent_means[action_index] = np.mean(recent_scores) if recent_scores else 0\n\n    # Dynamic exploration bonus\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n\n    # Weight based on time progression\n    time_weight = current_time_slot / total_time_slots\n\n    # Composite score calculation with bias for recent performance\n    adjusted_scores = (1 - time_weight) * action_means + time_weight * recent_means + exploration_bonus\n\n    # Epsilon-greedy approach for exploration\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n    \n    # Random selection for exploration\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": -449.73975775493517,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_means = []\n    action_counts = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_count = len(scores)\n        action_mean = np.mean(scores) if action_count > 0 else 0\n        action_means.append(action_mean)\n        action_counts.append(action_count)\n\n    action_means = np.array(action_means)\n    action_counts = np.array(action_counts)\n\n    # Recent performance emphasis using exponential decay\n    recent_weight = 0.8\n    recent_performances = [\n        np.mean(scores[-3:]) if len(scores) >= 3 else action_means[i] \n        for i, scores in score_set.items()\n    ]\n    recent_performances = np.array(recent_performances)\n    \n    # Scale recent performances into current means\n    effective_means = (1 - recent_weight) * action_means + recent_weight * recent_performances\n\n    # Promote exploration of less chosen actions\n    selection_bonus = np.where(action_counts < np.mean(action_counts) - 1, 0.1, 0)\n    adjusted_scores = effective_means + selection_bonus\n\n    # Dynamic epsilon for exploration\n    epsilon = max(0.1, 0.5 * (current_time_slot / total_time_slots))\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, 8)  # Explore: choose a random action\n    else:\n        action_index = np.argmax(adjusted_scores)  # Exploit: choose the best action based on adjusted scores\n\n    return action_index",
          "objective": -449.7201941314905,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Calculate historical performance and counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Smooth historical means to avoid zero division\n    smoothed_means = action_means / (action_counts + 1e-5)\n\n    # Recent performance calculation, using the last 10 scores or all if fewer exist\n    recent_scores = np.array([\n        np.mean(score_set.get(action_index, [])[-10:]) if action_counts[action_index] > 0 else 0\n        for action_index in range(action_count)\n    ])\n    \n    # Define the weight for recent versus historical performance\n    recent_weight = min(1, current_time_slot / (total_time_slots * 0.5))  # Linearly increases until halfway\n    combined_scores = recent_weight * recent_scores + (1 - recent_weight) * smoothed_means\n\n    # Exploration bonus\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n\n    # Final score calculation\n    final_scores = combined_scores + exploration_bonus\n\n    # Epsilon for exploration-exploitation\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n\n    # Decision making: Exploration vs Exploitation\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -449.71451961515754,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n    \n    # Calculate means and counts for each action\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Recent performance evaluation\n    recent_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        recent_scores = score_set.get(action_index, [])[-5:]  # Last 5 scores\n        recent_means[action_index] = np.mean(recent_scores) if recent_scores else 0\n\n    # Dynamic weighting based on time\n    recency_weight = 0.6 if current_time_slot > total_time_slots * 0.6 else 0.4\n    historical_weight = 1 - recency_weight\n    weighted_scores = recency_weight * recent_means + historical_weight * action_means\n\n    # Exploration bonus using UCB approach\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n    final_scores = weighted_scores + exploration_bonus\n\n    # Epsilon-greedy strategy for exploration\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.05)\n    \n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(np.arange(action_count))\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -449.7130744211852,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n    \n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    recent_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        recent_scores = score_set.get(action_index, [])[-5:]  # Using last 5 scores for recent mean\n        recent_means[action_index] = np.mean(recent_scores) if recent_scores else 0\n\n    recency_weight = 0.7 if current_time_slot > total_time_slots * 0.5 else 0.3\n    historical_weight = 1 - recency_weight\n\n    weighted_scores = recency_weight * recent_means + historical_weight * action_means\n\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n\n    final_scores = weighted_scores + exploration_bonus\n\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.05)\n    \n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(np.arange(action_count))\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -449.7127900375931,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_means = np.zeros(8)\n    action_counts = np.zeros(8)\n\n    # Evaluate historical performance\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Dynamic epsilon for exploration\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n    \n    # Recent performance sensitivity\n    recent_scores_means = np.zeros(8)\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        recent_scores = scores[-10:] if len(scores) > 10 else scores\n        recent_mean = np.mean(recent_scores) if recent_scores else 0\n        recent_scores_means[action_index] = recent_mean\n\n    # Confidence bonus for underutilized actions\n    bonus_factor = np.log(total_selection_count + 1) / (action_counts + 1e-5)\n    \n    # Calculate final scores combining historical means and bonuses\n    final_scores = 0.7 * recent_scores_means + 0.3 * action_means * bonus_factor\n\n    # Decision based on exploration or exploitation\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, 8)  # Explore\n    else:\n        action_index = np.argmax(final_scores)  # Exploit\n    \n    return action_index",
          "objective": -449.7064270548744,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Recent score weighting using exponential decay\n    decay_factor = 0.9\n    weighted_means = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        weighted_score = 0\n        for weight, score in enumerate(reversed(score_set.get(action_index, []))):\n            weighted_score += decay_factor ** weight * score\n        weighted_means[action_index] = weighted_score / (1 - decay_factor ** action_counts[action_index]) if action_counts[action_index] > 0 else 0\n\n    # Dynamic epsilon based on time slots\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n\n    # Exploration-exploitation decision\n    if np.random.rand() < epsilon:\n        # Exploration: Prefer actions that have been selected less\n        exploration_probabilities = (1 / (action_counts + 1))\n        action_index = np.random.choice(np.arange(num_actions), p=exploration_probabilities / np.sum(exploration_probabilities))\n    else:\n        # Exploitation: select best action based on weighted means\n        action_index = np.argmax(weighted_means)\n\n    return action_index",
          "objective": -449.68121293803915,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Calculate historical means and counts for each action\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Define exploration-exploitation strategy using Thompson Sampling approach\n    # Use Beta distribution parameters for assumed success/failure counts\n    successes = action_means * action_counts\n    failures = action_counts - successes\n    \n    # Generate random samples from the Beta distribution for exploration\n    beta_samples = np.random.beta(successes + 1, failures + 1)\n\n    # Incorporate recency weighting\n    recency_weight = max(0, 1 - (current_time_slot / total_time_slots))\n    recent_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        recent_scores = score_set.get(action_index, [])[-5:]  # Last 5 scores\n        recent_means[action_index] = np.mean(recent_scores) if recent_scores else 0\n    \n    # Combine historical means and recent means\n    combined_scores = recency_weight * recent_means + (1 - recency_weight) * action_means\n    \n    # Final score computation considering exploration\n    final_scores = beta_samples + combined_scores\n\n    # Epsilon-greedy approach to promote exploration\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.05)\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(np.arange(action_count))\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -449.67943012716194,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Calculate historical performance and counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Adjusted average score considering action selection frequency\n    adjusted_scores = (action_means * action_counts) / (action_counts + 1e-5)\n\n    # Recent performance weight\n    recent_length = min(10, action_counts.max())  # Consider most recent scores up to last 10 scores\n    recent_scores = np.array([\n        np.mean(score_set.get(action_index, [])[-recent_length:]) if action_counts[action_index] > 0 else 0\n        for action_index in range(action_count)\n    ])\n\n    # Weighting recent performance more heavily\n    recent_weight = 0.6\n    dynamic_scores = (recent_weight * recent_scores) + ((1 - recent_weight) * adjusted_scores)\n\n    # Exploration bonus for less frequently chosen actions\n    exploration_bonus = np.log(total_selection_count + 1) / (action_counts + 1e-5)\n\n    # Combine dynamic scores with exploration bonus\n    final_scores = dynamic_scores + exploration_bonus\n\n    # Epsilon for exploration\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -449.67422465379366,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    recent_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        recent_scores = score_set.get(action_index, [])[-5:]\n        recent_means[action_index] = np.mean(recent_scores) if recent_scores else 0\n\n    recency_weight = min(1, current_time_slot / (total_time_slots * 0.5))\n    historical_weight = 1 - recency_weight\n\n    weighted_scores = (recency_weight * recent_means) + (historical_weight * action_means)\n\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n\n    final_scores = weighted_scores + exploration_bonus\n\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.05)\n\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(np.arange(action_count))\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -449.6737083213009,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.array([np.mean(scores) if scores else 0 for scores in score_set.values()])\n    action_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Calculate the recent performance\n    recent_means = np.array([\n        np.mean(scores[-3:]) if len(scores) >= 3 else (np.mean(scores) if scores else 0) for scores in score_set.values()\n    ])\n    \n    # Dynamic weighting based on total time slots\n    weight_recent = min(current_time_slot / total_time_slots, 1)  # Cap at 1\n    weight_historical = 1 - weight_recent\n    \n    # Combined score calculation with recent and historical means\n    combined_scores = (weight_recent * recent_means) + (weight_historical * action_means)\n    \n    # Exploration bonus\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n    final_scores = combined_scores + exploration_bonus\n\n    # Epsilon-greedy selection with adaptive probability\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n    \n    return action_index",
          "objective": -449.6614504824634,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Compute means and counts for each action\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Calculate recent means for more dynamic decision making\n    recent_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        recent_scores = score_set.get(action_index, [])[-5:]\n        recent_means[action_index] = np.mean(recent_scores) if recent_scores else 0\n\n    # Weighting factors for historical and recent means\n    if total_time_slots > 0:  \n        recency_weight = (current_time_slot / total_time_slots)**2\n    else:\n        recency_weight = 0\n    historical_weight = 1 - recency_weight\n\n    # Compute weighted scores\n    weighted_scores = (historical_weight * action_means) + (recency_weight * recent_means)\n\n    # Exploration bonus to encourage exploration\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n    \n    final_scores = weighted_scores + exploration_bonus\n\n    # Epsilon-greedy strategy for exploration vs. exploitation\n    exploration_factor = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.05)\n    \n    if np.random.rand() < exploration_factor:\n        action_index = np.random.choice(np.arange(action_count))\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -449.65496228897644,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Calculate historical means and counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Normalizing means\n    normalized_means = action_means / (action_counts + 1e-5)\n    \n    # Recent performance consideration with recency bias\n    recent_scores = np.array([\n        np.mean(score_set.get(action_index, [])[-10:]) if action_counts[action_index] > 0 else 0\n        for action_index in range(action_count)\n    ])\n    \n    recency_weight = current_time_slot / total_time_slots\n    performance_scores = (recency_weight * recent_scores) + ((1 - recency_weight) * normalized_means)\n\n    # Exploration bonus\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n\n    # Combine scores with exploration bonus\n    final_scores = performance_scores + exploration_bonus\n\n    # Epsilon for probabilistic exploration\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n    \n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -449.6541645235745,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Calculate action means and counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Normalize means to prevent bias from highly selected actions\n    normalized_means = action_means / (action_counts + 1)\n\n    # Recent performance evaluation\n    recent_scores = np.zeros(action_count)\n    for action_index in range(action_count):\n        total_scores = score_set.get(action_index, [])\n        recent_weights = total_scores[-10:] if len(total_scores) > 10 else total_scores\n        recent_scores[action_index] = np.mean(recent_weights) if recent_weights else 0\n\n    # Balancing historical and recent scores\n    recent_weight = current_time_slot / total_time_slots\n    historical_weight = 1 - recent_weight\n    weighted_scores = (recent_weight * recent_scores) + (historical_weight * normalized_means)\n\n    # Exploration bonus\n    exploration_bonus = (np.log(total_selection_count + 1) + 1) / (action_counts + 1)\n\n    # Combine scores and exploration\n    final_scores = weighted_scores + exploration_bonus\n\n    # Epsilon-greedy approach for action selection\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -449.6518956969883,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n    weights = np.zeros(action_count)\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    recent_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        recent_scores = score_set.get(action_index, [])[-5:]\n        recent_means[action_index] = np.mean(recent_scores) if recent_scores else 0\n\n    recency_weight = min(1, current_time_slot / (total_time_slots * 0.5))\n    historical_weight = 1 - recency_weight\n    weighted_scores = (recency_weight * recent_means) + (historical_weight * action_means)\n\n    bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n    \n    final_scores = weighted_scores + bonus\n\n    exploration_probability = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.05)\n    if np.random.rand() < exploration_probability:\n        action_index = np.random.choice(np.arange(action_count))\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -449.6497145823713,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Calculate means and counts for historical scores\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n    \n    # Calculate recent means and apply a recent performance emphasis\n    recent_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        recent_scores = score_set.get(action_index, [])[-10:]  # last 10 scores\n        recent_means[action_index] = np.mean(recent_scores) if recent_scores else 0\n\n    # Combine historical and recent means using a sigmoid weighting\n    recent_weight = 1 / (1 + np.exp(-2 * (current_time_slot / total_time_slots - 0.5)))\n    historical_weight = 1 - recent_weight\n    combined_scores = (recent_weight * recent_means) + (historical_weight * action_means)\n\n    # Exploration bonus with a safe division to avoid division by zero\n    exploration_bonus = np.log(total_selection_count + 1) / (action_counts + 1e-5)\n\n    # Final scores\n    final_scores = combined_scores + exploration_bonus\n\n    # Epsilon-greedy exploration strategy\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n    \n    # Action selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -449.64212703429274,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if scores else 0.0\n\n    recent_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        recent_scores = score_set.get(action_index, [])[-5:]\n        recent_means[action_index] = np.mean(recent_scores) if recent_scores else 0.0\n\n    recency_weight = min(1.0, current_time_slot / (total_time_slots / 2))\n    historical_weight = 1.0 - recency_weight\n\n    weighted_scores = (recency_weight * recent_means) + (historical_weight * action_means)\n\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n    final_scores = weighted_scores + exploration_bonus\n\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -449.6389479125536,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_means = np.zeros(8)\n    action_counts = np.zeros(8)\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Recent score weighting\n    recent_weight = 1 + (current_time_slot / total_time_slots)\n    weighted_means = action_means * recent_weight\n\n    # Dynamic epsilon based on time slots\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n\n    # Exploration-exploitation decision\n    if np.random.rand() < epsilon:\n        exploration_probabilities = (1 / (action_counts + 1))\n        action_index = np.random.choice(np.arange(8), p=exploration_probabilities / np.sum(exploration_probabilities))\n    else:\n        action_index = np.argmax(weighted_means)\n\n    return action_index",
          "objective": -449.6299129010771,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    action_means = np.zeros(n_actions)\n    action_counts = np.zeros(n_actions)\n\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n    decision_scores = action_means + exploration_bonus\n\n    # Gradually increase exploitation as time progresses\n    time_weight = current_time_slot / total_time_slots\n    adjusted_scores = decision_scores * time_weight + action_means * (1 - time_weight)\n\n    action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": -449.60936354509005,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    recent_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        recent_scores = score_set.get(action_index, [])[-10:]\n        recent_means[action_index] = np.mean(recent_scores) if recent_scores else 0\n\n    recency_weight = min(1, current_time_slot / total_time_slots)\n    historical_weight = 1 - recency_weight\n\n    weighted_scores = recency_weight * recent_means + historical_weight * action_means\n    exploration_bonus = np.log(total_selection_count + 1) / (action_counts + 1e-5)  # Avoid division by zero\n\n    final_scores = weighted_scores + exploration_bonus\n\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -449.45407963764416,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Calculate means and counts for each action\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Weighted averaging for recent and historical performance\n    recent_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        recent_scores = score_set.get(action_index, [])[-5:]  # Last 5 scores for recent means\n        recent_means[action_index] = np.mean(recent_scores) if recent_scores else 0\n\n    # Dynamic weights adjusted by current time\n    recency_weight = min(1, current_time_slot / (total_time_slots * 0.75))\n    historical_weight = 1 - recency_weight\n    weighted_scores = recency_weight * recent_means + historical_weight * action_means\n\n    # Exploration bonus using a modified UCB approach\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n    final_scores = weighted_scores + exploration_bonus\n\n    # Probabilistic selection using softmax for exploration-exploitation trade-off\n    exp_scores = np.exp(final_scores - np.max(final_scores))  # stabilize softmax\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    action_index = np.random.choice(np.arange(action_count), p=probabilities)\n\n    return action_index",
          "objective": -449.3073033518824,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Normalize the means to avoid bias from the number of selections\n    normalized_means = action_means / (action_counts + 1)  # Avoid division by zero\n\n    # Recent scores consideration (last 5 scores for responsiveness)\n    recent_scores = np.zeros(action_count)\n    for action_index in range(action_count):\n        total_scores = score_set.get(action_index, [])\n        recent_weights = total_scores[-5:] if len(total_scores) > 5 else total_scores\n        recent_scores[action_index] = np.mean(recent_weights) if recent_weights else 0\n\n    # Compute exploration bonus to favor less selected actions\n    exploration_bonus = (np.log(total_selection_count + 1) + 1) / (action_counts + 1)  # Avoid division by zero\n\n    # Weighting recent and historical performance dynamically\n    dynamic_weight = np.clip(current_time_slot / total_time_slots, 0.2, 0.8)\n    weighted_scores = (dynamic_weight * recent_scores) + ((1 - dynamic_weight) * normalized_means)\n\n    # Final scores with exploration bonus\n    final_scores = weighted_scores + exploration_bonus\n\n    # Probabilistic exploration-exploitation\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.05)\n    \n    # Select action index\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Calculate means and counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Recent mean calculation (last 10 scores or all scores available)\n    recent_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        recent_scores = score_set.get(action_index, [])[-10:]\n        recent_means[action_index] = np.mean(recent_scores) if recent_scores else 0\n\n    # Dynamic weights for exploration vs exploitation\n    exploration_weight = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n    recency_weight = 0.7  # weight more on recency\n    historical_weight = 0.3\n    \n    # Composite score calculation\n    adjusted_scores = (recency_weight * recent_means + historical_weight * action_means) + exploration_weight\n\n    # Probabilistic decision-making\n    epsilon_decay = 0.2 * (1 - (current_time_slot / total_time_slots))\n    epsilon = max(epsilon_decay, 0.05)\n\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": -449.65842218610516,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Weighted average with exponential decay for recent scores\n    decay_rate = 0.9\n    decay_weights = np.array([(decay_rate ** (len(scores) - 1)) if len(scores) > 0 else 0 for scores in score_set.values()])\n    weighted_means = action_means * decay_weights\n\n    # Dynamic epsilon-greedy strategy\n    epsilon_max = 0.5\n    epsilon_min = 0.01\n    epsilon_decay = (epsilon_max - epsilon_min) * (current_time_slot / total_time_slots)\n    epsilon = epsilon_max - epsilon_decay\n\n    # Exploration-exploitation decision\n    if np.random.rand() < epsilon:\n        # Explore: select an action that has been chosen less often\n        unexplored_actions = np.where(action_counts < (total_selection_count / num_actions))[0]\n        if len(unexplored_actions) > 0:\n            action_index = np.random.choice(unexplored_actions)\n        else:\n            action_index = np.random.randint(0, num_actions)  # fallback\n    else:\n        # Exploit: select the action with the highest weighted mean\n        action_index = np.argmax(weighted_means)\n\n    return action_index",
          "objective": -449.63811729031954,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.array([np.mean(scores) if scores else 0 for scores in score_set.values()])\n    action_counts = np.array([len(scores) for scores in score_set.values()])\n\n    # Calculation of recent means (last 3 scores)\n    recent_means = np.array([\n        np.mean(scores[-3:]) if len(scores) >= 3 else (np.mean(scores) if scores else 0) for scores in score_set.values()\n    ])\n    \n    # Dynamically weight recent and historical scores\n    weight_recent = (current_time_slot / total_time_slots)\n    weight_historical = 1 - weight_recent\n\n    # Compute combined weighted scores\n    combined_scores = (weight_recent * recent_means) + (weight_historical * action_means)\n\n    # Exploration bonus based on selection count with a small epsilon to avoid division by zero\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n    final_scores = combined_scores + exploration_bonus\n\n    # Epsilon-greedy selection mechanism\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n    randomized_selection = np.random.rand() < epsilon\n    \n    if randomized_selection:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -449.6099258404526,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate action means and counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Emphasize recent performance\n    recent_scores = np.array([scores[-1] if len(scores) > 0 else 0 for scores in score_set.values()])\n    recent_trend_weights = 0.5  # Weight for recent scores in the overall score calculation\n    weighted_means = (1 - recent_trend_weights) * action_means + recent_trend_weights * recent_scores\n\n    # Dynamic exploration factor\n    epsilon = np.clip(1.0 - (current_time_slot / total_time_slots), 0.1, 1.0)\n    exploration_values = np.sqrt(action_counts + 1) / (total_selection_count + 1)\n    exploration_gain = epsilon * np.clip(exploration_values, 0, 1)\n\n    # Combine weighted means with exploration gain\n    exploration_scores = weighted_means + exploration_gain\n    action_index = np.random.choice(num_actions, p=exploration_scores / np.sum(exploration_scores))\n\n    return action_index",
          "objective": -449.58631451355205,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Adjusting for the number of times each action has been selected\n    adjusted_means = action_means * np.sqrt((total_selection_count + 1) / (action_counts + 1))\n\n    # Recent score weighting\n    weight_factor = current_time_slot / total_time_slots\n    weighted_means = adjusted_means * (1 + weight_factor)\n\n    # Dynamic exploration strategy based on time slot\n    epsilon = 1.0 / (current_time_slot + 1)\n    exploration_gain = np.random.rand() < epsilon\n\n    if exploration_gain:\n        action_index = np.random.randint(0, num_actions)  # Explore\n    else:\n        action_index = np.argmax(weighted_means)  # Exploit\n\n    return action_index",
          "objective": -449.58415322307707,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n    \n    # Calculate means and counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n    \n    # Avoiding division by zero for adjusted means\n    adjusted_means = action_means / (action_counts + 1e-5)\n    \n    # Dynamic exploration factor\n    epsilon = 0.1 * (1 - (current_time_slot / total_time_slots))\n    \n    # Recent performance weighting\n    recent_weight = 0.7 * (current_time_slot / total_time_slots)\n    weighted_performance = adjusted_means * (1 + recent_weight)\n    \n    # Exploration factor\n    exploration_value = np.random.rand()\n    if exploration_value < epsilon:\n        action_index = np.random.randint(0, num_actions)  # Explore\n    else:\n        action_index = np.argmax(weighted_performance)  # Exploit\n    \n    return action_index",
          "objective": -449.5736909302401,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n    \n    # Calculate historical means and counts for each action\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Incorporate a score normalization to avoid large discrepancies\n    min_mean = np.min(action_means) if action_counts.any() else 0\n    max_mean = np.max(action_means) if action_counts.any() else 1\n    normalized_means = (action_means - min_mean) / (max_mean - min_mean + 1e-5)\n\n    # Recent performance evaluation focusing on last scores\n    recent_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        recent_scores = score_set.get(action_index, [])[-5:]  # Last 5 scores\n        recent_means[action_index] = np.mean(recent_scores) if recent_scores else 0\n    \n    # Combine historical and recent performance\n    recent_weight = max(0.1 * (current_time_slot / total_time_slots), 0.05)\n    combined_scores = (1 - recent_weight) * normalized_means + recent_weight * recent_means\n\n    # Exploration bonus using UCB method\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n    \n    # Final scores integration\n    final_scores = combined_scores + exploration_bonus\n\n    # Phased exploration strategy\n    exploration_phase = max(0, 1 - (current_time_slot / total_time_slots))  # Reduces exploration over time\n    if np.random.rand() < exploration_phase:\n        action_index = np.random.choice(np.arange(action_count))\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -449.55353227580434,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0.0\n\n    # Recent performance weighting with an adaptive approach\n    recent_weight = 0.6  # More weight on recent scores\n    recent_scores = [scores[-1] if scores else 0 for scores in score_set.values()]\n    action_means = (1 - recent_weight) * action_means + recent_weight * np.array(recent_scores)\n\n    # Epsilon decay function\n    epsilon = 1.0 - (current_time_slot / total_time_slots) * 0.9\n    epsilon = max(0.1, epsilon)\n\n    # Selection bonus to encourage exploring less frequent actions\n    selection_bonus = np.where(action_counts > 0, (1.0 - (action_counts / total_selection_count)), 1.0)\n    adjusted_scores = action_means + selection_bonus * 0.1 * (1 - action_means)\n\n    # Probability distribution for action selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, num_actions)  # Explore\n    else:\n        action_index = np.random.choice(num_actions, p=adjusted_scores / np.sum(adjusted_scores))  # Probabilistic exploit\n\n    return action_index",
          "objective": -449.54998981147355,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n    \n    # Normalize the average scores\n    normalized_means = action_means / (action_counts + 1)  # Prevent division by zero\n\n    # Recent performance weights\n    recent_scores = np.zeros(action_count)\n    for action_index in range(action_count):\n        total_scores = score_set.get(action_index, [])\n        recent_weights = total_scores[-10:] if len(total_scores) > 10 else total_scores\n        recent_scores[action_index] = np.mean(recent_weights) if recent_weights else 0\n\n    # Weighting factors for scores\n    recent_weight = np.clip(current_time_slot / total_time_slots, 0, 1)\n    historical_weight = 1 - recent_weight\n    weighted_scores = (recent_weight * recent_scores) + (historical_weight * normalized_means)\n\n    # Exploration bonus\n    exploration_bonus = (np.log(total_selection_count + 1) + 1) / (action_counts + 1)\n    \n    # Final scores with exploration\n    final_scores = weighted_scores + exploration_bonus\n\n    # Epsilon for exploration/exploitation balance\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -449.5478621029811,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Weighting for recent performance\n    recent_weight = 0.5\n    if current_time_slot > 0:\n        recent_scores = np.array([score_set.get(i, [])[-1] for i in range(num_actions) if len(score_set.get(i, [])) > 0])\n        recent_means = np.zeros(num_actions)\n        recent_means[[i for i in range(num_actions) if len(score_set.get(i, [])) > 0]] = recent_scores\n        action_means += recent_weight * (recent_means - action_means)\n\n    # Exploration factor\n    exploration_factor = max(0.1 * (1 - (total_selection_count / (total_time_slots + 1))), 0.01)\n\n    # Selection probabilities\n    selection_scores = action_means + (exploration_factor * (1 / (1 + action_counts)))\n    selection_scores -= np.min(selection_scores)  # Shift to non-negative\n    probabilities = selection_scores / np.sum(selection_scores)\n\n    action_index = np.random.choice(num_actions, p=probabilities)\n\n    return action_index",
          "objective": -449.5347827919213,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n    \n    # Calculate average scores and counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Recent score weighting with exponential decay\n    decay_factor = 0.9\n    weighted_means = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        weighted_score = sum(decay_factor ** weight * score for weight, score in enumerate(reversed(scores)))\n        if action_counts[action_index] > 0:\n            weighted_means[action_index] = weighted_score / (1 - decay_factor ** action_counts[action_index])\n    \n    # Dynamic epsilon for exploration-exploitation tradeoff\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n\n    if np.random.rand() < epsilon:\n        # Favor less-selected actions for exploration\n        exploration_probabilities = (1 / (action_counts + 1))\n        action_index = np.random.choice(np.arange(num_actions), p=exploration_probabilities / np.sum(exploration_probabilities))\n    else:\n        # Select action based on weighted means for exploitation\n        action_index = np.argmax(weighted_means)\n    \n    return action_index",
          "objective": -449.5155947397753,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n    \n    # Calculate means and counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n    \n    # Recent mean calculation (consider last 5 scores for faster adaptation)\n    recent_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        recent_scores = score_set.get(action_index, [])[-5:]\n        recent_means[action_index] = np.mean(recent_scores) if recent_scores else 0\n\n    # Exploration bonus (encourages exploration of less selected actions)\n    exploration_bonus = np.log(total_selection_count + 1) / (action_counts + 1e-5)\n\n    # Adaptive weighting system based on the time slot\n    recency_weight = (current_time_slot / total_time_slots) ** 2\n    historical_weight = 1 - recency_weight\n    \n    # Composite score calculation\n    adjusted_scores = (recency_weight * recent_means + \n                       historical_weight * action_means + \n                       exploration_bonus)\n\n    # Stochastic decision-making via epsilon-greedy approach\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n    \n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": -449.46993942519083,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n    \n    # Compute means and counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n    \n    # Calculate recent means\n    recent_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        recent_scores = score_set.get(action_index, [])[-10:]  # last 10 scores\n        recent_means[action_index] = np.mean(recent_scores) if recent_scores else 0\n    \n    # Combine historical and recent means with dynamic weights\n    recent_weight = min(1, current_time_slot / total_time_slots)\n    historical_weight = 1 - recent_weight\n    combined_scores = (recent_weight * recent_means) + (historical_weight * action_means)\n\n    # Exploration bonus\n    exploration_bonus = np.log(total_selection_count + 1) / (action_counts + 1e-5)  # Avoid division by zero\n\n    # Final scores\n    final_scores = combined_scores + exploration_bonus\n    \n    # Dynamic epsilon for exploration\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n\n    # Action selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -449.464688514562,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Calculate historical performance and counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if scores else 0\n\n    # Normalize the means\n    normalized_means = action_means / np.maximum(action_counts, 1)  # Avoid division by zero\n\n    # Recent performance based on the last 5 scores\n    recent_scores = np.array([\n        np.mean(score_set.get(action_index, [])[-5:]) if len(score_set.get(action_index, [])) >= 5 else 0\n        for action_index in range(action_count)\n    ])\n\n    # Weight for recent performance\n    recent_weight = max(0.1, current_time_slot / total_time_slots)\n    combined_scores = (recent_weight * recent_scores) + ((1 - recent_weight) * normalized_means)\n\n    # Exploration bonus calculation\n    exploration_bonus = np.log(total_selection_count + 1) / np.maximum(action_counts + 1, 1)\n\n    # Compute final scores\n    final_scores = combined_scores + exploration_bonus\n\n    # Epsilon for exploration-exploitation\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -449.3934483151065,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Recent performance emphasis using exponential decay\n    decay_factor = 0.9\n    recent_means = np.array([\n        np.mean(scores[-3:]) if len(scores) >= 3 else action_means[i] for i, scores in score_set.items()\n    ])\n    \n    effective_means = decay_factor * recent_means + (1 - decay_factor) * action_means\n\n    # Dynamic exploration-exploitation strategy\n    epsilon = max(0.1, 0.5 * (total_selection_count / (total_time_slots * num_actions)))\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, num_actions)  # Explore\n    else:\n        action_index = np.argmax(effective_means)  # Exploit\n    \n    # Encourage action variety\n    adjustment_factor = 0.1\n    action_probabilities = np.ones(num_actions) / num_actions + adjustment_factor * (1 - (action_counts / total_selection_count))\n    action_probabilities = np.clip(action_probabilities, 0, None)\n    action_probabilities /= np.sum(action_probabilities)  # Normalize\n\n    # Potential mixture of exploration and optimized selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(np.arange(num_actions), p=action_probabilities)  # Explore with adjusted probability\n\n    return action_index",
          "objective": -449.3754339200653,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n    \n    # Calculate historical means and counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if scores else 0.0\n\n    # Calculate recent means (last 5 scores)\n    recent_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        recent_scores = score_set.get(action_index, [])[-5:]\n        recent_means[action_index] = np.mean(recent_scores) if recent_scores else 0.0\n\n    # Define recency and historical weighting\n    recency_weight = min(1.0, current_time_slot / (total_time_slots / 2))\n    historical_weight = 1.0 - recency_weight\n\n    # Compute weighted scores\n    weighted_scores = (recency_weight * recent_means) + (historical_weight * action_means)\n\n    # Exploration bonus (using a simple scale)\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n    final_scores = weighted_scores + exploration_bonus\n\n    # Dynamic epsilon-greedy strategy\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n    \n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -449.31033656816703,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Calculate action means and counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Normalize action means based on counts\n    normalized_means = action_means / (1 + action_counts)\n\n    # Recent performance evaluation\n    recent_scores = np.zeros(action_count)\n    for action_index in range(action_count):\n        total_scores = score_set.get(action_index, [])\n        recent_weights = total_scores[-10:] if len(total_scores) > 10 else total_scores\n        recent_scores[action_index] = np.mean(recent_weights) if recent_weights else 0\n\n    # Combining recent and historical scores\n    recent_weight = current_time_slot / (total_time_slots + 1)\n    historical_weight = 1 - recent_weight\n    weighted_scores = (recent_weight * recent_scores) + (historical_weight * normalized_means)\n\n    # Exploration bonus\n    exploration_bonus = (np.log(total_selection_count + 1) + 1) / (1 + action_counts)\n\n    # Final scores calculation\n    final_scores = weighted_scores + exploration_bonus\n\n    # Epsilon-greedy approach for action selection\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -449.30615384237103,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Calculate action means and counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Calculate recent means\n    recent_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        recent_scores = score_set.get(action_index, [])[-5:]  # last 5 scores\n        recent_means[action_index] = np.mean(recent_scores) if recent_scores else 0\n\n    # Determine weights based on time\n    recency_weight = min(1, current_time_slot / (total_time_slots * 0.5))\n    historical_weight = 1 - recency_weight\n\n    # Calculate combined scores\n    weighted_scores = (recency_weight * recent_means) + (historical_weight * action_means)\n\n    # Exploration bonus\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n\n    final_scores = weighted_scores + exploration_bonus\n    \n    # Epsilon-greedy strategy for exploration\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.05)\n    \n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(np.arange(action_count))\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -449.30283311219745,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Calculate means and counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Recent performance weighting\n    recent_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        recent_scores = score_set.get(action_index, [])[-5:]\n        recent_means[action_index] = np.mean(recent_scores) if recent_scores else 0\n\n    # Adaptive weights based on time\n    recency_weight = np.clip(current_time_slot / (total_time_slots + 1e-5), 0, 1)\n    historical_weight = 1 - recency_weight\n\n    # Calculate combined scores with exploration bonus\n    weighted_scores = (recency_weight * recent_means) + (historical_weight * action_means)\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n\n    # Final scores for selection\n    final_scores = weighted_scores + exploration_bonus\n\n    # Epsilon-greedy strategy\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.05)\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(np.arange(action_count))\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -449.29900557767377,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.array([np.mean(scores) if scores else 0 for scores in score_set.values()])\n    action_counts = np.array([len(scores) for scores in score_set.values()])\n\n    # Calculation of recent means (last 3 scores)\n    recent_means = np.array([\n        np.mean(scores[-3:]) if len(scores) >= 3 else (np.mean(scores) if scores else 0) for scores in score_set.values()\n    ])\n    \n    # Dynamically weight recent and historical scores\n    weight_recent = current_time_slot / total_time_slots\n    weight_historical = 1 - weight_recent\n\n    # Compute combined scores\n    combined_scores = (weight_recent * recent_means) + (weight_historical * action_means)\n\n    # Exploration bonus based on selection count\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n    final_scores = combined_scores + exploration_bonus\n\n    # Epsilon-greedy selection mechanism\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -449.28241851065223,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate action means and counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if scores else 0.0\n\n    # Emphasize recent performance with a weighted recent score\n    recent_scores = np.array([scores[-1] if scores else 0 for scores in score_set.values()])\n    recent_weight = 0.6  # Increased weight for recent performance\n    weighted_means = (1 - recent_weight) * action_means + recent_weight * recent_scores\n\n    # Adaptive epsilon for exploration\n    epsilon = max(0.05, 1.0 - (current_time_slot / total_time_slots))\n\n    # Exploration bonus based on action counts and total selections\n    exploration_bonus = np.sqrt(action_counts + 1) / (total_selection_count + 1)\n\n    # Combine weighted means with exploration\n    total_values = weighted_means + epsilon * exploration_bonus\n\n    # Normalize to create a probability distribution\n    total_values_normalized = total_values / np.sum(total_values)\n\n    # Select action based on probabilities\n    action_index = np.random.choice(np.arange(num_actions), p=total_values_normalized)\n    \n    return action_index",
          "objective": -449.24494709499965,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    recent_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        recent_scores = score_set.get(action_index, [])[-10:]\n        recent_means[action_index] = np.mean(recent_scores) if recent_scores else 0\n\n    recency_weight = min(1, current_time_slot / total_time_slots)\n    historical_weight = 1 - recency_weight\n\n    weighted_scores = recency_weight * recent_means + historical_weight * action_means\n\n    # Using square root of counts for exploration bonus to balance exploration better\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n\n    final_scores = weighted_scores + exploration_bonus\n\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -449.2193990701642,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Calculate means and counts for each action\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Calculate recent means with more weight on the last 5 scores or all if less than 5\n    recent_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        recent_scores = score_set.get(action_index, [])[-5:]\n        recent_means[action_index] = np.mean(recent_scores) if recent_scores else 0\n\n    # Dynamic weight towards recent performance\n    recency_weight = min(1, current_time_slot / (total_time_slots * 0.66))\n    historical_weight = 1 - recency_weight\n\n    # Calculate combined performance scores\n    combined_scores = (recency_weight * recent_means) + (historical_weight * action_means)\n\n    # Exploration bonus based on action counts\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n\n    # Add exploration bonus to combined scores\n    final_scores = combined_scores + exploration_bonus\n\n    # Probabilistic selection with a decreasing exploration rate\n    exploration_rate = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.05)\n    \n    if np.random.rand() < exploration_rate:\n        action_index = np.random.choice(np.arange(action_count))\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -449.1174281321212,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_counts = np.zeros(8)\n    action_means = np.zeros(8)\n\n    # Calculate action means and counts\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Recent score weighting\n    recency_weight = 1 - (current_time_slot / total_time_slots)\n    weighted_means = action_means * (1 + recency_weight)\n\n    # Dynamic exploration-exploitation factor\n    base_epsilon = 0.1\n    epsilon = base_epsilon * np.exp(-total_selection_count / 100)  # Decrease over time\n\n    # Select action based on exploration-exploitation strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, 8)  # Explore\n    else:\n        action_index = np.random.choice(np.flatnonzero(weighted_means == np.max(weighted_means)))  # Exploit\n\n    return action_index",
          "objective": -449.11263055906113,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Calculate means and counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Recent mean calculation\n    recent_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        recent_scores = score_set.get(action_index, [])[-10:]\n        recent_means[action_index] = np.mean(recent_scores) if recent_scores else 0\n\n    # Compute dynamic weights for exploration and recency\n    exploration_weight = np.log(total_selection_count + 1) / (action_counts + 1e-5)\n    recency_factor = current_time_slot / total_time_slots\n    historical_factor = 1 - recency_factor\n    \n    # Composite score calculation\n    adjusted_scores = (recency_factor * recent_means + historical_factor * action_means) + exploration_weight\n\n    # Stochastic exploration with adaptive epsilon\n    epsilon_decay = 0.1 * (1 - (current_time_slot / total_time_slots))\n    epsilon = max(epsilon_decay, 0.01)\n\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": -449.0474120497334,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Recent performance emphasis\n    recent_weight = 1 + (current_time_slot / total_time_slots)\n    weighted_means = action_means * recent_weight\n\n    # Dynamic epsilon for exploration\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n\n    # Exploration probability distribution\n    exploration_probabilities = (1 / (action_counts + 1)) if total_selection_count > 0 else np.ones(num_actions)\n    exploration_probabilities /= np.sum(exploration_probabilities)\n\n    # Total probabilities for selection\n    total_probabilities = np.zeros(num_actions)\n    total_probabilities = (1 - epsilon) * (weighted_means / np.sum(weighted_means)) + \\\n                         epsilon * exploration_probabilities\n\n    action_index = np.random.choice(np.arange(num_actions), p=total_probabilities)\n\n    return action_index",
          "objective": -449.00401097166855,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.array([np.mean(scores) if scores else 0 for scores in score_set.values()])\n    action_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Calculate recent means (last 3 scores or fewer)\n    recent_means = np.array([\n        np.mean(scores[-3:]) if len(scores) > 0 else 0 for scores in score_set.values()\n    ])\n    \n    # Weighting system for recent and historical data\n    weight_recent = min(1, current_time_slot / total_time_slots)\n    weight_historical = 1 - weight_recent\n\n    # Compute weighted scores\n    weighted_scores = (weight_recent * recent_means) + (weight_historical * action_means)\n\n    # Exploration bonus\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n    final_scores = weighted_scores + exploration_bonus\n\n    # Epsilon-greedy for action selection\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -448.92313974411405,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Calculate means and counts for each action\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Recent performance evaluation\n    recent_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        recent_scores = score_set.get(action_index, [])[-5:]  # Last 5 scores\n        recent_means[action_index] = np.mean(recent_scores) if recent_scores else 0\n\n    # Dynamic weighting based on recency and historical evaluations\n    recency_weight = min(1, current_time_slot / (total_time_slots * 0.5))\n    historical_weight = 1 - recency_weight\n    combined_scores = (recency_weight * recent_means + historical_weight * action_means)\n\n    # Exploration bonus using UCB\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n    final_scores = combined_scores + exploration_bonus\n    \n    # Epsilon-greedy strategy for exploration\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.05)\n    \n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(np.arange(action_count))\n    else:\n        action_index = np.argmax(final_scores)\n    \n    return action_index",
          "objective": -448.89370939333764,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Calculate means and counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Dynamic parameters for exploration-exploitation trade-off\n    exploration_constant = np.log(total_selection_count + 1) / (action_counts + 1e-5)\n    recency_factor = np.sqrt(current_time_slot / total_time_slots)\n    \n    # Composite score calculation\n    composite_scores = (0.7 * action_means + 0.3 * exploration_constant) + recency_factor\n\n    # Epsilon-greedy for exploration\n    epsilon_decay = 0.1 * (1 - (current_time_slot / total_time_slots))\n    epsilon = max(epsilon_decay, 0.01)\n    \n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(composite_scores)\n\n    return action_index",
          "objective": -448.88899496399205,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate average scores and counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if scores else 0.0\n\n    # Weight recent performance with exponential decay\n    recent_weight = 0.5\n    for action_index in range(num_actions):\n        if action_counts[action_index] > 0:\n            recent_score = scores[-1] if scores else 0\n            action_means[action_index] = (1 - recent_weight) * action_means[action_index] + recent_weight * recent_score\n\n    # Dynamic exploration parameter\n    epsilon = max(0.1, 1.0 - (current_time_slot / total_time_slots) * 0.9)\n\n    # Bonus for less frequently chosen actions\n    selection_bonus = (1.0 - (action_counts / total_selection_count)) if total_selection_count > 0 else np.ones(num_actions)\n\n    # Adjusted scores\n    adjusted_scores = action_means + selection_bonus * (1 - action_means) * 0.1\n\n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, num_actions)  # Explore\n    else:\n        action_index = np.argmax(adjusted_scores)  # Exploit\n\n    return action_index",
          "objective": -448.7611522319338,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Calculate means and counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Recent mean calculation for the last 5 scores\n    recent_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        recent_scores = score_set.get(action_index, [])[-5:]\n        recent_means[action_index] = np.mean(recent_scores) if recent_scores else 0\n    \n    # Dynamic weights for the exploration-exploitation trade-off\n    exploration_constant = np.log(total_selection_count + 1) / (action_counts + 1e-5)\n\n    # Composite score calculation: blend recent and historical means with exploration\n    total_weight = 0.8\n    adjusted_scores = (total_weight * recent_means + (1 - total_weight) * action_means) + exploration_constant\n\n    # Epsilon-greedy exploration\n    epsilon_decay = 0.1 * (1 - (current_time_slot / total_time_slots))\n    epsilon = max(epsilon_decay, 0.05)\n\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": -448.74976797397227,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Calculate action means and counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Recent performance evaluation with a focus on the last 10 scores\n    recent_scores = np.zeros(action_count)\n    for action_index in range(action_count):\n        total_scores = score_set.get(action_index, [])\n        recent_weights = total_scores[-10:] if len(total_scores) > 10 else total_scores\n        recent_scores[action_index] = np.mean(recent_weights) if recent_weights else 0\n\n    # Dynamic weights based on time slot\n    recent_weight = current_time_slot / total_time_slots\n    historical_weight = 1 - recent_weight\n    weighted_scores = (recent_weight * recent_scores) + (historical_weight * action_means)\n\n    # Exploration bonus\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1))\n\n    # Combine scores and exploration\n    final_scores = weighted_scores + exploration_bonus\n    \n    # Probabilistic selection using softmax for a better exploration-exploitation balance\n    exp_scores = np.exp(final_scores - np.max(final_scores))  # Shift for numerical stability\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    action_index = np.random.choice(action_count, p=probabilities)\n\n    return action_index",
          "objective": -448.7023334214463,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_means = np.zeros(8)\n    action_counts = np.zeros(8)\n    \n    # Calculate means and counts\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Weight recent performance more heavily\n    recent_weight = 0.5\n    for action_index in range(8):\n        if action_counts[action_index] > 0:\n            action_means[action_index] = (1 - recent_weight) * action_means[action_index] + \\\n                                          recent_weight * scores[-1]\n\n    # Dynamic exploration-exploitation ratio using softmax\n    exploration_factor = 0.1\n    exp_means = np.exp(action_means / exploration_factor)\n    probabilities = exp_means / np.sum(exp_means)\n\n    # Select action based on probabilities\n    action_index = np.random.choice(range(8), p=probabilities)\n\n    return action_index",
          "objective": -448.7011491427396,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.array([np.mean(scores) if scores else 0 for scores in score_set.values()])\n    action_counts = np.array([len(scores) for scores in score_set.values()])\n\n    # Calculation of recent means (last 3 scores)\n    recent_means = np.array([\n        np.mean(scores[-3:]) if len(scores) >= 3 else (np.mean(scores) if scores else 0) for scores in score_set.values()\n    ])\n    \n    # Dynamically weight recent and historical scores\n    weight_recent = current_time_slot / total_time_slots\n    weight_historical = 1 - weight_recent\n\n    # Compute combined scores\n    combined_scores = (weight_recent * recent_means) + (weight_historical * action_means)\n\n    # Exploration bonus\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n    final_scores = combined_scores + exploration_bonus\n\n    # Epsilon-greedy selection mechanism\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -448.6613022557835,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    scores = np.zeros(action_count)\n    counts = np.zeros(action_count)\n\n    # Calculate the mean scores and handle empty selections\n    for action_index in range(action_count):\n        action_scores = score_set.get(action_index, [])\n        counts[action_index] = len(action_scores)\n        scores[action_index] = np.mean(action_scores) if counts[action_index] > 0 else 0\n\n    # Normalize scores to avoid division by zero\n    normalized_scores = scores / (counts + 1e-5)\n\n    # Recent performance emphasis\n    recent_scores = np.array([\n        np.mean(score_set.get(action_index, [])[-10:]) if counts[action_index] > 0 else 0\n        for action_index in range(action_count)\n    ])\n    \n    # Dynamic weighting of recent performance\n    recent_weight = current_time_slot / total_time_slots\n    combined_scores = (1 - recent_weight) * normalized_scores + recent_weight * recent_scores\n\n    # Exploration factor based on visit counts\n    exploration_bonus = (1 + np.log(total_selection_count + 1)) / (counts + 1e-5)\n\n    # Compute final scores\n    final_scores = combined_scores + exploration_bonus\n\n    # Epsilon for exploratory behavior\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n\n    # Select action using weighted probabilities\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -448.5800256995601,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.array([np.mean(scores) if scores else 0 for scores in score_set.values()])\n    action_counts = np.array([len(scores) for scores in score_set.values()])\n\n    # Compute recent means with added weight\n    recent_weight_factor = 0.5\n    recent_means = np.array([\n        np.mean(scores[-3:]) * recent_weight_factor if len(scores) >= 3 else (np.mean(scores) * recent_weight_factor if scores else 0) \n        for scores in score_set.values()\n    ])\n    \n    # Calculate historical means\n    historical_means = action_means * (1 - recent_weight_factor)\n\n    # Combine recent and historical means\n    combined_scores = recent_means + historical_means\n\n    # Exploration bonus\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n    final_scores = combined_scores + exploration_bonus\n\n    # Epsilon-greedy exploration strategy\n    exploration_rate = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n    if np.random.rand() < exploration_rate:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -448.57252615228685,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Calculate historical performance and counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Recent performance using the last few scores\n    recent_scores = np.array([\n        np.mean(score_set.get(action_index, [])[-10:]) if action_counts[action_index] > 0 else 0\n        for action_index in range(action_count)\n    ])\n\n    # Combining recent and historical means with progressive weighting\n    recent_weight = current_time_slot / total_time_slots\n    historical_weight = 1 - recent_weight\n    combined_scores = (historical_weight * action_means) + (recent_weight * recent_scores)\n\n    # Exploration bonus\n    exploration_bonus = (np.log(total_selection_count + 1) + 1) / (action_counts + 1e-5)\n\n    # Final scores\n    final_scores = combined_scores + exploration_bonus\n\n    # Epsilon-greedy strategy for exploration vs. exploitation\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -448.5119158430678,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    recent_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        recent_scores = score_set.get(action_index, [])[-10:]\n        recent_means[action_index] = np.mean(recent_scores) if recent_scores else 0\n\n    recency_weight = min(1, current_time_slot / total_time_slots)\n    historical_weight = 1 - recency_weight\n\n    weighted_scores = recency_weight * recent_means + historical_weight * action_means\n    exploration_bonus = np.log(total_selection_count + 1) / (action_counts + 1e-5)\n\n    final_scores = weighted_scores + exploration_bonus\n\n    # Epsilon decay strategy\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -448.42856839292614,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Calculate means and counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Recent mean calculation (only consider last 10 scores)\n    recent_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        recent_scores = score_set.get(action_index, [])[-10:]\n        recent_means[action_index] = np.mean(recent_scores) if recent_scores else 0\n\n    # Dynamic weights for exploration and historical preference\n    exploration_bonus = np.log(total_selection_count + 1) / (action_counts + 1e-5)\n    \n    # Weighting based on time\n    recency_weight = (current_time_slot / total_time_slots)**2\n    historical_weight = 1 - recency_weight\n\n    # Composite score calculation\n    adjusted_scores = (recency_weight * recent_means + historical_weight * action_means + exploration_bonus)\n\n    # Epsilon-greedy exploration strategy\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n    \n    # Random selection for exploration\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": -448.4101652475827,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n    \n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    recent_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        recent_scores = score_set.get(action_index, [])[-5:]\n        recent_means[action_index] = np.mean(recent_scores) if recent_scores else 0\n\n    recency_weight = min(1, current_time_slot / (total_time_slots * 0.5))\n    historical_weight = 1 - recency_weight\n\n    weighted_scores = (recency_weight * recent_means) + (historical_weight * action_means)\n\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n\n    final_scores = weighted_scores + exploration_bonus\n\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.05)\n    \n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(np.arange(action_count))\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -448.4100415655145,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate action means and counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Emphasize recent performance\n    recent_scores = np.array([scores[-1] if len(scores) > 0 else 0 for scores in score_set.values()])\n    weighted_means = 0.7 * action_means + 0.3 * recent_scores\n\n    # Dynamic exploration factor\n    epsilon = 1.0 - (current_time_slot / total_time_slots)\n    exploration_values = np.sqrt(action_counts + 1) / (total_selection_count + 1)\n    exploration_gain = epsilon * np.clip(exploration_values, 0, 1)\n\n    exploration_probabilities = exploration_gain / np.sum(exploration_gain)\n\n    # Stochastic decision-making\n    if np.random.rand() < exploration_probabilities[np.argmax(weighted_means)]:\n        action_index = np.random.choice(num_actions, p=exploration_probabilities)  # Explore\n    else:\n        action_index = np.argmax(weighted_means)  # Exploit\n\n    return action_index",
          "objective": -448.40814504439163,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Compute historical means and counts for each action\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Weighted scores with recent performance emphasizing the last few scores\n    recent_weights = np.clip(current_time_slot / total_time_slots, 0, 1)\n    historical_weights = 1 - recent_weights\n\n    recent_mean_scores = np.zeros(action_count)\n    for action_index in range(action_count):\n        recent_scores = score_set.get(action_index, [])[-10:]\n        recent_mean_scores[action_index] = np.mean(recent_scores) if recent_scores else 0\n\n    weighted_action_scores = (recent_weights * recent_mean_scores +\n                              historical_weights * action_means)\n\n    # Exploration bonus based on action selection counts\n    epsilon_bonus = np.log(total_selection_count + 1) / (action_counts + 1e-5)\n\n    # Final decision scores\n    final_scores = weighted_action_scores + epsilon_bonus\n\n    # Epsilon for exploration versus exploitation\n    epsilon = max(0.05, 0.2 * (1 - (current_time_slot / total_time_slots)))\n\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -448.3657006277262,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    for action_index in range(action_count):\n        if action_index in score_set and score_set[action_index]:\n            scores = score_set[action_index]\n            action_counts[action_index] = len(scores)\n            action_means[action_index] = np.mean(scores)\n    \n    recent_means = np.zeros(action_count)\n    recent_window_size = min(5, total_selection_count)  # Limit recent scores to actual available ones\n    for action_index in range(action_count):\n        recent_scores = score_set.get(action_index, [])[-recent_window_size:]\n        recent_means[action_index] = np.mean(recent_scores) if recent_scores else 0\n\n    recency_weight = 0.7 if current_time_slot > total_time_slots * 0.5 else 0.3\n    historical_weight = 1 - recency_weight\n\n    weighted_scores = recency_weight * recent_means + historical_weight * action_means\n\n    # Exploration bonus based on selection counts\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n    \n    # Final scores combining both parts\n    final_scores = weighted_scores + exploration_bonus\n\n    # Epsilon-greedy strategy for exploration-exploitation balance\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.05)\n    \n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(np.arange(action_count))\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -448.34007358820577,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Compute average scores and counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Recent score weighting (consider last 3 scores)\n    decay_factor = 0.5\n    recent_weighted_means = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        if action_counts[action_index] > 0:\n            recent_scores = score_set[action_index][-min(3, len(scores)):]  # Get last up to 3 scores\n            recent_mean = np.mean(recent_scores) if len(recent_scores) > 0 else 0\n            recent_weighted_means[action_index] = (1 - decay_factor) * recent_mean + decay_factor * action_means[action_index]\n\n    # Adaptive exploration-exploitation strategy\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.05)\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, num_actions)  # Explore\n    else:\n        # Confidence score enhances exploitation\n        confidence_scores = (recent_weighted_means + np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5)))\n        action_index = np.argmax(confidence_scores)  # Exploit\n\n    return action_index",
          "objective": -448.2808256136027,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if scores else 0\n\n    recent_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        recent_scores = score_set.get(action_index, [])[-5:]\n        recent_means[action_index] = np.mean(recent_scores) if recent_scores else 0\n\n    recency_weight = min(1, current_time_slot / (total_time_slots * 0.5))\n    historical_weight = 1 - recency_weight\n\n    weighted_scores = (recency_weight * recent_means) + (historical_weight * action_means)\n\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n    \n    final_scores = weighted_scores + exploration_bonus\n\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.05)\n\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(np.arange(action_count))\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -448.0558152759925,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Recent score weighting (exponential decay)\n    decay_factor = 0.8\n    recent_weighted_means = np.zeros(num_actions)\n    \n    for action_index in range(num_actions):\n        if action_counts[action_index] > 0:\n            recent_scores = score_set[action_index][-min(3, len(scores)):]  # Last 3 scores or less\n            recent_mean = np.mean(recent_scores) if len(recent_scores) > 0 else 0\n            recent_weighted_means[action_index] = (1 - decay_factor) * recent_mean + decay_factor * action_means[action_index]\n    \n    # Exploration-Exploitation\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.05)  # Gradually decrease exploration\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, num_actions)  # Explore\n    else:\n        action_index = np.argmax(recent_weighted_means)  # Exploit\n    \n    return action_index",
          "objective": -447.9856443914451,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8  # Fixed number of actions\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n    \n    # Calculate effective scores with weighted averages\n    recent_weight = min(1, current_time_slot / total_time_slots)\n    historical_weight = 1 - recent_weight\n    \n    recent_weighted_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        recent_scores = scores[-10:] if len(scores) > 10 else scores\n        recent_mean = np.mean(recent_scores) if recent_scores else 0\n        recent_weighted_means[action_index] = (recent_weight * recent_mean + \n                                                historical_weight * action_means[action_index])\n    \n    # Exploration bonus for underutilized actions\n    exploration_bonus = np.log(total_selection_count + 1) / (action_counts + 1)\n    \n    # Final scores combining historical and exploration aspects\n    final_scores = recent_weighted_means + exploration_bonus\n    \n    # Dynamic epsilon for exploration\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n\n    if np.random.rand() < epsilon:\n        # Exploration\n        action_index = np.random.randint(0, action_count)\n    else:\n        # Exploitation\n        action_index = np.argmax(final_scores)\n    \n    return action_index",
          "objective": -447.95961364222154,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate action means and counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Exponential smoothing for recent performance emphasis\n    recent_weight = 0.9\n    smoothed_means = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        if action_counts[action_index] == 0:\n            smoothed_means[action_index] = action_means[action_index]\n        else:\n            smoothed_means[action_index] = (recent_weight * action_means[action_index] + \n                                            (1 - recent_weight) * np.mean(scores[-min(5, len(scores)):]) if len(scores) > 0 else 0)\n\n    # Dynamic exploration rate\n    epsilon = (total_time_slots - current_time_slot) / total_time_slots\n\n    # Selection bias for underutilized actions\n    selection_bonus = np.where(action_counts > 0, 0, 1)  # Encourage actions not selected\n    exploration_values = smoothed_means + selection_bonus\n\n    # Probabilistic action selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, num_actions)  # Explore\n    else:\n        action_index = np.argmax(exploration_values)  # Exploit\n\n    return action_index",
          "objective": -447.7240060206827,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.array([np.mean(scores) if scores else 0 for scores in score_set.values()])\n    action_counts = np.array([len(scores) for scores in score_set.values()])\n\n    # Enhanced recent performance (last 5 scores)\n    recent_means = np.array([\n        np.mean(scores[-5:]) if len(scores) >= 5 else (np.mean(scores) if scores else 0) for scores in score_set.values()\n    ])\n    \n    # Weighted average\n    weight_recent = min(1, current_time_slot / total_time_slots)\n    weight_historical = 1 - weight_recent\n\n    # Combine recent and historical scores\n    combined_scores = (weight_recent * recent_means) + (weight_historical * action_means)\n\n    # Exploration bonus: More significant impact for less explored actions\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n    final_scores = combined_scores + exploration_bonus\n\n    # Epsilon-greedy strategy adjusted for adaptive exploration\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -447.66484204626653,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    action_means = np.zeros(n_actions)\n    action_counts = np.zeros(n_actions)\n\n    # Compute action means and counts\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Adaptive epsilon-greedy strategy with time-dependent exploration factor\n    epsilon = max(0.1, min(0.5, (total_selection_count / (total_time_slots * n_actions)) * 0.5))\n\n    # Recent score weighting with exponential smoothing\n    recent_weight = 0.5\n    for action_index in range(n_actions):\n        if action_counts[action_index] > 0:\n            recent_score = score_set[action_index][-1]\n            action_means[action_index] = (1 - recent_weight) * action_means[action_index] + \\\n                                          recent_weight * recent_score\n\n    # Enhance diversity in action selection for less frequently tried actions\n    selection_bonus = 0.2 * (1 / (1 + action_counts))\n    adjusted_scores = action_means + selection_bonus\n\n    # Probabilistic adjustment using exploration term\n    exploration_term = np.random.rand(n_actions) * epsilon\n    final_scores = adjusted_scores + exploration_term\n\n    # Select action based on the highest score\n    action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -447.441968250157,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Calculate historical performance and counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Ensure no division by zero\n    normalized_means = np.divide(action_means, (action_counts + 1e-5))\n\n    # Recent performance with discount on past scores\n    recent_scores = np.array([\n        np.mean(score_set.get(action_index, [])[-10:]) if action_counts[action_index] > 0 else 0\n        for action_index in range(action_count)\n    ])\n    \n    # Dynamic weighting based on time slot progression\n    recent_weight = min(1.0, current_time_slot / total_time_slots)\n    historical_weight = 1 - recent_weight\n    weighted_scores = (recent_weight * recent_scores) + (historical_weight * normalized_means)\n\n    # Confidence interval adjustment (exploration factor)\n    confidence_intervals = np.sqrt((np.var(score_set.get(action_index, [])) if action_counts[action_index] > 0 else 1) / (action_counts + 1e-5))\n    exploration_adjustment = 1.5 * confidence_intervals\n    \n    # Combining scores with exploration factors\n    final_scores = weighted_scores + exploration_adjustment\n\n    # Epsilon for exploration-exploitation strategy\n    epsilon = 0.1 * (1 - (current_time_slot / total_time_slots))\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -447.40030489628424,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Recent performance focus\n    time_weight = np.sqrt((current_time_slot + 1) / (total_time_slots + 1))\n    recent_weighted_means = action_means * (1 + time_weight)\n\n    # Variance as an exploration factor\n    action_variances = np.array([\n        np.var(scores) if action_counts[action_index] > 0 else 1 for action_index, scores in score_set.items()\n    ])\n    \n    exploration_bonus = (np.log(total_selection_count + 1) / (action_counts + 1e-5)) * (1 + action_variances)\n\n    # Composite score calculation\n    composite_scores = recent_weighted_means + exploration_bonus\n\n    # Decay factor for exploration\n    exploration_strength = 0.1 * (1 - (current_time_slot / total_time_slots))\n    exploration_threshold = max(exploration_strength, 0.05)\n\n    # Epsilon-greedy selection\n    if np.random.rand() < exploration_threshold:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(composite_scores)\n\n    return action_index",
          "objective": -447.15210705643835,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Calculate historical means and counts for each action\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Recent performance evaluation focusing on the last few scores\n    recent_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        recent_scores = score_set.get(action_index, [])[-5:]  # Last 5 scores\n        recent_means[action_index] = np.mean(recent_scores) if recent_scores else 0\n\n    # Combine historical and recent performance with a focus on recency\n    recency_weight = np.clip(current_time_slot / (total_time_slots * 0.5), 0, 1)\n    historical_weight = 1 - recency_weight\n    combined_scores = (recency_weight * recent_means) + (historical_weight * action_means)\n\n    # Exploration bonus using a modified UCB method\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n\n    # Final scores integration\n    final_scores = combined_scores + exploration_bonus\n\n    # Epsilon-greedy mechanism to encourage exploration\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.05)\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(np.arange(action_count))\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -447.1096651445477,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Calculate historical performance and counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Adjusting for variance with counts\n    smoothed_means = action_means * (action_counts / (action_counts + 1))  \n\n    # Recent performance weighting\n    recent_scores = np.array([\n        np.mean(score_set.get(action_index, [])[-10:]) if len(score_set.get(action_index, [])) > 0 else 0\n        for action_index in range(action_count)\n    ])\n    \n    recent_weight = min(1.0, (current_time_slot / total_time_slots) ** 2)  # More weight to recent performance in early time slots\n    historical_weight = 1 - recent_weight\n    weighted_scores = (recent_weight * recent_scores) + (historical_weight * smoothed_means)\n\n    # Exploration bonus\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1))\n\n    # Final scores combining weighted and exploration factors\n    final_scores = weighted_scores + exploration_bonus\n\n    # Epsilon for exploration-exploitation\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -447.09114728850506,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Calculate historical performance and counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Normalize means to prevent zero division\n    normalized_means = action_means / (action_counts + 1e-5)\n\n    # Recent performance weighted by the last few selections\n    recent_scores = np.array([\n        np.mean(scores[-10:]) if len(scores) > 0 else 0\n        for scores in score_set.values()\n    ])\n\n    # Weight recent and historical performance\n    recent_weight = current_time_slot / total_time_slots\n    historical_weight = 1 - recent_weight\n    weighted_scores = (recent_weight * recent_scores) + (historical_weight * normalized_means)\n\n    # Exploration bonus\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n\n    # Final scores combination\n    final_scores = weighted_scores + exploration_bonus\n\n    # Epsilon-greedy exploration strategy\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n    \n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -446.7374791961482,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n    \n    # Calculate action means and counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if scores else 0\n    \n    # Emphasize recent performance\n    recent_weight = 0.7  # Adjusted for more recent emphasis\n    recent_scores = np.array([scores[-1] if scores else 0 for scores in score_set.values()])\n    weighted_means = (1 - recent_weight) * action_means + recent_weight * recent_scores\n    \n    # Dynamic epsilon for exploration, encouraging more exploration in earlier time slots\n    epsilon = max(0.1, 1.0 - (current_time_slot / total_time_slots))\n    \n    # Exploration bonus\n    exploration_bonus = np.sqrt(action_counts + 1) / (total_selection_count + 1)\n    \n    # Combine weighted means and exploration values\n    total_values = weighted_means + epsilon * exploration_bonus\n    \n    # Normalize to create a probability distribution\n    total_values_normalized = total_values / np.sum(total_values)\n    \n    # Select action based on probabilities\n    action_index = np.random.choice(np.arange(num_actions), p=total_values_normalized)\n    \n    return action_index",
          "objective": -446.7139853792934,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    action_means = np.zeros(n_actions)\n    action_counts = np.zeros(n_actions)\n\n    # Compute action means and counts\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Dynamic epsilon decay with time\n    epsilon = max(0.1, min(0.5, (total_selection_count / (total_time_slots * n_actions)) * 0.5))\n\n    # Recent performance weighting\n    if total_selection_count > 0:  # Avoid recent weighting if no selections made\n        recent_weight = 0.3\n        for action_index in range(n_actions):\n            if action_counts[action_index] > 0:  # Use most recent score\n                recent_scores = score_set[action_index][-1] if action_counts[action_index] > 0 else 0\n                action_means[action_index] = (\n                    (1 - recent_weight) * action_means[action_index] + \n                    recent_weight * recent_scores\n                )\n\n    # Selection bonus for untried actions\n    selection_bonus = 0.1\n    action_means += selection_bonus * (action_counts == 0)\n\n    # Probabilistic adjustment using exploration term\n    exploration_term = np.random.rand(n_actions) * epsilon\n    adjusted_scores = action_means + exploration_term\n\n    # Select action based on the highest score\n    action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": -446.4043604445471,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    action_means = np.zeros(n_actions)\n    action_counts = np.zeros(n_actions)\n\n    # Compute action means and counts\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Weight recent performance with a decay factor\n    recent_weight = 0.5\n    if total_selection_count > 0:\n        for action_index in range(n_actions):\n            if action_counts[action_index] > 0:\n                action_means[action_index] = (1 - recent_weight) * action_means[action_index] + \\\n                                              recent_weight * scores[-1]\n    \n    # Dynamic epsilon based on the current time slot\n    epsilon = max(0.1, min(0.5, (current_time_slot / total_time_slots) * 0.5))\n\n    # Selection bonus for actions that have never been selected\n    selection_bonus = 0.1\n    for action_index in range(n_actions):\n        if action_counts[action_index] == 0:\n            action_means[action_index] += selection_bonus\n\n    # Combined strategy for action selection:\n    # Epsilon-greedy approach mixing recent performance and exploration\n    exploration_term = np.random.rand(n_actions) * epsilon  # Random noise for exploration\n    adjusted_scores = action_means + exploration_term  # Add noise for exploration\n\n    # Select action based on the highest score\n    action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": -446.1874651316047,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n    \n    # Calculate means and counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Avoid division by zero for adjusted means\n    adjusted_means = action_means / (action_counts + 1e-5)\n\n    # Dynamic exploration factor based on total selections and time\n    epsilon = 0.1 * (1 - (current_time_slot / total_time_slots)) \n\n    # Recent performance weighting\n    recent_weight = 0.7 * (current_time_slot / total_time_slots)\n    weighted_performance = adjusted_means * (1 + recent_weight)\n\n    # Calculate selection probabilities with softmax to encourage exploration\n    selection_scores = weighted_performance / np.sum(weighted_performance)\n    \n    # Exploration decision based on epsilon\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, num_actions)  # Explore\n    else:\n        action_index = np.random.choice(num_actions, p=selection_scores)  # Exploit\n\n    return action_index",
          "objective": -445.7503401185869,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8  # Fixed number of actions\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Compute the historical means and counts for each action\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n    \n    # Calculate weights based on recency and historical performance\n    recent_weight = min(1, current_time_slot / total_time_slots)\n    historical_weight = 1 - recent_weight\n\n    # Calculate recent and historical scores\n    recent_weighted_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        recent_scores = scores[-10:] if len(scores) > 10 else scores\n        recent_mean = np.mean(recent_scores) if recent_scores else 0\n        recent_weighted_means[action_index] = (recent_weight * recent_mean + \n                                                historical_weight * action_means[action_index])\n    \n    # Exploration bonus\n    exploration_bonus = np.log(total_selection_count + 1) / (action_counts + 1e-5)  # Added small value to avoid division by zero\n\n    # Final scores\n    final_scores = recent_weighted_means + exploration_bonus\n\n    # Dynamic epsilon for exploration\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n\n    if np.random.rand() < epsilon:\n        # Exploration\n        action_index = np.random.randint(0, action_count)\n    else:\n        # Exploitation\n        action_index = np.argmax(final_scores)\n    \n    return action_index",
          "objective": -445.7487725709488,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Calculate dynamic exploration-exploitation balance\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1))\n    \n    # Temporal weighting for recent scores\n    recent_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        recent_scores = scores[-10:] if len(scores) > 10 else scores\n        recent_means[action_index] = np.mean(recent_scores) if recent_scores else 0\n\n    # Adjusting weights dynamically based on the time slot\n    recent_weight = min(1, current_time_slot / total_time_slots)\n    historical_weight = 1 - recent_weight\n    \n    # Calculate final scores based on weighted means and exploration bonuses\n    final_scores = (recent_weight * recent_means + historical_weight * action_means + exploration_bonus)\n    \n    # Epsilon-greedy strategy for exploration\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n    \n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n    \n    return action_index",
          "objective": -445.7325604239437,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n    \n    # Calculate means and counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n    \n    # Normalize action means by selection counts\n    adjusted_means = action_means / (1 + action_counts)\n    \n    # Dynamic exploration factor\n    epsilon = 0.1 * (1 - (current_time_slot / total_time_slots))\n    \n    # Recent performance weighting\n    weights = np.linspace(0.5, 1.0, num_actions)\n    weighted_performance = adjusted_means * weights\n    \n    # Probabilistic selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, num_actions)  # Explore\n    else:\n        action_index = np.argmax(weighted_performance)  # Exploit\n    \n    return action_index",
          "objective": -445.6019377016559,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Calculate means and counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Weighted recent mean calculation (considering last 5 scores)\n    recent_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        recent_scores = score_set.get(action_index, [])[-5:]\n        recent_means[action_index] = np.mean(recent_scores) if recent_scores else 0\n\n    # Exploration term\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n    \n    # Dynamically adjust weights based on recency\n    recency_weight = (current_time_slot / total_time_slots) ** 1.5\n    historical_weight = 1 - recency_weight\n\n    # Composite adjusted scores\n    adjusted_scores = (recency_weight * recent_means +\n                       historical_weight * action_means +\n                       exploration_bonus)\n\n    # Epsilon-greedy strategy for exploration\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n    \n    # Choose action\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": -445.47321950835885,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate action means and counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Emphasize recent performance\n    recent_scores = np.array([scores[-1] if len(scores) > 0 else 0 for scores in score_set.values()])\n    weighted_means = 0.7 * action_means + 0.3 * recent_scores\n\n    # Adaptive exploration factor\n    epsilon = 1.0 - (current_time_slot / total_time_slots)\n    exploration_values = np.sqrt(action_counts + 1) / (total_selection_count + 1)\n    exploration_gain = epsilon * np.clip(exploration_values, 0, 1)\n\n    # Stochastic decision-making\n    if np.random.rand() < exploration_gain[np.argmax(weighted_means)]:\n        action_index = np.random.choice(num_actions)  # Explore\n    else:\n        action_index = np.argmax(weighted_means)  # Exploit\n\n    return action_index",
          "objective": -445.3548226219764,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Calculate means and counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Score adjustment for selection count\n    adjusted_counts = np.where(action_counts > 0, action_counts, 1)  # Avoid division by zero\n    exploration_factor = np.log(total_selection_count + 1) / adjusted_counts\n    recency_weight = np.sqrt(current_time_slot / total_time_slots)\n\n    # Calculate composite scores using weighted mean\n    composite_scores = (0.6 * action_means + 0.4 * exploration_factor) + recency_weight\n\n    # Epsilon-greedy strategy for exploration\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n    \n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(composite_scores)\n\n    return action_index",
          "objective": -445.25650145452556,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Weighted score calculation to balance historical performance with count\n    weighted_scores = action_means / (action_counts + 1e-5)\n\n    # Enhance with recent performance trends\n    recent_mean = np.array([\n        np.mean(score_set.get(action_index, [])[-10:]) if action_counts[action_index] > 0 else 0\n        for action_index in range(action_count)\n    ])\n\n    # Combine both scores with a temporal adaptability feature\n    recent_weight = current_time_slot / (total_time_slots + 1e-5)\n    combined_scores = (1 - recent_weight) * weighted_scores + recent_weight * recent_mean\n\n    # Exploration bonus based on frequency of selection\n    exploration_bonus = np.log(total_selection_count + 1) / (action_counts + 1)\n    final_scores = combined_scores + exploration_bonus\n\n    # Implementing a softmax approach for selection probabilities\n    probabilities = np.exp(final_scores - np.max(final_scores))\n    probabilities /= np.sum(probabilities)\n\n    # Stochastic selection based on computed probabilities\n    action_index = np.random.choice(action_count, p=probabilities)\n\n    return action_index",
          "objective": -445.20814162810007,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate means and counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Recent scores weighted more heavily\n    recent_weights = np.array([0.7 if len(scores) >= 3 else 0.3 for scores in score_set.values()])\n    recent_means = np.array([np.mean(scores[-3:]) if len(scores) >= 3 else action_means[i] for i, scores in score_set.items()])\n    weighted_recent_means = (recent_means * recent_weights) + (action_means * (1 - recent_weights))\n\n    # Exploration-exploitation strategy\n    epsilon = 0.1 + (0.9 * (current_time_slot / total_time_slots))\n    \n    # Encourage exploration of less frequently selected actions\n    action_bonus = np.maximum(0, (1 - action_counts / (total_selection_count + 1)))  # Prevent division by zero\n    effective_scores = weighted_recent_means + action_bonus\n\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, num_actions)  # Explore\n    else:\n        action_index = np.argmax(effective_scores)  # Exploit\n\n    return action_index",
          "objective": -445.0241062240202,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate action means and counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if scores else 0.0\n\n    # Implement recent performance weighting\n    recent_scores = np.array([scores[-1] if scores else 0 for scores in score_set.values()])\n    recent_weight = 0.7  # Heavy weighting for recent performance\n    weighted_means = (1 - recent_weight) * action_means + recent_weight * recent_scores\n\n    # Dynamic exploration rate with adaptive epsilon\n    epsilon = max(0.01, 1.0 - (current_time_slot / total_time_slots))\n\n    # Exploration bonus adjusted for the total selection counts\n    exploration_bonus = np.sqrt(action_counts + 1) / (total_selection_count + 1)\n\n    # Combine weighted means with exploration values\n    total_values = weighted_means + epsilon * exploration_bonus\n    \n    # Normalize values to create a probability distribution\n    total_values_normalized = total_values / np.sum(total_values) if np.sum(total_values) > 0 else np.ones(num_actions) / num_actions\n\n    # Action selection based on the computed probabilities\n    action_index = np.random.choice(np.arange(num_actions), p=total_values_normalized)\n\n    return action_index",
          "objective": -444.97579922859404,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n    \n    # Calculate means and counts for each action\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Recent performance evaluation\n    recent_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        recent_scores = score_set.get(action_index, [])[-5:]  # Last 5 scores\n        recent_means[action_index] = np.mean(recent_scores) if recent_scores else 0\n\n    # Dynamic weighting based on time and thresholds\n    recency_weight = min(1, current_time_slot / (total_time_slots * 0.5))\n    historical_weight = 1 - recency_weight\n    weighted_scores = recency_weight * recent_means + historical_weight * action_means\n\n    # Exploration bonus using UCB approach\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n    final_scores = weighted_scores + exploration_bonus\n\n    # Epsilon-greedy strategy for exploration\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.05)\n    \n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(np.arange(action_count))\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -444.8944228978997,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Calculate means and counts with appropriate handling for empty score sets\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Recent performance weighting\n    weighted_action_means = action_means * (1 + (0.5 * np.sqrt(current_time_slot / total_time_slots)))\n\n    # Dynamic exploration term based on counts\n    exploration_bonus = np.log(total_selection_count + 1) / (action_counts + 1e-5) \n\n    # Composite score incorporating means and exploration\n    composite_scores = weighted_action_means + exploration_bonus\n\n    # Epsilon-greedy for exploration with a decaying epsilon\n    epsilon_decay = 0.1 * (1 - (current_time_slot / total_time_slots))\n    epsilon = max(epsilon_decay, 0.01)\n    \n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(composite_scores)\n\n    return action_index",
          "objective": -444.8838883659481,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Calculate means and counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Recent performance focus\n    recent_weighted_means = action_means * (1 + (current_time_slot / total_time_slots))\n\n    # Exploration term\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n\n    # Composite score\n    composite_scores = recent_weighted_means + exploration_bonus\n\n    # Epsilon-greedy selection\n    exploration_strength = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.05)\n    if np.random.rand() < exploration_strength:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(composite_scores)\n\n    return action_index",
          "objective": -443.82820806374104,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Compute action means and counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n    \n    # Recent scores consideration using a weighted average\n    recency_weight_factor = 0.1\n    recent_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        recent_scores = score_set.get(action_index, [])[-5:]  # Last 5 scores\n        recent_means[action_index] = np.mean(recent_scores) if recent_scores else 0\n    \n    adaptive_means = (1 - recency_weight_factor) * action_means + recency_weight_factor * recent_means\n\n    # Confidence interval for exploration\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n    \n    # Final score calculation\n    final_scores = adaptive_means + exploration_bonus\n\n    # Probabilistic selection framework with an exploration factor\n    exploration_rate = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n    if np.random.rand() < exploration_rate:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -443.6662528878648,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    action_means = np.zeros(n_actions)\n    action_counts = np.zeros(n_actions)\n\n    # Compute action means and counts\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Adaptive epsilon decay\n    epsilon = 1.0 - (current_time_slot / total_time_slots)  # Start high, decrease over time\n    epsilon = max(0.1, min(0.5, epsilon))\n\n    # Recent performance weighting (exponential decay)\n    recent_weight = np.exp(-0.1 * action_counts)  # Less weight for older scores\n    for action_index in range(n_actions):\n        if action_counts[action_index] > 0:\n            recent_scores = score_set[action_index][-1]  # Use most recent score\n            action_means[action_index] = (\n                (1 - recent_weight[action_index]) * action_means[action_index] + \n                recent_weight[action_index] * recent_scores\n            )\n\n    # Selection bonus for untried actions\n    selection_bonus = 0.1\n    for action_index in range(n_actions):\n        if action_counts[action_index] == 0:\n            action_means[action_index] += selection_bonus\n            \n    # Exploration term\n    exploration_term = np.random.rand(n_actions) * epsilon\n    adjusted_scores = action_means + exploration_term\n\n    # Select action based on the highest score\n    action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": -443.5971194100019,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n    \n    # Calculate action means and counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Normalizing the means for stability\n    normalized_means = action_means / (action_counts + 1e-5)\n\n    # Weight recent performance more\n    recent_scores = np.array([\n        np.mean(score_set.get(action_index, [])[-10:]) if action_counts[action_index] > 0 else 0\n        for action_index in range(action_count)\n    ])\n\n    # Recent performance weighting\n    recent_weight = min(current_time_slot / total_time_slots, 1.0)\n    combined_scores = (recent_weight * recent_scores) + ((1 - recent_weight) * normalized_means)\n\n    # Exploration bonus for less-explored actions\n    exploration_bonus = (np.log(total_selection_count + 1) + 1) / (action_counts + 1e-5)\n\n    # Final score calculation \n    final_scores = combined_scores + exploration_bonus\n    \n    # Epsilon for exploration-exploitation\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n\n    # Probabilistic decision making\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -443.59116820336277,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    \n    # Calculate average scores and counts\n    action_means = np.array([np.mean(scores) if scores else 0 for scores in score_set.values()])\n    action_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Calculate recent means (last 3 scores or fewer)\n    recent_means = np.array([\n        np.mean(scores[-3:]) if len(scores) > 0 else 0 for scores in score_set.values()\n    ])\n    \n    # Weighting system for recent and historical data\n    weight_recent = 1 - (current_time_slot / total_time_slots)  # More recent weighted initially\n    weight_historical = 1 - weight_recent\n\n    # Compute weighted scores\n    weighted_scores = (weight_recent * recent_means) + (weight_historical * action_means)\n\n    # Exploration bonus\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n    \n    # Combine scores for final decision\n    final_scores = weighted_scores + exploration_bonus\n    \n    # Dynamic epsilon for exploration\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n    \n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -443.5896973493014,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n    \n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if scores else 0\n\n    recent_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        recent_scores = score_set.get(action_index, [])[-5:]\n        recent_means[action_index] = np.mean(recent_scores) if recent_scores else 0\n    \n    recency_weight = min(1, current_time_slot / (total_time_slots * 0.5))\n    historical_weight = 1 - recency_weight\n\n    performance_scores = (recency_weight * recent_means) + (historical_weight * action_means)\n\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n    \n    final_scores = performance_scores + exploration_bonus\n\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.05)\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(np.arange(action_count))\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -443.3062191125171,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.array([np.mean(scores) if scores else 0 for scores in score_set.values()])\n    action_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Using at least two scores for better mean estimation\n    adjusted_means = np.where(action_counts >= 2, action_means, np.zeros(action_count))\n    \n    # Recent means focusing on the last score if available\n    recent_means = np.array([\n        scores[-1] if scores else 0 for scores in score_set.values()\n    ])\n    \n    # Dynamic weighting\n    weight_recent = min(1, current_time_slot / (total_time_slots * 0.5))  # Transition to exploitation\n    weight_historical = 1 - weight_recent\n    \n    # Compute combined scores using adjusted means\n    combined_scores = (weight_recent * recent_means) + (weight_historical * adjusted_means)\n\n    # Exploration bonus\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n    final_scores = combined_scores + exploration_bonus\n    \n    # Epsilon-greedy selection with a decaying epsilon\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -442.9041897270389,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Calculate historical performance and counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Handle cases of actions with no selections\n    safe_action_counts = np.maximum(action_counts, 1)  # Avoid division by zero\n    normalized_means = action_means / safe_action_counts\n\n    # Dynamic weighting based on recent scores\n    recent_scores = np.array([\n        np.mean(score_set.get(action_index, [])[-10:]) if action_counts[action_index] > 0 else 0\n        for action_index in range(action_count)\n    ])\n\n    # Importance of recent versus historical data\n    recent_weight = np.clip(current_time_slot / total_time_slots, 0.1, 0.9)\n    historical_weight = 1 - recent_weight\n    weighted_scores = (recent_weight * recent_scores) + (historical_weight * normalized_means)\n\n    # Exploration Bonus (Upper Confidence Bound approach)\n    exploration_bonus = (np.log(total_selection_count + 1) + 1) / (action_counts + 1)\n\n    # Combine final scores\n    final_scores = weighted_scores + exploration_bonus\n\n    # Epsilon for exploration-exploitation\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -442.4273950954114,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Fairness adjustment\n    adjusted_means = action_means * (action_counts / (total_selection_count + 1))\n\n    # Recent performance adaptation\n    recent_scores = np.zeros(action_count)\n    for action_index in range(action_count):\n        total_scores = score_set.get(action_index, [])\n        recent_weights = total_scores[-5:] if len(total_scores) > 5 else total_scores\n        recent_scores[action_index] = np.mean(recent_weights) if recent_weights else 0\n\n    # Combined scores with historical and recent performance\n    weighting_factor = 1 - (current_time_slot / total_time_slots)\n    combined_scores = (weighting_factor * adjusted_means) + (1 - weighting_factor) * recent_scores\n    \n    # Exploration bonus based on selection counts\n    exploration_bonus = np.log(total_selection_count + 1) / (action_counts + 1)\n\n    # Final scores with exploration bonuses\n    final_scores = combined_scores + exploration_bonus\n\n    # Epsilon for exploration\n    epsilon = 0.1 * (1 - (current_time_slot / total_time_slots))\n\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -442.3347783738544,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.array([np.mean(scores) if scores else 0 for scores in score_set.values()], dtype=np.float32)\n    action_counts = np.array([len(scores) for scores in score_set.values()], dtype=np.float32)\n    \n    # Compute recent means (last 3 scores)\n    recent_means = np.array([\n        np.mean(scores[-3:]) if len(scores) >= 3 else (np.mean(scores) if scores else 0) \n        for scores in score_set.values()\n    ], dtype=np.float32)\n    \n    # Weighting factors for recent and historical scores\n    weight_recent = current_time_slot / total_time_slots\n    weight_historical = 1 - weight_recent\n    \n    # Combined score calculation\n    combined_scores = (weight_recent * recent_means) + (weight_historical * action_means)\n\n    # Exploration bonus\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n    final_scores = combined_scores + exploration_bonus\n\n    # Epsilon-greedy mechanism\n    epsilon_start = 0.2\n    epsilon_end = 0.01\n    decay_steps = total_time_slots * 0.8  # Decaying exploration over 80% of the time slots\n    epsilon = max(epsilon_start * (1 - current_time_slot / decay_steps), epsilon_end)\n\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -440.8905644730941,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    action_means = np.zeros(n_actions)\n    action_counts = np.zeros(n_actions)\n\n    # Compute average scores and counts\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Dynamic epsilon based on time\n    epsilon = max(0.05 * (1 - (current_time_slot / total_time_slots)), 0.01)\n\n    # Recent weighted scores, favoring the last 10 scores\n    recent_weighted_means = np.zeros(n_actions)\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        recent_scores = scores[-10:] if len(scores) > 10 else scores\n        recent_mean = np.mean(recent_scores) if recent_scores else 0\n        recent_weighted_means[action_index] = (0.8 * recent_mean + 0.2 * action_means[action_index])\n    \n    # Bonus for under-selected actions\n    bonus_factor = (1 + np.log(total_selection_count + 1)) / (1 + action_counts)\n    final_scores = recent_weighted_means * bonus_factor\n\n    # Exploration-exploitation decision\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, n_actions)  # Exploration\n    else:\n        action_index = np.argmax(final_scores)  # Exploitation\n    \n    return action_index",
          "objective": -440.8803708243693,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate action means and counts, handling empty score sets\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if scores else 0.0\n\n    # Emphasize recent scores with a focus on the last three scores (or all if fewer than three)\n    recent_scores = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        recent_scores[action_index] = np.mean(scores[-3:]) if len(scores) > 0 else 0.0\n\n    # Adjust recent scores contribution\n    recent_weight = 0.7\n    total_means = (1 - recent_weight) * action_means + recent_weight * recent_scores\n\n    # Exploration bonus based on counts and total selections\n    exploration_bonus = np.sqrt(np.maximum(action_counts, 0)) / (total_selection_count + 1)\n\n    # Combine calculated values for final decision\n    total_values = total_means + exploration_bonus\n\n    # Use softmax for probabilistic action selection\n    exp_values = np.exp(total_values - np.max(total_values))  # Stability in softmax\n    probabilities = exp_values / np.sum(exp_values)\n\n    # Select an action based on the computed probabilities\n    action_index = np.random.choice(np.arange(num_actions), p=probabilities)\n    \n    return action_index",
          "objective": -440.5750264992394,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n    \n    # Dynamic weights\n    recent_weight = min(1, current_time_slot / total_time_slots)\n    historical_weight = 1 - recent_weight\n    \n    # Enhanced recent scores\n    recent_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        recent_scores = scores[-10:] if len(scores) > 10 else scores\n        recent_means[action_index] = np.mean(recent_scores) if recent_scores else 0\n\n    # Weighted scores\n    weighted_scores = recent_weight * recent_means + historical_weight * action_means\n    \n    # Exploration bonus\n    exploration_bonus = np.log(total_selection_count + 1) / (action_counts + 1)\n    \n    # Final scores including exploration\n    final_scores = weighted_scores + exploration_bonus\n    \n    # Dynamic epsilon\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n    \n    return action_index",
          "objective": -440.0415042783999,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Calculate historical performance, action means, and counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Normalize means to avoid division by zero\n    normalized_means = action_means / (action_counts + 1e-5)\n\n    # Recent score emphasis (last 3 scores)\n    recent_scores = np.array([\n        np.mean(score_set.get(action_index, [])[-3:]) if action_counts[action_index] > 0 else 0\n        for action_index in range(action_count)\n    ])\n\n    # Weight recent and historical scores\n    recent_weight = 0.7 + (0.3 * (current_time_slot / total_time_slots))\n    historical_weight = 1 - recent_weight\n    weighted_scores = (recent_weight * recent_scores) + (historical_weight * normalized_means)\n\n    # Exploration bonus based on the number of times actions have been taken\n    exploration_bonus = (np.log(total_selection_count + 1 + 1e-5) + 1) / (action_counts + 1)\n\n    # Combined final scores\n    final_scores = weighted_scores + exploration_bonus\n\n    # Epsilon for exploration-exploitation\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -439.89237107629117,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Recent performance weighting using exponential decay\n    decay_factor = 0.9  # Adjust the decay factor as necessary\n    weighted_means = np.zeros_like(action_means)\n    \n    for action_index in range(num_actions):\n        recent_scores = scores[-min(len(scores), 5):]  # Take last 5 scores for recent performance\n        recent_mean = np.mean(recent_scores) if len(recent_scores) > 0 else 0\n        weighted_means[action_index] = decay_factor * action_means[action_index] + (1 - decay_factor) * recent_mean\n\n    # Adaptive exploration-exploitation strategy\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n    exploration_gain = epsilon * (1 + (total_selection_count / 10))  # Encouraging exploration\n\n    if np.random.rand() < exploration_gain:\n        action_index = np.random.randint(0, num_actions)  # Explore\n    else:\n        action_index = np.argmax(weighted_means)  # Exploit\n\n    # Encourage diversity by applying a bonus to less frequently selected actions\n    selection_bonus = np.maximum(0, 1 - action_counts / total_selection_count) if total_selection_count > 0 else np.ones(num_actions)\n    final_scores = weighted_means + selection_bonus\n\n    action_index = np.argmax(final_scores)  # Select based on the final scores    \n\n    return action_index",
          "objective": -439.8330017504125,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n    \n    # Compute historical means and counts for each action\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n    \n    # Calculate recency weighting\n    recency_weight = 0.7 * (1 - (current_time_slot / total_time_slots)) + 0.3\n    recent_weighted_means = np.zeros(action_count)\n    \n    for action_index in range(action_count):\n        recent_scores = score_set.get(action_index, [])[-10:]\n        recent_mean = np.mean(recent_scores) if recent_scores else 0\n        recent_weighted_means[action_index] = (recency_weight * recent_mean +\n                                               (1 - recency_weight) * action_means[action_index])\n    \n    # Exploration bonus\n    exploration_bonus = np.log(total_selection_count + 1) / (action_counts + 1e-5)\n    \n    # Final scores\n    final_scores = recent_weighted_means + exploration_bonus\n    \n    # Dynamic epsilon for exploration vs. exploitation\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n    \n    # Stochastic action selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n    \n    return action_index",
          "objective": -439.3435387200333,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate means and counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Adjust means for selection bias using a small constant\n    adjusted_means = action_means / (action_counts + 1e-5)\n\n    # Dynamic exploration factor\n    epsilon = 0.1 * (1 - (current_time_slot / total_time_slots))  \n\n    # Recent performance weighting\n    recent_weight = 0.7 * (current_time_slot / total_time_slots)  \n    weighted_performance = adjusted_means * (1 + recent_weight) \n\n    # Exploration factor: likelihood of choosing a random action\n    exploration_value = np.random.rand()\n    if exploration_value < epsilon:\n        action_index = np.random.randint(0, num_actions)  # Explore\n    else:\n        action_index = np.argmax(weighted_performance)  # Exploit\n\n    return action_index",
          "objective": -439.31885999656595,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n    \n    # Calculate action means and counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if scores else 0.0\n\n    # Emphasize recent performance with a weighted recent score\n    recent_weights = 0.6\n    recent_scores = np.array([scores[-1] if scores else 0 for scores in score_set.values()])\n    weighted_means = (1 - recent_weights) * action_means + recent_weights * recent_scores\n\n    # Adaptive epsilon for exploration\n    epsilon = max(0.05, 1.0 - (current_time_slot / total_time_slots))\n\n    # Exploration bonus based on action counts and total selections\n    exploration_bonus = np.sqrt(action_counts + 1) / (total_selection_count + 1)\n\n    # Combine weighted means with exploration\n    total_values = weighted_means + epsilon * exploration_bonus\n\n    # Normalize to create a probability distribution\n    total_values_normalized = total_values / np.sum(total_values)\n\n    # Select action based on probabilities\n    action_index = np.random.choice(np.arange(num_actions), p=total_values_normalized)\n    \n    return action_index",
          "objective": -438.54639266897476,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Calculate action means and counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if scores else 0\n\n    # Recent performance adaptation\n    recent_weights = np.linspace(0.1, 1, num=total_time_slots)\n    weighted_means = action_means * recent_weights[current_time_slot]\n\n    # Exploration bonus with logarithmic scaling\n    exploration_bonus = (np.log(total_selection_count + 1) / (action_counts + 1e-5))\n\n    # Composite score calculation with a focus on weighted means and exploration\n    composite_scores = weighted_means + exploration_bonus\n\n    # Dynamic exploration probability\n    exploration_strength = min(1.0, max(0.1, 1 - (current_time_slot / total_time_slots)))\n\n    # Epsilon-greedy selection\n    if np.random.rand() < exploration_strength:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(composite_scores)\n\n    return action_index",
          "objective": -438.01950725607986,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    action_means = np.zeros(n_actions)\n    action_counts = np.zeros(n_actions)\n\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Calculate exploration rate\n    confidence_interval = np.sqrt(2 * np.log(total_selection_count + 1) / (action_counts + 1e-6))\n    exploration_scores = np.zeros(n_actions)\n\n    for action_index in range(n_actions):\n        if action_counts[action_index] > 0:\n            exploration_scores[action_index] = confidence_interval[action_index]\n\n    # Recent score weighting\n    recent_score_weight = min(current_time_slot / total_time_slots, 1)\n    weighted_means = (action_means * (1 + recent_score_weight)).clip(0, 1)\n\n    # Combine scores\n    total_scores = weighted_means + exploration_scores\n\n    action_index = np.argmax(total_scores)\n\n    return action_index",
          "objective": -437.8343129406003,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Calculate historical scores and counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Avoid division by zero and improve efficiency\n    adjusted_means = np.divide(action_means, action_counts + 1e-5)\n\n    # Recent performance with recency focus\n    recent_window = 10\n    recent_scores = np.array([\n        np.mean(score_set.get(action_index, [])[-recent_window:]) if action_counts[action_index] > 0 else 0\n        for action_index in range(action_count)\n    ])\n\n    # Calculate the recency weight\n    recency_weight = 1 - (current_time_slot / total_time_slots)\n\n    # Combined score calculation\n    weighted_scores = (recency_weight * recent_scores) + ((1 - recency_weight) * adjusted_means)\n\n    # Exploration bonus\n    exploration_bonus = np.log(total_selection_count + 1) / (action_counts + 1)\n    \n    # Final score aggregation\n    final_scores = weighted_scores + exploration_bonus\n\n    # Epsilon for random selection\n    exploration_rate = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n\n    if np.random.rand() < exploration_rate:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -437.22861942238114,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = len(score_set)\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Calculate action means and counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if scores else 0\n\n    # Emphasize recent performance\n    recent_scores = np.array([scores[-1] if scores else 0 for scores in score_set.values()])\n    weighted_means = 0.7 * action_means + 0.3 * recent_scores\n\n    # Exploration-exploitation balance\n    epsilon = np.clip(1.0 - (current_time_slot / total_time_slots), 0.1, 1.0)\n    exploration_values = np.sqrt(action_counts + 1) / (total_selection_count + 1)\n    exploration_gain = epsilon * np.clip(exploration_values, 0, 1)\n\n    # Score calculation\n    combined_scores = weighted_means + exploration_gain\n\n    # Stochastic decision-making\n    action_index = np.random.choice(np.arange(action_count), p=combined_scores / np.sum(combined_scores))\n\n    return action_index",
          "objective": -434.3907851367475,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Calculate historical performance and counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Normalizing the means for stability\n    normalized_means = action_means / (action_counts + 1e-5)\n\n    # Recent performance calculation\n    recent_scores = np.array([\n        np.mean(score_set.get(action_index, [])[-10:]) if action_counts[action_index] > 0 else 0\n        for action_index in range(action_count)\n    ])\n\n    # Determine weight between recent and historical scores\n    recent_weight = current_time_slot / total_time_slots\n    combined_scores = (recent_weight * recent_scores) + ((1 - recent_weight) * normalized_means)\n\n    # Exploration bonus for underexplored actions\n    exploration_bonus = (np.log(total_selection_count + 1) + 1) / (action_counts + 1)\n\n    # Final score calculation\n    final_scores = combined_scores + exploration_bonus\n\n    # Epsilon for exploration-exploitation\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -433.33977888724297,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Normalize means\n    normalized_means = action_means / (action_counts + 1)  # Prevent division by zero\n\n    # Recent performance adjustment\n    recent_scores = np.zeros(action_count)\n    for action_index in range(action_count):\n        total_scores = score_set.get(action_index, [])\n        recent_weights = total_scores[-5:] if len(total_scores) > 5 else total_scores\n        recent_scores[action_index] = np.mean(recent_weights) if recent_weights else 0\n\n    # Exploration factor and balancing\n    exploration_bonus = (np.log(total_selection_count + 1) + 1) / (action_counts + 1)\n    \n    # Weight factors\n    recent_weight = current_time_slot / total_time_slots\n    historical_weight = 1 - recent_weight\n    weighted_scores = (recent_weight * recent_scores) + (historical_weight * normalized_means) + exploration_bonus\n\n    # Epsilon for exploration\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(weighted_scores)\n\n    return action_index",
          "objective": -432.87451830623695,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Compute historical means and counts for each action\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Calculate weighted scores with recency bias\n    recency_factor = min(1, current_time_slot / total_time_slots)\n    historical_factor = 1 - recency_factor\n\n    recent_weighted_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        if action_counts[action_index] > 0:\n            recent_scores = score_set.get(action_index, [])[-10:]\n            recent_mean = np.mean(recent_scores) if recent_scores else 0\n            recent_weighted_means[action_index] = (recency_factor * recent_mean +\n                                                   historical_factor * action_means[action_index])\n        else:\n            recent_weighted_means[action_index] = action_means[action_index]  # Fallback to historical mean\n    \n    # Exploration bonus\n    exploration_bonus = np.log(total_selection_count + 1 + 1e-5) / (action_counts + 1e-5)\n\n    # Final scores\n    final_scores = recent_weighted_means + exploration_bonus\n\n    # Dynamic epsilon for exploration vs. exploitation\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -432.65484710789815,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_means = np.zeros(8)\n    action_counts = np.zeros(8)\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Recent score weighting\n    weight_factor = current_time_slot / total_time_slots\n    weighted_means = action_means * (1 + weight_factor)\n\n    # Exploration-exploitation parameters\n    epsilon = 0.1\n    exploration_gain = epsilon * (1 - (total_selection_count / (total_selection_count + 1)))\n\n    if np.random.rand() < exploration_gain:\n        action_index = np.random.randint(0, 8)  # Explore\n    else:\n        action_index = np.argmax(weighted_means)  # Exploit\n\n    return action_index",
          "objective": -431.25338818989377,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n    \n    # Compute action means and counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Weighted score using recent performance\n    recent_weight = min(1, current_time_slot / total_time_slots)\n    historical_weight = 1 - recent_weight\n    \n    recent_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        recent_scores = scores[-10:] if len(scores) > 10 else scores\n        recent_means[action_index] = np.mean(recent_scores) if recent_scores else 0\n\n    # Compute final weighted scores\n    weighted_scores = (recent_weight * recent_means) + (historical_weight * action_means)\n\n    # Exploration bonus for less-selected actions\n    exploration_bonus = np.log(total_selection_count + 1) / (action_counts + 1e-5)  # Avoid division by zero\n\n    # Final scores including exploration\n    final_scores = weighted_scores + exploration_bonus\n\n    # Dynamic epsilon for exploration\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n\n    # Stochastic selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -429.3394619818557,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Compute average scores and counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Dynamic epsilon calculation for exploration strategy\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n\n    # Recent performance weighted mean\n    recent_weighted_means = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        recent_scores = scores[-10:] if len(scores) > 10 else scores\n        recent_mean = np.mean(recent_scores) if recent_scores else 0\n        recent_weighted_means[action_index] = (0.7 * recent_mean + 0.3 * action_means[action_index])\n\n    # Exploration bonus for less frequently selected actions\n    bonus_factor = np.log(total_selection_count + 1) / (action_counts + 1)\n    final_scores = recent_weighted_means * bonus_factor\n\n    # Selection decision based on exploration-exploitation strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, num_actions)  # Explore\n    else:\n        action_index = np.argmax(final_scores)  # Exploit\n\n    return action_index",
          "objective": -427.82276894301503,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n    \n    # Calculate action means and counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n    \n    # Recent score means using the latest 5 scores for sensitivity\n    recent_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        recent_scores = score_set.get(action_index, [])[-5:]\n        recent_means[action_index] = np.mean(recent_scores) if recent_scores else 0\n    \n    # Dynamic weights and combined scores\n    recent_weight = min(1, (current_time_slot / total_time_slots) ** 2)\n    historical_weight = 1 - recent_weight\n    combined_scores = (recent_weight * recent_means) + (historical_weight * action_means)\n\n    # Exploration bonus\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n    \n    # Final scores\n    final_scores = combined_scores + exploration_bonus\n\n    # Probabilistic selection framework\n    epsilon = max(0.05, 0.1 * (1 - (current_time_slot / total_time_slots)))\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -427.3514272778396,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8  # Fixed number of actions\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Compute historical means and counts for each action\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Recent performance weighting\n    recent_weight = min(1, current_time_slot / total_time_slots)\n    historical_weight = 1 - recent_weight\n\n    # Adjust recent means\n    recent_weighted_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        recent_scores = scores[-10:] if len(scores) > 10 else scores\n        recent_mean = np.mean(recent_scores) if recent_scores else 0\n        recent_weighted_means[action_index] = (recent_weight * recent_mean +\n                                                historical_weight * action_means[action_index])\n\n    # Exploration bonus (inverted for lower counts)\n    exploration_bonus = np.log(total_selection_count + 1) / (action_counts + 1e-5)\n    exploration_bonus[action_counts == 0] = 1e5  # High bonus for untried actions\n\n    # Final scores calculation\n    final_scores = recent_weighted_means + exploration_bonus\n\n    # Probabilistic selection\n    prob_scores = final_scores - np.max(final_scores)  # Shift for numerical stability\n    exp_scores = np.exp(prob_scores)\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Select action based on probabilities\n    action_index = np.random.choice(np.arange(action_count), p=probabilities)\n\n    return action_index",
          "objective": -427.1877734995191,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate means and counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Weight recent performance\n    recent_weight = 0.7\n    recent_means = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        if action_counts[action_index] > 0:\n            recent_means[action_index] = score_set[action_index][-1]\n    \n    action_means += recent_weight * (recent_means - action_means)\n\n    # Dynamic exploration factor\n    exploration_factor = max(0.1 * (1 - (total_selection_count / max(total_time_slots, 1))), 0.01)\n    \n    if np.random.rand() < exploration_factor:\n        action_index = np.random.randint(0, num_actions)  # Explore\n    else:\n        action_index = np.argmax(action_means)  # Exploit\n\n    return action_index",
          "objective": -426.50961403636796,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_means = np.zeros(8)\n    action_counts = np.zeros(8)\n\n    # Calculate means and counts for each action\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Normalize means to account for selection bias\n    normalized_means = np.where(action_counts > 0, action_means / (action_counts + 1), 0)\n\n    # Dynamic exploration factor\n    exploration_factor = 1 / (1 + total_selection_count)\n    epsilon = 0.1 * (1 - (current_time_slot / total_time_slots))  # Decay epsilon over time\n\n    # Recent performance weighting\n    recent_weight = 0.5 * (current_time_slot / total_time_slots)  # More weight to recent scores\n    weighted_performance = normalized_means * (1 + recent_weight)\n\n    # Probabilistic selection framework\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, 8)  # Explore\n    else:\n        action_index = np.argmax(weighted_performance)  # Exploit\n\n    return action_index",
          "objective": -426.4326573403568,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Calculate means and counts for historical scores\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Consider recent performance for adaptive learning\n    recent_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        recent_scores = score_set.get(action_index, [])[-5:]\n        recent_means[action_index] = np.mean(recent_scores) if recent_scores else 0\n\n    # Calculate adaptive weights\n    recency_weight = min(1, current_time_slot / (total_time_slots * 0.5))\n    historical_weight = 1 - recency_weight\n\n    # Calculate combined scores\n    adaptive_scores = (recency_weight * recent_means) + (historical_weight * action_means)\n\n    # Exploration term based on selection frequency\n    with np.errstate(divide='ignore', invalid='ignore'):\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n    \n    # Final score computation\n    final_scores = adaptive_scores + exploration_bonus\n\n    # Epsilon-greedy strategy for balanced selection\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.05)\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(np.arange(action_count))\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -425.91291667231,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_means = np.zeros(8)\n    action_counts = np.zeros(8)\n\n    # Calculate action means and counts with handling for empty score lists\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Emphasize recent performance\n    recent_scores = np.array([scores[-1] if len(scores) > 0 else 0 for scores in score_set.values()])\n    weighted_means = 0.6 * action_means + 0.4 * recent_scores\n\n    # Dynamic exploration factor\n    epsilon = np.clip(1.0 - (current_time_slot / total_time_slots), 0.1, 1.0)  # Decay exploration over time\n    exploration_values = np.sqrt(action_counts + 1) / (total_selection_count + 1)\n    exploration_gain = epsilon * np.clip(exploration_values, 0, 1)\n\n    # Stochastic decision-making\n    if np.random.rand() < exploration_gain[np.argmax(weighted_means)]:\n        action_index = np.random.randint(0, 8)  # Explore\n    else:\n        action_index = np.argmax(weighted_means)  # Exploit\n\n    return action_index",
          "objective": -425.7431223691632,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_means = np.zeros(8)\n    action_counts = np.zeros(8)\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n    \n    # Recent performance with exponential decay\n    recent_weight = 0.5\n    if total_selection_count > 0:\n        for action_index in range(8):\n            if action_counts[action_index] > 0:\n                action_means[action_index] = (1 - recent_weight) * action_means[action_index] + \\\n                                              recent_weight * scores[-1]\n    \n    # Dynamic exploration-exploitation ratio\n    epsilon = max(0.1, min(0.5, (current_time_slot / total_time_slots) * 0.5))\n    \n    # Selection bonus for underutilized actions\n    selection_bonus = 0.1\n    for action_index in range(8):\n        if action_counts[action_index] == 0:\n            action_means[action_index] += selection_bonus\n\n    # Epsilon-greedy exploration\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, 8)  # Explore\n    else:\n        action_index = np.argmax(action_means)  # Exploit\n    \n    return action_index",
          "objective": -424.5295309586598,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.array([np.mean(scores) if scores else 0 for scores in score_set.values()])\n    action_counts = np.array([len(scores) for scores in score_set.values()])\n\n    # Recent means for the last 3 scores\n    recent_means = np.array([\n        np.mean(scores[-3:]) if len(scores) > 0 else 0 for scores in score_set.values()\n    ])\n\n    # Weighting for recent vs historical data\n    weight_recent = min(1, current_time_slot / total_time_slots)\n    weight_historical = 1 - weight_recent\n\n    # Adjust actions based on means\n    means = (weight_recent * recent_means + weight_historical * action_means)\n\n    # Explore-exploit balance\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n    final_scores = means + exploration_bonus\n\n    # Softmax for probability distribution\n    scaled_scores = final_scores - np.max(final_scores)\n    probabilities = np.exp(scaled_scores) / np.sum(np.exp(scaled_scores))\n\n    # Epsilon-greedy for selection\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.random.choice(range(action_count), p=probabilities)\n\n    return action_index",
          "objective": -423.69035710396884,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Normalize means for better scaling\n    normalized_means = np.where(action_counts > 0, action_means / (action_counts + 1e-5), 0)\n\n    # Recent performance assessment with flexible window\n    recent_window = min(5, action_counts.max())  # flexibility in the recent window size\n    recent_scores = np.zeros(action_count)\n    \n    for action_index in range(action_count):\n        total_scores = score_set.get(action_index, [])\n        recent_weights = total_scores[-recent_window:] if len(total_scores) >= recent_window else total_scores\n        recent_scores[action_index] = np.mean(recent_weights) if recent_weights else 0\n\n    # Dynamic weighting using time slot progress\n    time_factor = current_time_slot / total_time_slots\n    weighted_scores = (1 - time_factor) * normalized_means + time_factor * recent_scores\n\n    # Exploration bonus calculated using counts and selection frequency\n    exploration_bonus = np.log(total_selection_count + 1) / (action_counts + 1e-5)\n    \n    # Combine performance evaluation with exploration\n    final_scores = weighted_scores + exploration_bonus\n    \n    # Epsilon-greedy selection based on the dynamic probability\n    epsilon = max(0.1, 0.5 * (total_selection_count / (total_time_slots * action_count)))\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -422.803857705483,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if scores else 0\n\n    recent_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        recent_scores = score_set.get(action_index, [])[-5:]\n        recent_means[action_index] = np.mean(recent_scores) if recent_scores else 0\n\n    recency_weight = min(1, current_time_slot / (total_time_slots * 0.5))\n    historical_weight = 1 - recency_weight\n    weighted_scores = (recency_weight * recent_means) + (historical_weight * action_means)\n\n    bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n    \n    final_scores = weighted_scores + bonus\n\n    exploration_prob = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.05)\n    if np.random.rand() < exploration_prob:\n        action_index = np.random.choice(np.arange(action_count))\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -422.02269857284324,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_means = np.zeros(8)\n    action_counts = np.zeros(8)\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Recent score emphasis using exponential decay\n    recent_scores = np.array([scores[-1] if len(scores) > 0 else 0 for scores in score_set.values()])\n    decay_factor = 0.9\n    weighted_means = decay_factor * action_means + (1 - decay_factor) * recent_scores\n\n    # Dynamic epsilon-greedy exploration-exploitation balance\n    epsilon = 1.0 * (1 - (current_time_slot / total_time_slots))  # Linearly decay epsilon\n    exploration_values = np.sqrt(action_counts + 1) / (total_selection_count + 1)\n    exploration_bonus = epsilon * np.clip(exploration_values, 0, 1)\n\n    # Selection probabilities\n    probabilities = weighted_means + exploration_bonus\n    probabilities /= probabilities.sum()  # Normalize probabilities\n\n    # Select an action based on the computed probabilities\n    action_index = np.random.choice(range(8), p=probabilities)\n\n    return action_index",
          "objective": -421.56985058684756,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n    \n    # Calculate historical performance and counts\n    for index in range(action_count):\n        scores = score_set.get(index, [])\n        action_counts[index] = len(scores)\n        action_means[index] = np.mean(scores) if action_counts[index] > 0 else 0\n\n    # Normalization to avoid division by zero\n    normalized_means = action_means / (action_counts + 1e-5)\n\n    # Recent performance calculation\n    recent_scores = np.array([\n        np.mean(score_set.get(index, [])[-10:]) if action_counts[index] > 0 else 0\n        for index in range(action_count)\n    ])\n    \n    # Dynamically weighting between recent and historical scores\n    recent_weight = current_time_slot / total_time_slots\n    combined_scores = (recent_weight * recent_scores) + ((1 - recent_weight) * normalized_means)\n\n    # Exploration bonus based on selection counts\n    exploration_bonus = np.log(total_selection_count + 1) / (action_counts + 1e-5) ** 0.5\n\n    # Final score calculation\n    final_scores = combined_scores + exploration_bonus\n\n    # Epsilon for exploration-exploitation trade-off\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n\n    # Action selection based on exploration-exploitation balance\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -419.8766631192343,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_means = []\n    action_counts = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_count = len(scores)\n        action_mean = np.mean(scores) if action_count > 0 else 0\n        action_means.append(action_mean)\n        action_counts.append(action_count)\n\n    action_means = np.array(action_means)\n    action_counts = np.array(action_counts)\n\n    # Calculate recent performance emphasis with a decay factor\n    decay_weight = 0.7\n    recent_means = np.array([np.mean(scores[-3:]) if len(scores) >= 3 else action_means[i] for i, scores in score_set.items()])\n    \n    # Adjusting total weights\n    recent_weighting = 1 + (current_time_slot / total_time_slots)\n    effective_means = action_means * decay_weight + recent_means * (1 - decay_weight)\n    weighted_scores = effective_means * recent_weighting\n\n    # Dynamic epsilon based on selection count\n    epsilon = max(0.1, 0.5 * (total_selection_count / (total_time_slots * 8)))\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, 8)  # Explore: choose a random action\n    else:\n        action_index = np.argmax(weighted_scores)  # Exploit: choose the best action based on weighted scores\n\n    return action_index",
          "objective": -419.5879477433276,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.array([np.mean(scores) if scores else 0 for scores in score_set.values()])\n    action_counts = np.array([len(scores) for scores in score_set.values()])\n\n    # Calculate recent performances\n    recent_means = np.array([\n        np.mean(scores[-3:]) if len(scores) >= 3 else (np.mean(scores) if scores else 0) for scores in score_set.values()\n    ])\n    \n    # Weighting factors based on time slot progression\n    weight_recent = min(current_time_slot / total_time_slots, 1.0)\n    weight_historical = 1 - weight_recent\n\n    # Combine recent and historical scores\n    combined_scores = (weight_recent * recent_means) + (weight_historical * action_means)\n\n    # UCB exploration bonus\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n    final_scores = combined_scores + exploration_bonus\n\n    # Epsilon-greedy selection mechanism\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -418.9118140502268,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    action_means = np.zeros(n_actions)\n    action_counts = np.zeros(n_actions)\n    \n    # Calculate means and counts\n    for action_index in range(n_actions):\n        if action_index in score_set:\n            scores = score_set[action_index]\n            action_counts[action_index] = len(scores)\n            action_means[action_index] = np.mean(scores) if scores else 0\n    \n    # Dynamic epsilon decay based on time and selection count\n    epsilon = max(0.05, min(0.5, (total_selection_count / (total_time_slots * n_actions)) * 0.5))\n\n    # Recent performance weighting\n    recent_weight = 0.4  # Increased weight to recent scores\n    for action_index in range(n_actions):\n        if action_counts[action_index] > 0:  # Use recent score if at least one score exists\n            recent_score = score_set[action_index][-1]  # Use most recent score\n            action_means[action_index] = (\n                (1 - recent_weight) * action_means[action_index] + \n                recent_weight * recent_score\n            )\n    \n    # Selection bonus for untried actions\n    selection_bonus = 0.15\n    for action_index in range(n_actions):\n        if action_counts[action_index] == 0:\n            action_means[action_index] += selection_bonus\n            \n    # Probabilistic adjustment based on exploration\n    exploration_term = np.random.rand(n_actions) * epsilon\n    adjusted_scores = action_means + exploration_term\n\n    # Select action based on the highest score\n    action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": -418.8706012943772,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_means = np.zeros(8)\n    action_counts = np.zeros(8)\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Adaptive exploration rate\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)  # Lower bound for exploration\n    exploration_terms = np.random.rand(8) * epsilon\n    \n    # Recent score weighting\n    recent_weight = min(current_time_slot / total_time_slots, 1)\n    weighted_means = action_means * (1 + recent_weight)\n\n    # Combine scores for selection\n    total_scores = weighted_means + exploration_terms\n    \n    action_index = np.argmax(total_scores)\n\n    return action_index",
          "objective": -417.21786941885335,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    action_means = np.zeros(n_actions)\n    action_counts = np.zeros(n_actions)\n\n    # Calculate action means and counts\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Epsilon value decreasing over time for exploration\n    epsilon = max(0.1, min(0.5, 1 - (current_time_slot / total_time_slots)))\n\n    # Recent performance reinforcement with decay\n    recent_weight = 0.7\n    for action_index in range(n_actions):\n        if action_counts[action_index] > 0:\n            recent_score = score_set[action_index][-1]\n            action_means[action_index] = (1 - recent_weight) * action_means[action_index] + \\\n                                          recent_weight * recent_score\n\n    # Adjust scores with exploration bonus\n    exploration_bonus = (1 / (1 + action_counts)) * 0.2\n    combined_scores = action_means + exploration_bonus\n\n    # Probabilistic adjustment using exploration term\n    exploration_term = np.random.rand(n_actions) * epsilon\n    final_scores = combined_scores + exploration_term\n\n    # Select action based on highest score\n    action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -414.4288621778921,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n    \n    # Calculate means and counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n    \n    # Use the counts to determine a confidence interval approach\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n    \n    # Dynamic exploration factor\n    epsilon = max(0.1, 1.0 - (current_time_slot / total_time_slots))\n    \n    # Recent performance weighting\n    recent_weight = 0.7 * (current_time_slot / total_time_slots)\n    adjusted_scores = action_means + recent_weight * action_means / (action_counts + 1e-5)\n    \n    # Total score with exploration\n    total_scores = adjusted_scores + exploration_bonus\n    \n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)  # Explore\n    else:\n        action_index = np.argmax(total_scores)  # Exploit\n    \n    return action_index",
          "objective": -409.79230248286365,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Calculate means and counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Recent performance emphasis\n    recent_weight = min(1, current_time_slot / total_time_slots)\n    historical_weight = 1 - recent_weight\n\n    recent_weighted_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        recent_scores = scores[-10:] if len(scores) > 10 else scores\n        recent_mean = np.mean(recent_scores) if recent_scores else 0\n        recent_weighted_means[action_index] = (recent_weight * recent_mean +\n                                                historical_weight * action_means[action_index])\n\n    # Normalized exploration bonus\n    exploration_bonus = np.log(total_selection_count + 1) / (action_counts + 1e-5)\n    \n    # Final score calculation\n    adjusted_scores = recent_weighted_means + exploration_bonus\n\n    # Epsilon for exploration\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": -405.20633754538505,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate means and counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Calculate normalized scores to mitigate selection bias\n    normalized_means = action_means / (action_counts + 1e-5)\n    \n    # Recent performance weighting\n    recent_weight = 0.5 * (current_time_slot / total_time_slots)  \n    recent_scores_adjusted = np.zeros(num_actions)\n\n    # Apply recent score emphasis for better responsiveness\n    for action_index in range(num_actions):\n        if action_counts[action_index] > 0:\n            recent_scores_adjusted[action_index] = np.mean(score_set[action_index][-min(5, action_counts[action_index]):])\n        else:\n            recent_scores_adjusted[action_index] = 0\n\n    # Combine normalized historical means with recent performance scores\n    combined_performance = normalized_means * (1 - recent_weight) + recent_scores_adjusted * recent_weight\n\n    # Calculate dynamic epsilon for exploration\n    epsilon = 0.1 * (1 - (current_time_slot / total_time_slots))\n    \n    # Exploration vs. exploitation decision\n    exploration_value = np.random.rand()\n    if exploration_value < epsilon:\n        action_index = np.random.randint(0, num_actions)  # Explore\n    else:\n        action_index = np.argmax(combined_performance)  # Exploit\n\n    return action_index",
          "objective": -404.80083488478954,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n    \n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    recent_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        recent_scores = score_set.get(action_index, [])[-5:]\n        recent_means[action_index] = np.mean(recent_scores) if recent_scores else 0\n\n    recency_weight = 0.6 if current_time_slot > total_time_slots * 0.5 else 0.4\n    historical_weight = 1 - recency_weight\n\n    weighted_scores = recency_weight * recent_means + historical_weight * action_means\n\n    exploration_factor = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n    exploitation_factor = action_means\n    \n    final_scores = weighted_scores + exploration_factor\n\n    epsilon = max(0.05, 0.1 * (1 - (current_time_slot / total_time_slots)))\n\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(np.arange(action_count))\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -401.13584874125286,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        if action_counts[action_index] > 0:\n            action_means[action_index] = np.mean(scores)\n\n    # Weight recent performance with exponential decay\n    recent_weight = 0.5  # weight for the most recent score\n    for action_index in range(num_actions):\n        if action_counts[action_index] > 0:\n            recent_score = scores[-1] if scores else 0\n            action_means[action_index] = (1 - recent_weight) * action_means[action_index] + recent_weight * recent_score\n\n    # Dynamic epsilon based on the current time slot\n    epsilon = max(0.1, min(1.0, (current_time_slot / total_time_slots) * 0.9))\n\n    # Bonus for less frequently chosen actions\n    selection_bonus = (1.0 - (action_counts / total_selection_count)) if total_selection_count > 0 else np.ones(num_actions)\n    \n    # Combine the means with the selection bonus\n    adjusted_scores = action_means + selection_bonus * (1 - action_means) * 0.1  # encourage exploration of less chosen actions\n\n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, num_actions)  # Explore\n    else:\n        action_index = np.argmax(adjusted_scores)  # Exploit\n\n    return action_index",
          "objective": -398.45803303309157,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_means = np.zeros(8)\n    action_counts = np.zeros(8)\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Adaptation of exploration rate\n    epsilon = 0.1 * (1 - (current_time_slot / total_time_slots))\n    exploration_gain = (epsilon * (1 - (total_selection_count / (total_selection_count + 1)))) if total_selection_count > 0 else 1\n\n    # Recent score weighting\n    recent_score_weight = min(current_time_slot / total_time_slots, 1)\n    weighted_means = action_means * (1 + recent_score_weight)\n\n    # Combined scores with exploration\n    exploration_scores = np.random.rand(8) * exploration_gain\n    total_scores = weighted_means + exploration_scores\n\n    action_index = np.argmax(total_scores)\n\n    return action_index",
          "objective": -395.92605344308,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_means = np.zeros(8)\n    action_counts = np.zeros(8)\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Dynamic exploration-exploitation trade-off\n    epsilon = max(0.1, 1 - (total_selection_count / (total_time_slots * 2)))\n    \n    # Recent performance adjustment\n    weight_factor = current_time_slot / total_time_slots\n    weighted_means = action_means * (1 + weight_factor)\n\n    # Probabilistic selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, 8)  # Explore\n    else:\n        action_index = np.argmax(weighted_means)  # Exploit\n\n    return action_index",
          "objective": -394.46771555978887,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate means and counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Avoid division by zero and dynamic adjustment of means\n    adjusted_means = action_means / np.maximum(action_counts, 1e-5)\n\n    # Dynamic exploration factor\n    epsilon = 0.1 * (1 - (current_time_slot / total_time_slots))\n\n    # Recent performance weighting\n    recent_weight = 0.7 * (current_time_slot / total_time_slots)\n    weighted_performance = adjusted_means * (1 + recent_weight)\n\n    # Probabilistic action selection\n    exploration_threshold = np.random.rand()\n    if exploration_threshold < epsilon:\n        action_index = np.random.randint(0, num_actions)  # Explore\n    else:\n        action_index = np.argmax(weighted_performance)  # Exploit\n\n    return action_index",
          "objective": -394.2572699378761,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_means = np.zeros(8)\n    action_counts = np.zeros(8)\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Adjusting the means to weight recent performance more heavily\n    recent_weight = (current_time_slot + 1) / total_time_slots\n    weighted_means = action_means * (1 + recent_weight)\n\n    # Confidence intervals for exploration\n    exploration_factor = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-10))\n    adjusted_scores = weighted_means + exploration_factor\n\n    # Epsilon decreases over time to limit exploration\n    epsilon = max(0.01, 1 - (current_time_slot / total_time_slots))\n\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, 8)  # Explore\n    else:\n        action_index = np.argmax(adjusted_scores)  # Exploit\n\n    return action_index",
          "objective": -390.1367838675988,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate means and counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Recent scores weighting with decaying factor\n    if total_selection_count > 0:\n        decay_factor = (current_time_slot + 1) / (total_time_slots + 1)\n    else:\n        decay_factor = 1\n        \n    weighted_means = action_means * (1 + decay_factor)\n\n    # Adaptive exploration parameter\n    epsilon = max(0.1, min(1.0, 1.0 - (total_selection_count / (total_time_slots * 0.5))))\n    \n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, num_actions)  # Explore\n    else:\n        action_index = np.argmax(weighted_means)  # Exploit\n\n    return action_index",
          "objective": -390.03158274864194,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    action_means = np.zeros(n_actions)\n    action_counts = np.zeros(n_actions)\n\n    # Compute action means and counts\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Dynamic epsilon decay with time\n    epsilon = max(0.1, min(0.5, (total_selection_count / (total_time_slots * n_actions)) * 0.5))\n\n    # Recent performance weighting\n    recent_weight = 0.3\n    for action_index in range(n_actions):\n        if action_counts[action_index] > 1:  # Use recent score if more than one score exists\n            recent_scores = score_set[action_index][-1]  # Use most recent score\n            action_means[action_index] = (\n                (1 - recent_weight) * action_means[action_index] + \n                recent_weight * recent_scores\n            )\n    \n    # Selection bonus for untried actions\n    selection_bonus = 0.1\n    for action_index in range(n_actions):\n        if action_counts[action_index] == 0:\n            action_means[action_index] += selection_bonus\n            \n    # Exploration term\n    exploration_term = np.random.rand(n_actions) * epsilon\n    adjusted_scores = action_means + exploration_term\n\n    # Select action based on the highest score\n    action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": -384.3433812357695,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    \n    # Initialize means, counts, and variance\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n    \n    # Calculate action means and counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if scores else 0\n    \n    # Emphasize recent performance\n    recent_scores = np.array([scores[-1] if scores else 0 for scores in score_set.values()])\n    recent_weight = 0.7\n    weighted_means = (1 - recent_weight) * action_means + recent_weight * recent_scores\n\n    # Dynamic epsilon for exploration\n    epsilon = max(0.05, 1.0 - (current_time_slot / total_time_slots))\n    \n    # Exploration bonus based on action counts and total selections\n    exploration_bonus = np.sqrt(action_counts + 1) / (total_selection_count + 1)\n    \n    # Combine weighted means and exploration\n    total_values = weighted_means + epsilon * exploration_bonus\n    \n    # Normalize to create a probability distribution\n    total_values_normalized = total_values / total_values.sum() if total_values.sum() > 0 else np.ones(num_actions) / num_actions\n    \n    # Select action based on probabilities\n    action_index = np.random.choice(np.arange(num_actions), p=total_values_normalized)\n    \n    return action_index",
          "objective": -383.95678734739863,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate action means, counts, and handle empty score lists\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Emphasize recent performance (last score is given more weight)\n    recent_scores = np.array([scores[-1] if len(scores) > 0 else 0 for scores in score_set.values()])\n    recent_weight = 0.4\n    weighted_means = (1 - recent_weight) * action_means + recent_weight * recent_scores\n\n    # Dynamic exploration factor\n    epsilon = 1.0 - (total_selection_count / (total_time_slots + 1))  # Scale epsilon\n    exploration_values = np.sqrt(action_counts + 1) / (total_selection_count + 1)\n    exploration_gain = epsilon * np.clip(exploration_values, 0, 1)\n\n    # Combine means with exploration gain\n    decision_scores = weighted_means + exploration_gain\n\n    # Probabilistic selection of an action\n    probabilities = decision_scores / np.sum(decision_scores)  # Normalize scores to sum to 1\n    action_index = np.random.choice(range(num_actions), p=probabilities)\n\n    return action_index",
          "objective": -383.42249405771884,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8  # Fixed number of actions\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Compute historical means and counts for each action\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Calculate weights based on recency\n    recent_weight = min(1, current_time_slot / total_time_slots)\n    historical_weight = 1 - recent_weight\n\n    # Calculate recent performance\n    recent_weighted_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        recent_scores = scores[-10:] if len(scores) > 10 else scores\n        recent_mean = np.mean(recent_scores) if recent_scores else 0\n        recent_weighted_means[action_index] = (recent_weight * recent_mean + \n                                                historical_weight * action_means[action_index])\n    \n    # Exploration bonus\n    exploration_bonus = np.log(total_selection_count + 1) / (action_counts + 1e-5)\n\n    # Final scores\n    final_scores = recent_weighted_means + exploration_bonus\n\n    # Normalizing scores for probability\n    prob_scores = final_scores - np.max(final_scores)  # Shift for numerical stability\n    exp_scores = np.exp(prob_scores)\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Select action based on calculated probabilities\n    action_index = np.random.choice(np.arange(action_count), p=probabilities)\n    \n    return action_index",
          "objective": -380.8276143992001,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n    \n    # Calculate means and counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Normalize means to mitigate biases\n    normalized_means = action_means / (action_counts + 1e-5)\n\n    # Dynamic exploration factor\n    epsilon = 0.1 * (1 - (current_time_slot / total_time_slots))\n\n    # Weight recent performance more significantly\n    recent_weight = 0.5 * (current_time_slot / total_time_slots)\n    adjusted_scores = normalized_means * (1 + recent_weight)\n\n    # Calculate selection probabilities\n    selection_probs = adjusted_scores / adjusted_scores.sum()\n\n    # Exploration vs Exploitation\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, num_actions)  # Explore\n    else:\n        action_index = np.random.choice(num_actions, p=selection_probs)  # Exploit using weighted sampling\n\n    return action_index",
          "objective": -377.64047058032395,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate average scores and counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Recent score weighting using exponential decay\n    decay_factor = 0.9\n    weighted_means = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        weighted_score = sum(decay_factor ** weight * score \n                              for weight, score in enumerate(reversed(score_set.get(action_index, []))))\n        if action_counts[action_index] > 0:\n            weighted_means[action_index] = weighted_score / (1 - decay_factor ** action_counts[action_index])\n    \n    # Dynamic epsilon based on time slots and total selections\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n    exploration_probabilities = (1 / (action_counts + 1)) / np.sum(1 / (action_counts + 1))\n\n    # Exploration-exploitation decision\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(np.arange(num_actions), p=exploration_probabilities)\n    else:\n        action_index = np.argmax(weighted_means)\n\n    return action_index",
          "objective": -374.7088068039059,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate means and counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Adjusted scores considering selection bias\n    adjusted_scores = action_means * (action_counts + 1) / (total_selection_count + num_actions)\n\n    # Variable exploration rate\n    exploration_rate = 1.0 / (1 + total_selection_count)\n    epsilon = exploration_rate * (1 - (current_time_slot / total_time_slots))\n\n    # Recent performance prioritization\n    recent_boost = 0.5 * (current_time_slot / total_time_slots)\n    boosted_scores = adjusted_scores * (1 + recent_boost)\n\n    # Probabilistic selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, num_actions)  # Explore\n    else:\n        action_index = np.argmax(boosted_scores)  # Exploit\n\n    return action_index",
          "objective": -366.5848685376948,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_means = np.zeros(8)\n    action_counts = np.zeros(8)\n\n    # Calculate action means and counts\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Recent score emphasis: Give more weight to the latest scores\n    recent_weights = np.clip(action_counts / (total_selection_count + 1), 0, 1)\n    weighted_means = action_means * (1 + recent_weights)\n\n    # Dynamic exploration-exploitation parameters\n    epsilon = 1.0 / (current_time_slot + 1)\n    exploration_gain = epsilon * (1 - (total_selection_count / (total_selection_count + 1)))\n\n    # Probabilistic action selection\n    if np.random.rand() < exploration_gain:\n        action_index = np.random.randint(0, 8)  # Explore\n    else:\n        action_index = np.argmax(weighted_means)  # Exploit\n\n    return action_index",
          "objective": -362.7218874304253,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n    \n    # Calculate means and counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n    \n    # Recent performance weighting\n    recent_weight = 0.5 * (current_time_slot / total_time_slots)\n    weighted_means = action_means * (1 + recent_weight)\n\n    # Adaptive exploration based on total selection\n    epsilon = max(0.1, min(1.0, (total_selection_count / (total_selection_count + 1)) * 0.5))\n    explore = np.random.rand() < epsilon\n\n    if explore:\n        action_index = np.random.randint(0, num_actions)\n    else:\n        action_index = np.argmax(weighted_means)\n\n    return action_index",
          "objective": -361.8797257690754,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    recent_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        recent_scores = score_set.get(action_index, [])[-5:]\n        recent_means[action_index] = np.mean(recent_scores) if recent_scores else 0\n\n    recency_weight = min(1, current_time_slot / (total_time_slots * 0.5))\n    historical_weight = 1 - recency_weight\n\n    weighted_scores = (recency_weight * recent_means) + (historical_weight * action_means)\n\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n\n    final_scores = weighted_scores + exploration_bonus\n\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.05)\n    \n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(np.arange(action_count))\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -345.65931222733184,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate action means and counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Emphasize recent performance by giving weight to the last score\n    recent_weights = np.array([0.7 * action_means[action_index] + 0.3 * scores[-1] if scores else 0 \n                               for action_index, scores in score_set.items()])\n    \n    # Calculate exploration factor based on selection count\n    exploration_factor = np.sqrt(action_counts + 1) / (total_selection_count + 1)\n    epsilon = max(0.05, 1.0 - (current_time_slot / total_time_slots))\n    \n    # Combine the values for selection\n    total_values = recent_weights + epsilon * exploration_factor\n    \n    # Compute probabilities for each action\n    probabilities = total_values / total_values.sum()\n    \n    # Stochastic decision-making based on computed probabilities\n    action_index = np.random.choice(np.arange(num_actions), p=probabilities)\n    \n    return action_index",
          "objective": -335.1066232710373,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.array([np.mean(scores) if scores else 0 for scores in score_set.values()])\n    action_counts = np.array([len(scores) for scores in score_set.values()])\n\n    # Calculation of recent means (latest score or average if less than 3)\n    recent_means = np.array([\n        np.mean(scores[-3:]) if len(scores) >= 3 else (np.mean(scores) if scores else 0) for scores in score_set.values()\n    ])\n\n    # Weighting recent vs historical scores\n    weight_recent = min(current_time_slot / total_time_slots, 1.0)\n    weight_historical = 1 - weight_recent\n    \n    combined_scores = (weight_recent * recent_means) + (weight_historical * action_means)\n\n    # Exploration factor based on action counts\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n    weighted_scores = combined_scores + exploration_bonus\n\n    # Epsilon-greedy mechanism for exploration\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n    \n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(weighted_scores)\n\n    return action_index",
          "objective": -330.3671132045762,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Calculate historical means and counts for each action\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Handling division by zero for normalized means\n    normalized_means = np.divide(action_means, (action_counts + 1e-5))\n\n    # Recent performance using last few scores\n    recent_scores = np.array([\n        np.mean(score_set.get(action_index, [])[-10:]) if action_counts[action_index] > 0 else 0\n        for action_index in range(action_count)\n    ])\n\n    # Weighted scores combining historical and recent performance\n    recent_weight = current_time_slot / total_time_slots\n    weighted_scores = (recent_weight * recent_scores) + ((1 - recent_weight) * normalized_means)\n\n    # Exploration bonus\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1))\n\n    # Final scores\n    final_scores = weighted_scores + exploration_bonus\n\n    # Epsilon for exploration-exploitation\n    epsilon = np.maximum(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n\n    # Select action based on epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -329.0905954145125,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Calculate historical performance and counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Normalize means\n    normalized_means = np.divide(action_means, (action_counts + 1e-5))\n\n    # Recent performance consideration\n    recent_scores = np.array([\n        np.mean(score_set.get(action_index, [])[-10:]) if action_counts[action_index] > 0 else 0\n        for action_index in range(action_count)\n    ])\n\n    # Weighting scores based on time progression\n    recent_weight = 1 - (current_time_slot / total_time_slots)\n    weighted_scores = (recent_weight * recent_scores) + (current_time_slot / total_time_slots * normalized_means)\n\n    # Exploration bonus calculation\n    exploration_bonus = np.log(total_selection_count + 1) / (action_counts + 1e-5)\n\n    # Combine scores\n    final_scores = weighted_scores + exploration_bonus\n\n    # Probabilistic exploration\n    exploration_factor = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n    if np.random.rand() < exploration_factor:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -326.04690915522315,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    action_means = np.zeros(n_actions)\n    action_counts = np.zeros(n_actions)\n\n    # Compute action means and counts\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Dynamic epsilon decay with time\n    exploration_rate = max(0.1, (total_selection_count / (total_time_slots * n_actions)) * 0.5)\n\n    # Recent performance weighting\n    recent_weight = 0.5\n    for action_index in range(n_actions):\n        if action_counts[action_index] > 0:  # Use recent score if it exists\n            recent_scores = score_set[action_index][-1]  # Use most recent score\n            action_means[action_index] = (\n                (1 - recent_weight) * action_means[action_index] + \n                recent_weight * recent_scores\n            )\n    \n    # Selection bonus for untried actions\n    selection_bonus = 0.2\n    for action_index in range(n_actions):\n        if action_counts[action_index] == 0:\n            action_means[action_index] += selection_bonus\n            \n    # Exploration term\n    exploration_term = np.random.rand(n_actions) * exploration_rate\n    adjusted_scores = action_means + exploration_term\n\n    # Select action based on the highest adjusted score\n    action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": -323.30581866851816,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate action counts and normalized means\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Calculate normalized performance to prevent bias\n    normalized_scores = action_means * (action_counts / (total_selection_count + 1))\n\n    # Emphasize recent performance\n    recent_scores = np.array([scores[-1] if len(scores) > 0 else 0 for scores in score_set.values()])\n    recent_weighted_scores = 0.6 * normalized_scores + 0.4 * recent_scores  # Adjusted weights\n\n    # Adaptive exploration rate\n    epsilon = max(0.1, 1.0 - (current_time_slot / total_time_slots))  # Ensures minimum exploration\n    exploration_bonus = np.sqrt(action_counts + 1) / (total_selection_count + 1)  # Exploration component\n\n    # Combining exploitation and exploration\n    total_values = recent_weighted_scores + epsilon * exploration_bonus\n\n    # Stochastic decision-making: softmax selection\n    action_probabilities = np.exp(total_values) / np.sum(np.exp(total_values))\n    action_index = np.random.choice(np.arange(num_actions), p=action_probabilities)\n    \n    return action_index",
          "objective": -321.01501189173496,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_means = np.zeros(8)\n    action_counts = np.zeros(8)\n\n    # Calculate action means and counts\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Apply a recent score emphasis\n    recent_scores = np.array([scores[-1] if len(scores) > 0 else 0 for scores in score_set.values()])\n    weighted_means = 0.7 * action_means + 0.3 * recent_scores\n\n    # Adaptive exploration strategy\n    epsilon = 1.0 / (current_time_slot + 1 + np.sqrt(total_selection_count))  # Dynamic exploration adjustment\n    exploration_values = np.sqrt(action_counts + 1) / (total_selection_count + 1)\n    exploration_gain = epsilon * np.clip(exploration_values, 0, 1)\n\n    # Stochastic decision-making\n    if np.random.rand() < exploration_gain[np.argmax(weighted_means)]:\n        action_index = np.random.randint(0, 8)  # Explore\n    else:\n        action_index = np.argmax(weighted_means)  # Exploit\n\n    return action_index",
          "objective": -314.44038851287905,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Avoid division by zero\n    normalized_means = np.where(action_counts > 0, action_means / (action_counts + 1e-5), 0)\n\n    # Recent score average with decay\n    recent_weight = 0.6 * (1 - (current_time_slot / total_time_slots))\n    recent_scores = np.array([\n        np.mean(score_set.get(action_index, [])[-3:]) if action_counts[action_index] > 0 else 0\n        for action_index in range(action_count)\n    ])\n    \n    # Combined scores with weighting\n    weighted_scores = (1 - recent_weight) * normalized_means + recent_weight * recent_scores\n\n    # Exploration bonus\n    exploration_bonus = (np.log(total_selection_count + 1) + 1) / (action_counts + 1)\n\n    # Final scores\n    final_scores = weighted_scores + exploration_bonus\n\n    # Epsilon-greedy strategy for exploration vs exploitation\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01)\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -311.88311448679855,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_means = np.zeros(8)\n    action_counts = np.zeros(8)\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n    \n    recent_weighted_means = np.zeros(8)\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        recent_scores = scores[-10:] if len(scores) > 10 else scores\n        recent_mean = np.mean(recent_scores) if recent_scores else 0\n        recent_weighted_means[action_index] = (0.7 * recent_mean + 0.3 * action_means[action_index])\n\n    exploration_bonus = np.log(total_selection_count + 1) / (action_counts + 1)\n    total_scores = recent_weighted_means * exploration_bonus\n\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, 8)  # Explore\n    else:\n        action_index = np.argmax(total_scores)  # Exploit\n    \n    return action_index",
          "objective": -306.31909626639435,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    recent_means = np.array([np.mean(scores[-3:]) if len(scores) >= 3 else action_means[i] for i, scores in score_set.items()])\n    \n    decay_weight = 0.7\n    effective_means = (1 - decay_weight) * recent_means + decay_weight * action_means\n    \n    recent_weighting = 1 + (current_time_slot / total_time_slots)\n    weighted_scores = effective_means * recent_weighting\n\n    epsilon = max(0.1, 0.5 * (total_selection_count / (total_time_slots * num_actions)))\n    \n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, num_actions)  # Explore\n    else:\n        action_index = np.argmax(weighted_scores)  # Exploit\n\n    return action_index",
          "objective": -305.60281536929347,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = len(score_set)\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Performance analysis\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Normalizing scores for fair comparison\n    normalized_scores = np.zeros(action_count)\n    for i in range(action_count):\n        if action_counts[i] > 0:\n            normalized_scores[i] = action_means[i] / (1 + action_counts[i])\n        else:\n            normalized_scores[i] = action_means[i]  # Avoid division by zero\n    \n    # Dynamic epsilon calculation\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n    \n    # Recent performance emphasis\n    recent_weight = 0.7\n    historical_weight = 1 - recent_weight\n    recent_weighted_means = np.zeros(action_count)\n    \n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        recent_scores = scores[-10:] if len(scores) > 10 else scores\n        recent_mean = np.mean(recent_scores) if recent_scores else 0\n        recent_weighted_means[action_index] = (recent_weight * recent_mean + historical_weight * action_means[action_index])\n\n    # Bonus for underutilized actions\n    bonus_factor = np.log(total_selection_count + 1) / (action_counts + 1)\n    final_scores = recent_weighted_means * bonus_factor + normalized_scores\n\n    # Exploration-exploitation decision\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)  # Explore\n    else:\n        action_index = np.argmax(final_scores)  # Exploit\n    \n    return action_index",
          "objective": -305.1782560869994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_means = np.zeros(8)\n    action_counts = np.zeros(8)\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_count = len(scores)\n        action_counts[action_index] = action_count\n        action_means[action_index] = np.mean(scores) if action_count > 0 else 0\n\n    # Weight for recent performance\n    recent_weight = 0.5\n    recent_scores = np.array([\n        np.mean(scores[-3:]) if len(scores) >= 3 else action_means[action_index]\n        for action_index, scores in score_set.items()\n    ])\n    \n    # Combine recent performance and means\n    effective_means = (1 - recent_weight) * action_means + recent_weight * recent_scores\n    \n    # Exploration bonus\n    selection_bonus = np.log(total_selection_count + 1) / (action_counts + 1)\n    adjusted_scores = effective_means + selection_bonus\n\n    # Dynamic epsilon for exploration\n    exploration_rate = max(0.1, 1 - current_time_slot / total_time_slots)\n    if np.random.rand() < exploration_rate:\n        action_index = np.random.randint(0, 8)\n    else:\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": -296.6909968845932,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate action means and counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if scores else 0.0\n\n    # Emphasize recent performance\n    recent_scores = np.array([scores[-1] if scores else 0.0 for scores in score_set.values()])\n    weighted_means = 0.75 * action_means + 0.25 * recent_scores\n\n    # Adjust exploration based on time\n    epsilon = max(0.1, 1.0 - (current_time_slot / total_time_slots))\n    exploration_bonus = np.sqrt(action_counts + 1) / (total_selection_count + 1) \n\n    # Combine the values for selection\n    total_values = weighted_means + epsilon * exploration_bonus\n    total_values /= total_values.sum()  # Normalize for probability distribution\n    \n    # Select action based on weighted probabilities\n    action_index = np.random.choice(np.arange(num_actions), p=total_values)\n    \n    return action_index",
          "objective": -287.2457740727763,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate means and counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Weighted average score to account for selection bias\n    weights = np.array([(action_counts[i] / (total_selection_count + 1e-5)) if total_selection_count > 0 else 0 for i in range(num_actions)])\n    weighted_scores = action_means * weights\n\n    # Dynamic exploration factor\n    epsilon = 0.1 * (1 - (current_time_slot / total_time_slots))\n    \n    # Recency weight\n    recency_weight = 0.5 * (current_time_slot / total_time_slots)  \n    recent_adjusted_scores = weighted_scores * (1 + recency_weight)\n\n    # Probabilistic selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, num_actions)  # Explore\n    else:\n        action_index = np.argmax(recent_adjusted_scores)  # Exploit\n\n    return action_index",
          "objective": -283.1466684588995,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Dynamic epsilon calculation\n    max_epsilon = 0.5\n    min_epsilon = 0.05\n    epsilon = max(min_epsilon, max_epsilon * (1 - current_time_slot / total_time_slots))\n\n    # Weighted average incorporating exploration bonus and recent performance emphasis\n    exploration_bonus = (1 / (action_counts + 1e-5))  # Prevent division by zero\n    recent_performance_weight = np.log1p(action_counts)  # Favors actions that have seen more selection\n    weighted_scores = action_means + exploration_bonus + recent_performance_weight\n\n    # Probabilistic selection between exploration and exploitation\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(num_actions)  # Explore\n    else:\n        action_index = np.argmax(weighted_scores)  # Exploit\n\n    return action_index",
          "objective": -278.2848737494438,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    action_means = np.zeros(n_actions)\n    action_counts = np.zeros(n_actions)\n    \n    # Calculate action means and counts\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if scores else 0\n\n    # Dynamic epsilon calculation\n    epsilon = 0.1 + (0.4 * (total_selection_count / (total_time_slots * n_actions))) if total_selection_count > 0 else 0.5\n    \n    # Recent performance weighting\n    recent_weight = 0.5\n    for action_index in range(n_actions):\n        if action_counts[action_index] > 0:\n            recent_scores = score_set[action_index][-1]  # Use most recent score\n            action_means[action_index] = (\n                (1 - recent_weight) * action_means[action_index] + \n                recent_weight * recent_scores\n            )\n    \n    # Selection bonus for untried actions\n    selection_bonus = 0.1\n    for action_index in range(n_actions):\n        if action_counts[action_index] == 0:\n            action_means[action_index] += selection_bonus\n            \n    # Combine action means and exploration factor\n    exploration_term = np.random.rand(n_actions) * epsilon\n    adjusted_scores = action_means + exploration_term\n\n    # Select action based on the highest adjusted score\n    action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": -276.9480902682967,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Compute historical means and counts for each action\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Normalize action means by counts to avoid bias towards less selected actions\n    normalized_scores = action_means / (action_counts + 1e-5)\n\n    # Epsilon for exploration-exploitation trade-off\n    epsilon = max(0.05, 0.2 * (1 - (current_time_slot / total_time_slots)))\n\n    # Weighted recent scores (last 5 scores, if available)\n    recent_scores = np.zeros(action_count)\n    for action_index in range(action_count):\n        recent_action_scores = score_set.get(action_index, [])[-5:]\n        recent_scores[action_index] = np.mean(recent_action_scores) if recent_action_scores else 0\n\n    # Combine recent and historical performance\n    combined_scores = (0.7 * recent_scores + 0.3 * normalized_scores)\n\n    # Exploration bonus based on action selection counts\n    exploration_bonus = np.log(total_selection_count + 1) / (action_counts + 1e-5)\n\n    # Final decision scores\n    final_scores = combined_scores + exploration_bonus\n\n    # Selection process\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -275.8037703773274,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Dynamic weights for historical and recent performance\n    recent_weight = current_time_slot / total_time_slots\n    historical_weight = 1 - recent_weight\n\n    recent_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        if action_counts[action_index] > 0:\n            recent_scores = scores[-10:] if len(scores) > 10 else scores\n            recent_means[action_index] = np.mean(recent_scores)\n\n    weighted_scores = (recent_weight * recent_means) + (historical_weight * action_means)\n\n    # Exploration bonus to encourage trying less frequent actions\n    exploration_bonus = np.log(total_selection_count + 1) / (action_counts + 1e-5)\n    \n    # Final score calculation\n    final_scores = weighted_scores + exploration_bonus\n    \n    # Epsilon for exploration probability\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -275.49517109064317,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n    \n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if scores else 0\n\n    recent_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        recent_scores = score_set.get(action_index, [])[-5:]  \n        recent_means[action_index] = np.mean(recent_scores) if recent_scores else 0\n\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n    \n    # Combine recent means and action means\n    dynamic_recent_weight = (1 - current_time_slot / total_time_slots) if current_time_slot < total_time_slots * 0.7 else 0.3\n    weighted_scores = (dynamic_recent_weight * recent_means + \n                       (1 - dynamic_recent_weight) * action_means + \n                       exploration_bonus)\n\n    # Epsilon-greedy for exploration\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.05)\n    \n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(np.arange(action_count))\n    else:\n        action_index = np.argmax(weighted_scores)\n\n    return action_index",
          "objective": -275.35238291467056,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_means = np.zeros(8)\n    action_counts = np.zeros(8)\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    recent_scores = np.array([scores[-1] if len(scores) > 0 else 0 for scores in score_set.values()])\n    weighted_means = 0.6 * action_means + 0.4 * recent_scores\n\n    epsilon = 1.0 / (1 + np.sqrt(total_selection_count))  # Dynamic exploration adjustment\n    exploration_factor = np.log(1 + total_selection_count) / (action_counts + 1)\n    exploration_values = np.clip(exploration_factor, 0, 1)\n    \n    exploration_gain = epsilon * exploration_values\n\n    probabilities = (1 - exploration_gain) * weighted_means + exploration_gain * (1 - action_means)\n    probabilities = probabilities / np.sum(probabilities)  # Normalize probabilities\n\n    if np.random.rand() < np.max(exploration_gain):\n        action_index = np.random.randint(0, 8)  # Explore\n    else:\n        action_index = np.random.choice(np.arange(8), p=probabilities)  # Exploit based on probabilities\n\n    return action_index",
          "objective": -270.2763668393354,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    action_means = np.zeros(n_actions)\n    action_counts = np.zeros(n_actions)\n\n    # Compute action means and counts\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Dynamic epsilon based on selection count and time slot\n    epsilon = max(0.1, min(0.5, 0.5 * (total_selection_count / (total_time_slots * n_actions))))\n\n    # Recent performance weighting\n    recent_weight = 0.5\n    for action_index in range(n_actions):\n        if action_counts[action_index] > 0:\n            recent_score = score_set[action_index][-1]\n            action_means[action_index] = (\n                (1 - recent_weight) * action_means[action_index] + \n                recent_weight * recent_score\n            )\n\n    # Selection bonus for untried actions\n    selection_bonus = 0.2\n    action_means += selection_bonus * (action_counts == 0)\n\n    # Exploration term\n    exploration_values = np.random.rand(n_actions) * epsilon\n    adjusted_scores = action_means + exploration_values\n\n    # Select action based on the highest adjusted score\n    action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": -264.8689934324451,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize means and counts\n    action_means = np.zeros(8)\n    action_counts = np.zeros(8)\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Recent score weighting (exponential decay)\n    decay_factor = 0.9\n    recent_weighted_means = np.zeros(8)\n    \n    for action_index in range(8):\n        if action_counts[action_index] > 0:\n            recent_scores = scores[-min(3, len(scores)):]  # Last 3 scores or less\n            recent_mean = np.mean(recent_scores) if len(recent_scores) > 0 else 0\n            recent_weighted_means[action_index] = (1 - decay_factor) * recent_mean + decay_factor * action_means[action_index]\n    \n    # Exploration-exploitation\n    epsilon = 0.1\n    exploration_threshold = epsilon * (1 - (total_selection_count / (total_selection_count + 1)))\n\n    if np.random.rand() < exploration_threshold:\n        action_index = np.random.randint(0, 8)  # Explore\n    else:\n        action_index = np.argmax(recent_weighted_means)  # Exploit\n\n    return action_index",
          "objective": -251.72318664384147,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0.0\n\n    # Weight to give more importance to recent performances\n    recent_weights = (1 + np.arange(num_actions)) / np.sum(np.arange(num_actions) + 1)\n\n    # Calculate weighted means\n    weighted_means = action_means * (recent_weights * (action_counts / (np.sum(action_counts) + 1e-10)))\n\n    # Adjust exploration strategy\n    exploration_rate = min(1.0, 0.5 + (current_time_slot / total_time_slots) * 0.5)\n    if np.random.rand() < exploration_rate:\n        action_index = np.random.randint(num_actions)  # Explore\n    else:\n        action_index = np.argmax(weighted_means)  # Exploit\n\n    return action_index",
          "objective": -236.875330152912,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_means = np.zeros(8)\n    action_counts = np.zeros(8)\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Weight recent performance more heavily\n    recent_weight = 0.3  # can be adjusted\n    if total_selection_count > 0:\n        for action_index in range(8):\n            if action_counts[action_index] > 0:\n                action_means[action_index] = (1 - recent_weight) * action_means[action_index] + \\\n                                              recent_weight * scores[-1]  # use the most recent score\n\n    # Dynamic exploration-exploitation ratio\n    epsilon = max(0.1, min(0.5, (total_selection_count / total_time_slots) * 0.5))\n\n    # Score with epsilon-greedy exploration\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, 8)  # Explore\n    else:\n        action_index = np.argmax(action_means)  # Exploit\n\n    return action_index",
          "objective": -231.82220614917878,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    action_means = np.zeros(n_actions)\n    action_counts = np.zeros(n_actions)\n\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Exploration rate that decays over time\n    epsilon = 0.1 * (1 - (current_time_slot / total_time_slots))\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))  # Avoid division by zero\n    exploration_scores = epsilon * exploration_bonus\n    \n    # Recent performance weighting\n    recent_weight = min(current_time_slot / total_time_slots, 1)\n    weighted_means = action_means * (1 + recent_weight)\n\n    # Total scores with combined components\n    total_scores = weighted_means + exploration_scores\n\n    action_index = np.argmax(total_scores)\n\n    return action_index",
          "objective": -225.4044138559754,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_means = np.zeros(8)\n    action_counts = np.zeros(8)\n\n    # Calculate action means and counts\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Recent performance weighting\n    recent_weight = 0.75  # Increased weight for more sensitivity to recent trends\n    decay_factor = 0.9  # Decay factor to reduce influence of older scores\n    recent_scores = np.array([score_set.get(i, [])[-1] for i in range(8) if len(score_set.get(i, [])) > 0])\n    if current_time_slot > 0:\n        for i in range(8):\n            if len(score_set.get(i, [])) > 0:\n                action_means[i] = (action_means[i] * (action_counts[i] ** decay_factor) + recent_scores[i]) / (action_counts[i] ** decay_factor + 1)\n                action_means[i] += recent_weight * (recent_scores[i] - action_means[i])\n    \n    # Dynamic exploration factor\n    epsilon = max(0.1 * (1 - (total_selection_count / (total_time_slots + 1))), 0.01)\n    \n    # Probabilistic action selection: Exploration vs. Exploitation\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, 8)  # Explore\n    else:\n        action_index = np.argmax(action_means)  # Exploit\n\n    return action_index",
          "objective": -223.407895719053,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n    \n    # Calculate historical means and counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Calculate recent means (last 5 scores)\n    recent_means = np.zeros(action_count)\n    for action_index in range(action_count):\n        recent_scores = score_set.get(action_index, [])[-5:]\n        recent_means[action_index] = np.mean(recent_scores) if recent_scores else 0\n\n    # Combine historical and recent means\n    recency_weight = np.clip(current_time_slot / total_time_slots, 0.1, 1.0)\n    historical_weight = 1 - recency_weight\n    weighted_scores = (recency_weight * recent_means) + (historical_weight * action_means)\n\n    # Exploration bonus based on selection counts\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n    final_scores = weighted_scores + exploration_bonus\n\n    # Softmax for probabilistic selection\n    exp_scores = np.exp(final_scores - np.max(final_scores))  # Stabilized softmax\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Select action based on computed probabilities\n    action_index = np.random.choice(action_count, p=probabilities)\n\n    return action_index",
          "objective": -222.989072052714,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize action statistics\n    action_means = []\n    action_counts = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_count = len(scores)\n        action_mean = np.mean(scores) if action_count > 0 else 0\n        action_means.append(action_mean)\n        action_counts.append(action_count)\n    \n    action_means = np.array(action_means)\n    action_counts = np.array(action_counts)\n\n    # Calculate dynamic epsilon based on current time slot\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01)\n\n    # Adjust averages with respect to action counts for exploration\n    exploration_bonus = (1 - action_counts / (total_selection_count + 1)) * 0.5\n    adjusted_scores = action_means + exploration_bonus\n\n    # Temporal weighting for recency\n    time_decay_factor = current_time_slot / total_time_slots\n    weighted_scores = adjusted_scores * (1 + time_decay_factor)\n\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, 8)  # Explore\n    else:\n        action_index = np.argmax(weighted_scores)  # Exploit\n    \n    return action_index",
          "objective": -221.52251271477428,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_means = np.zeros(8)\n    action_counts = np.zeros(8)\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Dynamic epsilon based on time slot\n    epsilon = (total_time_slots - current_time_slot) / total_time_slots\n    exploration_threshold = np.random.rand() < epsilon\n\n    # Recent trends emphasis using a weighted average approach\n    recent_weights = np.array([0.1, 0.2, 0.3, 0.4])\n    recent_scores = np.zeros(8)\n    \n    for action_index in range(8):\n        if len(score_set.get(action_index, [])) > 0:\n            recent_scores[action_index] = np.average(score_set[action_index][-min(len(score_set[action_index]), 4):], weights=recent_weights[-len(score_set[action_index][-4:]):])\n        else:\n            recent_scores[action_index] = 0\n\n    # Combine action means and recent scores for a final decision\n    combined_scores = action_means * 0.5 + recent_scores * 0.5\n\n    # Reward for underselected actions\n    underselection_bonus = 0.1 * (1 / (action_counts + 1e-5))  # Avoid division by zero\n    adjusted_scores = combined_scores + underselection_bonus\n\n    # Action selection\n    if exploration_threshold:\n        action_index = np.random.randint(0, 8)  # Explore\n    else:\n        action_index = np.argmax(adjusted_scores)  # Exploit\n\n    return action_index",
          "objective": -218.55340642974582,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate action means and counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if scores else 0\n\n    # Recent performance weighting\n    recent_weights = 0.7\n    recent_scores = np.array([scores[-1] if scores else 0 for scores in score_set.values()])\n    \n    # Calculate a weighted mean incorporating recent scores\n    combined_means = recent_weights * recent_scores + (1 - recent_weights) * action_means\n\n    # Decaying epsilon for exploration\n    epsilon = max(0.05, (1.0 - (current_time_slot / total_time_slots)))\n\n    # Exploration bonus influenced by how often an action has been chosen\n    exploration_bonus = np.sqrt((action_counts + 1) / (total_selection_count + 1))\n\n    # Combine scores and exploration bonuses\n    total_values = combined_means + epsilon * exploration_bonus\n\n    # Normalize total values to create a probability distribution\n    total_values_normalized = total_values / total_values.sum()\n\n    # Select action based on the probability distribution\n    action_index = np.random.choice(np.arange(num_actions), p=total_values_normalized)\n    \n    return action_index",
          "objective": -217.6261596637852,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_means = np.zeros(8)\n    action_counts = np.zeros(8)\n\n    # Calculate means and counts for each action\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Dynamic exploration-exploitation adjustment\n    base_exploration_prob = 0.2  # Base exploration probability\n    exploration_decay = 1 - (current_time_slot / total_time_slots)\n    epsilon = base_exploration_prob * exploration_decay\n\n    # Recent performance emphasis\n    alpha = 0.3  # Weight for recent performance\n    recent_means = np.zeros(8)\n\n    for action_index in range(8):\n        if action_counts[action_index] > 0:\n            recent_scores = score_set.get(action_index, [])[-min(3, action_counts[action_index]):]\n            recent_means[action_index] = np.mean(recent_scores) if len(recent_scores) > 0 else 0\n\n    # Weighted performance based on average and recent scores\n    performance_scores = (1 - alpha) * action_means + alpha * recent_means\n\n    # Probabilistic selection framework\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, 8)  # Explore\n    else:\n        action_index = np.argmax(performance_scores)  # Exploit\n\n    return action_index",
          "objective": -207.9393991595919,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate means and counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Normalize means to avoid bias\n    normalized_means = np.where(action_counts > 0, action_means / (action_counts + 1), 0)\n\n    # Epsilon decay for exploration\n    epsilon = 0.1 * (1 - (current_time_slot / total_time_slots))\n    \n    # Weight recent scores more heavily\n    recency_factor = 0.5 * (current_time_slot / total_time_slots)\n    recent_performance = normalized_means * (1 + recency_factor)\n\n    # Calculate exploration factor\n    exploration_factor = 1 / (1 + total_selection_count ** 0.5)\n    \n    # Create a probability distribution for selecting the action\n    weighted_probs = (1 - epsilon) * recent_performance + epsilon * (1 / num_actions)\n    \n    # Normalize probabilities\n    weighted_probs /= weighted_probs.sum()\n\n    # Select action based on computed probabilities\n    action_index = np.random.choice(num_actions, p=weighted_probs)\n\n    return action_index",
          "objective": -176.76242558888066,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_means = np.zeros(8)\n    action_counts = np.zeros(8)\n    \n    # Calculate means and counts for each action\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Initialize recent score emphasis\n    recent_weight = 0.5  # Adjust this value to control emphasis on recent scores\n    weighted_means = (1 - recent_weight) * action_means.copy()\n\n    # Add a boosted recent score component\n    for action_index in range(8):\n        if action_counts[action_index] > 0:\n            recent_mean = np.mean(scores[-1:])  # Use the most recent score only\n            weighted_means[action_index] += recent_weight * recent_mean\n\n    # Dynamic exploration-exploitation factor\n    epsilon = 0.1 * (1 - (total_selection_count / (total_selection_count + 1))) * (current_time_slot / total_time_slots)\n\n    # Stochastic selection based on exploration-exploitation balance\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, 8)  # Explore\n    else:\n        action_index = np.argmax(weighted_means)  # Exploit\n\n    return action_index",
          "objective": -165.05587537935565,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate action means and counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Normalizing averages to handle selection frequency\n    normalized_means = action_means * (1.0 + (action_counts / (total_selection_count + 1)))\n\n    # Emphasize recent performance\n    recent_scores = np.array([scores[-1] if len(scores) > 0 else 0 for scores in score_set.values()])\n    weighted_means = 0.6 * normalized_means + 0.4 * recent_scores  # More emphasis on recent scores\n\n    # Adaptive epsilon based on current time slot\n    epsilon = max(0.1, 1.0 - (current_time_slot / total_time_slots))  # Minimum exploration constant\n    exploration_bonus = np.sqrt(action_counts + 1) / (total_selection_count + 1)  # Exploration factor\n\n    # Combine exploitation and exploration\n    total_values = weighted_means + epsilon * exploration_bonus\n\n    # Stochastic decision-making based on computed values\n    probabilities = total_values / total_values.sum()\n    action_index = np.random.choice(np.arange(num_actions), p=probabilities)\n    \n    return action_index",
          "objective": -135.44249366003703,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    action_means = np.zeros(n_actions)\n    action_counts = np.zeros(n_actions)\n    \n    # Calculate mean scores and counts\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Dynamic exploration rate based on current time slot\n    exploration_rate = 1.0 / (1.0 + np.log(1 + total_selection_count))\n    \n    # Recent score weighting\n    recent_weight = (current_time_slot / total_time_slots)\n    weighted_means = action_means * (1 + recent_weight)\n\n    # Probabilistic choice between exploration and exploitation\n    if np.random.rand() < exploration_rate:\n        action_index = np.random.randint(0, n_actions)  # Explore\n    else:\n        action_index = np.argmax(weighted_means)  # Exploit\n    \n    return action_index",
          "objective": -106.10816531013222,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n    \n    # Calculate mean scores and counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if scores else 0\n\n    # Recent performance emphasis with weighted averages\n    recent_weights = np.linspace(0.1, 1, min(3, max(1, len(scores))))  # Adjust weight based on recent scores\n    recent_means = np.array([np.sum(np.array(scores[-3:]) * recent_weights[-len(scores[-3:]):]) / np.sum(recent_weights[-len(scores[-3:]):]) if len(scores) >= 3 else action_means[i] for i, scores in score_set.items()])\n    \n    # Apply decay to combine recent and historical means\n    decay_weight = 0.7\n    effective_means = (1 - decay_weight) * recent_means + decay_weight * action_means\n    \n    # Dynamic exploration factor\n    exploration_ratio = min(1, current_time_slot / (total_time_slots / 2))\n    epsilon = 0.5 * (1 - exploration_ratio) + 0.1  # Lower epsilon as time progresses\n\n    # Selection bonus for infrequently selected actions\n    selection_bonus = (1 / (1 + action_counts)) * 0.5  # Encouraging lesser-chosen actions\n    \n    # Weighted scores with exploration and bonus\n    weighted_scores = effective_means + selection_bonus\n\n    # Epsilon-greedy strategy for action selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, num_actions)  # Explore\n    else:\n        action_index = np.argmax(weighted_scores)  # Exploit\n\n    return action_index",
          "objective": -105.92678223250749,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    # Calculate historical performance, action means, and counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Calculate score variances to evaluate reliability\n    action_variances = np.array([\n        np.var(scores) if action_counts[action_index] > 1 else 0\n        for action_index, scores in score_set.items()\n    ])\n\n    # Normalize means with counts\n    normalized_means = action_means / (action_counts + 1e-5)\n\n    # Recent score emphasis (last 3 scores) with more recent importance\n    recent_scores = np.array([\n        np.mean(score_set.get(action_index, [])[-3:]) if action_counts[action_index] > 0 else 0\n        for action_index in range(action_count)\n    ])\n\n    # Weighting recent and historical scores\n    recent_weight = 0.8 * (1 - (current_time_slot / total_time_slots)) + 0.5\n    historical_weight = 1 - recent_weight\n    weighted_scores = (recent_weight * recent_scores) + (historical_weight * normalized_means)\n\n    # Exploration bonus based on action variety explored\n    exploration_bonus = (np.log1p(total_selection_count) / (action_counts + 1)) * (1 + 0.5 * np.std(action_counts) / (action_counts + 1))\n\n    # Update final scores with exploration bonus\n    final_scores = weighted_scores + exploration_bonus - (action_variances / (action_counts + 1e-5))\n\n    # Epsilon for exploration-exploitation\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": -68.05324673955153,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_means = np.zeros(8)\n    action_counts = np.zeros(8)\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n        \n        # Recent score weighting: 2 most recent scores given more weight\n        if len(scores) >= 2:\n            recent_scores = scores[-2:]  # Last two scores\n            action_means[action_index] = (np.mean(recent_scores) * 1.5 + action_means[action_index]) / 2\n        \n    # Adaptive exploration strategy\n    exploration_epsilon = max(0.05, 1 - total_selection_count / (total_selection_count + 10))\n    \n    # Calculate probabilities for each action based on weighted means and exploration\n    action_probabilities = (1 - exploration_epsilon) * (action_means / np.sum(action_means)) + \\\n                           (exploration_epsilon / 8)\n\n    # Select action based on calculated probabilities\n    action_index = np.random.choice(range(8), p=action_probabilities)\n\n    return action_index",
          "objective": -64.11925570932851,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_means = []\n    action_counts = []\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_count = len(scores)\n        action_mean = np.mean(scores) if action_count > 0 else 0\n        action_means.append(action_mean)\n        action_counts.append(action_count)\n\n    action_means = np.array(action_means)\n    action_counts = np.array(action_counts)\n\n    # Dynamic weighting of scores\n    recent_weighting = 1 + (current_time_slot / total_time_slots)\n    weighted_scores = action_means * recent_weighting\n\n    # Epsilon-greedy exploration-exploitation\n    epsilon = 0.1\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, 8)  # Explore: choose a random action\n    else:\n        action_index = np.argmax(weighted_scores)  # Exploit: choose the best action based on weighted scores\n\n    return action_index",
          "objective": -37.5427607256762,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate means and counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Recent score weighting\n    recent_factor = 0.7  # Determines the weight of recent scores\n    weighted_means = action_means * (recent_factor + (1 - recent_factor))\n\n    # Adaptive exploration-exploitation strategy\n    epsilon = 1 - (current_time_slot / total_time_slots)\n    epsilon = max(epsilon, 0.05)  # Lower bound for exploration rate\n\n    if np.random.rand() < epsilon:\n        # Encourage diversity by promoting less frequently chosen actions\n        action_diversity_bonus = 1.0 / (action_counts + 1)  # to avoid division by zero\n        action_scores = weighted_means + action_diversity_bonus\n        action_index = np.random.choice(np.flatnonzero(action_scores == action_scores.max()))  # Randomly select among ties\n    else:\n        action_index = np.argmax(weighted_means)\n\n    return action_index",
          "objective": -28.934865599191085,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize variables\n    action_means = np.zeros(8)\n    action_counts = np.zeros(8)\n    \n    # Calculate means and counts for all actions\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Dynamic epsilon adjustment\n    epsilon = max(0.1 * (total_time_slots - current_time_slot) / total_time_slots, 0.01)\n\n    # Weighted recent performance (exponential decay)\n    weights = 0.9 ** np.array(action_counts)\n    weighted_means = action_means * weights\n    \n    # Exploration-exploitation decision\n    if np.random.rand() < epsilon:\n        # Encourage exploration of less chosen actions\n        selection_bonus = (8 - action_counts) / (8 - np.sum(action_counts > 0))\n        adjusted_scores = weighted_means + selection_bonus\n        action_index = np.random.choice(np.arange(8), p=adjusted_scores/np.sum(adjusted_scores))  # Explore\n    else:\n        action_index = np.argmax(weighted_means)  # Exploit\n\n    return action_index",
          "objective": -24.212356083487748,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate action means and counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Recent score emphasis\n    recent_weight = np.clip(action_counts / (total_selection_count + 1), 0, 1)\n    weighted_means = action_means * (1 + recent_weight)\n\n    # Dynamic exploration-exploitation parameters\n    exploration_probability = 1 / (current_time_slot + 1)\n    exploration_gain = exploration_probability * (1 - (total_selection_count / (total_selection_count + 1)))\n\n    # Probabilistic action selection\n    if np.random.rand() < exploration_gain:\n        action_index = np.random.randint(0, num_actions)  # Explore\n    else:\n        action_index = np.argmax(weighted_means)  # Exploit\n    \n    return action_index",
          "objective": -4.0876076446616025,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        if action_counts[action_index] > 0:\n            action_means[action_index] = np.mean(scores)\n\n    # Weight recent performance with exponential decay\n    recent_weight = 0.6  # Adjust weight for the most recent score\n    for action_index in range(num_actions):\n        if action_counts[action_index] > 0:\n            recent_score = scores[-1] if scores else 0\n            action_means[action_index] = (1 - recent_weight) * action_means[action_index] + recent_weight * recent_score\n\n    # Dynamic epsilon based on the current time slot\n    epsilon = max(0.1, (current_time_slot / total_time_slots) * 0.9)\n\n    # Bonus for less frequently chosen actions\n    selection_bonus = (1.0 - (action_counts / total_selection_count)) if total_selection_count > 0 else np.ones(num_actions)\n\n    # Combine means with a stronger exploration factor\n    adjusted_scores = action_means + selection_bonus * 0.1  # Encourage exploration of less chosen actions\n\n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, num_actions)  # Explore\n    else:\n        action_index = np.argmax(adjusted_scores)  # Exploit\n\n    return action_index",
          "objective": 3.3428828759736007,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate means and counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if scores else 0.0\n\n    # Incorporate recent scores\n    recent_scores = np.array([scores[-1] if scores else 0 for scores in score_set.values()])\n    \n    # Lambda for recent influence\n    recent_lambda = 0.7\n    adjusted_means = (1 - recent_lambda) * action_means + recent_lambda * recent_scores\n    \n    # Exploration factor inversely proportional to selection count\n    exploration_factor = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1))\n\n    # Time-decaying epsilon for exploration\n    epsilon = max(0.05, 1.0 - (current_time_slot / total_time_slots))\n\n    # Combined value\n    total_values = adjusted_means + epsilon * exploration_factor\n    \n    # Normalize\n    total_values_normalized = total_values / np.sum(total_values)\n\n    # Stochastic selection of action\n    action_index = np.random.choice(np.arange(num_actions), p=total_values_normalized)\n    \n    return action_index",
          "objective": 3.8450738842797705,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    action_means = np.zeros(n_actions)\n    action_counts = np.zeros(n_actions)\n\n    # Compute action means and counts\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Adaptive epsilon decay based on time progression\n    epsilon = max(0.1, min(0.5, (total_selection_count / total_time_slots) * 0.5))\n\n    # Recent performance weighting\n    recent_weight = 0.5\n    recent_scores = np.array([scores[-1] if scores else 0 for scores in score_set.values()])\n    action_means = (1 - recent_weight) * action_means + recent_weight * recent_scores\n\n    # Selection bonus for untried actions\n    action_means += (action_counts == 0).astype(float) * 0.2\n\n    # Probabilistic adjustment using exploration term\n    exploration_term = np.random.rand(n_actions) * epsilon\n    adjusted_scores = action_means + exploration_term\n\n    # Select action based on adjusted scores\n    action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 71.98722692582567,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate action means and counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if scores else 0.0\n\n    # Emphasize recent performance with a weighted recent score\n    recent_weight = 0.7\n    recent_scores = np.array([scores[-1] if scores else 0 for scores in score_set.values()])\n    weighted_means = (1 - recent_weight) * action_means + recent_weight * recent_scores\n\n    # Adjust for selection frequency\n    alpha = 1.0\n    adjusted_means = weighted_means * (total_selection_count / (action_counts + alpha))\n\n    # Exploration term: a fraction of the number of times each action has been selected\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1))\n\n    # Combine adjusted means with exploration\n    total_values = adjusted_means + exploration_bonus\n\n    # Normalize to create a probability distribution\n    total_values_normalized = total_values / np.sum(total_values)\n\n    # Select action based on probabilities\n    action_index = np.random.choice(np.arange(num_actions), p=total_values_normalized)\n\n    return action_index",
          "objective": 117.13596477721728,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n    \n    # Calculate the historical performance and counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0.0\n    \n    # Normalized performance evaluation\n    normalized_performance = action_means / (action_counts + 1e-5)\n\n    # Recent performance evaluation\n    recent_scores = np.array([\n        np.mean(score_set.get(action_index, [])[-5:]) if action_counts[action_index] > 0 else 0.0\n        for action_index in range(action_count)\n    ])\n\n    # Adaptive recency weighting\n    recent_weight = min(1.0, max(0.0, current_time_slot / total_time_slots))\n    weighted_scores = (recent_weight * recent_scores) + ((1 - recent_weight) * normalized_performance)\n\n    # Exploration term\n    exploration_bonus = (np.log(total_selection_count + 1) + 1) / (action_counts + 1e-5)\n\n    # Combined final scores with exploration\n    final_scores = weighted_scores + exploration_bonus\n\n    # Stochastic decision algorithm\n    selection_probabilities = np.exp(final_scores - np.max(final_scores))  # softmax\n    selection_probabilities /= np.sum(selection_probabilities)\n\n    action_index = np.random.choice(action_count, p=selection_probabilities)\n\n    return action_index",
          "objective": 142.7198018561428,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize action statistics\n    action_means = np.zeros(8)\n    action_counts = np.zeros(8)\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Weight recent scores more heavily\n    decay_factor = 0.8\n    recent_weighted_means = np.zeros(8)\n    \n    for action_index in range(8):\n        if action_counts[action_index] > 0:\n            recent_scores = scores[-min(3, len(scores)):]  # Use last 3 scores or less\n            recent_mean = np.mean(recent_scores) if len(recent_scores) > 0 else 0\n            recent_weighted_means[action_index] = (1 - decay_factor) * recent_mean + decay_factor * action_means[action_index]\n\n    # Dynamic exploration-exploitation strategy\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01)\n    exploration_threshold = epsilon * (1 - (total_selection_count / (total_selection_count + 1)))\n    \n    # Enhance selection for less frequently chosen actions\n    selection_bonus = 1 + (1 / (action_counts + 1e-5))  # Prevent division by zero\n    \n    if np.random.rand() < exploration_threshold:\n        action_index = np.random.randint(0, 8)  # Explore\n    else:\n        # Exploit: choose action based on weighted means and selection bonus\n        adjusted_scores = recent_weighted_means * selection_bonus\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 149.00110395698061,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_means = np.zeros(8)\n    action_counts = np.zeros(8)\n\n    # Calculate average scores and counts for each action\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Weigh recent performance more heavily\n    recent_scores = np.array([scores[-1] if len(scores) > 0 else 0 for scores in score_set.values()])\n    weighted_means = 0.7 * action_means + 0.3 * recent_scores\n\n    # Dynamic exploration factor\n    epsilon = np.clip(1.0 - (current_time_slot / total_time_slots), 0.1, 1.0)\n    exploration_values = np.sqrt(action_counts + 1) / (total_selection_count + 1)\n    exploration_gain = epsilon * np.clip(exploration_values, 0, 1)\n\n    # Calculate probabilities for exploration and exploitation\n    probabilities = weighted_means + exploration_gain\n    probabilities /= np.sum(probabilities)  # Normalize to get probabilities\n    \n    # Stochastic decision-making based on the calculated probabilities\n    action_index = np.random.choice(8, p=probabilities)\n\n    return action_index",
          "objective": 200.94064418168887,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate action means and counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Recent performance emphasis\n    recent_scores = np.array([scores[-1] if len(scores) > 0 else 0 for scores in score_set.values()])\n    weighted_means = 0.65 * action_means + 0.35 * recent_scores\n\n    # Exploration factor\n    epsilon = np.clip(1.0 - (current_time_slot / total_time_slots), 0.1, 1.0)\n    exploration_values = np.sqrt(action_counts + 1) / (total_selection_count + 1)\n    exploration_gain = epsilon * exploration_values\n\n    # Stochastic decision-making\n    probabilities = weighted_means + exploration_gain\n    probabilities /= np.sum(probabilities)  # Normalize to avoid large values affecting selection\n\n    action_index = np.random.choice(num_actions, p=probabilities)\n\n    return action_index",
          "objective": 336.8852163005979,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate action means and counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Recent performance with exponential decay\n    recent_weight = 0.3\n    for action_index in range(num_actions):\n        if action_counts[action_index] > 0:\n            action_means[action_index] = (1 - recent_weight) * action_means[action_index] + \\\n                                          recent_weight * scores[-1]\n\n    # Dynamic exploration-exploitation ratio\n    epsilon = 0.1 + 0.4 * (current_time_slot / total_time_slots)\n\n    # Selection bonus for underutilized actions\n    diversity_bonus = np.where(action_counts == 0, 0.2, 0)\n    adjusted_means = action_means + diversity_bonus\n\n    # Epsilon-greedy exploration\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, num_actions)  # Explore\n    else:\n        action_index = np.argmax(adjusted_means)  # Exploit\n\n    return action_index",
          "objective": 412.72891196114495,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    action_means = np.zeros(n_actions)\n    action_counts = np.zeros(n_actions)\n\n    # Calculate action means and counts\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Dynamic epsilon decay based on the total selections\n    epsilon = max(0.1, 0.5 - (total_selection_count / (total_time_slots * n_actions)) * (0.4))\n\n    # Emphasize recent performance (last score)\n    recent_weight = 0.4\n    for action_index in range(n_actions):\n        if action_counts[action_index] > 0:\n            recent_score = score_set[action_index][-1]\n            action_means[action_index] = (\n                (1 - recent_weight) * action_means[action_index] + \n                recent_weight * recent_score\n            )\n\n    # Selection bonus for untried actions\n    action_means += (action_counts == 0) * 0.1\n\n    # Exploration term\n    exploration_term = np.random.rand(n_actions) * epsilon\n    adjusted_scores = action_means + exploration_term\n\n    # Select action with the highest adjusted score\n    action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 451.22279325555905,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n    \n    # Compute means and counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Recent score weighting\n    weight_factor = (total_time_slots - current_time_slot) / total_time_slots\n    weighted_means = action_means * (1 + weight_factor)\n\n    # Dynamic exploration factor\n    epsilon = max(0.1 * (1 - (total_selection_count / (total_selection_count + 1))), 0.01)\n\n    # Calculate probabilities for exploration vs exploitation\n    exploration_probs = np.random.rand(num_actions) < epsilon\n    action_probabilities = np.zeros(num_actions)\n\n    # Assign probabilities\n    action_probabilities[exploration_probs] = 1 / np.sum(exploration_probs) if np.sum(exploration_probs) > 0 else 0\n    action_probabilities[~exploration_probs] = weighted_means[~exploration_probs] / np.sum(weighted_means[~exploration_probs]) if np.sum(weighted_means[~exploration_probs]) > 0 else 0\n    \n    # Select action based on probabilities\n    action_index = np.random.choice(num_actions, p=action_probabilities / np.sum(action_probabilities))\n\n    return action_index",
          "objective": 494.7357432375127,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_means = np.zeros(8)\n    action_counts = np.zeros(8)\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n    \n    # Weighting scores based on recency\n    weight_factor = current_time_slot / total_time_slots\n    recent_performance = action_means * (1 + weight_factor)\n\n    # Dynamic exploration parameter\n    epsilon = min(1.0, np.log(total_selection_count + 1) / np.log(total_time_slots))\n    \n    # Probability of exploration\n    exploration_threshold = np.random.rand()\n    \n    if exploration_threshold < epsilon:\n        action_index = np.random.randint(0, 8)  # Explore\n    else:\n        # Calculate adjusted scores for selection\n        adjusted_scores = recent_performance + np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n        action_index = np.argmax(adjusted_scores)  # Exploit\n    \n    return action_index",
          "objective": 514.9423332701846,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate action means and counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if scores else 0.0\n\n    # Recent performance emphasis\n    recent_weight = 0.75\n    recent_scores = np.array([scores[-1] if scores else 0 for scores in score_set.values()])\n    weighted_means = (1 - recent_weight) * action_means + recent_weight * recent_scores\n\n    # Selection frequency adjustment\n    epsilon = 0.1\n    adjusted_means = weighted_means * (total_selection_count / (action_counts + 1 + epsilon))\n\n    # Dynamic exploration term\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1)) * (current_time_slot / total_time_slots)\n\n    # Combine adjusted means with exploration\n    total_values = adjusted_means + exploration_bonus\n\n    # Normalize values to create a probability distribution\n    total_values_normalized = total_values / np.sum(total_values)\n\n    # Select action based on the probabilities\n    action_index = np.random.choice(np.arange(num_actions), p=total_values_normalized)\n\n    return action_index",
          "objective": 637.6749982437675,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate action means and counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Recent performance weighting: Exponential decay factor\n    recent_weight = 0.5\n    recent_performance = np.zeros(num_actions)\n    \n    for action_index in range(num_actions):\n        if action_counts[action_index] > 0:\n            recent_scores = scores[-1:]  # Only consider the most recent score\n            recent_performance[action_index] = np.mean(recent_scores) * (1 - recent_weight) + action_means[action_index] * recent_weight\n        else:\n            recent_performance[action_index] = action_means[action_index]\n\n    # Dynamic epsilon value based on time slot\n    epsilon = 1.0 * (total_time_slots - current_time_slot) / total_time_slots\n    selection_bonus = np.clip(1.0 / (action_counts + 1), 0, 1)  # Bonus for infrequently selected actions\n\n    # Weighted means that incorporate recent performance and selection bonuses\n    weighted_means = recent_performance + selection_bonus\n\n    # Probabilistic action selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, num_actions)  # Explore\n    else:\n        action_index = np.argmax(weighted_means)  # Exploit\n\n    return action_index",
          "objective": 730.8749975512826,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    recent_weight = 0.7  # Increased weight for the most recent score\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        if action_counts[action_index] > 0:\n            recent_score = scores[-1] if scores else 0\n            action_means[action_index] = (1 - recent_weight) * action_means[action_index] + recent_weight * recent_score\n            \n    epsilon = max(0.05, 0.9 * (1 - current_time_slot / total_time_slots))  # Gradual decay of exploration\n\n    selection_bonus = np.clip(1.0 - (action_counts / total_selection_count), 0, 1) if total_selection_count > 0 else np.ones(num_actions)\n    \n    adjusted_scores = action_means + selection_bonus * 0.1 * (1 - action_means)\n    adjusted_scores = np.clip(adjusted_scores, 0, 1)  # Ensure scores are within valid range\n    \n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, num_actions)  # Explore\n    else:\n        action_index = np.argmax(adjusted_scores)  # Exploit\n        \n    return action_index",
          "objective": 774.6970861442085,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate the average score and number of selections for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Recent score weighting using exponential decay\n    decay_factor = 0.9\n    weighted_means = np.zeros(num_actions)\n    \n    for action_index in range(num_actions):\n        if action_counts[action_index] > 0:\n            recent_scores = np.array(scores[-min(5, len(scores)):])  # Last 5 scores or fewer\n            weighted_mean = np.mean(recent_scores) if len(recent_scores) > 0 else 0\n            weighted_means[action_index] = weighted_mean ** (1 + current_time_slot / total_time_slots)\n        else:\n            weighted_means[action_index] = 0\n\n    # Adjust exploration rate dynamically\n    epsilon = max(0.1, 0.5 * (total_time_slots - current_time_slot) / total_time_slots)\n    \n    # Explore less frequently selected actions\n    selection_bonus = 1.0 / (action_counts + 1e-5)  # Small constant to avoid division by zero\n    exploration_adjustment = selection_bonus ** 2\n    \n    # Final selection criteria combining exploitation and exploration\n    adjusted_scores = weighted_means + exploration_adjustment\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, num_actions)  # Explore\n    else:\n        action_index = np.argmax(adjusted_scores)  # Exploit\n\n    return action_index",
          "objective": 799.0961059217855,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate action means and counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if scores else 0\n\n    # Emphasize recent performance\n    recent_scores = np.array([scores[-1] if scores else 0 for scores in score_set.values()])\n    recent_weight = 0.6  # Increased weight for recent scores\n    weighted_means = (1 - recent_weight) * action_means + recent_weight * recent_scores\n\n    # Dynamic exploration parameter\n    epsilon = max(0.1, 1.0 - (current_time_slot / total_time_slots))\n\n    # Exploration bonus based on action counts\n    exploration_bonus = np.sqrt(action_counts + 1) / (total_selection_count + 1)\n\n    # Combined value for action selection\n    total_values = weighted_means + epsilon * exploration_bonus\n\n    # Normalize total values for probability distribution\n    total_values_normalized = total_values / np.sum(total_values)\n\n    # Select action based on the probability distribution\n    action_index = np.random.choice(np.arange(num_actions), p=total_values_normalized)\n\n    return action_index",
          "objective": 920.8702865563544,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate action means and counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if scores else 0.0\n\n    # Recent performance emphasis\n    recent_scores = np.array([scores[-1] if scores else 0 for scores in score_set.values()])\n    recent_weight = 0.8  # Emphasis on recent performance\n    adjusted_scores = (1 - recent_weight) * action_means + recent_weight * recent_scores\n\n    # Epsilon adjustment\n    epsilon = max(0.1, 1.0 - (current_time_slot / total_time_slots) * 0.9)\n\n    # Exploration bonus normalized by selection counts\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1))\n\n    # Combine adjusted scores with exploration factor\n    total_values = adjusted_scores + epsilon * exploration_bonus\n\n    # Safe normalization\n    total_values_normalized = total_values / np.sum(total_values) if np.sum(total_values) > 0 else np.zeros(num_actions)\n\n    # Action selection based on probabilities\n    action_index = np.random.choice(np.arange(num_actions), p=total_values_normalized)\n    \n    return action_index",
          "objective": 1007.2990425656865,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_means = np.zeros(8)\n    action_counts = np.zeros(8)\n\n    # Calculate mean scores and counts\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Dynamic Epsilon Adjustment\n    epsilon = 1.0 - (current_time_slot / total_time_slots)  # Decrease exploration over time\n    epsilon = max(epsilon, 0.1)  # Ensure a minimum exploration level\n\n    # Recent trends improvement - Exponential decay on means\n    decay_factor = 0.9  # Weight for recent scores\n    recent_weighted_means = np.zeros(8)\n\n    for action_index in range(8):\n        if action_counts[action_index] > 0:\n            recent_weighted_means[action_index] = np.mean(np.array(scores[-min(5, len(scores)):])) # Last 5 scores\n            recent_weighted_means[action_index] = decay_factor * recent_weighted_means[action_index] + (1 - decay_factor) * action_means[action_index]\n        else:\n            recent_weighted_means[action_index] = action_means[action_index]\n\n    # Diversification Incentive\n    diversity_bonus = np.where(action_counts < np.median(action_counts), 0.1, 0)\n\n    # Compute final weighted scores\n    final_scores = recent_weighted_means + diversity_bonus\n    \n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, 8)  # Explore\n    else:\n        action_index = np.argmax(final_scores)  # Exploit\n\n    return action_index",
          "objective": 1029.4589014337068,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate action means and counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Emphasize more recent performance\n    recent_scores = np.array([scores[-1] if len(scores) > 0 else 0 for scores in score_set.values()])\n    recent_weighted_means = 0.8 * action_means + 0.2 * recent_scores  # More emphasis on recent scores\n\n    # Dynamic epsilon for exploration\n    exploration_rate = max(0.1, 1.0 - (current_time_slot / total_time_slots))  # Ensure minimum exploration\n    \n    # Exploration bonus that increases with more actions taken\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1))  # Logarithmic bonus\n\n    # Combining historical performance with exploration\n    total_values = recent_weighted_means + exploration_rate * exploration_bonus\n\n    # Normalize to create a probability distribution\n    total_values /= total_values.sum()\n    \n    # Stochastic decision-making based on computed values\n    action_index = np.random.choice(num_actions, p=total_values)\n    \n    return action_index",
          "objective": 1327.4192678978245,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    action_means = np.zeros(n_actions)\n    action_counts = np.zeros(n_actions)\n\n    # Calculate historical means and counts for each action\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Recency weighting on the last score if available\n    recency_weight = 0.5\n    for action_index in range(n_actions):\n        if action_counts[action_index] > 0:\n            last_score = scores[-1] if scores else 0\n            action_means[action_index] = (1 - recency_weight) * action_means[action_index] + \\\n                                          recency_weight * last_score\n\n    # Dynamic epsilon\n    epsilon = max(0.1, min(0.5, (current_time_slot / total_time_slots) * 0.5))\n    \n    # Selection bonus for actions that have never been chosen\n    selection_bonus = 0.1\n    adjusted_means = action_means + np.where(action_counts == 0, selection_bonus, 0)\n\n    # Combine exploration noise\n    exploration_term = np.random.rand(n_actions) * epsilon\n    total_scores = adjusted_means + exploration_term\n\n    # Action selection based on the highest score\n    action_index = np.argmax(total_scores)\n\n    return action_index",
          "objective": 1425.8106338677596,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_sums = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n    \n    # Calculate means and counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_sums[action_index] = np.sum(scores)\n    \n    # Compute average scores while avoiding division by zero\n    action_means = np.divide(action_sums, action_counts, out=np.zeros_like(action_sums), where=action_counts > 0)\n    \n    # Weight for more recent scores\n    weight_factor = 0.7 ** action_counts  # Exponential decay based on the count of selections\n    adjusted_means = action_means * weight_factor\n    \n    # Epsilon decay for exploration-exploitation\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n    \n    # Random selection for exploration\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, num_actions)  # Explore\n    else:\n        action_index = np.argmax(adjusted_means)  # Exploit\n    \n    # Encourage diversity by boosting less selected actions\n    diversity_bonus = 1.0 / (action_counts + 1)  # Small bonus for less-frequent actions\n    final_scores = adjusted_means + diversity_bonus * 0.1\n    \n    # Select action based on final scores\n    action_index = np.argmax(final_scores)\n    \n    return action_index",
          "objective": 1484.406108435301,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    action_means = np.zeros(n_actions)\n    action_counts = np.zeros(n_actions)\n\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Recent performance emphasis\n    if total_selection_count > 0:\n        recent_weights = action_counts / (total_selection_count + 1)\n    else:\n        recent_weights = np.zeros(n_actions)\n    weighted_means = action_means + (0.5 * recent_weights)\n\n    # Adaptive exploration strategy\n    epsilon = 1.0 - (current_time_slot / total_time_slots)\n    exploration_gain = np.clip(epsilon, 0.1, 1.0)\n\n    # Probabilistic action selection\n    probabilities = np.ones(n_actions) * (1 - exploration_gain) / n_actions\n    best_action_index = np.argmax(weighted_means)\n    probabilities[best_action_index] += exploration_gain\n\n    action_index = np.random.choice(np.arange(n_actions), p=probabilities)\n\n    return action_index",
          "objective": 1537.4183081592537,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize action statistics\n    action_means = np.zeros(8)\n    action_counts = np.zeros(8)\n    \n    # Calculate mean scores and counts for each action\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_count = len(scores)\n        action_mean = np.mean(scores) if action_count > 0 else 0\n        action_means[action_index] = action_mean\n        action_counts[action_index] = action_count\n\n    # Handle case where total selection count is zero\n    if total_selection_count > 0:\n        # Exploration factor based on total selections\n        exploration_factors = 1 / (action_counts + 1)  # Smoothing term to avoid division by zero\n        exploration_bonus = exploration_factors / np.sum(1 / (action_counts + 1))  # Normalize to sum to 1\n    else:\n        exploration_bonus = np.full(8, 1 / 8)  # Uniform exploration if no actions have been taken\n        \n    # Time-weighted means\n    time_weight = (1 + current_time_slot / total_time_slots)\n    weighted_means = action_means * time_weight + exploration_bonus\n\n    # Epsilon-greedy approach\n    epsilon = 0.1\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(8)  # Explore: random action\n    else:\n        action_index = np.argmax(weighted_means)  # Exploit: best action based on weighted mean\n\n    return action_index",
          "objective": 1787.1953795436557,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n    \n    # Calculate action means and counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if scores else 0.0\n\n    # Weight recent scores more\n    recent_scores = np.array([scores[-1] if scores else 0 for scores in score_set.values()])\n    recent_weight = 0.7  # Emphasis on recent performance\n    adjusted_scores = (1 - recent_weight) * action_means + recent_weight * recent_scores\n\n    # Action selection with exploration factor\n    exploration_factor = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1))\n    \n    # Incorporate a dynamic exploration coefficient\n    dynamic_exploration = 1.0 / (current_time_slot + 1) * exploration_factor\n\n    # Combine adjusted scores with exploration factor\n    total_values = adjusted_scores + dynamic_exploration\n\n    # Safe normalization\n    total_values_normalized = total_values / np.sum(total_values) if np.sum(total_values) > 0 else np.zeros(num_actions)\n\n    # Action selection based on probabilities\n    action_index = np.random.choice(np.arange(num_actions), p=total_values_normalized)\n    \n    return action_index",
          "objective": 1850.841112704792,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n    \n    # Calculate action means and counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0.0\n    \n    # Weight recent scores more heavily\n    recent_means = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        if len(scores) >= 3:\n            recent_means[action_index] = np.mean(scores[-3:])\n        else:\n            recent_means[action_index] = action_means[action_index]\n    \n    # Combine means with recency weighting\n    decay_weight = 0.8\n    weighted_means = (1 - decay_weight) * recent_means + decay_weight * action_means\n\n    # Dynamically adjust epsilon based on the current time slot\n    epsilon = max(0.1, 0.5 * (total_selection_count / (total_time_slots * num_actions)))\n    \n    # Select action based on epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)  # Explore\n    else:\n        action_index = np.argmax(weighted_means)  # Exploit\n\n    return action_index",
          "objective": 2083.908252867458,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_means = np.zeros(8)\n    action_counts = np.zeros(8)\n\n    # Calculate action means and counts\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n        \n    # Recent score emphasis\n    recent_scores = np.array([scores[-1] if len(scores) > 0 else 0 for scores in score_set.values()])\n    weighted_means = 0.75 * action_means + 0.25 * recent_scores\n    \n    # Adaptive exploration strategy\n    epsilon = max(0.1, 1.0 - (current_time_slot / total_time_slots))  # Decrease exploration over time\n    exploration_values = np.sqrt(action_counts + 1) / (total_selection_count + 1)\n    exploration_gain = epsilon * np.clip(exploration_values, 0, 1)\n    \n    # Compute probabilities\n    probabilities = weighted_means + exploration_gain\n    probabilities /= np.sum(probabilities)  # Normalize probabilities\n    \n    # Stochastic decision-making\n    action_index = np.random.choice(range(8), p=probabilities)\n\n    return action_index",
          "objective": 2155.1701066063565,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate average scores and counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if scores else 0.0\n\n    # Weight recent performance with exponential decay\n    recent_weight = 0.7\n    for action_index in range(num_actions):\n        if action_counts[action_index] > 0:\n            recent_score = score_set[action_index][-1] if scores else 0\n            action_means[action_index] = (1 - recent_weight) * action_means[action_index] + recent_weight * recent_score\n\n    # Dynamic exploration rate\n    epsilon = max(0.05, 1.0 - (current_time_slot / total_time_slots) * 0.95)\n\n    # Bonus for less frequently chosen actions\n    selection_bonus = np.zeros(num_actions)\n    if total_selection_count > 0:\n        selection_bonus = (1.0 - (action_counts / total_selection_count)) * 0.1\n        \n    # Calculate adjusted scores\n    adjusted_scores = action_means + selection_bonus * (1 - action_means)\n\n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, num_actions)  # Explore\n    else:\n        action_index = np.argmax(adjusted_scores)  # Exploit\n\n    return action_index",
          "objective": 2463.723346444002,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate action means and counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Emphasize more recent performance\n    recent_scores = np.array([scores[-1] if len(scores) > 0 else 0 for scores in score_set.values()])\n    weighted_means = 0.7 * action_means + 0.3 * recent_scores  # More emphasis on recent scores\n\n    # Epsilon-greedy exploration approach\n    epsilon = max(0.05, 1.0 - (current_time_slot / total_time_slots))  # Ensure minimum exploration\n    exploration_bonus = np.sqrt(action_counts + 1) / (total_selection_count + 1)  # Exploration factor\n\n    # Combining exploitation and exploration\n    total_values = weighted_means + epsilon * exploration_bonus\n\n    # Stochastic decision-making based on computed values\n    action_index = np.random.choice(np.arange(num_actions), p=total_values / total_values.sum())\n    \n    return action_index",
          "objective": 2498.471340328989,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n    \n    # Calculate average scores and counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n    \n    # Dynamic exploration rate\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n    \n    # Recent score weighting\n    weighted_means = action_means * (1 + (current_time_slot / total_time_slots))\n    \n    # Encouraging diversity in action selection\n    diversity_bonus = np.where(action_counts < np.mean(action_counts), 0.1, 0)\n    enhanced_scores = weighted_means + diversity_bonus\n    \n    # Action selection based on exploration-exploitation trade-off\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, num_actions)  # Explore\n    else:\n        action_index = np.argmax(enhanced_scores)  # Exploit\n    \n    return action_index",
          "objective": 2792.832410173414,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    action_means = np.zeros(n_actions)\n    action_counts = np.zeros(n_actions)\n    \n    # Calculate total scores and counts for average calculation\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Decay factor for recent performance\n    recent_weight = 0.7\n    for action_index in range(n_actions):\n        if action_counts[action_index] > 0:\n            recent_score = scores[-1] if scores else 0\n            action_means[action_index] = (1 - recent_weight) * action_means[action_index] + \\\n                                          recent_weight * recent_score\n\n    # Dynamic epsilon based on time slot progression\n    initial_epsilon = 0.5\n    final_epsilon = 0.1\n    epsilon = initial_epsilon - (initial_epsilon - final_epsilon) * (current_time_slot / total_time_slots)\n\n    # Selection bonus for unselected actions\n    selection_bonus = 0.2\n    for action_index in range(n_actions):\n        if action_counts[action_index] == 0:\n            action_means[action_index] += selection_bonus\n            \n    # Exploration term\n    exploration_term = np.random.rand(n_actions) * epsilon\n    adjusted_scores = action_means + exploration_term\n\n    # Select action based on highest adjusted score\n    action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 3265.9176617203925,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    action_means = np.zeros(n_actions)\n    action_counts = np.zeros(n_actions)\n\n    # Calculate means and counts for each action\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Normalize means to account for selection bias\n    normalized_means = np.where(action_counts > 0, action_means / (action_counts + 1), 0)\n\n    # Dynamic exploration factor\n    epsilon = np.clip(0.1 * (1 - (current_time_slot / total_time_slots)), 0, 0.1)\n\n    # Recent performance weighting\n    recent_weight = 0.5 * (current_time_slot / total_time_slots)\n    weighted_performance = normalized_means * (1 + recent_weight)\n\n    # Add a term for exploration based on counts\n    exploration_bonus = (1 - (action_counts / np.max(action_counts + 1))) if total_selection_count > 0 else np.ones(n_actions)\n    exploration_bonus = exploration_bonus / np.max(exploration_bonus)  # Normalize exploration bonus\n\n    # Final performance score combining exploitation and exploration\n    final_scores = weighted_performance + exploration_bonus\n\n    # Probabilistic selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(n_actions)  # Explore\n    else:\n        action_index = np.argmax(final_scores)  # Exploit\n\n    return action_index",
          "objective": 3361.85901979344,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    action_means = np.zeros(n_actions)\n    action_counts = np.zeros(n_actions)\n\n    # Calculate action means and counts\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n    \n    # Adaptive epsilon based on total_selection_count\n    epsilon = max(0.1, min(1.0, 1 - (total_selection_count / (total_time_slots * n_actions))))\n\n    # Recent performance weighting\n    recent_weight = 0.4\n    for action_index in range(n_actions):\n        if action_counts[action_index] > 0:\n            recent_score = score_set[action_index][-1]  # Most recent score\n            action_means[action_index] = (\n                (1 - recent_weight) * action_means[action_index] + \n                recent_weight * recent_score\n            )\n\n    # Selection bonus for untried actions\n    selection_bonus = 0.2\n    action_means += np.where(action_counts == 0, selection_bonus, 0)\n\n    # Exploration term \n    exploration_term = np.random.rand(n_actions) * epsilon\n    adjusted_scores = action_means + exploration_term\n\n    # Select action based on the highest score\n    action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 3362.210785972962,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_means = np.zeros(8)\n    action_counts = np.zeros(8)\n\n    # Calculate average scores and counts for each action\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0.0\n\n    # Apply exponential decay to prioritize recent performances\n    decay_factor = 0.9\n    recent_weights = np.power(decay_factor, action_counts)\n    weighted_performance = action_means * recent_weights\n\n    # Epsilon-greedy strategy with decreasing exploration over time\n    epsilon_max = 0.8\n    epsilon_min = 0.05\n    epsilon_decay = epsilon_max - (epsilon_max * (current_time_slot / total_time_slots))\n    epsilon = max(epsilon_min, epsilon_decay)\n\n    # Action diversity bonus for underutilized actions\n    underutilization_bonus = (total_selection_count / (action_counts + 1e-5))  # Avoid division by zero\n\n    adjusted_scores = weighted_performance + underutilization_bonus * epsilon\n\n    # Epsilon-greedy selection approach\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(np.arange(8), p=underutilization_bonus/np.sum(underutilization_bonus))  # Explore\n    else:\n        action_index = np.argmax(adjusted_scores)  # Exploit\n\n    return action_index",
          "objective": 3425.580217218746,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    action_means = np.zeros(n_actions)\n    action_counts = np.zeros(n_actions)\n\n    # Calculate average scores and counts for each action\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Recent performance weighting\n    recent_weights = np.power(0.9, action_counts)\n    weighted_means = action_means * recent_weights\n\n    # Adaptive exploration-exploitation parameter\n    exploration_factor = 1 + np.log(1 + total_selection_count) / (1 + total_selection_count)\n    ucb_values = weighted_means + exploration_factor * np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n\n    # Select action based on calculated UCB values\n    action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 3475.836050092889,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate means and counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if scores else 0\n\n    # Weight recent performance\n    recent_weight = 0.6  # increased weight for recent scores\n    for action_index in range(num_actions):\n        if scores:\n            recent_score = scores[-1] if scores else 0\n            action_means[action_index] = (1 - recent_weight) * action_means[action_index] + recent_weight * recent_score\n\n    # Dynamic epsilon\n    epsilon = max(0.1, min(1.0, (current_time_slot / total_time_slots) * 0.9))\n\n    # Selection bonus for underexplored actions\n    selection_bonus = np.where(total_selection_count > 0, 1.0 - (action_counts / total_selection_count), 1.0)\n\n    # Combine means with bonuses\n    adjusted_scores = action_means + selection_bonus * (1 - action_means) * 0.1  \n\n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, num_actions)  # Explore\n    else:\n        action_index = np.argmax(adjusted_scores)  # Exploit\n\n    return action_index",
          "objective": 4038.106178849155,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        if action_counts[action_index] > 0:\n            action_means[action_index] = np.mean(scores)\n\n    # Weight recent performance with exponential decay\n    recent_weight = 0.5\n    for action_index in range(num_actions):\n        if action_counts[action_index] > 0:\n            recent_score = scores[-1] if scores else 0\n            action_means[action_index] = (1 - recent_weight) * action_means[action_index] + recent_weight * recent_score\n\n    # Adaptive epsilon based on the current time slot\n    epsilon = max(0.1, (current_time_slot / total_time_slots) * 0.9)\n\n    # Bonus for less frequently chosen actions\n    selection_bonus = (1.0 - (action_counts / total_selection_count)) if total_selection_count > 0 else np.ones(num_actions)\n    \n    # Compute adjusted scores\n    adjusted_scores = action_means + selection_bonus * 0.1 * (1 - action_means)\n\n    # Probabilistic selection based on epsilon-greedy\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(num_actions)  # Explore\n    else:\n        action_index = np.argmax(adjusted_scores)  # Exploit\n\n    return action_index",
          "objective": 4200.037322682761,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Recent score weighting with exponential decay\n    decay_factor = 0.95\n    discounted_scores = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        weighted_score = sum(decay_factor ** weight * score for weight, score in enumerate(reversed(scores)))\n        discounted_scores[action_index] = weighted_score\n\n    # Normalized weighted means\n    normalized_weights = discounted_scores / np.sum(discounted_scores) if np.sum(discounted_scores) > 0 else np.zeros(num_actions)\n\n    # Dynamic epsilon for exploration-exploitation balance\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n\n    # Softmax to compute probabilities for actions\n    exp_values = np.exp(normalized_weights / (1 + epsilon))  # Add epsilon for conversion stability\n    softmax_probabilities = exp_values / np.sum(exp_values)\n\n    # Encouraging exploration by introducing a diversity factor\n    diversity_bonus = (1 - (action_counts / (total_selection_count + 1)))  # Bonus for less selected actions\n    combined_probabilities = softmax_probabilities + diversity_bonus\n    combined_probabilities /= np.sum(combined_probabilities)  # Normalize again\n\n    # Action selection based on the combined probabilities\n    action_index = np.random.choice(np.arange(num_actions), p=combined_probabilities)\n    \n    return action_index",
          "objective": 4255.996890287051,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    action_means = np.zeros(n_actions)\n    action_counts = np.zeros(n_actions)\n\n    # Compute action means and counts\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Weight recent performance with a decay factor\n    recent_weight = 0.7  # increase emphasis on recent scores\n    if total_selection_count > 0:\n        for action_index in range(n_actions):\n            if action_counts[action_index] > 0:\n                action_means[action_index] = (1 - recent_weight) * action_means[action_index] + \\\n                                              recent_weight * scores[-1]\n\n    # Adaptive exploration parameter based on time\n    epsilon = max(0.05, min(0.3, (current_time_slot / total_time_slots) * 0.35))\n\n    # Selection bonus for unselected actions\n    selection_bonus = 0.2\n    for action_index in range(n_actions):\n        if action_counts[action_index] == 0:\n            action_means[action_index] += selection_bonus\n\n    # Stochastic selection strategy\n    exploration_term = np.random.rand(n_actions) * epsilon\n    adjusted_scores = action_means + exploration_term\n\n    # Select action based on the highest score\n    action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 4472.962405958116,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate means and counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Prioritize actions with fewer counts\n    adjusted_means = action_means / (action_counts + 1e-5)\n\n    # Dynamic exploration factor\n    epsilon = 0.1 * (1 - (current_time_slot / total_time_slots))\n\n    # Recent performance weighting\n    recent_weight = 0.7 * (current_time_slot / total_time_slots)  \n    weighted_performance = adjusted_means * (1 + recent_weight)\n\n    # Combine exploration and exploitation using a softmax approach for better distribution\n    scaled_scores = weighted_performance / (1 + epsilon * (1 - weighted_performance))\n    action_probabilities = np.exp(scaled_scores) / np.sum(np.exp(scaled_scores))\n\n    # Select action based on computed probabilities\n    action_index = np.random.choice(num_actions, p=action_probabilities)\n\n    return action_index",
          "objective": 4750.504929867994,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_means = np.zeros(8)\n    action_counts = np.zeros(8)\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Recent score weighting\n    decay_factor = 0.5  # Example of decay factor for recent performance\n    recent_scores = np.zeros(8)\n    for action_index, scores in score_set.items():\n        recent_weighted_scores = [score * (decay_factor ** (len(scores) - i - 1)) for i, score in enumerate(scores)]\n        recent_scores[action_index] = np.mean(recent_weighted_scores) if scores else 0\n\n    # Dynamic exploration-exploitation parameter\n    epsilon = min(1, 0.1 * (total_time_slots - current_time_slot) / total_time_slots)\n\n    # Weighted selection based on recent performance and average scores\n    weighted_scores = (1 - epsilon) * action_means + epsilon * recent_scores\n\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, 8)  # Explore\n    else:\n        action_index = np.argmax(weighted_scores)  # Exploit\n\n    return action_index",
          "objective": 4956.807781940815,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    action_means = np.zeros(n_actions)\n    action_counts = np.zeros(n_actions)\n\n    # Compute action means and counts\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Dynamic epsilon decay for exploration\n    epsilon = max(0.1, min(0.5, (total_selection_count / (total_time_slots * n_actions)) * 0.5))\n\n    # Recent performance prioritization (exponential decay)\n    recent_weight = 0.7\n    recent_scores = np.zeros(n_actions)\n    for action_index in range(n_actions):\n        if action_counts[action_index] > 0:\n            recent_scores[action_index] = score_set[action_index][-1]\n            action_means[action_index] = (1 - recent_weight) * action_means[action_index] + \\\n                                          recent_weight * recent_scores[action_index]\n\n    # Encourage action diversity\n    exploration_bonus = 0.2 * (1 - (action_counts / np.max(action_counts, initial=1)))\n    adjusted_scores = action_means + exploration_bonus + np.random.rand(n_actions) * epsilon\n\n    # Select action based on the highest score\n    action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 5776.3292867471355,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    action_means = np.zeros(n_actions)\n    action_counts = np.zeros(n_actions)\n\n    # Calculate action means and counts\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Recent performance emphasis using exponential decay\n    recent_scores = np.zeros(n_actions)\n    decay_factor = 0.9  # Example decay factor\n    for action_index in range(n_actions):\n        if action_counts[action_index] > 0:\n            recent_scores[action_index] = np.mean(scores[-min(5, len(scores)):])  # Last 5 scores\n\n    # Combined weighted means with recent emphasis\n    weighted_means = action_means * 0.7 + recent_scores * 0.3\n\n    # Dynamic exploration-exploitation parameters\n    epsilon = 1.0 - (current_time_slot / total_time_slots)\n    exploration_bonus = np.clip(1.0 - (action_counts / (total_selection_count + 1)), 0, 1)\n\n    # Adding exploration bonus to weighted means\n    adjusted_means = weighted_means + exploration_bonus * 0.2\n\n    # Probabilistic action selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, n_actions)  # Explore\n    else:\n        action_index = np.argmax(adjusted_means)  # Exploit\n\n    return action_index",
          "objective": 6120.065809519159,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate average scores and count for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        if action_counts[action_index] > 0:\n            action_means[action_index] = np.mean(scores)\n\n    # Apply recency bias to estimated performances\n    recent_weight = 0.6\n    for action_index in range(num_actions):\n        if action_counts[action_index] > 0:\n            recent_score = scores[-1] if scores else 0\n            action_means[action_index] = (1 - recent_weight) * action_means[action_index] + recent_weight * recent_score\n\n    # Dynamic epsilon based on current time slot to adjust exploration\n    epsilon = max(0.05, min(0.9, (current_time_slot / total_time_slots) * 0.85))\n\n    # Calculate exploration bonus for less frequently chosen actions\n    selection_bonus = np.zeros(num_actions)\n    if total_selection_count > 0:\n        selection_bonus = (1.0 - (action_counts / total_selection_count)) * 0.2\n\n    # Adjusted scores considering mean and selection bonus\n    adjusted_scores = action_means + selection_bonus\n\n    # Epsilon-greedy selection with a probabilistic approach\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, num_actions)  # Exploration\n    else:\n        action_probabilities = adjusted_scores / np.sum(adjusted_scores)  # Normalize scores for probabilistic selection\n        action_index = np.random.choice(num_actions, p=action_probabilities)  # Exploitation\n\n    return action_index",
          "objective": 6206.22421368759,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate means and counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        if action_counts[action_index] > 0:\n            action_means[action_index] = np.mean(scores)\n\n    # Weight recent performance with exponential decay\n    recent_weight = 0.6  # Adjusted weight for more emphasis on the recent score\n    for action_index in range(num_actions):\n        if action_counts[action_index] > 0:\n            recent_score = scores[-1] if scores else 0\n            action_means[action_index] = (1 - recent_weight) * action_means[action_index] + recent_weight * recent_score\n\n    # Dynamic epsilon for exploration\n    epsilon = max(0.05, min(1.0, (current_time_slot / total_time_slots) * 0.8))\n    \n    # Bonus for less frequently selected actions\n    selection_bonus = np.zeros(num_actions)\n    if total_selection_count > 0:\n        selection_bonus = (1.0 - (action_counts / total_selection_count)) * 0.2\n    \n    # Combine means with exploration bonus\n    adjusted_scores = action_means + selection_bonus\n\n    # Probabilistic selection based on epsilon\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, num_actions)  # Explore\n    else:\n        action_index = np.argmax(adjusted_scores)  # Exploit\n\n    return action_index",
          "objective": 6270.596361170681,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Weighted recent performance calculation using exponential decay\n    decay_factor = 0.95\n    weighted_means = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        weighted_score = np.sum([decay_factor ** weight * score for weight, score in enumerate(reversed(score_set.get(action_index, [])))])\n        weighted_means[action_index] = weighted_score if action_counts[action_index] > 0 else 0\n\n    # Dynamic epsilon based on total selection count and progression through time slots\n    exploration_rate = max(0.1 * (1 - (total_selection_count / (total_time_slots * 10))), 0.01)\n\n    # Exploration bonus for underexploration\n    exploration_bonus = 1.0 / (action_counts + 1)\n    exploration_bonus /= np.sum(exploration_bonus)  # Normalize\n\n    # Select action based on exploration-exploitation strategy\n    if np.random.rand() < exploration_rate:\n        action_index = np.random.choice(np.arange(num_actions), p=exploration_bonus)\n    else:\n        action_index = np.random.choice(np.arange(num_actions), p=weighted_means / np.sum(weighted_means))\n\n    return action_index",
          "objective": 6459.6274598411,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_means = np.zeros(8)\n    action_counts = np.zeros(8)\n    \n    # Calculate average scores and counts for each action\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Recent score weighting (exponential decay)\n    decay_factor = 0.9\n    recent_weights = np.power(decay_factor, action_counts)\n    weighted_means = action_means * recent_weights\n\n    # Dynamic epsilon-greedy strategy\n    epsilon = 1 - (current_time_slot / total_time_slots)  # Exploration decreases over time\n    exploration_threshold = total_selection_count / (total_selection_count + 1)\n\n    # Exploration bonus for underutilized actions\n    selection_bonus = (1 - (action_counts / np.max(action_counts)) + 1e-5) if total_selection_count > 0 else np.ones(8)\n    \n    # Adjust weighted means with selection bonus\n    adjusted_scores = weighted_means + selection_bonus * epsilon\n\n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(np.arange(8), p=selection_bonus / np.sum(selection_bonus))  # Explore\n    else:\n        action_index = np.argmax(adjusted_scores)  # Exploit\n\n    return action_index",
          "objective": 7277.787787539722,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate means and counts while handling empty lists\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Recent score weighting (more recent scores get higher weight)\n    recent_weight = 0.8  \n    decay_factor = np.array([recent_weight**(len(score_set[action_index]) - 1) for action_index in range(num_actions)])\n    weighted_means = action_means * decay_factor\n\n    # Adaptive exploration strategy based on time\n    exploration_rate = max(0.1, 1 - (current_time_slot / total_time_slots))\n    \n    # Calculate exploration bonuses\n    exploration_bonus = 1 / (action_counts + 1)  # Avoid division by zero\n\n    # Combine scores for selection\n    combined_scores = weighted_means + exploration_bonus\n\n    # Stochastic choice mechanism\n    if np.random.rand() < exploration_rate:\n        action_index = np.random.choice(np.flatnonzero(combined_scores == combined_scores.max()))  # Randomly select among ties\n    else:\n        action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": 7535.997350612966,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Evaluate historical performance\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Weight more recent performance\n    recent_scores = np.array([scores[-1] if scores else 0 for scores in score_set.values()])\n    dynamic_weights = 0.6 * action_means + 0.4 * recent_scores\n\n    # Adaptive exploration strategy\n    epsilon_min = 0.05\n    epsilon_max = 0.5\n    epsilon = epsilon_min + (epsilon_max - epsilon_min) * (current_time_slot / total_time_slots)\n    exploration_bonus = np.sqrt(action_counts + 1) / (total_selection_count + 1)\n\n    # Combine weighted means and exploration factors\n    total_values = dynamic_weights + epsilon * exploration_bonus\n\n    # Normalize to create probability distribution\n    probabilities = total_values / total_values.sum()\n\n    # Stochastic action selection\n    action_index = np.random.choice(np.arange(num_actions), p=probabilities)\n\n    return action_index",
          "objective": 8179.927685096416,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    action_means = np.zeros(n_actions)\n    action_counts = np.zeros(n_actions)\n    \n    # Calculate average scores and counts for each action\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Dynamic exploration rate\n    epsilon = 1 - (current_time_slot / total_time_slots)\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n    \n    # Recent performance weighting\n    recent_weights = np.exp(-action_counts)  # Assign higher weight to recently selected\n    adjusted_means = action_means * recent_weights\n\n    # Combine exploration and recent performance\n    decision_scores = adjusted_means + epsilon * exploration_bonus\n\n    # Stochastic selection mechanism\n    decision_probs = decision_scores / np.sum(decision_scores) if np.sum(decision_scores) > 0 else np.ones(n_actions) / n_actions\n    action_index = np.random.choice(np.arange(n_actions), p=decision_probs)\n\n    return action_index",
          "objective": 10816.623870962598,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_means = np.zeros(8)\n    action_counts = np.zeros(8)\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if scores else 0\n\n    # Weight recent scores by a factor that decreases over time\n    recent_weight = 1 - (current_time_slot / total_time_slots)\n    weighted_means = action_means * (1 + recent_weight)\n\n    # Explore-exploit ratio that decays over time\n    epsilon = max(0.1 * (1 - (total_selection_count / (total_selection_count + 1))), 0.01)\n\n    # Calculate exploration probabilities\n    exploration_probabilities = np.ones(8) * (epsilon / 8)\n    for i in range(8):\n        exploration_probabilities[i] += (1 - epsilon) * weighted_means[i] / np.sum(weighted_means)\n\n    # Select action based on probabilities\n    action_index = np.random.choice(range(8), p=exploration_probabilities)\n\n    return action_index",
          "objective": 11315.231628463092,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    action_means = np.zeros(n_actions)\n    action_counts = np.zeros(n_actions)\n\n    # Calculate action means and counts\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Recent score weighting\n    recent_factor = (total_time_slots - current_time_slot) / total_time_slots\n    weighted_means = action_means * (1 + recent_factor)\n\n    # Adaptive exploration mechanism\n    epsilon = 1 / (1 + total_selection_count)  # Decreasing epsilon over time\n    probability = np.random.rand(n_actions)\n    exploration_masks = probability < epsilon\n\n    # Make the selection\n    if np.any(exploration_masks):\n        # Select an action randomly from the under-explored actions\n        action_index = np.random.choice(np.where(exploration_masks)[0]) \n    else:\n        # Select the best action based on weighted means\n        action_index = np.argmax(weighted_means)\n\n    return action_index",
          "objective": 14288.724282734845,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Recent score weighting using exponential decay\n    decay_factor = 0.95\n    weighted_means = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        weighted_score = sum(decay_factor ** weight * score for weight, score in enumerate(reversed(scores)))\n        weighted_means[action_index] = weighted_score / (1 - decay_factor ** len(scores)) if scores else 0\n\n    # Dynamic epsilon based on time slots\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n\n    # Exploration-exploitation decision\n    if np.random.rand() < epsilon:\n        # Exploration: Prefer actions that have been selected less\n        exploration_probabilities = (1 / (action_counts + 1))\n        action_index = np.random.choice(np.arange(num_actions), p=exploration_probabilities / exploration_probabilities.sum())\n    else:\n        # Exploitation: use both weighted means and counts for better action selection\n        selection_scores = weighted_means * (1 + (action_counts / total_selection_count))\n        action_index = np.argmax(selection_scores)\n\n    return action_index",
          "objective": 19203.20502797052,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate action means and counts, avoiding division by zero\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if scores else 0.0\n\n    # Recent performance emphasis\n    recent_scores = np.array([scores[-1] if scores else 0 for scores in score_set.values()])\n    recent_weight = 0.5  # Weight for recent performance\n    weighted_means = (1 - recent_weight) * action_means + recent_weight * recent_scores\n\n    # Adaptive exploration adjustment\n    epsilon = max(0.05, 1.0 - (current_time_slot / total_time_slots) * 0.9)\n\n    # Exploration bonus based on selection counts\n    exploration_bonus = np.sqrt((total_selection_count + 1) / (action_counts + 1))  # Avoid division by zero\n\n    # Combine performance with exploration\n    total_values = weighted_means + epsilon * exploration_bonus\n\n    # Normalize total values safely\n    total_values_normalized = total_values / np.sum(total_values) if np.sum(total_values) > 0 else np.ones(num_actions) / num_actions\n\n    # Action selection based on probabilities\n    action_index = np.random.choice(np.arange(num_actions), p=total_values_normalized)\n    \n    return action_index",
          "objective": 19734.19561764475,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n    \n    # Compute the average scores and counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Dynamic epsilon adjustment for exploration-exploitation balance\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n    \n    # Weighted means emphasizing recent performance\n    recent_weight = np.exp(-np.linspace(0, 1, len(scores) + 1)[1:-1])  # Exponential decay\n    weighted_means = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        if action_counts[action_index] > 0:\n            weighted_means[action_index] = np.sum(scores[-len(recent_weight):] * recent_weight[:len(scores[-len(recent_weight):])])\n            weighted_means[action_index] /= np.sum(recent_weight[:len(scores[-len(recent_weight):])])  # Normalize\n\n    # Encourage diversity for less selected actions\n    average_selections = total_selection_count / num_actions\n    diversity_bonus = (average_selections - action_counts) / (average_selections + 1e-5)  # Avoid division by zero\n    weighted_means += np.clip(diversity_bonus, 0, None)  # Only positive bonuses\n    \n    # Action selection based on exploration-exploitation\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, num_actions)  # Explore\n    else:\n        action_index = np.argmax(weighted_means)  # Exploit\n\n    return action_index",
          "objective": 22484.7816727459,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0.0\n\n    # Compute recent performance weights\n    recency_factor = current_time_slot / total_time_slots\n    recent_weights = (1 - recency_factor) + (recency_factor * (action_counts / (np.sum(action_counts) + 1e-10)))\n    \n    weighted_means = action_means * recent_weights\n\n    # Dynamic epsilon for exploration\n    epsilon = max(0.1, min(1, 0.2 * (1 - (total_selection_count / (total_time_slots + 1)))))\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)  # Explore randomly\n    else:\n        action_index = np.argmax(weighted_means)  # Exploit based on weighted scores\n\n    return action_index",
          "objective": 25033.958703306955,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_means = np.zeros(8)\n    action_counts = np.zeros(8)\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Recent score weighting\n    recent_weight = 1 + (current_time_slot / total_time_slots)\n    weighted_means = action_means * recent_weight\n\n    # Dynamic epsilon based on time slots\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n\n    # Exploration-exploitation decision\n    if np.random.rand() < epsilon:\n        # Prefer less selected actions for exploration\n        exploration_probabilities = (1 / (action_counts + 1))\n        action_index = np.random.choice(np.arange(8), p=exploration_probabilities / np.sum(exploration_probabilities))\n    else:\n        action_index = np.argmax(weighted_means)\n\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Recent score weighting\n    weight_factor = (current_time_slot / total_time_slots) ** 2  # Squared to give more weight to recent times\n    weighted_means = action_means * (1 + weight_factor)\n\n    # Dynamic exploration parameter\n    epsilon = max(0.1, (1 - (total_selection_count / (total_time_slots * num_actions))))  # Decreasing epsilon with more selections\n    exploration_gain = np.random.rand() < epsilon\n\n    if exploration_gain:\n        action_index = np.random.randint(0, num_actions)  # Explore\n    else:\n        action_index = np.argmax(weighted_means)  # Exploit\n    \n    return action_index",
          "objective": 8436.268343973057,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate action means and counts with safety checks\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if scores else 0.0\n\n    # Recent performance emphasis\n    recent_weights = 0.7  # Emphasis on recent performance\n    recent_scores = [scores[-1] if scores else 0 for scores in score_set.values()]\n    recent_performance = np.array(recent_scores)\n    \n    # Combine action means with recent performance\n    adjusted_scores = (1 - recent_weights) * action_means + recent_weights * recent_performance\n\n    # Adaptive exploration rate\n    epsilon = 0.1 + (0.9 * (current_time_slot / total_time_slots))\n\n    # Exploration bonus normalized by selection counts\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-10))\n\n    # Compute total scores integrating exploration\n    total_scores = adjusted_scores + epsilon * exploration_bonus\n\n    # Safe normalization to prevent division by zero\n    total_scores_normalized = total_scores / np.sum(total_scores) if np.sum(total_scores) > 0 else np.zeros(num_actions)\n\n    # Select action based on probabilities\n    action_index = np.random.choice(np.arange(num_actions), p=total_scores_normalized)\n    \n    return action_index",
          "objective": 23850.539431260706,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_means = np.zeros(8)\n    action_counts = np.zeros(8)\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_count = len(scores)\n        action_counts[action_index] = action_count\n        action_means[action_index] = np.mean(scores) if action_count > 0 else 0\n    \n    total_selections = total_selection_count + 1  # Avoid division by zero\n    action_means = np.array(action_means)\n    action_counts = np.array(action_counts)\n\n    # Dynamic epsilon calculation\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01)\n\n    # Weight adjustment for exploration\n    exploration_bonus = (1 - (action_counts / total_selections)) * 0.5\n    adjusted_scores = action_means + exploration_bonus\n    \n    # Recent performance weighting\n    decay_weight = np.exp(-action_counts / (total_selection_count + 1))\n    weighted_scores = adjusted_scores * (1 + decay_weight)\n    \n    # Action selection: Explore or Exploit\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, 8)  # Explore\n    else:\n        action_index = np.argmax(weighted_scores)  # Exploit\n    \n    return action_index",
          "objective": 28358.096337063376,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    action_means = np.zeros(n_actions)\n    action_counts = np.zeros(n_actions)\n\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Epsilon-greedy exploration strategy\n    epsilon = 1.0 - (current_time_slot / total_time_slots)\n    exploration_choice = np.random.rand() < epsilon\n\n    if exploration_choice:\n        # Encourage exploration by using a random action\n        action_index = np.random.choice(np.arange(n_actions))\n    else:\n        # Adjusted scores for exploitation\n        recent_weight = 0.7\n        weighted_means = recent_weight * action_means\n        \n        selection_bonus = (1.0 / (action_counts + 1e-5)) * (1.0 / total_selection_count if total_selection_count > 0 else 1)\n        adjusted_scores = weighted_means + selection_bonus\n\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 30427.850662652392,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Normalize counts for exploration\n    exploration_weights = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1))\n    \n    # Adaptive epsilon based on current time slot\n    max_epsilon = 0.5\n    min_epsilon = 0.1\n    epsilon = max_epsilon * (1 - (current_time_slot / total_time_slots)) + min_epsilon\n\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, num_actions)  # Explore\n    else:\n        # Combine historical performance and exploration weights\n        adjusted_means = action_means + exploration_weights\n        action_index = np.argmax(adjusted_means)  # Exploit\n\n    return action_index",
          "objective": 38017.66117669971,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0.0\n\n    recency_factor = current_time_slot / total_time_slots\n    recency_weights = (1 - recency_factor) + (recency_factor * (action_counts + 1e-10) / (np.sum(action_counts) + 1e-10))\n    adjusted_means = action_means * recency_weights\n\n    epsilon = max(0.05, min(1, 0.2 * (1 - (total_selection_count / (total_time_slots + 1)))))\n    \n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)  # Explore\n    else:\n        action_index = np.argmax(adjusted_means)  # Exploit\n\n    return action_index",
          "objective": 42132.1793029576,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    action_means = np.zeros(n_actions)\n    action_counts = np.zeros(n_actions)\n\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        action_count = len(scores)\n        action_counts[action_index] = action_count\n        action_means[action_index] = np.mean(scores) if action_count > 0 else 0\n\n    # Adaptive epsilon based on current time slot\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01)\n\n    # Dynamic weighting of means with recency consideration\n    recency_weighting = 1 + (current_time_slot / total_time_slots)\n    weighted_scores = action_means * recency_weighting\n\n    # Exploration of underutilized actions\n    exploration_bonus = (1 - action_counts / (1 + total_selection_count)) * np.max(weighted_scores)\n\n    # Combine weighted scores with exploration bonus\n    final_scores = weighted_scores + exploration_bonus\n\n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, n_actions)  # Explore\n    else:\n        action_index = np.argmax(final_scores)  # Exploit\n\n    return action_index",
          "objective": 47168.216127244305,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate action means and counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if scores else 0\n\n    # Recent performance emphasis using exponentially weighted averages\n    alpha = 0.9  # decay factor for recent scores\n    recent_scores = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        if scores:\n            recent_scores[action_index] = scores[-1] * (1 - alpha) + action_means[action_index] * alpha\n\n    # Dynamic epsilon for exploration\n    epsilon = 1.0 - (current_time_slot / total_time_slots) * 0.95  # adjust range [0.05, 1.0]\n\n    # Exploration bonus based on action counts\n    exploration_bonus = np.sqrt((total_selection_count + 1) / (action_counts + 1)) \n\n    # Combine means, recent scores and exploration bonus\n    total_values = recent_scores + exploration_bonus\n\n    # Normalize total values to create a probability distribution\n    total_values_normalized = total_values / total_values.sum() if total_values.sum() > 0 else np.zeros(num_actions)\n\n    # Epsilon-greedy decision making\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(np.arange(num_actions))\n    else:\n        action_index = np.random.choice(np.arange(num_actions), p=total_values_normalized)\n\n    return action_index",
          "objective": 49205.47479056838,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate mean scores and counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0.0\n\n    # Recent score emphasis\n    recent_weight = 0.7\n    adjusted_scores = (recent_weight * action_means) + ((1 - recent_weight) * np.random.rand(num_actions))\n\n    # Epsilon-greedy strategy\n    epsilon = max(0.05, 0.1 * (1 - (total_selection_count / (total_time_slots + 1))))\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)  # Explore\n    else:\n        action_index = np.argmax(adjusted_scores)  # Exploit\n\n    return action_index",
          "objective": 54150.82426508336,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if scores else 0.0\n\n    recent_weight = 0.7\n    recent_scores = [scores[-1] if scores else 0 for scores in score_set.values()]\n    for action_index in range(num_actions):\n        if action_counts[action_index] > 0:\n            action_means[action_index] = ((1 - recent_weight) * action_means[action_index] +\n                                           (recent_weight * recent_scores[action_index]))\n\n    epsilon = max(0.1, 1.0 - (current_time_slot / total_time_slots) * 0.8)\n    \n    selection_bonus = np.where(action_counts > 0, (1.0 - (action_counts / total_selection_count)), 1.0)\n    \n    adjusted_scores = action_means + selection_bonus * (1 - action_means) * 0.15\n\n    # Probabilistic selection using softmax\n    exp_scores = np.exp(adjusted_scores - np.max(adjusted_scores))  # Stability in softmax\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(num_actions)  # Explore\n    else:\n        action_index = np.random.choice(num_actions, p=probabilities)  # Exploit\n\n    return action_index",
          "objective": 60873.19342719217,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_means = np.zeros(8)\n    action_counts = np.zeros(8)\n\n    # Calculate means and counts for each action\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Recent performance weighting\n    recent_weights = np.array([0.1 if count == 0 else 1 / count for count in action_counts])\n    weighted_means = action_means * (1 + recent_weights)\n\n    # Dynamic exploration-exploitation parameters\n    epsilon = 1.0 / (current_time_slot + 1)\n    exploration_gain = epsilon * (1 - (total_selection_count / (total_selection_count + 1)))\n\n    # Probabilistic action selection\n    if np.random.rand() < exploration_gain:\n        action_index = np.random.randint(0, 8)  # Explore\n    else:\n        # Softmax function for selection to balance exploration and exploitation\n        exp_means = np.exp(weighted_means - np.max(weighted_means))\n        probabilities = exp_means / np.sum(exp_means)\n        action_index = np.random.choice(range(8), p=probabilities)\n\n    return action_index",
          "objective": 81156.157613418,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0.0\n\n    # Weighted mean calculation favoring recent scores\n    recency_factor = current_time_slot / total_time_slots\n    recency_weights = (1 - recency_factor) + (recency_factor * action_counts / (np.sum(action_counts) + 1e-10))\n    weighted_means = action_means * recency_weights\n\n    # Controlled exploration with epsilon\n    epsilon = max(0.05, min(1, 0.2 * (1 - (total_selection_count / (total_time_slots + 1)))))\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)  # Explore\n    else:\n        action_index = np.argmax(weighted_means)  # Exploit\n\n    return action_index",
          "objective": 87938.55810746111,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_means = np.zeros(8)\n    action_counts = np.zeros(8)\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Weight recent performance\n    recent_weight = 0.5\n    weighted_means = (1 - recent_weight) * action_means + recent_weight * (np.max(action_means) + 0.01)\n\n    # UCB exploration strategy\n    ucb_values = weighted_means + np.sqrt(2 * np.log(total_selection_count + 1) / (action_counts + 1e-5))\n\n    # Probabilistic selection\n    action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 145262.12615510589,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate means and counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Recent performance weighting\n    recent_weights = np.array([0.7**(total_selection_count - c) if c > 0 else 0 for c in action_counts])\n    weighted_means = action_means * recent_weights\n\n    # Exploration-exploitation strategy\n    epsilon = 1 - (current_time_slot / total_time_slots)\n    epsilon = max(epsilon, 0.05)\n\n    if np.random.rand() < epsilon:\n        # Encourage diversity among less frequently chosen actions\n        action_diversity_bonus = 1.0 / (action_counts + 1)  # Avoid division by zero\n        action_scores = weighted_means + action_diversity_bonus\n        action_index = np.random.choice(np.flatnonzero(action_scores == action_scores.max()))  # Randomly select among any ties\n    else:\n        action_index = np.argmax(weighted_means)\n\n    return action_index",
          "objective": 150044.800760385,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        if action_counts[action_index] > 0:\n            action_means[action_index] = np.mean(scores)\n\n    # Weight recent performance using exponential decay\n    recent_weight = 0.5\n    for action_index in range(num_actions):\n        if action_counts[action_index] > 0:\n            recent_score = scores[-1] if scores else 0\n            action_means[action_index] = (1 - recent_weight) * action_means[action_index] + recent_weight * recent_score\n\n    # Dynamic epsilon based on the current time slot\n    epsilon = max(0.1, min(1.0, 1.0 - (current_time_slot / total_time_slots) * 0.9))\n\n    # Bonus for less frequently chosen actions\n    selection_bonus = np.zeros(num_actions)\n    if total_selection_count > 0:\n        selection_bonus = (1.0 - (action_counts / total_selection_count)) * 0.1\n\n    # Adjust scores\n    adjusted_scores = action_means + selection_bonus\n    \n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, num_actions)  # Explore\n    else:\n        action_index = np.argmax(adjusted_scores)  # Exploit\n\n    return action_index",
          "objective": 153823.44710069362,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate mean scores and counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0.0\n\n    # Recent performance emphasis\n    recent_weight = 0.9\n    weighted_scores = (recent_weight * action_means) + (1 - recent_weight) * (1 - action_means)\n\n    # Epsilon-greedy strategy\n    epsilon = max(0.05, 0.2 * (1 - (total_selection_count / (total_time_slots + 1))))\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)  # Explore\n    else:\n        action_index = np.argmax(weighted_scores)  # Exploit\n\n    return action_index",
          "objective": 170825.83623199377,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate means and counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if scores else 0\n\n    # Apply exponential decay for weighted recent performance\n    decay_factor = 0.9\n    recent_weights = np.array([decay_factor ** (action_counts[i]) for i in range(num_actions)])\n    weighted_means = action_means * recent_weights\n\n    # Adaptive exploration\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n    \n    # Calculate exploration bonuses\n    selection_bonus = 1 / (action_counts + 1e-5)  # Avoid division by zero\n    adjusted_means = weighted_means + selection_bonus\n\n    # Selection strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, num_actions)  # Explore\n    else:\n        action_index = np.argmax(adjusted_means)  # Exploit\n\n    return action_index",
          "objective": 185793.61847968886,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n    \n    # Calculate mean scores and counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Adjust for recent performance using a time decay factor\n    time_decay_weight = np.exp(-(current_time_slot / total_time_slots))\n    adjusted_means = action_means * (1 + time_decay_weight)\n    \n    # Exploration vs Exploitation with Epsilon-Greedy\n    exploration_prob = max(0.1, 0.1 * (1 - (current_time_slot / total_time_slots)))  # Decaying exploration rate\n    if np.random.rand() < exploration_prob:\n        # Explore: select an action random from those with fewer selections\n        low_count_actions = np.where(action_counts < np.max(action_counts))[0]\n        action_index = np.random.choice(low_count_actions) if len(low_count_actions) > 0 else np.random.randint(0, num_actions)\n    else:\n        # Exploit: select the best action based on adjusted means\n        action_index = np.argmax(adjusted_means)\n    \n    return action_index",
          "objective": 190164.87708106643,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Apply recency weighting\n    recency_weight = 0.7  # Weight for recent scores\n    if total_selection_count > 0:\n        for action_index in range(num_actions):\n            if action_counts[action_index] > 0:\n                recent_scores = np.array(scores[-min(3, action_counts[action_index]):])  # Last 3 scores\n                if len(recent_scores) > 0:\n                    recent_average = np.mean(recent_scores)\n                    action_means[action_index] = (1 - recency_weight) * action_means[action_index] + recency_weight * recent_average\n\n    # Dynamic exploration-exploitation\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))  # Decrease exploration over time\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, num_actions)  # Explore\n    else:\n        action_index = np.argmax(action_means)  # Exploit\n\n    return action_index",
          "objective": 197858.50670629725,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate mean scores and counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Dynamic epsilon for exploration\n    max_epsilon = 0.5  # Starting exploration probability\n    min_epsilon = 0.05  # Minimum exploration probability\n    epsilon = max(min_epsilon, max_epsilon * (1 - current_time_slot / total_time_slots))\n\n    # Incentivizing exploration of lesser-selected actions\n    bonus_factor = 0.1  # Encourage exploration\n    exploration_scores = (1 / (1 + action_counts)) * bonus_factor\n\n    # Recent score weighting\n    recent_weight = 1 + (current_time_slot / total_time_slots)\n    weighted_scores = action_means * recent_weight + exploration_scores\n\n    # Explore or exploit based on epsilon\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, num_actions)  # Explore\n    else:\n        action_index = np.argmax(weighted_scores)  # Exploit\n\n    return action_index",
          "objective": 204245.64770323446,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_means = np.zeros(8)\n    action_counts = np.zeros(8)\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n    \n    # Apply recency weight\n    recency_factor = np.maximum(0, current_time_slot / (total_time_slots / 2) - 1)\n    weighted_means = action_means * (1 + recency_factor)\n\n    # Adaptive exploration-exploitation balance\n    epsilon = 0.1 * (total_time_slots - current_time_slot) / total_time_slots\n    exploration_threshold = np.random.rand()\n\n    if exploration_threshold < epsilon:\n        action_index = np.random.randint(0, 8)  # Explore\n    else:\n        action_index = np.argmax(weighted_means)  # Exploit\n\n    return action_index",
          "objective": 218392.31227125032,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_means = np.zeros(8)\n    action_counts = np.zeros(8)\n\n    # Calculate action means and counts\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Recent score emphasis: Give more weight to the latest scores\n    recent_weights = np.clip(action_counts / (total_selection_count + 1), 0, 1)\n    weighted_means = action_means * (1 + recent_weights)\n\n    # Adaptive exploration factor\n    epsilon = max(0.1, 1.0 - (current_time_slot / total_time_slots))\n    exploration_factor = np.sqrt(total_selection_count + 1) / (action_counts + 1)  # Bonus for less selected actions\n    exploration_adjusted_means = weighted_means + exploration_factor\n\n    # Probabilistic action selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, 8)  # Explore\n    else:\n        action_index = np.argmax(exploration_adjusted_means)  # Exploit\n\n    return action_index",
          "objective": 230552.36690683482,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    action_means = np.zeros(n_actions)\n    action_counts = np.zeros(n_actions)\n    \n    # Calculate average scores and counts\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Epsilon-greedy strategy implementation\n    epsilon = max(1 - (current_time_slot / total_time_slots), 0.1)  # Decrease epsilon over time\n    if np.random.rand() < epsilon:\n        # Encourage exploration by selecting a random action\n        action_index = np.random.randint(0, n_actions)\n    else:\n        # Utilize historical performance\n        recent_weights = np.linspace(1, 0, min(action_counts.max(), 5))  # Focus on recent scores\n        recent_means = np.array([np.mean(scores[-len(recent_weights):]) if len(scores) > 0 else 0 \n                                 for scores in [score_set.get(i, []) for i in range(n_actions)]])\n        combined_scores = (0.7 * recent_means + 0.3 * action_means)  # Combine recent and historical scores\n\n        # Encourage diversity by incorporating a penalty for frequently selected actions\n        diversity_penalty = 1 / (action_counts + 1)  # Penalize based on counts\n        adjusted_scores = combined_scores + diversity_penalty\n\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 283517.7697340398,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate mean scores and counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0.0\n\n    # Implement recent performance weighting\n    recent_weight = 0.8\n    recent_scores = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        if action_counts[action_index] > 0:\n            recent_scores[action_index] = scores[-1] if scores else 0.0\n\n    weighted_means = (recent_weight * recent_scores) + ((1 - recent_weight) * action_means)\n\n    # Dynamic epsilon for exploration\n    epsilon = max(0.05, 0.2 * (1 - (total_selection_count / total_time_slots)))\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)  # Explore\n    else:\n        action_index = np.argmax(weighted_means)  # Exploit\n\n    return action_index",
          "objective": 302307.9426618968,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_means = np.zeros(8)\n    action_counts = np.zeros(8)\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Time decay factor\n    time_decay_factor = current_time_slot / total_time_slots\n    adjusted_scores = action_means * (1 + time_decay_factor)\n\n    # Exploration-exploitation strategy\n    epsilon = 0.1\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, 8)  # Explore\n    else:\n        action_index = np.argmax(adjusted_scores)  # Exploit\n\n    return action_index",
          "objective": 305361.39765296975,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n    \n    # Calculate means and counts with handling for empty score lists\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Dynamic epsilon for exploration\n    max_epsilon = 0.5\n    min_epsilon = 0.05\n    epsilon = max(min_epsilon, max_epsilon * (1 - current_time_slot / total_time_slots))\n\n    # Incentivizing exploration of lesser-selected actions\n    exploration_bonus = (1 / (1 + action_counts + 1e-5))  # Adding a small constant to prevent division by zero\n\n    # Recent score weighting\n    recent_weight = 1 + (current_time_slot / total_time_slots)\n    weighted_scores = action_means * recent_weight + exploration_bonus\n\n    # Select action\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)  # Explore\n    else:\n        action_index = np.argmax(weighted_scores)  # Exploit\n\n    return action_index",
          "objective": 328705.48914012325,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_means = np.zeros(8)\n    action_counts = np.zeros(8)\n\n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Recent performance weighting - more recent scores have greater influence\n    time_weight_factor = (1 - (current_time_slot / total_time_slots)) ** 2\n    recent_adjusted_scores = action_means * (1 + time_weight_factor)\n\n    # Epsilon-greedy strategy\n    initial_epsilon = 0.2\n    final_epsilon = 0.05\n    decay_factor = (total_selection_count / total_time_slots)\n    epsilon = max(final_epsilon, initial_epsilon * (1 - decay_factor))\n\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, 8)  # Explore\n    else:\n        action_index = np.argmax(recent_adjusted_scores)  # Exploit\n\n    return action_index",
          "objective": 342240.9303164308,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize means and counts\n    action_means = np.zeros(8)\n    action_counts = np.zeros(8)\n\n    # Compute action means and counts\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Dynamic epsilon decay\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01)\n    \n    # Temporal weighting for recent scores\n    decay_factor = np.exp(-current_time_slot / total_time_slots)\n    adjusted_scores = action_means * (1 + decay_factor)\n\n    # Exploration-exploitation decision\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, 8)  # Explore\n    else:\n        action_index = np.argmax(adjusted_scores)  # Exploit\n\n    return action_index",
          "objective": 345772.1170600235,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0.0\n\n    # Recent performance emphasis\n    recent_weight = (current_time_slot / total_time_slots) ** 2\n    weighted_scores = (1 - recent_weight) * action_means + recent_weight * (action_counts / (np.sum(action_counts) + 1e-10))\n    \n    # Adjust exploration rate dynamically\n    exploration_rate = max(0.05, 1 - (total_selection_count / (total_time_slots + 1)))\n    if np.random.rand() < exploration_rate:\n        action_index = np.random.randint(num_actions)  # Explore randomly\n    else:\n        action_index = np.argmax(weighted_scores)  # Exploit based on weighted scores\n\n    return action_index",
          "objective": 385624.99433303386,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_means = []\n    action_counts = []\n    \n    # Calculate means and counts\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_count = len(scores)\n        action_mean = np.mean(scores) if action_count > 0 else 0\n        action_means.append(action_mean)\n        action_counts.append(action_count)\n\n    action_means = np.array(action_means)\n    action_counts = np.array(action_counts)\n\n    # Calculate dynamic epsilon\n    epsilon = max(0.1, 1.0 - (total_selection_count / (total_time_slots * 10)))\n\n    # Time-weighted performance\n    time_weight = 1 + (current_time_slot / total_time_slots)\n    weighted_scores = action_means * time_weight\n\n    # Penalizing for less selected actions to encourage exploration\n    selection_penalty = 1 / (1 + action_counts)\n    adjusted_scores = weighted_scores * selection_penalty\n\n    # Epsilon-greedy action selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(np.arange(8), p=selection_penalty / np.sum(selection_penalty))  # Explore\n    else:\n        action_index = np.argmax(adjusted_scores)  # Exploit\n\n    return action_index",
          "objective": 454943.5770187341,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration factor\n    action_indices = list(score_set.keys())\n    scores = []\n    \n    # Calculate weighted average score for each action\n    for index in action_indices:\n        if len(score_set[index]) > 0:\n            avg_score = np.mean(score_set[index])\n            # Apply time decay based on the current time slot\n            time_decay = (current_time_slot / total_time_slots)\n            weighted_score = avg_score * time_decay\n        else:\n            weighted_score = 0  # Action has no historical scores\n        scores.append(weighted_score)\n\n    # Exploration vs. exploitation decision\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        action_index = np.argmax(scores)  # Exploit the best-known action\n\n    return action_index",
          "objective": 513561.7735723915,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize action statistics\n    action_means = []\n    action_counts = []\n    \n    # Calculate mean scores and counts for each action\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_count = len(scores)\n        action_mean = np.mean(scores) if action_count > 0 else 0\n        action_means.append(action_mean)\n        action_counts.append(action_count)\n\n    action_means = np.array(action_means)\n    action_counts = np.array(action_counts)\n\n    # Normalized scores with time decay\n    time_decay_factor = current_time_slot / total_time_slots\n    decay_weighted_means = action_means * (1 + time_decay_factor)\n\n    # Exploration with epsilon-greedy\n    epsilon = 0.1\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(8)  # Explore: random action\n    else:\n        # Exploit: best action based on weighted mean\n        action_index = np.argmax(decay_weighted_means)\n\n    return action_index",
          "objective": 626669.5232653178,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize action statistics\n    action_means = []\n    action_counts = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_count = len(scores)\n        action_mean = np.mean(scores) if action_count > 0 else 0\n        action_means.append(action_mean)\n        action_counts.append(action_count)\n    \n    action_means = np.array(action_means)\n    action_counts = np.array(action_counts)\n\n    # Normalized scores with time decay\n    time_decay_factor = current_time_slot / total_time_slots\n    normalized_scores = action_means * (1 + time_decay_factor)\n\n    # Epsilon-greedy approach for exploration\n    epsilon = 0.1\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, 8)  # Explore: random action\n    else:\n        action_index = np.argmax(normalized_scores)  # Exploit: best action\n    \n    return action_index",
          "objective": 644110.7182040856,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_means = np.zeros(8)\n    action_counts = np.zeros(8)\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Time decay factor, giving more weight to recent performance\n    time_decay_factor = max(0.1, (total_time_slots - current_time_slot) / total_time_slots)\n    adjusted_scores = action_means * (1 + time_decay_factor)\n\n    # Epsilon-greedy strategy for exploration and exploitation\n    epsilon = 0.1\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, 8)  # Explore\n    else:\n        action_index = np.argmax(adjusted_scores)  # Exploit\n\n    return action_index",
          "objective": 680213.9543842317,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate action means and counts\n    action_means = []\n    action_counts = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_count = len(scores)\n        action_mean = np.mean(scores) if action_count > 0 else 0\n        action_means.append(action_mean)\n        action_counts.append(action_count)\n\n    action_means = np.array(action_means)\n    action_counts = np.array(action_counts)\n\n    # Normalize counts to avoid division by zero and create exploration bonus\n    exploration_bonus = (total_selection_count / (action_counts + 1)) if total_selection_count > 0 else np.zeros(8)\n\n    # Temporal weighting based on current time slot\n    time_decay_factor = current_time_slot / total_time_slots\n    weighted_scores = action_means * (1 + time_decay_factor) + exploration_bonus\n\n    # Epsilon-greedy approach for exploration\n    epsilon = 0.1\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, 8)  # Explore: random action\n    else:\n        action_index = np.argmax(weighted_scores)  # Exploit: best action\n\n    return action_index",
          "objective": 706979.8471263447,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_means = np.zeros(8)\n    action_counts = np.zeros(8)\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Time decay factor for dynamic scoring\n    time_decay_factor = 1 - (current_time_slot / total_time_slots)\n    adjusted_scores = action_means + time_decay_factor * (1 - action_means)\n\n    # Exploration-exploitation strategy \u2013 using epsilon-greedy approach\n    epsilon = 0.1\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, 8)  # Explore\n    else:\n        action_index = np.argmax(adjusted_scores)  # Exploit\n\n    return action_index",
          "objective": 789598.6586687026,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration factor\n    action_indices = list(score_set.keys())\n    scores = []\n    \n    # Calculate weighted average score for each action\n    for index in action_indices:\n        if len(score_set[index]) > 0:\n            avg_score = np.mean(score_set[index])\n            # Apply time decay based on the current time slot\n            time_decay = (current_time_slot / total_time_slots)\n            weighted_score = avg_score * time_decay\n        else:\n            weighted_score = 0  # No historical scores available\n        \n        # Incorporate selection frequency\n        selection_fraction = score_set.get(index, []).count(1) / total_selection_count if total_selection_count > 0 else 0\n        adjusted_score = weighted_score / (1 + selection_fraction)  # Penalize frequently selected actions\n        scores.append(adjusted_score)\n\n    # Exploration vs. exploitation decision\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        action_index = np.argmax(scores)  # Exploit the best-known action\n\n    return action_index",
          "objective": 795352.8268156699,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n    \n    # Calculate mean scores and counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_count = len(scores)\n        action_mean = np.mean(scores) if action_count > 0 else 0\n        action_means[action_index] = action_mean\n        action_counts[action_index] = action_count\n\n    # Prioritize recent performance with exponential weighting\n    time_decay_factor = 1 - (current_time_slot / total_time_slots)  # Higher weight for recent\n    weighted_scores = action_means * (1 + time_decay_factor)\n\n    # Exploration vs Exploitation\n    exploration_prob = 0.1  # Epsilon value for exploration\n    if np.random.rand() < exploration_prob:\n        action_index = np.random.randint(0, num_actions)  # Explore\n    else:\n        action_index = np.argmax(weighted_scores)  # Exploit\n\n    return action_index",
          "objective": 870726.3378675923,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Time decay factor to increase relevance of recent scores\n    time_decay = (1 - (current_time_slot / total_time_slots))\n    adjusted_scores = action_means * time_decay\n\n    # Tuning the effect of exploration vs exploitation\n    epsilon = 0.1\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, num_actions)  # Explore\n    else:\n        action_index = np.argmax(adjusted_scores)  # Exploit\n\n    return action_index",
          "objective": 1040201.1200868797,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize action statistics\n    action_means = np.zeros(8)\n    action_counts = np.zeros(8)\n    \n    # Calculate mean scores and counts for each action\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_count = len(scores)\n        action_mean = np.mean(scores) if action_count > 0 else 0\n        action_means[action_index] = action_mean\n        action_counts[action_index] = action_count\n\n    # Handle case where total selection count is zero\n    if total_selection_count > 0:\n        exploration_bonus = 1 / (action_counts + 1)  # Smoothed exploration factor\n        exploration_bonus /= np.sum(exploration_bonus)\n    else:\n        exploration_bonus = np.full(8, 1 / 8)  # Uniform exploration\n    \n    # Recent performance weighting\n    time_weight = (1 + current_time_slot / total_time_slots)\n    weighted_means = action_means * time_weight + exploration_bonus\n\n    # Epsilon-greedy selection\n    epsilon = 0.1\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(8)  # Exploration: select a random action\n    else:\n        action_index = np.argmax(weighted_means)  # Exploitation: select the best action based on weighted means\n\n    return action_index",
          "objective": 1051574.7470202672,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores for each action\n    average_scores = {}\n    for action_index, scores in score_set.items():\n        if len(scores) > 0:\n            average_scores[action_index] = np.mean(scores)\n        else:\n            average_scores[action_index] = 0  # If no scores, assume average score of 0\n    \n    # Exploration factor: e.g., epsilon could be 0.1\n    epsilon = 0.1 \n    if total_selection_count < 1:\n        action_index = np.random.randint(0, 8)  # Random choice if no selections made\n    else:\n        if np.random.rand() < epsilon:\n            # Exploration: choose a random action\n            action_index = np.random.randint(0, 8)\n        else:\n            # Exploitation: choose the best average score action\n            action_index = max(average_scores, key=average_scores.get)\n    \n    return action_index",
          "objective": 1075274.4052785703,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n    \n    # Calculate means and counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n    \n    # Epsilon decay for exploration\n    max_epsilon = 0.5\n    min_epsilon = 0.05\n    epsilon = max(min_epsilon, max_epsilon * (1 - current_time_slot / total_time_slots))\n    \n    # Recent score weighting\n    recent_weight = 1 + (current_time_slot / total_time_slots)\n    weighted_scores = action_means * recent_weight + (1 / (1 + action_counts + 1e-5))\n    \n    # Probabilistic selection strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(num_actions)  # Explore\n    else:\n        action_index = np.argmax(weighted_scores)  # Exploit\n    \n    return action_index",
          "objective": 1502542.4800830223,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate mean scores and counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Dynamic score adjustment\n    time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = action_means * (1 + time_decay_factor)\n\n    # Exploration vs Exploitation\n    epsilon = 0.1\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, num_actions)  # Explore\n    else:\n        # Compute exploration bonus\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (1 + action_counts))\n        final_scores = adjusted_scores + exploration_bonus\n        action_index = np.argmax(final_scores)  # Exploit\n\n    return action_index",
          "objective": 1958138.2579020113,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1\n    random_action = np.random.rand() < epsilon\n    \n    averages = {}\n    for action_index, scores in score_set.items():\n        if scores:\n            average_score = np.mean(scores)\n        else:\n            average_score = 0  # Handle cases where the action has never been selected\n        # Decay factor based on recency of action\n        decay = (total_time_slots - current_time_slot) / total_time_slots\n        averages[action_index] = average_score * decay\n    \n    if random_action:\n        action_index = np.random.randint(0, 8)  # Randomly select an action from 0 to 7\n    else:\n        action_index = max(averages, key=averages.get)  # Select action with max average score\n    \n    return action_index",
          "objective": 2176514.72225514,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    action_means = np.zeros(n_actions)\n    action_counts = np.zeros(n_actions)\n\n    # Calculate average scores and action counts\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Epsilon-greedy strategy implementation\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n\n    if np.random.rand() < epsilon:\n        # Exploration: Select a random action\n        action_index = np.random.randint(0, n_actions)\n    else:\n        # Exploitation: Utilize historical performance with recency weighting\n        recent_weights = np.linspace(1, 0, min(5, max(1, action_counts.max())))\n        recency_scores = np.array([np.mean(scores[-len(recent_weights):]) if len(scores) > 0 else 0 \n                                   for scores in [score_set.get(i, []) for i in range(n_actions)]])\n        combined_scores = 0.7 * recency_scores + 0.3 * action_means\n        \n        # Encourage diversity\n        diversity_penalty = 1 / (action_counts + 1e-5)  # Avoid division by zero\n        adjusted_scores = combined_scores + diversity_penalty\n        \n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 3283383.671796757,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_means = np.zeros(8)\n    action_counts = np.zeros(8)\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Recent score weighting\n    time_weight_factor = 1 - (current_time_slot / total_time_slots)\n    recent_weighted_means = action_means + (time_weight_factor * (1 - action_means))\n    \n    # Dynamic exploration factor\n    epsilon_base = 0.1\n    epsilon = epsilon_base * (1 - (current_time_slot / total_time_slots))\n    \n    # Probability distribution\n    probabilities = (1 - epsilon) * (recent_weighted_means / np.sum(recent_weighted_means + 1e-10)) + (epsilon / 8)\n    \n    action_index = np.random.choice(range(8), p=probabilities)\n\n    return action_index",
          "objective": 4911829.425951647,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize action statistics\n    action_means = []\n    action_counts = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_count = len(scores)\n        action_mean = np.mean(scores) if action_count > 0 else 0\n        action_means.append(action_mean)\n        action_counts.append(action_count)\n    \n    action_means = np.array(action_means)\n    action_counts = np.array(action_counts)\n\n    # Temporal weighting based on current time slot\n    time_decay_factor = current_time_slot / total_time_slots\n    weighted_scores = action_means * (1 + time_decay_factor)\n\n    # Epsilon-greedy approach for exploration\n    epsilon = 0.1\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, 8)  # Explore: random action\n    else:\n        action_index = np.argmax(weighted_scores)  # Exploit: best action\n    \n    return action_index",
          "objective": 7291876.1837006975,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_means = np.zeros(8)\n    action_counts = np.zeros(8)\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0.0\n\n    total_selections = total_selection_count + 1  # Avoid division by zero\n\n    # Adaptive epsilon calculation\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01)\n\n    # Recent performance weighting\n    recent_weight = np.array([0.7 if action_counts[i] > 0 else 0 for i in range(8)])\n    weighted_means = action_means * recent_weight + (1 - recent_weight) * 0.5  # Adjust for recent performance \n\n    # Informed exploration\n    unexplored_bonus = 0.5 * (1 - (action_counts / total_selections))\n    adjusted_scores = weighted_means + unexplored_bonus\n\n    # Action selection: Explore or Exploit\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, 8)  # Explore\n    else:\n        action_index = np.argmax(adjusted_scores)  # Exploit\n    \n    return action_index",
          "objective": 8980767.356178239,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_means = []\n    action_counts = []\n    \n    # Calculate mean scores and counts\n    for i in range(8):\n        scores = score_set.get(i, [])\n        action_counts.append(len(scores))\n        action_means.append(np.mean(scores) if action_counts[-1] > 0 else 0)\n    \n    action_means = np.array(action_means)\n    action_counts = np.array(action_counts)\n\n    # Dynamic epsilon decay over time\n    max_epsilon = 0.2  # Starting exploration rate\n    min_epsilon = 0.01  # Minimum exploration rate\n    epsilon_decay_rate = (max_epsilon - min_epsilon) / total_time_slots\n    epsilon = max(min_epsilon, max_epsilon - current_time_slot * epsilon_decay_rate)\n\n    if np.random.rand() < epsilon:\n        # Encourage selection of less frequently chosen actions\n        action_probabilities = (1 / (1 + action_counts)) / np.sum(1 / (1 + action_counts))\n        action_index = np.random.choice(range(8), p=action_probabilities)\n    else:\n        # Weighted means: prioritize recent performance with linear decay\n        recent_weighting = np.clip(current_time_slot / total_time_slots, 0.5, 1.0)\n        adjusted_means = action_means * recent_weighting\n        action_index = np.argmax(adjusted_means)\n\n    return action_index",
          "objective": 21052622.825443383,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate mean scores and counts\n    for action_index in range(num_actions):\n        scores = np.array(score_set.get(action_index, []))\n        action_count = len(scores)\n        action_mean = np.mean(scores) if action_count > 0 else 0\n        action_means[action_index] = action_mean\n        action_counts[action_index] = action_count\n\n    # Temporal adaptability\n    time_decay_factor = current_time_slot / total_time_slots\n    adjusted_scores = action_means * (1 + (1 - time_decay_factor))\n\n    # Epsilon-greedy strategy\n    epsilon = max(0.1, 0.1 * (1 - (total_selection_count / 1000)))  # Decrease exploration over time\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(num_actions, p=(1-action_counts/sum(action_counts + 1e-5)))\n    else:\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 25082228.453395326,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize action statistics\n    action_means = np.zeros(8)\n    action_counts = np.zeros(8)\n\n    # Calculate means and counts\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n    \n    # Calculate confidence for exploration (UCB style)\n    exploration_factor = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n    ucb_scores = action_means + exploration_factor\n\n    # Temporal weighting\n    time_decay_factor = current_time_slot / total_time_slots\n    adjusted_scores = ucb_scores * (1 + time_decay_factor)\n\n    # Epsilon-greedy approach\n    epsilon = 0.1\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(np.arange(8))  # Explore\n    else:\n        action_index = np.argmax(adjusted_scores)  # Exploit\n    \n    return action_index",
          "objective": 49514742.4237383,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Time decay factor to increase relevance of recent scores\n    time_decay = (1 - (current_time_slot / total_time_slots))\n    adjusted_scores = action_means * time_decay\n    \n    # Adding a bonus for unselected actions to encourage exploration\n    unselected_bonus = (1 - (action_counts / total_selection_count)) * (total_selection_count != 0)\n    adjusted_scores += unselected_bonus\n\n    # Epsilon-greedy strategy for exploration vs exploitation\n    epsilon = 0.1\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, num_actions)  # Explore\n    else:\n        action_index = np.argmax(adjusted_scores)  # Exploit\n\n    return action_index",
          "objective": 166229788.09677926,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0.0\n\n    # Dynamic adjustment based on time slot\n    recent_effectiveness = (1 - (current_time_slot / total_time_slots))\n    adjusted_scores = action_means + recent_effectiveness * (1 - action_means)\n\n    # Epsilon-greedy strategy\n    epsilon = max(0.05, 0.1 * (1 - (current_time_slot / total_time_slots)))  # Decreasing epsilon over time\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)  # Explore\n    else:\n        action_index = np.argmax(adjusted_scores)  # Exploit\n\n    return action_index",
          "objective": 186109748.61278418,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate mean scores and counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_count = len(scores)\n        action_mean = np.mean(scores) if action_count > 0 else 0\n        action_means[action_index] = action_mean\n        action_counts[action_index] = action_count\n\n    # Momentum: Adjust score based on recent performance (weight with a decay)\n    momentum_factor = 0.1\n    recent_scores = np.zeros(num_actions)\n    \n    for action_index in range(num_actions):\n        if action_counts[action_index] > 0:\n            recent_score = np.mean(scores[-min(3, action_counts[action_index]):])\n            recent_scores[action_index] = (1 - momentum_factor) * action_means[action_index] + momentum_factor * recent_score\n\n    # Combine historical performance and recent scores\n    combined_scores = (action_means + recent_scores) / 2\n\n    # Epsilon-greedy selection\n    epsilon = 0.1\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, num_actions)  # Explore\n    else:\n        action_index = np.argmax(combined_scores)  # Exploit\n\n    return action_index",
          "objective": 215447443.44252515,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize the action statistics\n    action_means = []\n    action_counts = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_count = len(scores)\n        action_mean = np.mean(scores) if action_count > 0 else 0\n        action_means.append(action_mean)\n        action_counts.append(action_count)\n    \n    # Calculate exploration factor\n    exploration_factor = np.sqrt(np.log(total_selection_count + 1) / (np.array(action_counts) + 1e-5))\n    \n    # Calculate selection score: UCB score with exploration\n    selection_scores = np.array(action_means) + exploration_factor\n    \n    # Select the action with the highest score\n    action_index = np.argmax(selection_scores)\n    \n    return action_index",
          "objective": 244872787.73776796,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1\n    exploration_factor = max(0.1, 1 - (total_selection_count / (total_time_slots * 8)))  # Adjust exploration over time\n    random_action = np.random.rand() < epsilon * exploration_factor\n    \n    averages = {}\n    action_counts = {action_index: len(scores) for action_index, scores in score_set.items()}\n\n    for action_index, scores in score_set.items():\n        if action_counts[action_index] > 0:\n            average_score = np.mean(scores)\n        else:\n            average_score = 0  # Handle unselected actions\n        \n        # Decay factor based on recency of action\n        decay = (total_time_slots - current_time_slot) / total_time_slots\n        \n        # Prioritize actions that have been selected less frequently\n        selection_bias = (total_selection_count / (action_counts[action_index] + 1)) if action_counts[action_index] > 0 else float('inf')\n        \n        averages[action_index] = average_score * decay * selection_bias\n\n    if random_action:\n        action_index = np.random.randint(0, 8)  # Random select from 0 to 7\n    else:\n        action_index = max(averages, key=averages.get)  # Select action with max weighted score\n    \n    return action_index",
          "objective": 254572800.6832503,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    action_means = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate mean scores and counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_count = len(scores)\n        action_mean = np.mean(scores) if action_count > 0 else 0\n        action_means[action_index] = action_mean\n        action_counts[action_index] = action_count\n\n    # Apply dynamic score decay based on recency\n    time_decay_factor = 1 - (current_time_slot / total_time_slots)\n    adjusted_scores = action_means * (1 + time_decay_factor)\n\n    # Exploration vs Exploitation\n    epsilon = 0.1\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, num_actions)  # Explore\n    else:\n        # Compute exploration factor based on selection counts\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (1 + action_counts))\n        final_scores = adjusted_scores + exploration_bonus\n        action_index = np.argmax(final_scores)  # Exploit\n\n    return action_index",
          "objective": 274744383.03220004,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_means = np.zeros(8)\n    action_counts = np.zeros(8)\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Time decay factor to encourage focusing on recent performance\n    time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = action_means * (1 + time_decay_factor)\n    \n    # Balance exploration and exploitation with epsilon-greedy strategy\n    epsilon = 0.1 * (1 - (current_time_slot / total_time_slots))  # Decrease epsilon over time\n    if np.random.rand() < epsilon:\n        # Explore: prioritize actions that have been selected less often\n        exploration_weight = (1 + action_counts.min() / (action_counts + 1))  # Favor less selected actions\n        selected_action = np.random.choice(np.arange(8), p=exploration_weight / exploration_weight.sum())\n    else:\n        # Exploit: choose the action with the highest adjusted mean score\n        selected_action = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 464170771.20413184,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration factor\n    action_indices = list(score_set.keys())\n    scores = np.array([np.mean(score_set[i]) if score_set[i] else 0 for i in action_indices])\n    selections = np.array([len(score_set[i]) for i in action_indices])\n    \n    # Calculate UCB-like values\n    ucb_values = scores + np.sqrt((2 * np.log(total_selection_count + 1)) / (selections + 1e-5))\n    \n    # Epsilon-greedy approach\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = action_indices[np.argmax(ucb_values)]\n    \n    return action_index",
          "objective": 482109302.4853227,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration factor\n    \n    # Calculate mean scores, applying recency weighting\n    avg_scores = np.zeros(8)\n    for action_index, scores in score_set.items():\n        if scores:\n            weight = (current_time_slot / total_time_slots)\n            recent_score = np.mean(scores[-max(1, len(scores)//5):]) * weight  # weight recent scores\n            historical_score = np.mean(scores) * (1 - weight)  # rest of the scores\n            avg_scores[action_index] = recent_score + historical_score\n            \n    # Normalize and adjust for exploration\n    total_avg_scores = np.sum(avg_scores)\n    selection_probabilities = avg_scores / total_avg_scores if total_avg_scores > 0 else np.ones(8) / 8\n    probabilities = (1 - epsilon) * selection_probabilities + (epsilon / 8)\n    \n    # Select action based on computed probabilities\n    action_index = np.random.choice(range(8), p=probabilities)\n    \n    return action_index",
          "objective": 509742938.604638,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize action statistics\n    action_means = []\n    action_counts = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_count = len(scores)\n        action_counts.append(action_count)\n        action_mean = np.mean(scores) if action_count > 0 else 0\n        action_means.append(action_mean)\n    \n    action_means = np.array(action_means)\n    action_counts = np.array(action_counts)\n\n    # Temporal weighting (recent scores more influential)\n    decay_factor = 1 - (current_time_slot / total_time_slots)  # Inverse of progress\n    weighted_means = action_means * decay_factor + (1 - decay_factor) * (action_counts / (total_selection_count + 1))\n\n    # Dynamic epsilon based on selection experience\n    epsilon = 0.1 * (1 - (current_time_slot / total_time_slots))\n\n    # Exploration vs exploitation\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, 8)  # Explore\n    else:\n        action_index = np.argmax(weighted_means)  # Exploit\n\n    return action_index",
          "objective": 524794443.51068866,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores for each action, handling cases with zero selections\n    average_scores = []\n    for i in range(8):\n        if len(score_set[i]) == 0:\n            average_scores.append(0)  # No scores, so average is 0\n        else:\n            average_scores.append(np.mean(score_set[i]))\n    \n    # Explore factor based on the current time slot\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n    \n    # Weight the actions: combine average scores with exploration\n    exploration_weighted_scores = [\n        average_scores[i] + exploration_factor * (1 - (len(score_set[i]) / (total_selection_count + 1)))\n        for i in range(8)\n    ]\n    \n    # Choose the action with the highest weighted score\n    action_index = np.argmax(exploration_weighted_scores)\n    \n    return action_index",
          "objective": 840424516.750607,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    action_means = np.zeros(n_actions)\n    action_counts = np.zeros(n_actions)\n\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if scores else 0\n\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-5))\n    ucb_values = action_means + exploration_bonus\n\n    if total_selection_count < 20:  # Encourage exploration in early slots\n        epsilon = 0.2\n        if np.random.rand() < epsilon:\n            action_index = np.random.randint(0, n_actions)  # Explore\n        else:\n            action_index = np.argmax(ucb_values)  # Exploit\n    else:\n        action_index = np.argmax(ucb_values)  # Focus on the best action\n\n    return action_index",
          "objective": 857219703.939933,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Temperature parameter for exploration\n    temperature = max(0.1, 1 - (current_time_slot / total_time_slots))  # Decreases over time\n\n    # Calculate average scores for each action\n    avg_scores = []\n    for action_idx in range(8):\n        scores = score_set.get(action_idx, [])\n        if scores:\n            avg_score = np.mean(scores)\n        else:\n            avg_score = 0\n        avg_scores.append(avg_score)\n\n    # Apply softmax function for selection probabilities\n    exp_scores = np.exp(np.array(avg_scores) / temperature)\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Select action based on probabilities\n    action_index = np.random.choice(range(8), p=probabilities)\n\n    return action_index",
          "objective": 1019452333.7821996,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_means = np.zeros(8)\n    action_counts = np.zeros(8)\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Weighting recent scores more heavily\n    recent_score_weights = np.linspace(1, 0, num=len(scores)) if action_counts[action_index] > 0 else np.array([])\n    weighted_means = np.array([\n        np.sum(np.array(scores) * recent_score_weights) / np.sum(recent_score_weights)\n        if action_counts[action_index] > 0 else 0\n        for action_index in range(8)\n    ])\n    \n    # Dynamic score adjustment\n    time_decay_factor = 1 - (current_time_slot / total_time_slots)\n    adjusted_scores = weighted_means + time_decay_factor * (1 - weighted_means)\n\n    # Epsilon-greedy exploration-exploitation strategy\n    epsilon = 0.1\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, 8)  # Explore\n    else:\n        action_index = np.argmax(adjusted_scores)  # Exploit\n\n    return action_index",
          "objective": 1371452790.434557,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    scores = []\n    \n    # Calculate the average score for each action\n    for index in action_indices:\n        if len(score_set[index]) > 0:\n            avg_score = np.mean(score_set[index])\n        else:\n            avg_score = 0  # Handle case of no historical scores\n        scores.append(avg_score)\n    \n    # If total_selection_count is 0, we should explore all actions equally\n    if total_selection_count == 0:\n        action_index = np.random.choice(action_indices)\n        return action_index\n    \n    # Calculate exploratory values\n    exploration_values = []\n    for index in action_indices:\n        if len(score_set[index]) > 0:\n            exploration_value = 1 / (len(score_set[index]) + 1)  # Encourages exploring less selected actions\n        else:\n            exploration_value = 1  # If action has never been selected, fully explore it\n        exploration_values.append(exploration_value)\n    \n    # Combine scores and exploration values\n    combined_values = np.array(scores) + np.array(exploration_values)\n    \n    # Select action based on combined strategy\n    action_index = np.argmax(combined_values)\n    \n    return action_index",
          "objective": 1524098471.974799,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    \n    # Calculate average scores for available actions\n    avg_scores = []\n    for action in action_indices:\n        scores = score_set[action]\n        avg_score = np.mean(scores) if len(scores) > 0 else 0\n        avg_scores.append(avg_score)\n\n    # Epsilon-greedy strategy\n    epsilon = max(0.1, 1 - (total_selection_count / (total_time_slots * 5)))\n    \n    if np.random.rand() < epsilon:\n        # Exploration: select action randomly\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploitation: select the action with the highest average score\n        max_score = max(avg_scores)\n        best_actions = [index for index, score in zip(action_indices, avg_scores) if score == max_score]\n        action_index = np.random.choice(best_actions)\n\n    return action_index",
          "objective": 3388781812.8172116,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    scores = np.zeros(num_actions)\n    exploration_values = np.zeros(num_actions)\n    \n    # Calculate average scores and exploration values\n    for i, index in enumerate(action_indices):\n        if len(score_set[index]) > 0:\n            scores[i] = np.mean(score_set[index])\n            exploration_values[i] = 1 / (len(score_set[index]) + 1)  # Encourages exploration of less selected actions\n        else:\n            exploration_values[i] = 1  # Fully explore actions never selected\n    \n    # Calculate a softmax value for more balanced exploration and exploitation\n    combined_scores = scores + exploration_values    \n    softmax_probs = np.exp(combined_scores - np.max(combined_scores))  # Stability for large values\n    softmax_probs /= np.sum(softmax_probs)\n\n    # Select action based on softmax probabilities\n    action_index = np.random.choice(action_indices, p=softmax_probs)\n    \n    return action_index",
          "objective": 3806526866.687254,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_factor = 0.1\n    epsilon = exploration_factor * (total_time_slots - current_time_slot) / total_time_slots\n    \n    # Calculate average scores for each action\n    average_scores = []\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        if len(scores) > 0:\n            average_scores.append(np.mean(scores))\n        else:\n            average_scores.append(0)  # Default to 0 if no scores available\n    \n    # Convert to probabilities (softmax function)\n    max_score = np.max(average_scores)\n    exp_scores = np.exp(average_scores - max_score)  # Subtract max to prevent overflow\n    probabilities = exp_scores / np.sum(exp_scores)\n    \n    # Incorporate exploration\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(range(8))\n    else:\n        action_index = np.random.choice(range(8), p=probabilities)\n        \n    return action_index",
          "objective": 3846980842.7848754,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration factor\n    \n    # Calculate average scores for each action\n    avg_scores = np.zeros(8)\n    for action_index, scores in score_set.items():\n        if scores:  # Avoid division by zero\n            avg_scores[action_index] = np.mean(scores)\n    \n    # Selection probabilities (exploitation)\n    selection_probabilities = avg_scores / np.sum(avg_scores) if np.sum(avg_scores) > 0 else avg_scores\n    \n    # Adjust probabilities for exploration\n    probabilities = (1 - epsilon) * selection_probabilities + (epsilon / 8)\n    \n    # Select action based on computed probabilities\n    action_index = np.random.choice(range(8), p=probabilities)\n    \n    return action_index",
          "objective": 4276680416.038046,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = len(score_set)\n    action_means = np.zeros(action_count)\n    action_counts = np.zeros(action_count)\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Apply a time decay factor to the mean scores\n    time_decay_factor = (current_time_slot + 1) / (total_time_slots + 1)  # To avoid division by zero\n    adjusted_scores = action_means * (1 + time_decay_factor)\n\n    # Exploration-exploitation strategy\n    epsilon = max(0.1, 1 - (total_selection_count / (total_time_slots * 8)))  # Decaying epsilon\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, action_count)\n    else:\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 5123217888.022673,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize action statistics\n    action_means = np.zeros(8)\n    action_counts = np.zeros(8)\n    \n    # Calculate mean scores and counts for each action\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        action_means[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Dynamic epsilon based on the total selection count\n    epsilon = max(0.1, 1 - (total_selection_count / (total_time_slots * 10)))  # Example decay mechanism\n\n    # Apply decay to scores, prioritizing recent scores\n    decay_factor = (current_time_slot + 1) / total_time_slots\n    adjusted_means = action_means * (1 - decay_factor) + decay_factor * np.max(action_means)\n\n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(8)  # Explore: random action\n    else:\n        action_index = np.argmax(adjusted_means)  # Exploit: best action based on adjusted mean\n\n    return action_index",
          "objective": 10673231472.260986,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Define the exploration-exploitation balance factor\n    epsilon = 0.1\n    action_count = len(score_set)  # Assume action indices are 0 to 7\n\n    # Calculate average scores and selection counts\n    avg_scores = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n\n    for action_index, scores in score_set.items():\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n    \n    # Calculate selection probabilities (exploitation)\n    exploration_bonus = np.sqrt((np.log(total_selection_count + 1) / (selection_counts + 1e-5)))  # Avoid division by zero\n    selection_values = avg_scores + exploration_bonus\n    \n    # Normalize the selection values to get probabilities\n    if np.sum(selection_values) > 0:\n        selection_probabilities = selection_values / np.sum(selection_values)\n    else:\n        selection_probabilities = np.full(action_count, 1 / action_count)\n    \n    # Apply epsilon-greedy strategy for exploration\n    probabilities = (1 - epsilon) * selection_probabilities + (epsilon / action_count)\n    \n    # Select action based on computed probabilities\n    action_index = np.random.choice(range(action_count), p=probabilities)\n    \n    return action_index",
          "objective": 10771495355.481415,
          "other_inf": null
     }
]