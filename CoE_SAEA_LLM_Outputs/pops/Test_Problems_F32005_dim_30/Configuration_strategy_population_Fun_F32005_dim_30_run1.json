[
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    \n    # Initialize average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions, dtype=int)\n\n    # Compute average scores and selection counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        average_scores[action_index] = np.mean(scores) if selection_counts[action_index] > 0 else 0\n\n    # Exploration factor that decreases as time progresses\n    exploration_factor = max(0, (total_time_slots - current_time_slot) / total_time_slots)\n\n    # Calculate confidence adjusted scores\n    confidence_scores = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        if selection_counts[action_index] > 0:\n            confidence = np.sqrt(np.log(total_selection_count + 1) / selection_counts[action_index])\n            confidence_scores[action_index] = average_scores[action_index] + confidence\n        else:\n            confidence_scores[action_index] = 1.0  # Encourage exploration of unselected actions\n\n    # Combine average scores and exploration\n    combined_scores = (1 - exploration_factor) * confidence_scores + exploration_factor * np.random.rand(num_actions)\n\n    # Assign higher weight to average scores as the total selection count increases\n    if total_selection_count > 0:\n        combined_scores += (average_scores * (total_selection_count / (total_selection_count + 1)))\n\n    # Select action based on the combined scores\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": 13693587.803639943,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions, dtype=int)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        average_scores[action_index] = np.mean(scores) if selection_counts[action_index] > 0 else 0\n\n    # Calculate exploration bonus which diminishes over time\n    exploration_bonus = np.log((total_selection_count + 1) / (selection_counts + 1))  \n    exploration_bonus[selection_counts == 0] = np.inf  # Infinite bonus for unselected actions\n\n    # Adjust scores by exploration factor and confidence\n    confidence_scores = average_scores + exploration_bonus\n    time_weighting = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = confidence_scores * time_weighting + average_scores * (1 - time_weighting)\n\n    # Action selection: explore or exploit\n    exploration_threshold = np.random.rand()\n    exploration_factor = max(0.1, 1.0 - (total_selection_count / (total_time_slots + 1)))\n    \n    if exploration_threshold < exploration_factor:\n        action_index = np.random.choice(np.arange(num_actions))  # Explore\n    else:\n        action_index = np.argmax(adjusted_scores)  # Exploit\n\n    return action_index",
          "objective": 14364434.835857425,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    \n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions, dtype=int)\n    \n    # Compute average scores and selection counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n    \n    # Calculate exploration weights based on selection counts\n    exploration_weights = np.zeros(num_actions)\n    exploration_constant = 1.0\n    \n    for action_index in range(num_actions):\n        if selection_counts[action_index] > 0:\n            exploration_weights[action_index] = exploration_constant / np.sqrt(selection_counts[action_index])\n        else:\n            exploration_weights[action_index] = np.inf  # Very high weight for unselected actions\n\n    # Combine average scores with exploration weights\n    combined_scores = average_scores + exploration_weights\n    \n    # Calculate exploration probability based on current time slot\n    exploration_probability = max(0.05, min(1.0, (total_time_slots - current_time_slot) / total_time_slots))\n    \n    # Selection mechanism: Random exploration vs greedy exploitation\n    if np.random.rand() < exploration_probability:\n        action_index = np.random.choice(np.arange(num_actions))  # Random exploration\n    else:\n        action_index = np.argmax(combined_scores)  # Greedy exploitation\n\n    return action_index",
          "objective": 14447970.866042199,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    \n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions, dtype=int)\n\n    # Compute average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        average_scores[action_index] = np.mean(scores) if selection_counts[action_index] > 0 else 0\n\n    # Calculate exploration factor inversely proportional to total selection count\n    exploration_factor = max(0.1, 1.0 - (total_selection_count / (total_time_slots + 1)))\n\n    # Calculate confidence bonuses for each action\n    confidence_boost = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        if selection_counts[action_index] > 0:\n            variance = np.var(score_set[action_index], ddof=1) if selection_counts[action_index] > 1 else 0\n            confidence_boost[action_index] = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[action_index]) + (variance / selection_counts[action_index])\n        else:\n            confidence_boost[action_index] = np.inf  # High confidence for never selected actions\n\n    # Weight average scores by remaining time and confidence\n    time_weighting = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = average_scores + (confidence_boost * time_weighting)\n\n    # Combine exploration and exploitation\n    exploration_threshold = np.random.rand()\n    if exploration_threshold < exploration_factor:\n        action_index = np.random.choice(np.arange(num_actions))  # Random exploration\n    else:\n        action_index = np.argmax(adjusted_scores)  # Greedy exploitation\n\n    return action_index",
          "objective": 14477219.806886813,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    \n    # Initialize arrays for average scores, selection counts, and confidence scores\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions, dtype=int)\n    \n    # Calculate average scores and counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n    \n    # Calculate exploration factor and confidence scores\n    exploration_factor = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-6))  # Add small epsilon to avoid division by zero\n    confidence_scores = average_scores + exploration_factor * (1.0 / (selection_counts + 1e-6))\n    confidence_scores[np.isnan(confidence_scores)] = np.inf  # Handle cases for unselected actions\n    \n    # Dynamic exploration-exploitation balance\n    time_factor = current_time_slot / total_time_slots\n    combined_scores = (1 - time_factor) * average_scores + time_factor * confidence_scores\n    \n    # Normalize the scores to enhance stability in action selection\n    combined_scores = (combined_scores - np.min(combined_scores)) / (np.max(combined_scores) - np.min(combined_scores) + 1e-6)\n    \n    # Select action based on the combined scores\n    action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": 14541818.274766501,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    \n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions, dtype=int)\n    \n    # Compute average scores and selection counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n    \n    # Calculate exploration weights based on selection counts\n    exploration_weights = np.zeros(num_actions)\n    exploration_factor = 1.5  # Adjust exploration factor for balance\n\n    for action_index in range(num_actions):\n        if selection_counts[action_index] > 0:\n            # Inverse sqrt to reward under-selected actions\n            exploration_weights[action_index] = exploration_factor / np.sqrt(selection_counts[action_index])\n        else:\n            exploration_weights[action_index] = exploration_factor  # High weight for unselected actions\n\n    # Combine average scores with exploration weights\n    combined_scores = average_scores + exploration_weights\n\n    # Dynamic exploration probability based on current time slot\n    exploration_probability = max(0.05, min(1.0, (total_time_slots - current_time_slot + 1) / total_time_slots))\n\n    # Selection mechanism: Random exploration vs greedy exploitation\n    if np.random.rand() < exploration_probability:\n        action_index = np.random.choice(np.arange(num_actions))  # Random exploration\n    else:\n        action_index = np.argmax(combined_scores)  # Greedy exploitation\n\n    return action_index",
          "objective": 14894268.247582177,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    \n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions, dtype=int)\n    \n    # Compute average scores and count of selections\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Avoid division by zero for normalization\n    selection_counts_normalized = np.where(selection_counts > 0, selection_counts, 1)\n\n    # Dynamic exploration factor based on selection frequency\n    exploration_factor = np.sqrt(np.log(total_selection_count + 1) / selection_counts_normalized)\n\n    # Calculate a confidence score based on average score and selection count\n    confidence_scores = average_scores * (selection_counts_normalized ** 0.5)\n    \n    # Combine exploitation (confidence scored) and exploration\n    combined_scores = confidence_scores + exploration_factor\n\n    # Apply a decay function that transitions from exploration to exploitation\n    decay_weight = 1 - (current_time_slot / total_time_slots)\n    combined_scores *= (1 + decay_weight)\n\n    # Normalize combined scores\n    combined_scores_normalized = (combined_scores - np.min(combined_scores)) / (np.max(combined_scores) - np.min(combined_scores) + 1e-6)\n\n    # Select action with the highest normalized score\n    action_index = np.argmax(combined_scores_normalized)\n\n    return action_index",
          "objective": 15679759.3683119,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    \n    # Initialize average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions, dtype=int)\n\n    # Compute average scores and selection counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        average_scores[action_index] = np.mean(scores) if selection_counts[action_index] > 0 else 0\n\n    # Utilize total selection count to compute a dynamic exploration pressure\n    exploration_pressure = 1.0 / (1 + total_selection_count)\n    \n    # Calculate confidence adjusted scores with a balance between exploration and exploitation\n    confidence_scores = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        if selection_counts[action_index] > 0:\n            confidence = np.sqrt(np.log(total_selection_count + 1) / selection_counts[action_index])\n            confidence_scores[action_index] = average_scores[action_index] + confidence\n        else:\n            confidence_scores[action_index] = 1.0  # High score for untried actions\n\n    # Weighted score to combine both confidence scoring and exploration pressure\n    combined_scores = (1 - exploration_pressure) * confidence_scores + exploration_pressure * np.random.rand(num_actions)\n\n    # Select action based on the combined scores\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": 16871802.398903396,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    \n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions, dtype=int)\n\n    # Calculate scores and counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n    \n    # Calculate exploration weight based on total selections\n    exploration_weight = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1))\n    \n    # Confidence scores combining average scores and exploration\n    confidence_scores = average_scores + exploration_weight\n\n    # Adjust exploration factor over time\n    exploration_factor = 1 - (current_time_slot / total_time_slots)\n    adjusted_scores = confidence_scores * exploration_factor + average_scores * (1 - exploration_factor)\n\n    # Handle actions not yet picked, prefer them in the score\n    adjusted_scores[selection_counts == 0] += 1.0\n    \n    # Select action based on the adjusted scores\n    action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 16882729.16243151,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    exploration_rate = 1.0  # Start with full exploration\n    decay_rate = 0.99  # Exploration decay rate\n\n    # Arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions, dtype=int)\n\n    # Compute average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        average_scores[action_index] = np.mean(scores) if scores else 0.0\n\n    # Confidence interval calculations\n    confidence_intervals = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        if selection_counts[action_index] > 0:\n            variance = np.var(score_set[action_index], ddof=1) if len(score_set[action_index]) > 1 else 0\n            confidence_intervals[action_index] = np.sqrt(np.log(total_selection_count + 1) / selection_counts[action_index]) + (variance / selection_counts[action_index])\n        else:\n            confidence_intervals[action_index] = np.inf  # High confidence for unselected actions\n\n    # Adjust scores using confidence intervals\n    time_factor = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = average_scores + (confidence_intervals * time_factor)\n\n    # Gradual exploration-exploitation trade-off\n    exploration_factor = max(0.1, exploration_rate * (decay_rate ** total_selection_count))\n\n    # Action selection\n    if np.random.rand() < exploration_factor:\n        action_index = np.random.choice(np.arange(num_actions))  # Explore\n    else:\n        action_index = np.argmax(adjusted_scores)  # Exploit the highest score\n\n    return action_index",
          "objective": 18798736.697358638,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    \n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions, dtype=int)\n    \n    # Compute averages and counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n    \n    # Normalize selection counts to avoid division by zero and emphasize exploration\n    selection_counts_normalized = np.where(selection_counts > 0, selection_counts, 1)\n    \n    # Dynamic exploration factor\n    exploration_factor = np.sqrt(np.log(total_selection_count + 1) / selection_counts_normalized)\n\n    # Calculate a modified exploration score for actions with fewer selections\n    exploration_bonus = 1.0 / (1 + selection_counts_normalized)\n\n    # Combine exploration and exploitation scores\n    combined_scores = average_scores + exploration_factor * exploration_bonus\n    \n    # Time-based decay: encourage exploration initially, decay over time\n    time_factor = current_time_slot / total_time_slots\n    combined_scores *= (1 - time_factor) + (time_factor / 2)\n\n    # Normalize combined scores for stability in action selection\n    combined_scores_normalized = (combined_scores - np.min(combined_scores)) / (np.max(combined_scores) - np.min(combined_scores) + 1e-6)\n\n    # Select action based on combined normalized scores\n    action_index = np.argmax(combined_scores_normalized)\n\n    return action_index",
          "objective": 20611309.15340516,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    exploration_rate = 0.5  # Initial exploration rate\n    decay_rate = 0.99  # Rate at which exploration rate decreases\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions, dtype=int)\n\n    # Compute average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n        else:\n            average_scores[action_index] = 0  # Default to zero for unselected actions\n\n    # Calculate confidence bonuses for underutilized actions\n    confidence_boost = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        if selection_counts[action_index] > 0:\n            variance = np.var(score_set[action_index], ddof=1) if len(score_set[action_index]) > 1 else 0\n            confidence_boost[action_index] = np.sqrt(np.log(total_selection_count + 1) / selection_counts[action_index]) + (variance / selection_counts[action_index])\n        else:\n            confidence_boost[action_index] = np.inf  # High confidence boost for actions never selected\n\n    # Dynamic adjustments based on the current time slot\n    time_weighting = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = average_scores + (confidence_boost * time_weighting)\n\n    # Gradually decay the exploration rate\n    exploration_factor = max(0.1, exploration_rate * (decay_rate ** total_selection_count))\n\n    # Selection mechanism\n    if np.random.rand() < exploration_factor:\n        action_index = np.random.choice(np.arange(num_actions))  # Random exploration\n    else:\n        action_index = np.argmax(adjusted_scores)  # Greedy exploitation\n\n    return action_index",
          "objective": 21583542.980402112,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    exploration_decay = 0.99\n    exploration_factor = max(0.1, 1.0 * (exploration_decay ** total_selection_count))\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions, dtype=int)\n\n    # Compute average scores and handle zero-selection cases\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n        else:\n            average_scores[action_index] = 0  # Default to zero for unselected actions\n\n    # Calculate confidence bonuses for underutilized actions\n    confidence_boost = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        if selection_counts[action_index] > 0:\n            variance = np.var(score_set[action_index], ddof=1) if len(score_set[action_index]) > 1 else 0\n            confidence_boost[action_index] = np.sqrt(np.log(total_selection_count + 1) / selection_counts[action_index]) + (variance / selection_counts[action_index])\n        else:\n            confidence_boost[action_index] = np.inf  # High confidence boost for never selected actions\n\n    # Dynamic adjustments based on the current time slot\n    time_weighting = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = average_scores + (confidence_boost * time_weighting)\n\n    # Selection mechanism\n    if np.random.rand() < exploration_factor:\n        action_index = np.random.choice(np.arange(num_actions))  # Random exploration\n    else:\n        action_index = np.argmax(adjusted_scores)  # Greedy exploitation\n\n    return action_index",
          "objective": 25508600.048836153,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    \n    # Calculate average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions, dtype=int)\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n    \n    # Calculate confidence metric: average score + exploration term\n    confidence_scores = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        if selection_counts[action_index] > 0:\n            confidence_scores[action_index] = average_scores[action_index] + (np.sqrt(np.log(total_selection_count) / selection_counts[action_index]))\n        else:\n            confidence_scores[action_index] = 1.0 + (np.sqrt(np.log(total_selection_count) + 1))  # Encourage exploration for unselected actions\n            \n    # Adjust exploration based on current time slot\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = confidence_scores * (1 - exploration_factor) + average_scores * exploration_factor\n\n    # Select action based on adjusted scores\n    action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 25986239.286003817,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    initial_exploration_rate = 1.0\n    exploration_decay = 0.95\n    \n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions, dtype=int)\n\n    # Compute average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n        else:\n            average_scores[action_index] = 0  # Default to zero for unselected actions\n\n    # Calculate exploration factor\n    exploration_rate = initial_exploration_rate * (exploration_decay ** current_time_slot)\n    \n    # Boost factors for under-selected actions\n    selection_boost = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        if selection_counts[action_index] == 0:\n            selection_boost[action_index] = np.inf  # High boost for actions never selected\n        else:\n            selection_boost[action_index] = np.log(total_selection_count + 1) / (selection_counts[action_index] + 1)\n\n    # Compound scores with exploration factors and boosts\n    adjusted_scores = average_scores + selection_boost\n    \n    # Selection mechanism: exploration vs exploitation\n    if np.random.rand() < exploration_rate:\n        action_index = np.random.choice(np.arange(num_actions))  # Random exploration\n    else:\n        action_index = np.argmax(adjusted_scores)  # Greedy exploitation\n\n    return action_index",
          "objective": 27868828.26103014,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    \n    # Initialize arrays for average scores, selection counts, variances, and effective scores\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions, dtype=int)\n    variances = np.zeros(num_actions)\n\n    # Calculate average scores and counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n            variances[action_index] = np.var(scores)  # Calculate variance\n\n    # Calculate a confidence measure\n    delta = np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1e-6))\n    confidence_scores = average_scores + delta\n\n    # Combine mean scores and confidence scores dynamically\n    time_factor = current_time_slot / total_time_slots\n    effective_scores = (1 - time_factor) * average_scores + time_factor * confidence_scores\n    \n    # Normalize the effective scores for stability in selection\n    effective_scores = (effective_scores - np.min(effective_scores)) / (np.max(effective_scores) - np.min(effective_scores) + 1e-6)\n\n    # Select action based on the highest effective score\n    action_index = np.argmax(effective_scores)\n\n    return action_index",
          "objective": 29557784.50661838,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    exploration_weight = 1.0 + (total_selection_count ** -0.5)  # Decreasing weight as total selections grow\n    \n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions, dtype=int)\n    \n    # Calculate average scores and counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n    \n    # Calculate confidence metric with exploration\n    confidence_scores = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        if selection_counts[action_index] > 0:\n            confidence_scores[action_index] = (average_scores[action_index] +\n                                                exploration_weight * np.sqrt(np.log(total_selection_count + 1) / selection_counts[action_index]))\n        else:\n            confidence_scores[action_index] = np.inf  # Encourage exploration for unselected actions\n\n    # Adjust scores based on elapsed time for adaptive exploration vs exploitation\n    time_factor = current_time_slot / total_time_slots\n    scaled_average_scores = average_scores * (1 - time_factor) + confidence_scores * time_factor\n\n    # Select action based on the scores\n    action_index = np.argmax(scaled_average_scores)\n\n    return action_index",
          "objective": 29709186.290099777,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    \n    # Initialization\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions, dtype=int)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Confidence metric based on average scores and selection counts\n    confidence_scores = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        if selection_counts[action_index] > 0:\n            # Calculate confidence as average score adjusted by selection counts\n            confidence_scores[action_index] = average_scores[action_index] + \\\n                np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts[action_index])\n        else:\n            confidence_scores[action_index] = 1.0  # Default high confidence for untried actions\n\n    # Dynamic exploration factor\n    exploration_factor = max(0.1, 1.0 * (0.99 ** total_selection_count))\n    proportion_elapsed = current_time_slot / total_time_slots\n    adjusted_exploration_factor = exploration_factor * (1 - proportion_elapsed)\n\n    # Selection mechanism\n    if np.random.rand() < adjusted_exploration_factor:\n        action_index = np.random.choice(np.arange(num_actions))  # Random exploration\n    else:\n        action_index = np.argmax(confidence_scores)  # Greedy exploitation based on confidence\n\n    return action_index",
          "objective": 34692153.91221064,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    exploration_factor = max(0.1, 1.0 * (0.99 ** total_selection_count))\n    \n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions, dtype=int)\n\n    # Compute average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n    \n    # Calculate exploration bonuses\n    exploration_bonus = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        if selection_counts[action_index] > 0:\n            variance = np.var(score_set[action_index], ddof=1) if len(score_set[action_index]) > 1 else 0\n            exploration_bonus[action_index] = np.sqrt(np.log(total_selection_count + 1) / selection_counts[action_index]) + (variance / selection_counts[action_index])\n        else:\n            exploration_bonus[action_index] = np.inf  # High exploration for unselected actions\n\n    # Adjust scores with exploration bonus\n    adjusted_scores = average_scores + exploration_bonus\n    \n    # Adjust exploration factor based on time slot\n    proportion_elapsed = current_time_slot / total_time_slots\n    adjusted_exploration_factor = exploration_factor * (1 - proportion_elapsed)\n    \n    # Selection mechanism\n    if np.random.rand() < adjusted_exploration_factor:\n        action_index = np.random.choice(np.arange(num_actions))  # Random exploration\n    else:\n        action_index = np.argmax(adjusted_scores)  # Greedy exploitation\n\n    return action_index",
          "objective": 35010887.65357508,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    # Initialize arrays for average scores, selection counts, and confidence intervals\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions, dtype=int)\n\n    # Compute average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Calculate a confidence interval approach for exploration\n    exploration_bonus = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        if selection_counts[action_index] > 0:\n            variance = np.var(score_set[action_index], ddof=1) if len(score_set[action_index]) > 1 else 0\n            exploration_bonus[action_index] = np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts[action_index]) + (variance / selection_counts[action_index])\n        else:\n            exploration_bonus[action_index] = np.inf  # Encourage selection of unexplored actions\n\n    # Adjust scores with exploration bonus\n    adjusted_scores = average_scores + exploration_bonus\n\n    # Define exploration probability dynamically based on time and selection progress\n    exploration_probability = max(0.1, 1.0 * (0.99 ** total_selection_count))\n    time_factor = (total_time_slots - current_time_slot) / total_time_slots\n    combined_exploration_prob = exploration_probability * time_factor\n\n    # Selection mechanism: explore or exploit based on calculated exploration probability\n    if np.random.rand() < combined_exploration_prob:\n        action_index = np.random.choice(np.arange(num_actions))  # Random exploration\n    else:\n        action_index = np.argmax(adjusted_scores)  # Greedy exploitation\n\n    return action_index",
          "objective": 37467709.04118389,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon_initial = 1.0\n    epsilon_minimum = 0.1\n    epsilon_decay = 0.99  \n\n    # Initialize average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        average_scores[action_index] = np.mean(scores) if scores else 0.0\n\n    # Calculate dynamic epsilon for exploration\n    decay_factor = epsilon_decay ** current_time_slot\n    epsilon = max(epsilon_minimum, epsilon_initial * decay_factor)\n\n    # Compute exploration bonuses\n    exploration_bonus = np.zeros(num_actions)\n    if total_selection_count > 0:\n        for action_index in range(num_actions):\n            if selection_counts[action_index] > 0:\n                exploration_bonus[action_index] = np.sqrt((2 * np.log(total_selection_count)) / selection_counts[action_index])\n            else:\n                # If an action has never been selected, grant full exploration\n                exploration_bonus[action_index] = 1.0\n\n    # Adjusted scores combining average scores with exploration bonuses\n    adjusted_scores = average_scores + exploration_bonus\n\n    # Epsilon-greedy action selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(num_actions)\n    else:\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 38171395.541013725,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    exploration_factor = max(0.1, 1.0 * (0.99 ** total_selection_count))\n    \n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions, dtype=int)\n    \n    # Compute average scores and selection counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n    \n    # Calculating exploration bonuses\n    exploration_bonus = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        if selection_counts[action_index] > 0:\n            variance = np.var(score_set[action_index], ddof=1) if len(score_set[action_index]) > 1 else 0\n            exploration_bonus[action_index] = np.sqrt(np.log(total_selection_count + 1) / selection_counts[action_index]) + (variance / selection_counts[action_index])\n        else:\n            exploration_bonus[action_index] = np.inf  # High exploration for actions never selected\n\n    # Adjust scores with exploration bonus\n    adjusted_scores = average_scores + exploration_bonus\n\n    # Selection mechanism\n    if np.random.rand() < exploration_factor:\n        action_index = np.random.choice(np.arange(num_actions))  # Random exploration\n    else:\n        action_index = np.argmax(adjusted_scores)  # Greedy exploitation\n\n    return action_index",
          "objective": 38553962.57680462,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8  # Total number of actions\n\n    # Initialize average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        average_scores[action_index] = np.mean(scores) if scores else 0.0\n\n    # Set exploration factor\n    exploration_factor = 0.5 * (1 - (current_time_slot / total_time_slots))\n    exploration_threshold = max(0.1, exploration_factor)\n\n    # Calculate UCB values\n    ucb_values = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        if selection_counts[action_index] == 0:\n            ucb_values[action_index] = float('inf')  # Encourage exploration of untried actions\n        else:\n            ucb_values[action_index] = average_scores[action_index] + np.sqrt((2 * np.log(total_selection_count)) / selection_counts[action_index])\n\n    # Epsilon-greedy selection\n    if np.random.rand() < exploration_threshold:\n        action_index = np.random.choice(num_actions)\n    else:\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 38982972.68945281,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    initial_epsilon = 1.0\n    min_epsilon = 0.05\n    decay_rate = 0.99\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Epsilon reduction based on total selection count\n    exploration_factor = max(min_epsilon, initial_epsilon * (decay_rate ** total_selection_count))\n\n    # Calculate exploration bonus for actions based on selection counts\n    exploration_bonus = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        if selection_counts[action_index] > 0:\n            exploration_bonus[action_index] = np.sqrt(np.log(total_selection_count + 1) / (selection_counts[action_index] + 1))\n        else:\n            exploration_bonus[action_index] = np.inf  # Encourage selection of untried actions\n\n    # Adjusted scores\n    adjusted_scores = average_scores + exploration_bonus\n\n    # Epsilon-greedy selection\n    if np.random.rand() < exploration_factor:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 40560658.579404615,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    exploration_base = 1.5  # Base exploration factor\n    exploration_decay = 0.97  # Decay factor for exploration\n    exploitation_weight = 0.8  # Weight for exploitation contribution\n\n    # Calculate average scores and counts\n    average_scores = np.zeros(num_actions)\n    counts = np.zeros(num_actions, dtype=int)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        counts[action_index] = len(scores)\n        if counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Calculate exploration values based on counts\n    exploration_values = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        if counts[action_index] > 0:\n            exploration_values[action_index] = exploration_base / np.sqrt(counts[action_index])\n        else:\n            exploration_values[action_index] = np.inf  # Unexplored actions get high exploration value\n\n    # Calculate total exploitation and exploration influence\n    adjusted_exploration = (exploration_decay ** current_time_slot) * exploration_values\n    total_scores = (exploitation_weight * average_scores) + ((1 - exploitation_weight) * adjusted_exploration)\n\n    # Dynamic exploration versus exploitation decision\n    if np.random.rand() < (exploration_base / (1 + total_selection_count**0.5)):\n        action_index = np.random.choice(np.arange(num_actions))  # Pure exploration\n    else:\n        action_index = np.argmax(total_scores)  # Biased towards exploitation \n\n    return action_index",
          "objective": 42197706.32028387,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    exploration_factor_base = 1.5\n    exploration_factor_decay = 0.99\n    exploitation_weight = 0.7\n    epsilon_start = 0.2\n    epsilon_decay = 0.997\n\n    # Initialize average scores and counts\n    average_scores = np.zeros(num_actions)\n    counts = np.zeros(num_actions, dtype=int)\n\n    # Calculate average scores and counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        counts[action_index] = len(scores)\n        if counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Calculate exploration values\n    exploration_values = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        if counts[action_index] > 0:\n            exploration_values[action_index] = exploration_factor_base / np.sqrt(counts[action_index])\n        else:\n            exploration_values[action_index] = np.inf  # High exploration value for unexplored actions\n\n    # Apply exploration decay based on the current time slot\n    adjusted_exploration = (exploration_factor_decay ** current_time_slot) * exploration_values\n\n    # Calculate total scores considering both exploitation and exploration\n    total_scores = (exploitation_weight * average_scores) + ((1 - exploitation_weight) * adjusted_exploration)\n\n    # Epsilon-greedy selection strategy\n    epsilon = epsilon_start * (epsilon_decay ** total_selection_count)\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(np.arange(num_actions))  # Pure exploration\n    else:\n        action_index = np.argmax(total_scores)  # Biased towards exploitation \n\n    return action_index",
          "objective": 42296257.14095409,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8  # Total number of actions\n\n    # Initialize average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        average_scores[action_index] = np.mean(scores) if scores else 0.0\n\n    # Control exploration vs. exploitation based on time slot\n    if total_selection_count == 0:\n        return np.random.choice(num_actions)  # Exploratory selection if no selections made\n\n    # Epsilon decay strategy\n    epsilon = max(0.1, 1.0 * (1 - (current_time_slot / total_time_slots)))\n\n    # Calculate exploration bonus using UCB algorithm\n    ucb_values = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        if selection_counts[action_index] == 0:\n            ucb_values[action_index] = float('inf')  # Encourage exploration of untried actions\n        else:\n            ucb_values[action_index] = average_scores[action_index] + np.sqrt((2 * np.log(total_selection_count)) / selection_counts[action_index])\n\n    # Select action based on exploration (epsilon-greedy)\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(num_actions)\n    else:\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 42623906.16086336,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon_initial = 1.0\n    epsilon_minimum = 0.1\n    \n    # Decay epsilon based on time slot\n    epsilon = max(epsilon_minimum, epsilon_initial * (1 - current_time_slot / total_time_slots))\n\n    # Initialize average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Compute average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        average_scores[action_index] = np.mean(scores) if scores else 0.0\n\n    # Safety against division by zero for exploration bonus\n    exploration_bonus = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        if selection_counts[action_index] > 0:\n            exploration_bonus[action_index] = np.sqrt((2 * np.log(total_selection_count)) / selection_counts[action_index])\n        else:\n            # Favor unexplored actions by giving them high exploration bonus\n            exploration_bonus[action_index] = np.inf\n\n    # Adjust scores\n    adjusted_scores = average_scores + exploration_bonus\n\n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(num_actions)\n    else:\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 43123980.26075397,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    initial_epsilon = 1.0\n    final_epsilon = 0.1\n    exploration_decay_start = 0.75 * total_time_slots\n\n    # Calculate epsilon value that decays over time\n    if current_time_slot < exploration_decay_start:\n        epsilon = initial_epsilon - (initial_epsilon - final_epsilon) * (current_time_slot / exploration_decay_start)\n    else:\n        epsilon = final_epsilon\n\n    # Initialize average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Compute average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        average_scores[action_index] = np.mean(scores) if scores else 0.0\n\n    # Compute exploration bonuses\n    exploration_bonus = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        if selection_counts[action_index] > 0:\n            exploration_bonus[action_index] = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[action_index])\n        else:\n            # Favor unexplored actions by giving them a high exploration bonus\n            exploration_bonus[action_index] = np.inf\n\n    # Adjust scores with exploration bonuses\n    adjusted_scores = average_scores + exploration_bonus\n\n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(num_actions)\n    else:\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 49239824.086060815,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8  # Total number of actions\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        average_scores[action_index] = np.mean(scores) if scores else 0.0\n\n    # Exploration factor scaling with time\n    exploration_factor = np.clip(1 - (current_time_slot / total_time_slots), 0.1, 1.0)\n\n    # Calculate confidence intervals\n    confidence_intervals = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        if selection_counts[action_index] > 0:\n            confidence_intervals[action_index] = np.sqrt((2 * np.log(total_selection_count)) / selection_counts[action_index])\n        else:\n            confidence_intervals[action_index] = float('inf')  # Encourage exploration of untried actions\n\n    # Calculate UCB values\n    ucb_values = average_scores + exploration_factor * confidence_intervals\n\n    # Epsilon-greedy selection\n    epsilon = np.random.rand()\n    if epsilon < exploration_factor:\n        action_index = np.random.choice(num_actions)\n    else:\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 50261280.48133073,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon_initial = 1.0\n    epsilon_minimum = 0.1\n    epsilon_decay = 0.95\n\n    # Initialize average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        average_scores[action_index] = np.mean(scores) if scores else 0.0\n\n    # Dynamic epsilon calculation\n    if total_selection_count == 0:\n        epsilon = epsilon_initial\n    else:\n        epsilon = max(epsilon_minimum, epsilon_initial * (epsilon_decay ** total_selection_count))\n\n    # Exploration bonus to encourage exploration of less-selected actions\n    exploration_bonus = np.zeros(num_actions)\n    if len(average_scores) > 1:\n        total_variance = np.var(average_scores)\n\n        for action_index in range(num_actions):\n            if selection_counts[action_index] > 0:\n                exploration_bonus[action_index] = (\n                    np.sqrt(np.log(total_selection_count + 1) / (selection_counts[action_index] + 1))\n                    + np.std(score_set[action_index]) * (total_variance / 2)\n                )\n\n    # Adjusted scores calculation\n    adjusted_scores = average_scores + exploration_bonus\n\n    # Epsilon-greedy action selection\n    if np.random.rand() < epsilon:\n        # Favor exploration based on counts\n        probabilities = selection_counts / selection_counts.sum() if total_selection_count > 0 else np.ones(num_actions) / num_actions\n        action_index = np.random.choice(np.arange(num_actions), p=probabilities)\n    else:\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 51702233.01473452,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon_initial = 1.0\n    epsilon_minimum = 0.1\n    \n    # Linear decay strategy for epsilon based on time slot\n    epsilon = max(epsilon_minimum, epsilon_initial * (1 - current_time_slot / total_time_slots))\n\n    # Initialize average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        average_scores[action_index] = np.mean(scores) if scores else 0.0\n\n    # Avoid division by zero and calculate exploration bonus\n    exploration_bonus = np.zeros(num_actions)\n    if total_selection_count > 0:\n        for action_index in range(num_actions):\n            if selection_counts[action_index] > 0:\n                exploration_bonus[action_index] = np.sqrt((2 * np.log(total_selection_count)) / selection_counts[action_index])\n\n    # Adjusted scores incorporating exploration\n    adjusted_scores = average_scores + exploration_bonus\n\n    # Epsilon-greedy action selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(num_actions)\n    else:\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 54900392.44797925,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    initial_epsilon = 1.0\n    min_epsilon = 0.05\n    decay_rate = 0.99\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Dynamic epsilon calculation based on total selections\n    exploration_factor = max(min_epsilon, initial_epsilon * (decay_rate ** total_selection_count))\n    \n    # Calculate exploration bonuses\n    exploration_bonus = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        if selection_counts[action_index] > 0:\n            exploration_bonus[action_index] = np.sqrt(np.log(total_selection_count + 1) / (selection_counts[action_index] + 1))\n        else:\n            exploration_bonus[action_index] = np.inf  # Encourage selection if this action hasn't been chosen\n\n    # Adjusted scores for selection\n    adjusted_scores = average_scores + exploration_bonus\n\n    # Epsilon-greedy selection with improved exploration handling\n    if np.random.rand() < exploration_factor:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 58922869.193213545,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n\n    # Initialize average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    # Compute average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        average_scores[action_index] = np.mean(scores) if scores else 0.0\n    \n    # Exploration factor based on the current time slot\n    exploration_factor = max(0, 1.0 - (current_time_slot / total_time_slots)**2)\n\n    # Confidence term to encourage exploration of less-selected actions\n    confidence_scores = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        if selection_counts[action_index] > 0:\n            confidence_scores[action_index] = np.sqrt(np.log(total_selection_count + 1) / (selection_counts[action_index] + 1))\n\n    # Adjusted score with exploration and confidence\n    adjusted_scores = average_scores + (exploration_factor * confidence_scores)\n\n    # Action selection using a softmax approach for better exploration and exploitation balance\n    exp_scores = np.exp(adjusted_scores - np.max(adjusted_scores))  # Stability for large numbers\n    action_probabilities = exp_scores / np.sum(exp_scores)\n\n    action_index = np.random.choice(np.arange(num_actions), p=action_probabilities)\n\n    return action_index",
          "objective": 59596927.07606142,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    exploration_factor = max(0.1, 1.0 * (0.99 ** total_selection_count))\n    \n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions, dtype=int)\n    \n    # Compute average scores and selection counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n    \n    # Calculate exploration bonuses\n    exploration_bonus = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        if selection_counts[action_index] > 0:\n            variance = np.var(score_set[action_index], ddof=1) if len(score_set[action_index]) > 1 else 0\n            exploration_bonus[action_index] = np.sqrt(np.log(total_selection_count + 1) / selection_counts[action_index]) + (variance / selection_counts[action_index])\n        else:\n            exploration_bonus[action_index] = np.inf  # High exploration for actions never selected\n\n    # Adjust scores with exploration bonus\n    adjusted_scores = average_scores + exploration_bonus\n\n    # Dynamic exploration based on remaining time slots\n    exploration_weight = 1 - (current_time_slot / total_time_slots)  # Weight decreases over time\n    final_scores = (1 - exploration_weight) * adjusted_scores + exploration_weight * np.random.rand(num_actions)  # Incorporate randomness based on exploration weight\n\n    # Selection mechanism\n    if np.random.rand() < exploration_factor:\n        action_index = np.random.choice(np.arange(num_actions))  # Random exploration\n    else:\n        action_index = np.argmax(final_scores)  # Select the action with the highest final score\n\n    return action_index",
          "objective": 62908298.0946476,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    initial_epsilon = 1.0\n    min_epsilon = 0.05\n    decay_rate = 0.95  # Slightly slower decay for better exploration longer\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Dynamic exploration factor based on the current selection count\n    exploration_factor = max(min_epsilon, initial_epsilon * (decay_rate ** total_selection_count))\n\n    # Calculate exploration bonus\n    exploration_bonus = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        exploration_bonus[action_index] = np.sqrt(np.log(total_selection_count + 1) / (selection_counts[action_index] + 1)) if selection_counts[action_index] > 0 else np.inf\n\n    # Adjusted scores combining average scores and exploration bonuses\n    adjusted_scores = average_scores + exploration_bonus\n\n    # Epsilon-greedy selection mechanism\n    if np.random.rand() < exploration_factor:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 65957228.279623255,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    initial_epsilon = 1.0\n    min_epsilon = 0.05\n    decay_rate = 0.99\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Dynamic epsilon calculation based on total selections and time slot progress\n    exploration_factor = max(min_epsilon, initial_epsilon * (decay_rate ** total_selection_count))\n\n    # Calculate exploration bonuses using confidence intervals (UCB-like method)\n    upper_confidence_bounds = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        if selection_counts[action_index] > 0:\n            ucb = np.sqrt(2 * np.log(total_selection_count) / selection_counts[action_index])\n            upper_confidence_bounds[action_index] = average_scores[action_index] + ucb\n        else:\n            upper_confidence_bounds[action_index] = np.inf  # Encourage selection of untried actions\n\n    # Total scores combine average scores and exploration bonuses\n    total_scores = average_scores + upper_confidence_bounds\n\n    # Epsilon-greedy selection with a balance toward exploration and exploitation\n    if np.random.rand() < exploration_factor:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(total_scores)\n\n    return action_index",
          "objective": 66209800.48752693,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    initial_epsilon = 1.0\n    min_epsilon = 0.05\n    decay_rate = 0.99\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Dynamic epsilon calculation based on total selections\n    exploration_factor = max(min_epsilon, initial_epsilon * (decay_rate ** total_selection_count))\n\n    # Calculate exploration bonuses based on selection counts using UCB method\n    exploration_bonus = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        if selection_counts[action_index] > 0:\n            exploration_bonus[action_index] = np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts[action_index])\n        else:\n            exploration_bonus[action_index] = np.inf  # Encourage selection if this action hasn't been chosen\n\n    # Total scores combine average scores and exploration bonuses\n    total_scores = average_scores + exploration_bonus\n\n    # Epsilon-greedy selection with improved exploration handling\n    if np.random.rand() < exploration_factor:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(total_scores)\n\n    return action_index",
          "objective": 66565787.085134424,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon_initial = 1.0\n    epsilon_minimum = 0.05\n    epsilon_decay_rate = 0.97\n    \n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        average_scores[action_index] = np.mean(scores) if scores else 0.0\n\n    # Dynamic epsilon calculation\n    epsilon = max(epsilon_minimum, epsilon_initial * (epsilon_decay_rate ** total_selection_count))\n\n    # Calculate confidence scores for each action based on selection counts\n    confidence = np.zeros(num_actions)\n    total_actions_selected = total_selection_count + 1e-5  # avoid division by zero\n    \n    # Explore with a confidence interval\n    for action_index in range(num_actions):\n        if selection_counts[action_index] > 0:\n            confidence[action_index] = np.sqrt(np.log(total_actions_selected) / (selection_counts[action_index] + 1))\n        else:\n            confidence[action_index] = np.sqrt(np.log(total_actions_selected))  # High exploration for unselected actions\n\n    # Adjusted scores combine average scores with the confidence factor\n    adjusted_scores = average_scores + confidence\n\n    # Epsilon-greedy action selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(\n            np.arange(num_actions),\n            p=selection_counts / selection_counts.sum() if total_selection_count > 0 else np.ones(num_actions) / num_actions\n        )\n    else:\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 68422301.1638127,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    initial_exploration_rate = 1.0\n    min_exploration_rate = 0.1\n    decay_rate = 0.99\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions, dtype=int)\n\n    # Compute average scores and selection counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Dynamic exploration rate calculation\n    exploration_rate = max(min_exploration_rate, initial_exploration_rate * (decay_rate ** total_selection_count))\n\n    # Calculate exploration bonuses with a focus on actions with fewer selections\n    exploration_bonus = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        if selection_counts[action_index] > 0:\n            variance = np.var(score_set[action_index], ddof=1)\n            exploration_bonus[action_index] = np.sqrt(np.log(total_selection_count + 1) / selection_counts[action_index]) + (variance / selection_counts[action_index])\n        else:\n            exploration_bonus[action_index] = np.inf  # Bonus for actions that have never been selected\n\n    # Adjust scores with exploration bonus\n    adjusted_scores = average_scores + exploration_bonus\n\n    # Epsilon-greedy selection mechanism\n    if np.random.rand() < exploration_rate:\n        action_index = np.random.choice(np.arange(num_actions))  # Choose an action randomly\n    else:\n        action_index = np.argmax(adjusted_scores)  # Choose the action with the highest adjusted score\n\n    return action_index",
          "objective": 69964954.51797405,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    \n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        average_scores[action_index] = np.mean(scores) if scores else 0.0\n\n    # Dynamic epsilon calculation based on progress in time slots\n    epsilon_initial = 1.0\n    epsilon_minimum = 0.05\n    epsilon_decay_rate = 0.95\n    epsilon = max(epsilon_minimum, epsilon_initial * (epsilon_decay_rate ** (current_time_slot / total_time_slots)))\n\n    # Calculate confidence scores for each action based on selection counts\n    confidence = np.zeros(num_actions)\n    total_actions_selected = total_selection_count + 1e-5  # avoid division by zero\n    \n    # Explore with a confidence interval\n    for action_index in range(num_actions):\n        if selection_counts[action_index] > 0:\n            confidence[action_index] = np.sqrt(np.log(total_actions_selected) / (selection_counts[action_index] + 1))\n        else:\n            confidence[action_index] = np.sqrt(np.log(total_actions_selected))  # High exploration for unselected actions\n\n    # Adjusted scores combine average scores with the confidence factor\n    adjusted_scores = average_scores + confidence\n\n    # Epsilon-greedy action selection strategy\n    if np.random.rand() < epsilon:\n        # Uniform random selection from unselected or under-selected actions\n        action_index = np.random.choice(np.arange(num_actions))\n    else:\n        # Select the action with the highest adjusted score\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 74682017.77365628,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon_initial = 1.0\n    epsilon_minimum = 0.05\n    epsilon_decay_rate = 0.95\n\n    # Calculate average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        average_scores[action_index] = np.mean(scores) if scores else 0.0\n\n    # Dynamic epsilon calculation\n    epsilon = max(epsilon_minimum, epsilon_initial * (epsilon_decay_rate ** current_time_slot))\n\n    # Calculate exploration factors\n    exploration_factors = np.zeros(num_actions)\n    total_selection_count_adjusted = total_selection_count + 1  # to avoid division by zero\n\n    for action_index in range(num_actions):\n        if selection_counts[action_index] > 0:\n            exploration_factors[action_index] = np.sqrt(np.log(total_selection_count_adjusted) / (selection_counts[action_index] + 1))\n    \n    # Adjusted scores include average scores + exploration factors\n    adjusted_scores = average_scores + exploration_factors\n\n    # Epsilon-greedy action selection\n    if np.random.rand() < epsilon:\n        available_actions = np.nonzero(selection_counts)[0]\n        action_index = np.random.choice(available_actions) if available_actions.size > 0 else np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 75328176.34706487,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon_initial = 1.0\n    epsilon_minimum = 0.1\n    epsilon_decay = 0.01  # Rate of decay for epsilon\n\n    # Calculate current epsilon for exploration\n    epsilon = max(epsilon_minimum, epsilon_initial * (1 - (current_time_slot / total_time_slots)))\n\n    # Initialize average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Populate average scores and counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        average_scores[action_index] = np.mean(scores) if scores else 0.0\n\n    # Calculate exploration bonus\n    exploration_bonus = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        if selection_counts[action_index] > 0:\n            exploration_bonus[action_index] = np.sqrt((2 * np.log(total_selection_count)) / selection_counts[action_index])\n\n    # Compute adjusted scores for selection\n    adjusted_scores = average_scores + exploration_bonus\n\n    # Epsilon-greedy action selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(num_actions)\n    else:\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 75966723.48948435,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    initial_epsilon = 1.0\n    min_epsilon = 0.05\n    decay_rate = 0.99\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and populate selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        # Calculate average score only if the action has been selected at least once\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Dynamic epsilon calculation based on total selections\n    exploration_factor = max(min_epsilon, initial_epsilon * (decay_rate ** total_selection_count))\n    \n    # Calculate exploration bonuses for underexplored actions\n    exploration_bonus = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        if selection_counts[action_index] > 0:\n            exploration_bonus[action_index] = np.sqrt(np.log(total_selection_count + 1) / (selection_counts[action_index] + 1))\n        else:\n            exploration_bonus[action_index] = np.inf  # Encourage selection if this action hasn't been chosen\n\n    # Adjust scores for exploration\n    adjusted_scores = average_scores + exploration_bonus\n\n    # Epsilon-greedy selection mechanism\n    if np.random.rand() < exploration_factor:\n        action_index = np.random.choice(num_actions)\n    else:\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 79537906.86809066,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon_initial = 1.0\n    epsilon_minimum = 0.1\n    epsilon_decay_factor = 0.9  # Adjusted decay factor for exploring actions\n    epsilon_time_slot_factor = (total_time_slots - current_time_slot) / total_time_slots\n\n    # Initialize average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        average_scores[action_index] = np.mean(scores) if scores else 0.0\n\n    # Dynamic epsilon calculation\n    if total_selection_count == 0:\n        epsilon = epsilon_initial\n    else:\n        epsilon = max(epsilon_minimum, epsilon_initial * (epsilon_decay_factor ** (total_selection_count / (total_time_slots / 2))))\n\n    # Ensure that exploration remains viable for less-selected actions\n    exploration_bonus = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        if selection_counts[action_index] > 0:\n            exploration_bonus[action_index] = (\n                np.sqrt(np.log(total_selection_count) / (selection_counts[action_index] + 1)) \n                + (1 / selection_counts[action_index])\n            )\n\n    # Adjusted scores\n    adjusted_scores = average_scores + exploration_bonus\n\n    # Epsilon-greedy action selection\n    if np.random.rand() < epsilon:\n        probabilities = (1 / (selection_counts + 1e-5)) / np.sum(1 / (selection_counts + 1e-5))\n        action_index = np.random.choice(np.arange(num_actions), p=probabilities)\n    else:\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 82633810.39671998,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon_initial = 1.0\n    epsilon_minimum = 0.1\n    epsilon_decay = 0.99  # Decay factor for epsilon after each action selection\n\n    # Initialize average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        average_scores[action_index] = np.mean(scores) if scores else 0.0\n\n    # Calculate dynamic epsilon\n    epsilon = max(epsilon_minimum, epsilon_initial * (epsilon_decay ** current_time_slot))\n\n    # Upper confidence bound exploration\n    exploration_bonus = np.zeros(num_actions)\n    if total_selection_count > 0:\n        for action_index in range(num_actions):\n            if selection_counts[action_index] > 0:\n                exploration_bonus[action_index] = np.sqrt((2 * np.log(total_selection_count)) / selection_counts[action_index])\n    \n    # Adjusted scores incorporating exploration\n    adjusted_scores = average_scores + exploration_bonus\n\n    # Epsilon-greedy action selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(num_actions)\n    else:\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 83168390.77593885,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    \n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        average_scores[action_index] = np.mean(scores) if scores else 0.0\n\n    # Dynamic epsilon calculation based on time slots and selection count\n    epsilon_initial = 1.0\n    epsilon_minimum = 0.1\n    epsilon_decay = 0.99\n    decay_steps = 100\n    decay_factor = (total_selection_count + 1) / (decay_steps + 1)  # To scale decay\n    epsilon = max(epsilon_minimum, epsilon_initial * (epsilon_decay ** (decay_factor)))\n\n    # Exploration factor\n    exploration_bonus = np.zeros(num_actions)\n    exploration_constant = 2.0  # Encourages exploration more\n\n    for action_index in range(num_actions):\n        if selection_counts[action_index] > 0:\n            exploration_factor = exploration_constant * np.sqrt(np.log(total_selection_count + 1) / (selection_counts[action_index] + 1))\n            exploration_bonus[action_index] = exploration_factor\n\n    # Combine average scores and exploration bonuses\n    combined_scores = average_scores + exploration_bonus\n    \n    # Epsilon-greedy action selection logic\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(num_actions)\n    else:\n        action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": 85755567.11476079,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon = max(0.1, 1.0 - (current_time_slot / total_time_slots))  # Decrease exploration with more time slots\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions, dtype=int)\n\n    # Compute average scores and selection counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if scores:\n            average_scores[action_index] = np.mean(scores)\n\n    # Handling exploration bonus\n    exploration_bonus = np.zeros(num_actions)\n    total_variance = np.var([score for scores in score_set.values() for score in scores], ddof=1) if total_selection_count > 0 else 0\n\n    for action_index in range(num_actions):\n        if selection_counts[action_index] == 0:\n            exploration_bonus[action_index] = np.inf  # Forever encourage exploration for unselected actions\n        else:\n            variance = np.var(score_set[action_index], ddof=1) if selection_counts[action_index] > 1 else 0\n            exploration_bonus[action_index] = np.sqrt(np.log(total_selection_count) / selection_counts[action_index]) + (variance / selection_counts[action_index])\n\n    # Calculate adjusted scores with exploration bonuses\n    adjusted_scores = average_scores + exploration_bonus\n\n    # Epsilon-greedy selection strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(np.arange(num_actions))  # Random exploration\n    else:\n        action_index = np.argmax(adjusted_scores)  # Greedy exploitation\n\n    return action_index",
          "objective": 89175437.84727392,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon_initial = 1.0\n    epsilon_minimum = 0.01\n    epsilon_decay = 0.95\n    \n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        average_scores[action_index] = np.mean(scores) if scores else 0.0\n\n    # Dynamic epsilon calculation\n    epsilon = max(epsilon_minimum, epsilon_initial * (epsilon_decay ** total_selection_count))\n\n    # Calculate the exploration bonus\n    exploration_bonus = np.zeros(num_actions)\n    total_variance = np.var(average_scores) if len(average_scores) > 1 else 1.0\n\n    for action_index in range(num_actions):\n        if selection_counts[action_index] > 0:\n            exploration_bonus[action_index] = np.sqrt(np.log(total_selection_count + 1) / (selection_counts[action_index] + 1)) + \\\n                                                     np.std(score_set[action_index]) * (total_variance / 2)\n\n    # Adjusted scores\n    adjusted_scores = average_scores + exploration_bonus\n\n    # Epsilon-greedy action selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(np.arange(num_actions), p=selection_counts / selection_counts.sum() if total_selection_count > 0 else np.ones(num_actions) / num_actions)\n    else:\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 89525578.84148139,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    initial_exploration_rate = 1.0\n    min_exploration_rate = 0.05\n    decay_factor = 0.95\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions, dtype=int)\n\n    # Compute average scores and selection counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        average_scores[action_index] = np.mean(scores) if scores else 0.0\n\n    # Dynamic calculation of the exploration rate\n    exploration_rate = max(min_exploration_rate, initial_exploration_rate * (decay_factor ** total_selection_count))\n\n    # Calculate exploration bonuses\n    exploration_bonus = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        if selection_counts[action_index] > 0:\n            uncertainty = np.sqrt(np.log(total_selection_count + 1) / selection_counts[action_index])\n            exploration_bonus[action_index] = uncertainty\n        else:\n            exploration_bonus[action_index] = np.inf  # bonus for actions that have never been selected\n\n    # Adjust scores with exploration bonus\n    adjusted_scores = average_scores + exploration_bonus\n\n    # Epsilon-greedy selection mechanism\n    if np.random.rand() < exploration_rate:\n        action_index = np.random.choice(np.arange(num_actions))  # explore\n    else:\n        action_index = np.argmax(adjusted_scores)  # exploit\n\n    return action_index",
          "objective": 90248955.01535262,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    initial_epsilon = 1.0\n    min_epsilon = 0.1\n    decay_rate = 0.995\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions, dtype=int)\n\n    # Compute average scores and total selection counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Dynamic epsilon calculation\n    exploration_factor = max(min_epsilon, initial_epsilon * (decay_rate ** total_selection_count))\n\n    # Exploration bonus based on selection counts and variance\n    exploration_bonus = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        if selection_counts[action_index] > 0:\n            variance = np.var(score_set[action_index], ddof=1)  \n            exploration_bonus[action_index] = np.sqrt(np.log(total_selection_count + 1) / selection_counts[action_index]) + (variance / selection_counts[action_index])\n\n    # Adjust scores with exploration bonus\n    adjusted_scores = average_scores + exploration_bonus\n\n    # Epsilon-greedy selection mechanism\n    if np.random.rand() < exploration_factor:\n        action_index = np.random.randint(num_actions)  # Choose an action randomly\n    else:\n        action_index = np.argmax(adjusted_scores)  # Choose the action with the highest adjusted score\n\n    return action_index",
          "objective": 94644961.80609675,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8  # Total number of actions\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        average_scores[action_index] = np.mean(scores) if scores else 0.0\n\n    # Explore-exploit parameters\n    exploration_factor = max(0.1, 1 - (total_selection_count / (total_time_slots * 10)))\n\n    # Calculate confidence measure for actions\n    confidence_intervals = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        if selection_counts[action_index] > 0:\n            confidence_intervals[action_index] = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts[action_index] + 1))\n        else:\n            confidence_intervals[action_index] = float('inf')  # Encourage exploration of untried actions\n\n    # Compute weighted UCB values\n    ucb_values = average_scores + exploration_factor * confidence_intervals\n\n    # Choose action based on UCB values\n    action_index = np.argmax(ucb_values)\n    \n    # Incorporate an epsilon-greedy mechanism\n    epsilon = np.random.rand()\n    if epsilon < exploration_factor:\n        unselected_actions = np.where(selection_counts == 0)[0]\n        if unselected_actions.size > 0:\n            action_index = np.random.choice(unselected_actions)  # Randomly select from untried actions\n        else:\n            action_index = np.random.randint(num_actions)  # Fallback to random selection\n\n    return action_index",
          "objective": 96562768.84890538,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    initial_epsilon = 1.0\n    min_epsilon = 0.1\n    decay_rate = 0.99\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions, dtype=int)\n\n    # Compute average scores and selection counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Dynamic epsilon calculation\n    exploration_factor = max(min_epsilon, initial_epsilon * (decay_rate ** total_selection_count))\n\n    # Calculate exploration bonuses\n    exploration_bonus = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        if selection_counts[action_index] > 0:\n            variance = np.var(score_set[action_index], ddof=1)\n            exploration_bonus[action_index] = np.sqrt(np.log(total_selection_count + 1) / selection_counts[action_index]) + (variance / selection_counts[action_index])\n        else:\n            exploration_bonus[action_index] = np.inf  # Bonus for actions that have never been selected\n\n    # Adjust scores with exploration bonus\n    adjusted_scores = average_scores + exploration_bonus\n\n    # Epsilon-greedy selection mechanism\n    if np.random.rand() < exploration_factor:\n        action_index = np.random.choice(np.arange(num_actions))  # Choose an action randomly\n    else:\n        action_index = np.argmax(adjusted_scores)  # Choose the action with the highest adjusted score\n\n    return action_index",
          "objective": 99144068.98618457,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    base_exploration_rate = 0.7\n    min_exploration_rate = 0.1\n    epsilon_decay = 0.99\n\n    # Calculate the exploration factor based on the total number of selections\n    exploration_rate = max(min_exploration_rate, base_exploration_rate * (epsilon_decay ** total_selection_count))\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions, dtype=int)\n\n    # Compute average scores and selection counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if scores:\n            average_scores[action_index] = np.mean(scores)\n    \n    # Calculate exploration bonuses with bias towards less selected actions\n    exploration_bonus = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        if selection_counts[action_index] > 0:\n            variance = np.var(score_set[action_index], ddof=1) if len(score_set[action_index]) > 1 else 0\n            exploration_bonus[action_index] = np.sqrt(np.log(total_selection_count + 1) / selection_counts[action_index]) + (variance / selection_counts[action_index])\n        else:\n            exploration_bonus[action_index] = np.inf  # Encourage exploration for unselected actions\n            \n    # Adjust scores with exploration bonuses\n    adjusted_scores = average_scores + exploration_bonus\n\n    # Epsilon-greedy selection mechanism\n    if np.random.rand() < exploration_rate:\n        # Choose an action randomly (exploration)\n        action_index = np.random.choice(np.arange(num_actions))\n    else:\n        # Choose the best action based on adjusted scores (exploitation)\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 101605007.94871907,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon_initial = 1.0\n    epsilon_minimum = 0.01\n    epsilon_decay = 0.95\n\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    # Calculate average scores and selection counts\n    for action_index, scores in score_set.items():\n        selection_counts[action_index] = len(scores)\n        if scores:\n            average_scores[action_index] = np.mean(scores)\n    \n    # Dynamic epsilon calculation\n    epsilon = max(epsilon_minimum, epsilon_initial * (epsilon_decay ** current_time_slot))\n\n    # Calculate the exploration bonus\n    exploration_bonus = np.zeros(num_actions)\n    total_variance = np.var(average_scores) if len(average_scores) > 1 else 0\n\n    for action_index in range(num_actions):\n        if selection_counts[action_index] > 0:\n            exploration_bonus[action_index] = np.sqrt(np.log(total_selection_count + 1) / (selection_counts[action_index] + 1))\n        exploration_bonus[action_index] += np.std(score_set.get(action_index, [])) * (total_variance / 2)\n\n    # Adjusted scores\n    adjusted_scores = average_scores + exploration_bonus\n\n    # Epsilon-greedy action selection\n    if np.random.rand() < epsilon:\n        available_actions = np.nonzero(selection_counts)[0]\n        if available_actions.size > 0:\n            action_index = np.random.choice(available_actions)\n        else:\n            action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 123001041.8567559,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    initial_epsilon = 1.0\n    min_epsilon = 0.1\n    decay_rate = 0.995\n\n    # Initialize arrays for average scores, selection counts, and variances\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions, dtype=int)\n    score_variance = np.zeros(num_actions)\n\n    # Compute average scores, selection counts, and score variances\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        if len(scores) > 0:\n            average_scores[action_index] = np.mean(scores)\n            score_variance[action_index] = np.var(scores, ddof=1) if len(scores) > 1 else 0\n\n    # Calculate dynamic epsilon\n    exploration_factor = max(min_epsilon, initial_epsilon * (decay_rate ** total_selection_count))\n\n    # Calculate exploration bonuses\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1)) + \\\n                         (score_variance / (selection_counts + 1))\n    \n    # Adjusted scores for selection\n    adjusted_scores = average_scores + exploration_bonus\n\n    # Epsilon-greedy selection\n    if np.random.rand() < exploration_factor:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 124419290.60011886,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    alpha_initial = 1.0\n    alpha_minimum = 0.01\n    alpha_decay = 0.95\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    # Compute average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Dynamic exploration factor calculation\n    alpha = max(alpha_minimum, alpha_initial * (alpha_decay ** current_time_slot))\n\n    # Calculate the exploration bonus, incorporating selection frequency\n    exploration_bonus = np.zeros(num_actions)\n    total_count = total_selection_count + 1  # To avoid division by zero\n\n    for action_index in range(num_actions):\n        if selection_counts[action_index] > 0:\n            exploration_bonus[action_index] = np.sqrt(np.log(total_count) / (selection_counts[action_index] + 1))\n        \n        # Promote exploration of less selected actions\n        exploration_bonus[action_index] += (1 / (selection_counts[action_index] + 1))\n\n    # Combine average scores with exploration bonuses\n    adjusted_scores = average_scores + exploration_bonus\n    \n    # Epsilon-greedy action selection strategy\n    if np.random.rand() < alpha and total_selection_count < num_actions:\n        # Encouraging exploration when there are untried actions\n        action_index = np.argmin(selection_counts)  # Select the least selected action\n    else:\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 136062007.38397527,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    initial_epsilon = 1.0\n    min_epsilon = 0.1\n    decay_rate = 0.995\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Dynamic epsilon calculation based on total selections with a time-based adjustment\n    epsilon = max(min_epsilon, initial_epsilon * (decay_rate ** total_selection_count))\n    \n    # Calculate exploration bonuses using upper confidence bound (UCB) strategy\n    exploration_bonus = np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n    # Add a decay factor related to the time slot to encourage exploration at early stages\n    time_factor = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_exploration = exploration_bonus * time_factor\n\n    # Adjusted scores for selection incorporating exploration\n    adjusted_scores = average_scores + adjusted_exploration\n\n    # Epsilon-greedy action selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(num_actions, p=(selection_counts + 0.1) / np.sum(selection_counts + 0.1))\n    else:\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 143209407.5906874,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    initial_temperature = 1.0\n    min_temperature = 0.1\n    decay_rate = 0.95\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Dynamic temperature calculation\n    exploration_factor = max(min_temperature, initial_temperature * (decay_rate ** total_selection_count))\n\n    # Softmax calculation for adjusted scores\n    exp_scores = np.exp(average_scores / exploration_factor)\n    probability_distribution = exp_scores / np.sum(exp_scores)\n\n    # Use cumulative probability to select an action\n    cumulative_probability = np.cumsum(probability_distribution)\n    random_value = np.random.rand()\n\n    # Select action based on cumulative probability\n    for action_index in range(num_actions):\n        if random_value < cumulative_probability[action_index]:\n            return action_index\n\n    # Fallback in case of numerical instability\n    return action_index",
          "objective": 143904714.16108608,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon_initial = 1.0\n    epsilon_minimum = 0.1\n    epsilon_decay_factor = 0.95\n\n    # Initialize average scores and counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        average_scores[action_index] = np.mean(scores) if scores else 0.0\n\n    # Dynamic epsilon calculation\n    epsilon = max(epsilon_minimum, epsilon_initial * (epsilon_decay_factor ** total_selection_count))\n\n    # Compute exploration bonuses based on selection counts\n    exploration_bonus = np.zeros(num_actions)\n    total_counts = np.sum(selection_counts) if np.sum(selection_counts) > 0 else 1\n    \n    for action_index in range(num_actions):\n        if selection_counts[action_index] == 0:\n            exploration_bonus[action_index] = 1.0  # High exploration for unselected actions\n        else:\n            exploration_bonus[action_index] = np.sqrt(np.log(total_selection_count + 1) / selection_counts[action_index])\n\n    # Calculate adjusted scores\n    adjusted_scores = average_scores + exploration_bonus\n\n    # Epsilon-greedy action selection\n    if np.random.rand() < epsilon:\n        # Favor exploration by selecting uniformly from less selected actions\n        probabilities = selection_counts / total_counts\n        probabilities = np.nan_to_num(probabilities, nan=1/num_actions)  # Replace NaN with uniform probability if all counts are zero\n        action_index = np.random.choice(np.arange(num_actions), p=probabilities)\n    else:\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 144737769.24373686,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon_initial = 1.0\n    epsilon_minimum = 0.01\n    epsilon_decay = 0.9\n    \n    # Initialize arrays for average scores and selection counts\n    selection_counts = np.array([len(score_set.get(i, [])) for i in range(num_actions)], dtype=float)\n    average_scores = np.array([np.mean(score_set.get(i, [])) if selection_counts[i] > 0 else 0.0 for i in range(num_actions)])\n\n    # Dynamic epsilon calculation based on total_selection_count\n    epsilon = max(epsilon_minimum, epsilon_initial * (epsilon_decay ** (total_selection_count / (total_time_slots + 1))))\n\n    # Exploration term to encourage exploration of less-seen actions\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1)) * (selection_counts < np.median(selection_counts)).astype(float)\n\n    # Adjusted scores for action selection by combining average scores and exploration bonuses\n    adjusted_scores = average_scores + exploration_bonus\n\n    # Epsilon-greedy action selection with probabilistic exploration\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(num_actions, p=(selection_counts / selection_counts.sum()) if selection_counts.sum() > 0 else np.ones(num_actions) / num_actions)\n    else:\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 146331424.8973741,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon_initial = 1.0\n    epsilon_minimum = 0.01\n    epsilon_decay = 0.95\n\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Dynamic epsilon calculation\n    epsilon = max(epsilon_minimum, epsilon_initial * (epsilon_decay ** current_time_slot))\n\n    # Calculate the exploration bonus based on selection counts and variance of scores\n    exploration_bonuses = np.zeros(num_actions)\n    \n    if total_selection_count > 0:\n        total_variance = np.var(average_scores) if num_actions > 1 else 0\n        for action_index in range(num_actions):\n            if selection_counts[action_index] > 0:\n                exploration_bonuses[action_index] = np.sqrt(np.log(total_selection_count) / (selection_counts[action_index] + 1))\n                action_variance = np.std(score_set[action_index]) if score_set[action_index] else 0\n                exploration_bonuses[action_index] += action_variance * (total_variance / 2)\n\n    # Adjusted scores\n    adjusted_scores = average_scores + exploration_bonuses\n\n    # Epsilon-greedy action selection\n    if np.random.rand() < epsilon:\n        available_actions = np.nonzero(selection_counts)[0]\n        if available_actions.size > 0:\n            action_index = np.random.choice(available_actions)\n        else:\n            action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 150021763.85635412,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon_initial = 1.0\n    epsilon_minimum = 0.1\n    epsilon_decay = 0.99\n\n    # Initialize average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        average_scores[action_index] = np.mean(scores) if scores else 0.0\n\n    # Dynamic epsilon calculation\n    epsilon = max(epsilon_minimum, epsilon_initial * (epsilon_decay ** total_selection_count))\n\n    # Exploration bonus to encourage exploration of less-selected actions\n    exploration_bonus = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        if selection_counts[action_index] > 0:\n            exploration_factor = np.sqrt(np.log(total_selection_count) / (selection_counts[action_index] + 1))\n            exploration_bonus[action_index] = exploration_factor\n\n    # Combined scores\n    combined_scores = average_scores + exploration_bonus\n\n    # Epsilon-greedy action selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(np.arange(num_actions), p=selection_counts / selection_counts.sum() if total_selection_count > 0 else np.ones(num_actions) / num_actions)\n    else:\n        action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": 150161123.12658757,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    initial_epsilon = 1.0\n    min_epsilon = 0.05\n    decay_rate = 0.99\n    \n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions, dtype=int)\n\n    # Compute average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n        else:\n            average_scores[action_index] = 0  # Actions not selected have a score of 0\n\n    # Calculate dynamic epsilon based on total selections\n    exploration_factor = max(min_epsilon, initial_epsilon * (decay_rate ** total_selection_count))\n\n    # Calculate exploration bonuses\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1)) \n\n    # Adjusted scores for selection which amplifies scores of lesser selected actions\n    adjusted_scores = average_scores + exploration_bonus\n\n    # Epsilon-greedy selection\n    if np.random.rand() < exploration_factor:\n        # Select a random action, but promote diversity by weighting selection less for frequently chosen actions\n        probabilities = np.array(selection_counts + 1) / (np.sum(selection_counts) + num_actions)\n        action_index = np.random.choice(num_actions, p=probabilities)\n    else:\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 159053211.73641634,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    \n    # Dynamic exploration factor based on the current time slot in relation to total time slots\n    exploration_factor = max(0.1, 1.0 - (current_time_slot / total_time_slots))\n    \n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions, dtype=int)\n    \n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n    \n    # Weighted scoring: dynamically adjust the average score based on selection count\n    weighted_scores = average_scores * (selection_counts + 1) / (total_selection_count + num_actions)\n\n    # Calculate exploration bonuses\n    exploration_bonus = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        if selection_counts[action_index] > 0:\n            variance = np.var(score_set[action_index], ddof=1) if len(score_set[action_index]) > 1 else 0\n            exploration_bonus[action_index] = (0.1 + np.sqrt(np.log(total_selection_count + 1) / selection_counts[action_index]) + (variance / (selection_counts[action_index] + 1)))\n        else:\n            exploration_bonus[action_index] = np.inf  # High exploration for actions never selected\n\n    # Adjust scores with exploration bonus\n    adjusted_scores = weighted_scores + exploration_bonus\n\n    # Selection mechanism using dynamic exploration\n    if np.random.rand() < exploration_factor:\n        action_index = np.random.choice(np.arange(num_actions))  # Random exploration\n    else:\n        action_index = np.argmax(adjusted_scores)  # Greedy exploitation\n\n    return action_index",
          "objective": 160139391.21151078,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    initial_epsilon = 1.0\n    min_epsilon = 0.1\n    decay_rate = 0.99\n\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions, dtype=int)\n\n    # Compute average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        # Only calculate average scores if there's at least one score\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Dynamic epsilon calculation\n    exploration_factor = max(min_epsilon, initial_epsilon * (decay_rate ** (total_selection_count / (total_time_slots + 1))))\n\n    # Exploration bonuses calculation\n    exploration_bonus = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        if selection_counts[action_index] == 0:\n            exploration_bonus[action_index] = np.inf  # Untried actions get maximum bonus\n        else:\n            variance = np.var(score_set[action_index], ddof=1) if selection_counts[action_index] > 1 else 0\n            exploration_bonus[action_index] = np.sqrt(np.log(total_selection_count) / selection_counts[action_index]) + (variance / selection_counts[action_index])\n\n    # Adjust scores with exploration bonuses\n    adjusted_scores = average_scores + exploration_bonus\n\n    # Epsilon-greedy selection mechanism\n    if np.random.rand() < exploration_factor:\n        action_index = np.random.choice(np.arange(num_actions))  # Randomly select for exploration\n    else:\n        action_index = np.argmax(adjusted_scores)  # Select the best action based on adjusted scores\n\n    return action_index",
          "objective": 161516633.36748728,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    initial_epsilon = 1.0\n    min_epsilon = 0.05\n    decay_rate = 0.99\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions, dtype=int)\n\n    # Compute average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n        else:\n            average_scores[action_index] = -np.inf  # Use -inf to ensure unselected actions rank low\n\n    # Calculate dynamic epsilon based on total selections\n    exploration_factor = max(min_epsilon, initial_epsilon * (decay_rate ** total_selection_count))\n\n    # Exploration bonuses based on selection counts\n    exploration_bonus = np.where(selection_counts > 0, \n                                  np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1)), \n                                  np.inf)  # Allow unselected actions to take precedence\n\n    # Adjusted scores for selection\n    adjusted_scores = average_scores + exploration_bonus\n\n    # Epsilon-greedy selection\n    if np.random.rand() < exploration_factor:\n        # Select a random action based on a uniform distribution\n        action_index = np.random.choice(num_actions)\n    else:\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 162165602.24965626,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon_initial = 0.5\n    epsilon_minimum = 0.01\n    epsilon_decay = 0.95\n\n    # Initialize arrays for average scores, selection counts, and variances\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    score_variances = np.zeros(num_actions)\n\n    # Update averages, counts and calculate variances based on the score_set\n    for action_index in range(num_actions):\n        if action_index in score_set and score_set[action_index]:\n            scores = score_set[action_index]\n            selection_counts[action_index] = len(scores)\n            average_scores[action_index] = np.mean(scores)\n            score_variances[action_index] = np.var(scores, ddof=1) if selection_counts[action_index] > 1 else 0\n\n    # Dynamic epsilon value\n    epsilon = max(epsilon_minimum, epsilon_initial * (epsilon_decay ** (total_selection_count / num_actions)))\n\n    # Exploration bonus based on selection counts\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-6))\n\n    # Adjust scores by average scores and exploration bonus, penalizing high variances\n    adjusted_scores = (average_scores + exploration_bonus - \n                       (score_variances / (selection_counts + 1e-6)))\n\n    # Action selection\n    if np.random.rand() < epsilon:\n        available_actions = np.where(selection_counts > 0)[0]\n        action_index = np.random.choice(available_actions) if available_actions.size > 0 else np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 176618272.26915848,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon_initial = 0.5\n    epsilon_minimum = 0.01\n    epsilon_decay = 0.95\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Update averages and counts based on score_set\n    for action_index in range(num_actions):\n        if action_index in score_set:\n            scores = score_set[action_index]\n            selection_counts[action_index] = len(scores)\n            if selection_counts[action_index] > 0:\n                average_scores[action_index] = np.mean(scores)\n\n    # Calculate dynamic epsilon\n    epsilon = max(epsilon_minimum, epsilon_initial * (epsilon_decay ** (total_selection_count / num_actions)))\n\n    # Exploration bonus\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-6))\n\n    # Adjust scores with exploration bonus\n    adjusted_scores = average_scores + exploration_bonus\n\n    # Encourage diversity by considering the scores of less frequently selected actions\n    diversity_penalty = np.maximum(0, np.std(adjusted_scores, ddof=1) - exploration_bonus)\n    adjusted_scores -= diversity_penalty\n\n    # Action selection using epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        available_actions = np.where(selection_counts > 0)[0]\n        action_index = np.random.choice(available_actions) if available_actions.size > 0 else np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 181317275.4508103,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon_initial = 1.0\n    epsilon_decay = 0.95\n    epsilon_minimum = 0.01\n\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index, scores in score_set.items():\n        selection_counts[action_index] = len(scores)\n        average_scores[action_index] = np.mean(scores) if scores else 0\n\n    # Dynamically calculate epsilon\n    epsilon = max(epsilon_minimum, epsilon_initial * (epsilon_decay ** current_time_slot))\n\n    # Calculate exploration bonus using UCB-style logic\n    exploration_bonus = np.zeros(num_actions)\n    total_variance = np.var(average_scores) if np.size(average_scores) > 1 else 0\n\n    for action_index in range(num_actions):\n        if selection_counts[action_index] > 0:\n            exploration_bonus[action_index] = np.sqrt(np.log(total_selection_count + 1) / (selection_counts[action_index] + 1))\n        else:\n            exploration_bonus[action_index] = float('inf')  # Ensure untried actions are favored\n\n    # Adjust scores with the exploration bonus\n    adjusted_scores = average_scores + (exploration_bonus * (total_variance / 2))\n\n    # Epsilon-greedy action selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(num_actions)\n    else:\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 193722755.8748507,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon_initial = 1.0\n    epsilon_minimum = 0.01\n    epsilon_decay = 0.95\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Dynamic epsilon calculation\n    epsilon = max(epsilon_minimum, epsilon_initial * (epsilon_decay ** current_time_slot))\n    \n    # Calculate exploration bonus using counts and variance\n    exploration_bonus = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        count = selection_counts[action_index]\n        if count > 0:\n            # Calculate exploration factor\n            exploration_bonus[action_index] = np.sqrt(np.log(total_selection_count + 1) / (count + 1))\n            # Add score variance for additional exploration encouragement\n            if count > 1:\n                exploration_bonus[action_index] += np.std(score_set[action_index]) / np.sqrt(count)\n\n    # Adjust scores with exploration bonus\n    adjusted_scores = average_scores + exploration_bonus\n\n    # Action selection using epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        # Select a random action uniformly\n        action_index = np.random.randint(num_actions)\n    else:\n        # Select the action with the highest adjusted score\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 195305978.2511238,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon_initial = 1.0\n    epsilon_decay = 0.95\n    epsilon_minimum = 0.01\n\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index, scores in score_set.items():\n        selection_counts[action_index] = len(scores)\n        if scores:\n            average_scores[action_index] = np.mean(scores)\n\n    # Dynamically calculate epsilon\n    epsilon = max(epsilon_minimum, epsilon_initial * (epsilon_decay ** current_time_slot))\n\n    # Calculate exploration bonus using UCB-style\n    total_variance = np.var(average_scores) if np.size(average_scores) > 1 else 0\n    exploration_bonus = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        if selection_counts[action_index] > 0:\n            exploration_bonus[action_index] = np.sqrt(np.log(total_selection_count + 1) / (selection_counts[action_index] + 1))\n        else:\n            # If an action has never been selected, provide a high exploration score\n            exploration_bonus[action_index] = float('inf')\n\n    # Adjust scores with the exploration bonus\n    adjusted_scores = average_scores + exploration_bonus * (total_variance / 2)\n    \n    # Epsilon-greedy action selection with diverse exploration\n    if np.random.rand() < epsilon:\n        # Encourage diversity by randomly selecting from all actions\n        action_index = np.random.choice(num_actions)  # Allow any action including those not yet tried\n    else:\n        action_index = np.argmax(adjusted_scores)\n        \n    return action_index",
          "objective": 199224238.37713328,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    # Constants for exploration-exploitation dynamics\n    epsilon_initial = 1.0\n    epsilon_minimum = 0.01\n    epsilon_decay = 0.95\n    \n    # Initialize average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        average_scores[action_index] = np.mean(scores) if scores else 0.0\n\n    # Calculate epsilon for exploration\n    epsilon = max(epsilon_minimum, epsilon_initial * (epsilon_decay ** total_selection_count))\n\n    # Calculate exploration bonus for each action\n    exploration_bonus = np.zeros(num_actions)\n    if total_selection_count > 0:\n        for action_index in range(num_actions):\n            if selection_counts[action_index] > 0:\n                # Using standard deviation to provide a bonus for lesser-explored actions\n                exploration_bonus[action_index] = np.sqrt(np.log(total_selection_count) / selection_counts[action_index])\n\n    # Adjust scores by adding exploration bonuses \n    adjusted_scores = average_scores + exploration_bonus\n\n    # Epsilon-greedy action selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(np.arange(num_actions), \n                                         p=selection_counts / np.sum(selection_counts) if total_selection_count > 0 else np.ones(num_actions) / num_actions)\n    else:\n        action_index = np.argmax(adjusted_scores)\n        \n    return action_index",
          "objective": 201571759.5807286,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    initial_epsilon = 1.0\n    min_epsilon = 0.1\n    decay_rate = 0.99\n    \n    # Initialize metrics for actions\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions, dtype=int)\n    score_variance = np.zeros(num_actions)\n\n    # Calculate average scores, selection counts, and variances\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n            score_variance[action_index] = np.var(scores, ddof=1) if len(scores) > 1 else 0\n\n    # Calculate dynamic epsilon\n    exploration_factor = max(min_epsilon, initial_epsilon * (decay_rate ** total_selection_count))\n\n    # Calculate exploration bonuses\n    exploration_bonus = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        if selection_counts[action_index] > 0:\n            exploration_bonus[action_index] = (np.sqrt(np.log(total_selection_count + 1) / (selection_counts[action_index] + 1)) +\n                                                (score_variance[action_index] / (selection_counts[action_index] + 1)))\n        else:\n            # Encourage selection of untried actions\n            exploration_bonus[action_index] = np.inf  # Assign a high value to untried actions\n\n    # Calculate adjusted scores incorporating exploration bonuses\n    adjusted_scores = average_scores + exploration_bonus\n\n    # Epsilon-greedy selection\n    if np.random.rand() < exploration_factor:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 206024958.53743032,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon_initial = 1.0\n    epsilon_minimum = 0.01\n    epsilon_decay = 0.95\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Dynamic epsilon calculation\n    epsilon = max(epsilon_minimum, epsilon_initial * (epsilon_decay ** current_time_slot))\n\n    # Calculate exploration bonus using counts and variance\n    exploration_bonus = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        count = selection_counts[action_index]\n        if count > 0:\n            # Explore based on the number of selections\n            exploration_bonus[action_index] = np.sqrt(np.log(total_selection_count + 1) / (count + 1))\n            # Add variance for additional exploration encouragement\n            if count > 1:\n                exploration_bonus[action_index] += np.std(score_set[action_index]) / np.sqrt(count)\n\n    # Adjust scores with exploration bonus\n    adjusted_scores = average_scores + exploration_bonus\n\n    # Action selection using epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        available_actions = np.where(selection_counts > 0)[0]\n        # Select a random action if there are available actions, else choose randomly\n        action_index = np.random.choice(available_actions) if available_actions.size > 0 else np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 219043525.1779158,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon_initial = 1.0\n    epsilon_minimum = 0.05\n    epsilon_decay = 0.95\n\n    # Initialize average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        average_scores[action_index] = np.mean(scores) if scores else 0.0\n\n    # Dynamic epsilon calculation\n    decay_factor = current_time_slot / total_time_slots\n    epsilon = max(epsilon_minimum, epsilon_initial * (epsilon_decay ** (total_selection_count / (1 + total_selection_count))))\n\n    # Exploration bonus using UCB\n    exploration_bonus = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        if selection_counts[action_index] > 0:\n            exploration_factor = np.sqrt((2 * np.log(total_selection_count)) / selection_counts[action_index])\n            exploration_bonus[action_index] = exploration_factor\n    \n    # Combine scores and apply exploration bonuses\n    combined_scores = average_scores + exploration_bonus\n\n    # Epsilon-greedy action selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(num_actions)\n    else:\n        action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": 227848631.1481732,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon_initial = 0.5  # Initial exploration rate\n    epsilon_minimum = 0.05  # Minimum exploration rate\n    epsilon_decay = 0.99  # Decay factor for exploration\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Update averages and counts based on the score_set\n    for action_index in range(num_actions):\n        if action_index in score_set and score_set[action_index]:\n            scores = score_set[action_index]\n            selection_counts[action_index] = len(scores)\n            average_scores[action_index] = np.mean(scores)\n\n    # Calculate epsilon for exploration\n    epsilon = max(epsilon_minimum, epsilon_initial * (epsilon_decay ** (total_selection_count / num_actions)))\n\n    # Calculate the exploration bonus using selection counts\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-6))\n\n    # Calculate score variance for action adaptability\n    score_variance = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        if selection_counts[action_index] > 1:  # Variance only if more than one score available\n            score_variance[action_index] = np.var(score_set[action_index])\n\n    # Adjust scores with exploration bonus and variance\n    adjusted_scores = average_scores + exploration_bonus - score_variance * 0.5\n\n    # Action selection process with exploration\n    if np.random.rand() < epsilon:\n        # Select randomly from actions that have been tried at least once\n        available_actions = np.where(selection_counts > 0)[0]\n        if available_actions.size > 0:\n            action_index = np.random.choice(available_actions)\n        else:\n            action_index = np.random.randint(num_actions)\n    else:\n        # Select the action with the highest adjusted score\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 239162732.52545714,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    initial_epsilon = 1.0\n    min_epsilon = 0.1\n    decay_rate = 0.995\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions, dtype=int)\n\n    # Compute average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        # Compute average scores if there's any score\n        if len(scores) > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Calculate dynamic epsilon\n    exploration_factor = max(min_epsilon, initial_epsilon * (decay_rate ** total_selection_count))\n    \n    # Score-based adjustment to encourage exploration\n    exploration_bonus = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        if selection_counts[action_index] == 0:\n            exploration_bonus[action_index] = float('inf')  # Encourage selection of untried actions\n        else:\n            exploration_bonus[action_index] = np.sqrt(np.log(total_selection_count) / (selection_counts[action_index] + 1))\n    \n    # Calculate an adjusted score that incorporates the bonus\n    adjusted_scores = average_scores + exploration_bonus\n\n    # Epsilon-greedy selection\n    if np.random.rand() < exploration_factor:\n        action_index = np.random.randint(num_actions)  # Random selection\n    else:\n        action_index = np.argmax(adjusted_scores)  # Greedy selection based on adjusted scores\n\n    return action_index",
          "objective": 251440843.29418343,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon_initial = 0.2  # Initial exploration rate\n    epsilon_minimum = 0.05  # Minimum exploration rate\n\n    # Initialize average scores, selection counts, and variances\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    variances = np.zeros(num_actions)\n\n    # Calculate average scores, counts, and variances for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n            variances[action_index] = np.var(scores, ddof=1)  # Sample variance\n\n    # Dynamically calculate exploration rate based on total selections\n    exploration_percentage = 1 - (current_time_slot / total_time_slots)\n    epsilon = max(epsilon_minimum, epsilon_initial * exploration_percentage)\n\n    # Calculate exploration bonus using the number of selections for confidence\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-6))\n\n    # Consider the variances in the decision-making process\n    diversity_penalty = np.sqrt(variances + 1e-6)  # Smoothing to avoid zeros\n    upper_bounds = average_scores + exploration_bonus - diversity_penalty\n\n    # Action selection using epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        unexplored_actions = np.where(selection_counts == 0)[0]\n        if unexplored_actions.size > 0:\n            action_index = np.random.choice(unexplored_actions)\n        else:\n            action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(upper_bounds)\n\n    return action_index",
          "objective": 253239226.0521555,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon_initial = 0.5  # Initial exploration rate\n    epsilon_minimum = 0.05  # Minimum exploration rate\n    epsilon_decay = 0.95  # Decay over time\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Update averages and counts based on the score_set\n    for action_index in range(num_actions):\n        if action_index in score_set:\n            scores = score_set[action_index]\n            selection_counts[action_index] = len(scores)\n            if selection_counts[action_index] > 0:\n                average_scores[action_index] = np.mean(scores)\n\n    # Dynamic epsilon decay function based on total_selection_count\n    epsilon = max(epsilon_minimum, epsilon_initial * (epsilon_decay ** total_selection_count))\n\n    # Calculate exploration bonus based on selection counts\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-6))\n\n    # Calculate adjusted scores with exploration bonus\n    adjusted_scores = average_scores + exploration_bonus\n\n    # Action selection process with exploration\n    if np.random.rand() < epsilon:\n        # Picking an action either by exploring or by picking among the less chosen actions\n        if np.sum(selection_counts) > 0:\n            available_actions = np.where(selection_counts == 0)[0]\n            if available_actions.size > 0:\n                action_index = np.random.choice(available_actions)\n            else:\n                action_index = np.random.choice(num_actions)\n        else:\n            action_index = np.random.randint(num_actions)\n    else:\n        # Choose the action with the highest adjusted score\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 254913131.16343656,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    initial_epsilon = 1.0\n    min_epsilon = 0.1\n    decay_rate = 0.999\n    exploration_weight = 1.5\n\n    # Initialize arrays for average scores, selection counts, and score variances\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions, dtype=int)\n    score_variance = np.zeros(num_actions)\n\n    # Calculate average scores, selection counts, and score variances\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n            score_variance[action_index] = np.var(scores, ddof=1) if selection_counts[action_index] > 1 else 0\n\n    # Calculate dynamic epsilon\n    exploration_factor = max(min_epsilon, initial_epsilon * (decay_rate ** total_selection_count))\n\n    # Exploration bonuses based on variance and selection counts\n    exploration_bonus = exploration_weight * (np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-10)) + \n                                                (score_variance / (selection_counts + 1e-10)))\n\n    # Adjusted scores for selection\n    adjusted_scores = average_scores + exploration_bonus\n\n    # Epsilon-greedy selection\n    if np.random.rand() < exploration_factor:\n        action_index = np.random.randint(0, num_actions)\n    else:\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 277225295.204814,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon_initial = 1.0\n    epsilon_minimum = 0.01\n    epsilon_decay = 0.95\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    # Update average scores and counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Dynamic epsilon calculation\n    epsilon = max(epsilon_minimum, epsilon_initial * (epsilon_decay ** current_time_slot))\n\n    # Calculate adjusted exploration term (using counts and variance)\n    exploration_bonus = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        if selection_counts[action_index] > 0:\n            exploration_bonus[action_index] = np.sqrt(np.log(total_selection_count + 1) / (selection_counts[action_index] + 1)) \n        if selection_counts[action_index] > 1:\n            exploration_bonus[action_index] += np.std(score_set.get(action_index, []))\n\n    # Adjusted scores incorporate the exploration bonus\n    adjusted_scores = average_scores + exploration_bonus\n\n    # Action selection with epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        available_actions = np.where(selection_counts > 0)[0]\n        if len(available_actions) > 0:\n            action_index = np.random.choice(available_actions)\n        else:\n            action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 281993151.192687,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    initial_epsilon = 1.0\n    min_epsilon = 0.05\n    decay_rate = 0.99\n\n    # Initialize arrays for average scores, selection counts, and score variances\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    score_variance = np.zeros(num_actions)\n\n    # Calculate average scores, selection counts, and variances\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n            score_variance[action_index] = np.var(scores, ddof=1) if len(scores) > 1 else 0\n\n    # Dynamic epsilon calculation based on total selections\n    exploration_factor = max(min_epsilon, initial_epsilon * (decay_rate ** (total_selection_count)))\n    \n    # Calculate exploration bonuses\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1)) + score_variance / (selection_counts + 1)\n    \n    # Adjusted scores\n    adjusted_scores = average_scores + exploration_bonus\n\n    # Epsilon-greedy selection\n    if np.random.rand() < exploration_factor:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 289202107.11299723,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon_initial = 0.5  # Initial exploration rate\n    epsilon_minimum = 0.01  # Minimum exploration rate\n    epsilon_decay = 0.99  # Decay over time\n\n    # Initialize arrays for average scores, selection counts, and variances\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    score_variances = np.zeros(num_actions)\n\n    # Update averages, counts and calculate variances based on the score_set\n    for action_index in range(num_actions):\n        if action_index in score_set:\n            scores = score_set[action_index]\n            selection_counts[action_index] = len(scores)\n            if selection_counts[action_index] > 0:\n                average_scores[action_index] = np.mean(scores)\n                score_variances[action_index] = np.var(scores, ddof=1)  # Sample variance\n\n    # Dynamic epsilon decay function\n    epsilon = max(epsilon_minimum, epsilon_initial * (epsilon_decay ** (total_selection_count / num_actions)))\n\n    # Exploration term adds a score based on the number of selections\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-6))\n\n    # Adjust scores by both average scores and exploration bonus, and penalize high variance actions\n    adjusted_scores = average_scores + exploration_bonus - score_variances / (selection_counts + 1e-6)\n\n    # Action selection process with exploration\n    if np.random.rand() < epsilon:\n        # Pick from actions that have been selected at least once\n        available_actions = np.where(selection_counts > 0)[0]\n        if available_actions.size > 0:\n            action_index = np.random.choice(available_actions)\n        else:\n            action_index = np.random.randint(num_actions)  # fallback if no actions selected\n    else:\n        # Choose the action with the highest adjusted score\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 297883735.61976534,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    initial_epsilon = 1.0\n    min_epsilon = 0.1\n    decay_rate = 0.99\n    \n    # Adjust epsilon based on the current time slot and total time slots\n    exploration_factor = max(min_epsilon, initial_epsilon * (decay_rate ** (total_selection_count / total_time_slots)))\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Compute exploration bonus\n    exploration_bonus = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        if selection_counts[action_index] > 0:\n            exploration_bonus[action_index] = np.sqrt(np.log(total_selection_count + 1) / (selection_counts[action_index] + 1))\n        else:\n            exploration_bonus[action_index] = np.inf  # Zero count encourages selection of untried actions\n\n    # Calculate adjusted scores\n    adjusted_scores = average_scores + exploration_bonus\n\n    # Epsilon-greedy selection\n    if np.random.rand() < exploration_factor:\n        action_index = np.random.choice(num_actions)\n    else:\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 300404321.349166,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon_initial = 0.5  # Initial exploration rate\n    epsilon_minimum = 0.05  # Minimum exploration rate\n    epsilon_decay = 0.99  # Decay over time\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Update averages and counts based on the score_set\n    for action_index in range(num_actions):\n        if action_index in score_set:\n            scores = score_set[action_index]\n            selection_counts[action_index] = len(scores)\n            if selection_counts[action_index] > 0:\n                average_scores[action_index] = np.mean(scores)\n\n    # Dynamic epsilon decay function\n    epsilon = max(epsilon_minimum, epsilon_initial * (epsilon_decay ** (total_selection_count / num_actions)))\n\n    # Calculate exploration bonus based on selection counts\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-6))\n\n    # Calculate adjusted scores with exploration bonus\n    adjusted_scores = average_scores + exploration_bonus\n\n    # Action selection process with exploration\n    if np.random.rand() < epsilon:\n        # Pick from actions that have been selected at least once\n        available_actions = np.where(selection_counts > 0)[0]\n        action_index = np.random.choice(available_actions) if available_actions.size > 0 else np.random.randint(num_actions)\n    else:\n        # Choose the action with the highest adjusted score\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 308897603.0683922,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon_initial = 0.2  # Initial exploration rate\n    epsilon_minimum = 0.05  # Minimum exploration rate\n\n    # Initialize average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and counts for each action\n    for action_index in range(num_actions):\n        if action_index in score_set:\n            scores = score_set[action_index]\n            selection_counts[action_index] = len(scores)\n            average_scores[action_index] = np.mean(scores) if scores else 0.0\n\n    # Adjust exploration rate based on total selections\n    exploration_factor = 1 - total_selection_count / (total_time_slots * num_actions)\n    epsilon = max(epsilon_minimum, epsilon_initial * exploration_factor)\n\n    # Calculate the upper confidence bounds\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-6))\n    upper_bounds = average_scores + exploration_bonus\n\n    # Action selection\n    if np.random.rand() < epsilon:\n        # Explore: Choose randomly among less frequently selected actions\n        unexplored_actions = np.where(selection_counts < np.median(selection_counts[selection_counts > 0]))[0]\n        if unexplored_actions.size > 0:\n            action_index = np.random.choice(unexplored_actions)\n        else:\n            action_index = np.random.choice(num_actions)  # Randomly select from all actions\n    else:\n        # Exploit: Select the action with the highest upper bound\n        action_index = np.argmax(upper_bounds)\n\n    return action_index",
          "objective": 322076175.66652524,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon_initial = 0.5  \n    epsilon_minimum = 0.05  \n    epsilon_decay = 0.99  \n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores for each action and selection counts\n    for action_index, scores in score_set.items():\n        selection_counts[action_index] = len(scores)\n        \n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Calculate the current epsilon value\n    epsilon = max(epsilon_minimum, epsilon_initial * (epsilon_decay ** (total_selection_count / num_actions)))\n\n    # Calculate the upper confidence bound\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-6))\n    bounds = average_scores + exploration_bonus\n\n    # Action selection using epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        available_actions = np.where(selection_counts > 0)[0]\n        action_index = np.random.choice(available_actions) if available_actions.size > 0 else np.random.choice(num_actions)\n    else:\n        action_index = np.argmax(bounds)\n\n    return action_index",
          "objective": 338562173.3568069,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon_initial = 0.2  # Initial exploration rate\n    epsilon_minimum = 0.05  # Minimum exploration rate\n\n    # Initialize average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    # Compute average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Dynamically calculate exploration rate based on total selections\n    exploration_percentage = 1 - (current_time_slot / total_time_slots)\n    epsilon = max(epsilon_minimum, epsilon_initial * exploration_percentage)\n\n    # Weight for exploration based on selection counts\n    selection_weights = np.where(selection_counts == 0, 1, 1 / selection_counts)\n    selection_weights /= np.sum(selection_weights)  # Normalize to sum to 1\n\n    # Combine average scores and selection weights\n    combined_scores = average_scores + selection_weights\n\n    # Epsilon-Greedy Action Selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(num_actions, p=selection_weights)\n    else:\n        action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": 340504088.6651375,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon_initial = 0.5\n    epsilon_minimum = 0.1\n    epsilon_decay = 0.99\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Update averages and counts\n    for action_index in score_set:\n        scores = score_set[action_index]\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Calculate epsilon\n    epsilon = max(epsilon_minimum, epsilon_initial * (epsilon_decay ** total_selection_count))\n\n    # Calculate exploration bonus\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-6))\n\n    # Compute adjusted scores\n    adjusted_scores = average_scores + exploration_bonus\n\n    # Action selection\n    if np.random.rand() < epsilon:\n        # Explore: choose an action that has been selected at least once, or any random action if no prior selection\n        available_actions = np.where(selection_counts > 0)[0]\n        if len(available_actions) > 0:\n            action_index = np.random.choice(available_actions)\n        else:\n            action_index = np.random.randint(num_actions)\n    else:\n        # Exploit: choose the action with the highest adjusted score\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 372982048.48412555,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon_initial = 0.5\n    epsilon_minimum = 0.05\n    epsilon_decay = 0.99\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    # Update average scores and counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Dynamic epsilon calculation\n    epsilon = max(epsilon_minimum, epsilon_initial * (epsilon_decay ** current_time_slot))\n\n    # Variance calculation to encourage exploration\n    variances = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        if selection_counts[action_index] > 1:\n            variances[action_index] = np.var(score_set.get(action_index, []))\n    \n    # Exploration bonus based on counts and variance\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-6)) + variances\n    \n    # Adjusted scores incorporate exploration bonus\n    adjusted_scores = average_scores + exploration_bonus\n\n    # Action selection process with epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        available_actions = np.where(selection_counts > 0)[0]\n        action_index = np.random.choice(available_actions) if available_actions.size > 0 else np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 380453970.94267154,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon_initial = 0.5  # Higher initial exploration rate\n    epsilon_minimum = 0.05  # Minimum exploration rate\n    decay_factor = 0.95  # Decay factor for epsilon\n    variance_bonus_factor = 0.1  # Factor to encourage high variance selections\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    # Calculate average scores and counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Adjust exploration rate based on total selections\n    epsilon = max(epsilon_minimum, epsilon_initial * (decay_factor ** current_time_slot))\n\n    # Calculate the variance for each action's scores\n    score_variances = np.zeros(num_actions)\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        if selection_counts[action_index] > 0:\n            score_variances[action_index] = np.var(scores)\n\n    # Calculate exploration bonuses\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-6))\n    \n    # Adjust exploration bonuses with score variance\n    variance_bonus = variance_bonus_factor * score_variances\n\n    # Compute upper confidence bounds\n    upper_bounds = average_scores + exploration_bonus + variance_bonus\n\n    # Action selection\n    if np.random.rand() < epsilon:\n        # Explore: Randomly select from all actions\n        action_index = np.random.randint(num_actions)\n    else:\n        # Exploit: Select the action with the highest upper bound\n        action_index = np.argmax(upper_bounds)\n\n    return action_index",
          "objective": 409647960.690824,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    initial_epsilon = 1.0\n    min_epsilon = 0.1\n    decay_rate = 0.995\n    \n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions, dtype=int)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        if scores:\n            average_scores[action_index] = np.mean(scores)\n\n    # Calculate dynamic epsilon\n    exploration_factor = max(min_epsilon, initial_epsilon * (decay_rate ** total_selection_count))\n\n    # Calculate UCB-based exploration bonuses\n    bonuses = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        if selection_counts[action_index] > 0:\n            bonuses[action_index] = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[action_index])\n        else:\n            bonuses[action_index] = float('inf')  # Encourage exploration of unselected actions\n\n    # Influence of the score and selection count\n    adjusted_scores = average_scores + bonuses\n\n    # Epsilon-greedy action selection\n    if np.random.rand() < exploration_factor:\n        action_index = np.random.choice(num_actions, p=(1 - exploration_factor) * np.ones(num_actions) + exploration_factor / num_actions)\n    else:\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 473565301.4665036,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon_initial = 0.5  # Initial exploration rate\n    epsilon_minimum = 0.05  # Minimum exploration rate\n    epsilon_decay = 0.95  # Decay factor for exploration rate\n\n    # Initialize arrays for average scores, selection counts, and variances\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    score_variances = np.zeros(num_actions)\n\n    # Calculate average scores and variances for each action\n    for action_index in range(num_actions):\n        if action_index in score_set:\n            scores = score_set[action_index]\n            selection_counts[action_index] = len(scores)\n            \n            if selection_counts[action_index] > 0:\n                average_scores[action_index] = np.mean(scores)\n                score_variances[action_index] = np.var(scores) if len(scores) > 1 else 0.0\n\n    # Adjust exploration rate based on total selections\n    epsilon = max(epsilon_minimum, epsilon_initial * (epsilon_decay ** (total_selection_count / num_actions)))\n\n    # Calculate upper confidence bounds incorporating variance\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-6))\n    bounds = average_scores + exploration_bonus + 0.1 * score_variances\n\n    # Action selection\n    if np.random.rand() < epsilon:\n        # Explore: Select uniformly among available actions (non-zero selection counts)\n        available_actions = np.where(selection_counts > 0)[0]\n        action_index = np.random.choice(available_actions) if available_actions.size > 0 else np.random.choice(num_actions)\n    else:\n        # Exploit: Select the action with the highest adjusted score\n        action_index = np.argmax(bounds)\n\n    return action_index",
          "objective": 559788548.8024925,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon_minimum = 0.05  # Minimum exploration rate\n    epsilon_initial = 1.0  # Initial exploration rate\n    \n    # Initialize average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    # Calculate average scores and counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        average_scores[action_index] = np.mean(scores) if scores else 0.0\n\n    # Dynamic exploration rate based on total selections\n    epsilon = max(epsilon_minimum, epsilon_initial * (1 - total_selection_count / total_time_slots))\n    \n    # Calculate exploration bonus: UCB1 formula\n    exploration_bonus = np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1e-6))\n    \n    # Combine average scores with exploration bonus\n    upper_bounds = average_scores + exploration_bonus\n\n    # Action selection: explore or exploit\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(num_actions)  # Uniform exploration\n    else:\n        action_index = np.argmax(upper_bounds)  # Exploit best action\n    \n    return action_index",
          "objective": 563019797.8881199,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    initial_epsilon = 0.2  # Initial exploration rate\n    min_epsilon = 0.05  # Minimum exploration rate\n\n    # Initialize average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    # Calculate average scores and counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        average_scores[action_index] = np.mean(scores) if scores else 0.0\n\n    # Adjust exploration rate based on total selections\n    epsilon = max(min_epsilon, initial_epsilon * (1 - total_selection_count / (total_time_slots * num_actions)))\n\n    # Calculate the upper confidence bound (UCB) for each action\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-6))\n    upper_bounds = average_scores + exploration_bonus\n\n    # Action selection\n    if np.random.rand() < epsilon:\n        # Explore: Randomly select an action among those that have been explored before\n        explored_actions = np.where(selection_counts > 0)[0]\n        if explored_actions.size > 0:\n            action_index = np.random.choice(explored_actions)\n        else:\n            action_index = np.random.randint(num_actions)\n    else:\n        # Exploit: Select the action with the highest upper bound\n        action_index = np.argmax(upper_bounds)\n    \n    return action_index",
          "objective": 580691532.5727193,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon_initial = 0.3  # Initial exploration rate\n    epsilon_minimum = 0.05  # Minimum exploration rate\n\n    # Initialize average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Dynamic calculation of exploration rate based on total selections\n    exploration_percentage = 1 - (current_time_slot / total_time_slots)\n    epsilon = max(epsilon_minimum, epsilon_initial * exploration_percentage)\n\n    # Calculate exploration bonus using selection counts\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-6))\n\n    # Weighted success score considering average scores and exploration bonus\n    weighted_scores = average_scores + exploration_bonus\n\n    # Action selection using epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        unexplored_actions = np.where(selection_counts == 0)[0]\n        if unexplored_actions.size > 0:\n            action_index = np.random.choice(unexplored_actions)\n        else:\n            action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(weighted_scores)\n\n    return action_index",
          "objective": 598866951.8177748,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon_initial = 0.2  # Initial exploration rate\n    epsilon_minimum = 0.05  # Minimum exploration rate\n\n    # Initialize average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Dynamically calculate exploration rate based on total selections\n    exploration_percentage = 1 - (current_time_slot / total_time_slots)\n    epsilon = max(epsilon_minimum, epsilon_initial * exploration_percentage)\n\n    # Calculate exploration bonus using the number of selections for confidence\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-6))\n    \n    # Calculate upper confidence bounds\n    upper_bounds = average_scores + exploration_bonus\n\n    # Action selection using epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        unexplored_actions = np.where(selection_counts == 0)[0]\n        if unexplored_actions.size > 0:\n            action_index = np.random.choice(unexplored_actions)\n        else:\n            action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(upper_bounds)\n\n    return action_index",
          "objective": 611167665.3701444,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon_initial = 0.5  # Initial exploration rate\n    epsilon_minimum = 0.05  # Minimum exploration rate\n    decay_rate = 0.99  # Decay rate for exploration factor\n    exploration_bonus_factor = 0.1  # Factor to encourage exploration\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Compute average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Adjust exploration rate\n    epsilon = max(epsilon_minimum, epsilon_initial * (decay_rate ** current_time_slot))\n\n    # Calculate exploration bonuses based on selection counts\n    exploration_bonus = exploration_bonus_factor * np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-6))\n\n    # Compute adjusted scores\n    adjusted_scores = average_scores + exploration_bonus\n\n    # Action selection\n    if np.random.rand() < epsilon:\n        # Explore: Randomly select from all actions\n        action_index = np.random.randint(num_actions)\n    else:\n        # Exploit: Select the action with the highest adjusted score\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 611589684.5258412,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon_initial = 0.2  # Initial exploration rate\n    epsilon_minimum = 0.05  # Minimum exploration rate\n\n    # Initialize average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and counts for each action\n    for action_index in range(num_actions):\n        if action_index in score_set:\n            scores = score_set[action_index]\n            selection_counts[action_index] = len(scores)\n            if selection_counts[action_index] > 0:\n                average_scores[action_index] = np.mean(scores)\n\n    # Adjust exploration rate based on total selections\n    exploration_factor = 1 - total_selection_count / (total_time_slots * num_actions)\n    epsilon = max(epsilon_minimum, epsilon_initial * exploration_factor)\n\n    # Calculate the upper confidence bound for each action\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-6))\n    upper_bounds = average_scores + exploration_bonus\n\n    # Action selection\n    if np.random.rand() < epsilon:\n        # Explore: Select randomly among less frequently selected actions or uniformly\n        unexplored_actions = np.where(selection_counts == 0)[0]\n        if unexplored_actions.size > 0:\n            action_index = np.random.choice(unexplored_actions)\n        else:\n            action_index = np.random.choice(num_actions)  # Random among all if all explored\n    else:\n        # Exploit: Select the action with the highest upper bound\n        action_index = np.argmax(upper_bounds)\n\n    return action_index",
          "objective": 620928932.4543929,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon_minimum = 0.05  # Minimum exploration rate\n    exploration_factor = 1.0  # Base factor for exploration scaling\n    \n    # Initialize average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        average_scores[action_index] = np.mean(scores) if scores else 0.0\n\n    # Dynamic exploration rate based on total selections\n    epsilon = max(epsilon_minimum, 1.0 - (total_selection_count / total_time_slots))\n\n    # Calculate exploration bonus using selection counts\n    exploration_bonus = np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1))\n\n    # Combine average scores with exploration bonus\n    upper_bounds = average_scores + exploration_factor * exploration_bonus\n\n    # Action selection: choose whether to explore or exploit\n    if np.random.rand() < epsilon:\n        # Explore: select an action that has been explored less\n        low_count_actions = np.where(selection_counts < 3)[0]  # Less explored threshold\n        if len(low_count_actions) > 0:\n            action_index = np.random.choice(low_count_actions)\n        else:\n            action_index = np.random.choice(num_actions)\n    else:\n        # Exploit: select the action with the highest upper bound\n        action_index = np.argmax(upper_bounds)\n\n    return action_index",
          "objective": 622430107.2960545,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon_initial = 0.2\n    epsilon_minimum = 0.05\n\n    # Initialize average scores and counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Adaptive exploration rate\n    exploration_percentage = 1 - (current_time_slot / total_time_slots)\n    epsilon = max(epsilon_minimum, epsilon_initial * exploration_percentage)\n\n    # Exploration bonus\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-6))\n\n    # Calculate upper confidence bounds\n    upper_bounds = average_scores + exploration_bonus\n\n    # Epsilon-greedy action selection\n    if np.random.rand() < epsilon:\n        unexplored_actions = np.where(selection_counts == 0)[0]\n        if unexplored_actions.size > 0:\n            action_index = np.random.choice(unexplored_actions)\n        else:\n            action_index = np.random.choice(num_actions)\n    else:\n        action_index = np.argmax(upper_bounds)\n\n    return action_index",
          "objective": 632522277.4750034,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon_initial = 0.2  # Initial exploration rate\n    epsilon_minimum = 0.05  # Minimum exploration rate\n\n    # Initialize average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Dynamically calculate exploration rate based on total selections\n    exploration_percentage = 1 - (current_time_slot / total_time_slots)\n    epsilon = max(epsilon_minimum, epsilon_initial * exploration_percentage)\n\n    # Calculate upper confidence bounds for each action\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-6))\n    upper_bounds = average_scores + exploration_bonus\n\n    # Action selection based on exploration vs exploitation\n    if np.random.rand() < epsilon:\n        unexplored_actions = np.where(selection_counts == 0)[0]\n        if unexplored_actions.size > 0:\n            action_index = np.random.choice(unexplored_actions)\n        else:\n            action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(upper_bounds)\n\n    return action_index",
          "objective": 638064919.172405,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon_minimum = 0.05  # Minimum exploration rate\n    epsilon_initial = 1.0  # Initial exploration rate\n    \n    # Initialize average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        average_scores[action_index] = np.mean(scores) if scores else 0.0\n\n    # Dynamic exploration rate\n    epsilon = max(epsilon_minimum, epsilon_initial * (1 - total_selection_count / total_time_slots))\n\n    # Calculate exploration bonus using a refined approach\n    with np.errstate(divide='ignore'):  # Suppress warnings for division by zero\n        exploration_bonus = np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1e-6))\n\n    # Combined score with exploration bonus\n    upper_bounds = average_scores + exploration_bonus\n\n    # Action selection process\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(num_actions)  # Explore: Random selection\n    else:\n        action_index = np.argmax(upper_bounds)  # Exploit: Best action based on upper bounds\n    \n    return action_index",
          "objective": 649745160.6444756,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon_initial = 0.3  # Initial exploration rate\n    epsilon_minimum = 0.05  # Minimum exploration rate\n\n    # Initialize average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if len(scores) > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Adjust exploration rate based on total selections\n    exploration_factor = 1 - total_selection_count / (total_time_slots * num_actions)\n    epsilon = max(epsilon_minimum, epsilon_initial * exploration_factor)\n\n    # Calculate potential reward and exploration bonus\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-6))\n    potential_reward = average_scores + exploration_bonus\n\n    # Action selection\n    if np.random.rand() < epsilon:\n        # Explore: Select randomly among all actions\n        action_index = np.random.choice(num_actions)\n    else:\n        # Exploit: Select the action with the highest potential reward\n        action_index = np.argmax(potential_reward)\n\n    return action_index",
          "objective": 670357660.2623267,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon_initial = 0.2  # Initial exploration rate\n    epsilon_minimum = 0.05  # Minimum exploration rate\n\n    # Initialize average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Adjust exploration rate based on total selections\n    epsilon = max(epsilon_minimum, epsilon_initial * (1 - total_selection_count / (total_time_slots * num_actions)))\n\n    # Calculate the upper confidence bound for each action\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-6))\n    upper_bounds = average_scores + exploration_bonus\n\n    # Action selection\n    if np.random.rand() < epsilon:\n        # Explore: Randomly select an action\n        unexplored_actions = np.where(selection_counts == 0)[0]\n        if len(unexplored_actions) > 0:\n            action_index = np.random.choice(unexplored_actions)\n        else:\n            action_index = np.random.randint(num_actions)\n    else:\n        # Exploit: Select the action with the highest upper bound\n        action_index = np.argmax(upper_bounds)\n    \n    return action_index",
          "objective": 724714471.389124,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon_initial = 0.5  # Initial exploration rate\n    epsilon_minimum = 0.05  # Minimum exploration rate\n\n    # Initialize average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Adjust exploration rate using a nonlinear decay based on total selections\n    decay_factor = (total_selection_count / (total_time_slots * num_actions)) ** 1.5\n    epsilon = max(epsilon_minimum, epsilon_initial * (1 - decay_factor))\n\n    # Calculate the upper confidence bound for each action\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-6))\n    upper_bounds = average_scores + exploration_bonus\n\n    # Action selection\n    if np.random.rand() < epsilon:\n        # Explore: Randomly select an action\n        unexplored_actions = np.where(selection_counts == 0)[0]\n        if unexplored_actions.size > 0:\n            action_index = np.random.choice(unexplored_actions)\n        else:\n            action_index = np.random.randint(num_actions)\n    else:\n        # Exploit: Select the action with the highest upper bound\n        action_index = np.argmax(upper_bounds)\n    \n    return action_index",
          "objective": 779647040.5155439,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon_initial = 0.5  # Initial exploration rate\n    epsilon_minimum = 0.05  # Minimum exploration rate\n    epsilon_decay = 0.99  # Decay factor for exploration rate\n    \n    # Initialize average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    # Calculate average scores and counts for each action\n    for action_index in range(num_actions):\n        if action_index in score_set:\n            scores = score_set[action_index]\n            selection_counts[action_index] = len(scores)\n            if selection_counts[action_index] > 0:\n                average_scores[action_index] = np.mean(scores)\n\n    # Adjust exploration rate based on total selections\n    exploration_factor = epsilon_initial * (epsilon_decay ** (total_selection_count // num_actions))\n    epsilon = max(epsilon_minimum, exploration_factor)\n\n    # Calculate the upper confidence bound for each action\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-6))\n    upper_bounds = average_scores + exploration_bonus\n    \n    # Action selection\n    if np.random.rand() < epsilon:\n        # Explore: Select uniformly among available actions\n        action_index = np.random.choice(num_actions)\n    else:\n        # Exploit: Select the action with the highest upper bound\n        action_index = np.argmax(upper_bounds)\n        \n    return action_index",
          "objective": 798614563.0388052,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    base_epsilon = 0.2  # Start with a higher exploration rate\n    min_epsilon = 0.05   # Minimum exploration rate\n    \n    # Initialize selection counts and average scores\n    selection_counts = np.zeros(num_actions)\n    average_scores = np.zeros(num_actions)\n\n    # Calculate selection counts and average scores\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n    \n    # Dynamic epsilon based on the number of total selections\n    epsilon = max(min_epsilon, base_epsilon * (1 - total_selection_count / (total_time_slots + 1e-6)))\n\n    # Calculate upper confidence bounds\n    confidence_intervals = np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1e-6))\n    upper_bound_scores = average_scores + confidence_intervals\n\n    # Select action based on exploration-exploitation trade-off\n    if np.random.rand() < epsilon:\n        # Explore: select a random action\n        action_index = np.random.randint(num_actions)\n    else:\n        # Exploit: select action with the highest upper confidence bound\n        action_index = np.argmax(upper_bound_scores)\n\n    return action_index",
          "objective": 914817670.0478156,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    base_epsilon = 0.1  # Base exploration rate\n    min_epsilon = 0.01   # Minimum exploration rate\n\n    # Initialize average scores, selection counts, and variances\n    selection_counts = np.zeros(num_actions)\n    average_scores = np.zeros(num_actions)\n    variances = np.zeros(num_actions)\n\n    # Calculate average scores, variances, and counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        counts = len(scores)\n        selection_counts[action_index] = counts\n        if counts > 0:\n            average_scores[action_index] = np.mean(scores)\n            variances[action_index] = np.var(scores) if counts > 1 else 0\n\n    # Epsilon decay based on time slot\n    epsilon = max(min_epsilon, base_epsilon * (1 - current_time_slot / total_time_slots))\n\n    # Calculate confidence intervals and upper confidence bounds for each action\n    confidence_intervals = np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1e-6))\n    upper_bound_scores = average_scores + confidence_intervals + variances\n\n    # Select action based on the exploration-exploitation trade-off\n    if np.random.rand() < epsilon:\n        # Explore: select a random action\n        action_index = np.random.randint(num_actions)\n    else:\n        # Exploit: select action with the highest upper confidence bound\n        action_index = np.argmax(upper_bound_scores)\n\n    return action_index",
          "objective": 948496939.8048342,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    initial_epsilon = 1.0\n    min_epsilon = 0.1\n    epsilon_decay = 0.99  # Decay factor for epsilon after each action selection\n\n    # Initialize average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        average_scores[action_index] = np.mean(scores) if scores else 0.0\n\n    # Calculate dynamic epsilon\n    epsilon = max(min_epsilon, initial_epsilon * (epsilon_decay ** current_time_slot))\n    \n    # Adjust the exploration rate based on total selection count\n    exploration_factor = max(1, np.log(total_selection_count + 1))  # Smooth log growth\n\n    # Adjusted scores incorporating exploration\n    adjusted_scores = average_scores + (exploration_factor / (1 + selection_counts)) * (1 - epsilon)\n\n    # Epsilon-greedy action selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(num_actions)\n    else:\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 990145641.2803209,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon_initial = 0.5  # Initial exploration rate\n    epsilon_minimum = 0.05  # Minimum exploration rate\n    epsilon_decay = 0.99  # Decay over time\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Update averages and counts\n    for action_index in range(num_actions):\n        if action_index in score_set:\n            scores = score_set[action_index]\n            selection_counts[action_index] = len(scores)\n            if selection_counts[action_index] > 0:\n                average_scores[action_index] = np.mean(scores)\n\n    # Dynamic epsilon decay function considering current_time_slot\n    epsilon = max(epsilon_minimum, epsilon_initial * (epsilon_decay ** ((total_selection_count + 1) / (num_actions * current_time_slot + 1))))\n    \n    # Calculate exploration bonus based on selection counts\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-6))\n    \n    # Calculate adjusted scores with exploration bonus\n    adjusted_scores = average_scores + exploration_bonus\n\n    # Action selection process\n    if np.random.rand() < epsilon:\n        available_actions = np.where(selection_counts > 0)[0]\n        action_index = np.random.choice(available_actions) if available_actions.size > 0 else np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 996894304.3567879,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon_initial = 0.2  # Initial exploration rate\n    epsilon_minimum = 0.05  # Minimum exploration rate\n    decay_factor = 0.99  # Factor to decrease epsilon over time\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Adjust exploration rate based on total selections\n    epsilon = max(epsilon_minimum, epsilon_initial * (decay_factor ** current_time_slot))\n\n    # Calculate exploration bonuses\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-6))\n\n    # Compute upper confidence bounds\n    upper_bounds = average_scores + exploration_bonus\n\n    # Action selection\n    if np.random.rand() < epsilon:\n        # Explore: Randomly select from all actions\n        action_index = np.random.randint(num_actions)\n    else:\n        # Exploit: Select the action with the highest upper bound\n        action_index = np.argmax(upper_bounds)\n\n    return action_index",
          "objective": 1215559113.5416188,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon = max(0.1 * (total_time_slots - current_time_slot) / total_time_slots, 0.01)\n\n    # Initialize average scores and counts\n    average_scores = np.zeros(num_actions)\n    counts = np.zeros(num_actions)\n\n    # Calculate average scores and counts for each action\n    for action in range(num_actions):\n        if action in score_set and score_set[action]:\n            average_scores[action] = np.mean(score_set[action])\n            counts[action] = len(score_set[action])\n    \n    # Handle cases where action has never been selected\n    total_selection_count = total_selection_count + 1e-5  # To avoid division by zero in exploration bonus\n\n    # Epsilon-greedy decision making\n    if np.random.rand() < epsilon or total_selection_count == 0:\n        action_index = np.random.randint(0, num_actions)  # Explore: select a random action\n    else:\n        # Exploit: select action with the highest score plus exploration bonus\n        exploration_bonus = np.sqrt(np.log(total_selection_count) / (counts + 1e-5))\n        total_scores = average_scores + exploration_bonus\n        action_index = np.argmax(total_scores)\n\n    return action_index",
          "objective": 1236971994.2160017,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon_initial = 0.2  # Initial exploration rate\n    epsilon_minimum = 0.05  # Minimum exploration rate\n\n    # Initialize average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Adjust exploration rate based on total selections\n    epsilon = max(epsilon_minimum, epsilon_initial * (1 - total_selection_count / (total_time_slots * num_actions)))\n\n    # Calculate the upper confidence bound for each action\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-6))\n    upper_bounds = average_scores + exploration_bonus\n\n    # Action selection\n    if np.random.rand() < epsilon:\n        # Explore: Randomly select an action that has been selected less frequently\n        unexplored_actions = np.where(selection_counts == 0)[0]\n        if unexplored_actions.size > 0:\n            action_index = np.random.choice(unexplored_actions)\n        else:\n            action_index = np.random.randint(num_actions)\n    else:\n        # Exploit: Select the action with the highest upper bound\n        action_index = np.argmax(upper_bounds)\n    \n    return action_index",
          "objective": 1266313823.6237092,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon_initial = 0.2  # Initial exploration rate\n    epsilon_minimum = 0.05  # Minimum exploration rate\n    exploration_factor = 1.5  # Factor to balance exploration and exploitation\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Adjust exploration rate based on total selections\n    epsilon = max(epsilon_minimum, epsilon_initial * (1 - total_selection_count / (total_time_slots * num_actions)))\n\n    # Calculate exploration bonuses\n    exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-6))\n    \n    # Compute upper confidence bounds\n    upper_bounds = average_scores + exploration_bonus\n\n    # Action selection\n    if np.random.rand() < epsilon:\n        # Explore: Randomly select from underexplored actions first\n        unexplored_actions = np.where(selection_counts == 0)[0]\n        if unexplored_actions.size > 0:\n            action_index = np.random.choice(unexplored_actions)\n        else:\n            action_index = np.random.randint(num_actions)\n    else:\n        # Exploit: Select the action with the highest upper bound\n        action_index = np.argmax(upper_bounds)\n    \n    return action_index",
          "objective": 1309512210.8910778,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    base_exploration_rate = 0.1  # Base exploration rate\n    min_exploration_rate = 0.01   # Minimum exploration rate\n\n    # Initialize selection counts and average scores\n    selection_counts = np.zeros(num_actions)\n    average_scores = np.zeros(num_actions)\n\n    # Calculate average scores and counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        average_scores[action_index] = np.mean(scores) if scores else 0.0\n\n    # Adaptive exploration rate based on time slot\n    exploration_rate = max(min_exploration_rate, base_exploration_rate * \n                           (1 - (current_time_slot / total_time_slots)))\n\n    # Compute confidence intervals and upper confidence bounds\n    confidence_intervals = np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1e-6))\n    upper_bound_scores = average_scores + confidence_intervals\n\n    # Exploration-exploitation decision\n    if np.random.rand() < exploration_rate:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(upper_bound_scores)\n\n    return action_index",
          "objective": 1343750487.3267715,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    # Implement a dynamic epsilon value based on the total selection count\n    epsilon = max(0.1, 1 - (total_selection_count / (total_time_slots + 1)))  # Minimum exploration rate\n\n    # Use a softmax approach for action selection\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        if scores:\n            average_scores[action_index] = np.mean(scores)\n            selection_counts[action_index] = len(scores)\n\n    # Normalize selection counts to prevent division by zero\n    normalized_counts = selection_counts / (selection_counts.sum() + 1e-6)\n\n    # Calculate adjusted scores based on average scores and exploration factor\n    adjusted_scores = average_scores * (1 - normalized_counts) + epsilon * np.random.rand(num_actions)\n\n    # Select the action with the highest adjusted score\n    action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 1355643543.1671731,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    initial_epsilon = 0.2  # Initial exploration rate\n    min_epsilon = 0.05  # Minimum exploration rate\n\n    # Initialize average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        average_scores[action_index] = np.mean(scores) if scores else 0.0\n\n    # Adjust exploration rate based on total selections\n    epsilon = max(min_epsilon, initial_epsilon * (1 - total_selection_count / (total_time_slots * num_actions)))\n\n    # Calculate the upper confidence bound (UCB) for each action\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-6))\n    upper_bounds = average_scores + exploration_bonus\n\n    # Action selection\n    if np.random.rand() < epsilon:\n        # Explore: Randomly select an action among those that have been explored before\n        explored_actions = np.where(selection_counts > 0)[0]\n        if explored_actions.size > 0:\n            action_index = np.random.choice(explored_actions)\n        else:\n            action_index = np.random.randint(num_actions)\n    else:\n        # Exploit: Select the action with the highest upper bound\n        action_index = np.argmax(upper_bounds)\n\n    return action_index",
          "objective": 1453305539.8778305,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon = max(0.1 * (total_time_slots - current_time_slot) / total_time_slots, 0.01)  # Dynamic epsilon\n\n    # Initialize average scores and counts\n    average_scores = np.zeros(num_actions)\n    counts = np.zeros(num_actions)\n\n    # Calculate average scores for each action\n    for action in range(num_actions):\n        if action in score_set and score_set[action]:  # Check if action has scores\n            average_scores[action] = np.mean(score_set[action])\n            counts[action] = len(score_set[action])\n    \n    # Epsilon-greedy decision making\n    if np.random.rand() < epsilon or total_selection_count == 0:\n        # Explore: Select a random action\n        action_index = np.random.randint(0, num_actions)\n    else:\n        # Exploit: Select action with the highest average score\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (counts + 1e-5))  # +1 to avoid division by zero\n        total_scores = average_scores + exploration_bonus\n        action_index = np.argmax(total_scores)\n\n    return action_index",
          "objective": 1493673546.210477,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon_initial = 1.0\n    epsilon_minimum = 0.05\n    epsilon_decay = 0.9\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Update average scores and counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Adjust epsilon decay based on current time slot\n    epsilon = max(epsilon_minimum, epsilon_initial * (epsilon_decay ** (current_time_slot / total_time_slots)))\n\n    # Calculate exploration bonuses based on selection counts and variance\n    exploration_bonus = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        if selection_counts[action_index] > 0:\n            exploration_bonus[action_index] = np.sqrt(np.log(total_selection_count + 1) / (selection_counts[action_index] + 1)) \n        if selection_counts[action_index] > 1:\n            exploration_bonus[action_index] += np.std(score_set.get(action_index, []))\n\n    # Adjusted scores\n    adjusted_scores = average_scores + exploration_bonus\n\n    # Action selection with epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        # Exploration: select randomly among less explored actions\n        action_index = np.random.choice(np.flatnonzero(selection_counts == 0) if np.any(selection_counts == 0) else np.arange(num_actions))\n    else:\n        # Exploitation: select action with highest adjusted score\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 1506405788.2368934,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    initial_epsilon = 0.2  # Initial exploration rate\n    minimum_epsilon = 0.05  # Minimum exploration rate\n    decay_rate = 0.99  # Decay rate for epsilon over time\n\n    # Initialize average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Dynamically adjust exploration rate\n    epsilon = max(minimum_epsilon, initial_epsilon * (decay_rate ** current_time_slot))\n\n    # Calculate the upper confidence bound for each action\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-6))\n    upper_bounds = average_scores + exploration_bonus\n\n    # Action selection\n    if np.random.rand() < epsilon:\n        # Explore: Select a random action, prioritizing less frequently chosen actions\n        unexplored_actions = np.where(selection_counts == 0)[0]\n        if unexplored_actions.size > 0:\n            action_index = np.random.choice(unexplored_actions)\n        else:\n            action_index = np.random.choice(num_actions)\n    else:\n        # Exploit: Select the action with the highest upper bound\n        action_index = np.argmax(upper_bounds)\n    \n    return action_index",
          "objective": 1592139855.5065749,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    base_epsilon = 0.1  # Base exploration rate\n    min_epsilon = 0.01   # Minimum exploration rate\n\n    # Initialize average scores and selection counts\n    selection_counts = np.zeros(num_actions)\n    average_scores = np.zeros(num_actions)\n\n    # Calculate average scores and counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if scores:\n            average_scores[action_index] = np.mean(scores)\n\n    # Epsilon decay based on time slot\n    epsilon = max(min_epsilon, base_epsilon * (1 - current_time_slot / total_time_slots))\n\n    # Calculate confidence intervals and upper confidence bounds for each action\n    confidence_intervals = np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1e-6))\n    upper_bound_scores = average_scores + confidence_intervals\n\n    # Select action based on the exploration-exploitation trade-off\n    if np.random.rand() < epsilon:\n        # Explore: select a random action\n        action_index = np.random.randint(num_actions)\n    else:\n        # Exploit: select action with the highest upper confidence bound\n        action_index = np.argmax(upper_bound_scores)\n\n    return action_index",
          "objective": 1662000575.5173624,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon_base = 0.2  # Increased base exploration rate for more exploration initially\n    epsilon_minimum = 0.05  # Slightly higher minimum exploration rate to maintain exploration\n\n    # Initialize average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Adjust exploration rate based on total selections and time slot\n    epsilon = max(epsilon_minimum, epsilon_base * (1 - (total_selection_count / (total_time_slots * num_actions))))\n\n    # Calculate confidence intervals for each action\n    confidence_intervals = np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1e-6))\n    upper_bounds = average_scores + confidence_intervals\n\n    # Decide to explore or exploit\n    if np.random.rand() < epsilon:\n        # Explore: Randomly select an action\n        action_index = np.random.randint(num_actions)\n    else:\n        # Exploit: Select the action with the highest upper confidence bound\n        action_index = np.argmax(upper_bounds)\n\n    return action_index",
          "objective": 1663812318.0597255,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon_base = 0.1  # Base exploration rate\n    epsilon_minimum = 0.01  # Minimum exploration rate\n\n    # Initialize average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Adjust exploration rate based on total selections\n    epsilon = max(epsilon_minimum, epsilon_base * (1 - current_time_slot / total_time_slots))\n\n    # Calculate confidence intervals for each action to inform selection\n    confidence_intervals = np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1e-6))\n    upper_bounds = average_scores + confidence_intervals\n\n    # Decide whether to explore or exploit\n    if np.random.rand() < epsilon:\n        # Explore: Randomly select an action\n        action_index = np.random.randint(num_actions)\n    else:\n        # Exploit: Select the action with the highest upper confidence bound\n        action_index = np.argmax(upper_bounds)\n\n    return action_index",
          "objective": 1681607305.1998367,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    base_epsilon = 0.1  # Base exploration rate\n    min_epsilon = 0.01  # Minimum exploration rate\n\n    # Initialize average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        average_scores[action_index] = np.mean(scores) if scores else 0.0\n\n    # Calculate dynamic epsilon based on current time and history\n    epsilon = max(min_epsilon, base_epsilon * (1 - current_time_slot / total_time_slots))\n\n    # Use a log function for exploration bonus\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n\n    # Calculate adjusted scores\n    adjusted_scores = average_scores + exploration_bonus\n\n    # Determine whether to explore or exploit\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 1703891217.785438,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    base_epsilon = 0.1  # Base epsilon for exploration\n    min_epsilon = 0.01  # Minimum epsilon value to avoid too little exploration\n\n    # Initialize average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Calculate a dynamic epsilon value based on selection history\n    epsilon = max(min_epsilon, base_epsilon * (1 - current_time_slot / total_time_slots))\n    \n    # Calculate bonus for exploration as a factor of total selections\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1))\n\n    # Calculate adjusted scores that favor less-selected actions\n    adjusted_scores = average_scores + exploration_bonus\n\n    # Decide whether to explore or exploit\n    if np.random.rand() < epsilon:\n        # Explore: Randomly select an action\n        action_index = np.random.randint(num_actions)\n    else:\n        # Exploit: Select the action with the highest adjusted score\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 1746784599.0624325,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon = 0.1  # Exploration rate\n\n    # Calculate average scores for each action\n    average_scores = np.zeros(num_actions)\n    counts = np.zeros(num_actions)\n\n    for action in range(num_actions):\n        if action in score_set and score_set[action]:\n            average_scores[action] = np.mean(score_set[action])\n            counts[action] = len(score_set[action])\n\n    # Implement epsilon-greedy strategy\n    if np.random.rand() < epsilon or total_selection_count == 0:\n        # Explore: Select a random action\n        action_index = np.random.randint(0, num_actions)\n    else:\n        # Exploit: Select action with the highest average score, with consideration of counts\n        exploration_bonus = np.sqrt(np.log(total_selection_count) / (counts + 1e-5))  # Add small value to avoid division by zero\n        total_scores = average_scores + exploration_bonus\n        action_index = np.argmax(total_scores)\n\n    return action_index",
          "objective": 1785090307.4270353,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon_initial = 0.5  # Initial exploration rate\n    epsilon_minimum = 0.05  # Minimum exploration rate\n    epsilon_decay = 0.995  # Decay rate for epsilon\n\n    # Initialize arrays for average scores, selection counts, and variances\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    score_variance = np.zeros(num_actions)\n\n    # Update averages and counts based on the score_set\n    for action_index in range(num_actions):\n        if action_index in score_set and score_set[action_index]:  # Ensure scores list is non-empty\n            scores = score_set[action_index]\n            selection_counts[action_index] = len(scores)\n            average_scores[action_index] = np.mean(scores)\n            score_variance[action_index] = np.var(scores)\n\n    # Dynamic epsilon decay function\n    epsilon = max(epsilon_minimum, epsilon_initial * (epsilon_decay ** (total_selection_count / num_actions)))\n\n    # Calculate exploration bonus based on selection counts and variances\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-6)) * np.sqrt(score_variance + 1e-6)\n\n    # Calculate adjusted scores with exploration bonus\n    adjusted_scores = average_scores + exploration_bonus\n\n    # Action selection process with exploration\n    if np.random.rand() < epsilon:\n        avail_actions = np.where(selection_counts > 0)[0]\n        if avail_actions.size > 0:\n            action_index = np.random.choice(avail_actions)\n        else:\n            action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 1813752174.4796672,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon_initial = 0.2  # Initial exploration rate\n    epsilon_minimum = 0.05  # Minimum exploration rate\n\n    # Initialize average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Adjust exploration rate based on total selections\n    epsilon = max(epsilon_minimum, epsilon_initial * (1 - (total_selection_count / (total_time_slots * num_actions))))\n\n    # Upper confidence bound calculation with a small value to avoid division by zero\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-6))\n    upper_bounds = average_scores + exploration_bonus\n\n    # Epsilon-Greedy action selection\n    if np.random.rand() < epsilon:\n        unexplored_actions = np.where(selection_counts == 0)[0]\n        if unexplored_actions.size > 0:\n            action_index = np.random.choice(unexplored_actions)\n        else:\n            action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(upper_bounds)\n    \n    return action_index",
          "objective": 1854097663.2648816,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon = max(0.1 * (total_time_slots - current_time_slot) / total_time_slots, 0.01)  # Dynamic epsilon\n\n    # Initialize average scores, counts, and a flag for unexplored actions\n    average_scores = np.zeros(num_actions)\n    counts = np.zeros(num_actions)\n    unexplored_actions = []\n\n    # Calculate average scores for each action and identify unexplored actions\n    for action in range(num_actions):\n        if action in score_set and score_set[action]:  # Action has scores\n            average_scores[action] = np.mean(score_set[action])\n            counts[action] = len(score_set[action])\n        else:\n            unexplored_actions.append(action)\n    \n    # Epsilon-greedy decision making with the possibility of exploring unexplored actions\n    if np.random.rand() < epsilon or total_selection_count == 0 or unexplored_actions:\n        # Explore: Select a random unexplored action if available, else select a random action\n        if unexplored_actions:\n            action_index = np.random.choice(unexplored_actions)\n        else:\n            action_index = np.random.randint(0, num_actions)\n    else:\n        # Exploit: Select action with the highest average score\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (counts + 1e-5))  # +1e-5 to avoid division by zero\n        total_scores = average_scores + exploration_bonus\n        action_index = np.argmax(total_scores)\n\n    return action_index",
          "objective": 1906494249.102078,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon_base = 0.1  # Base epsilon for exploration\n\n    # Initialize average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Apply a small value for unselected actions to avoid division by zero in epsilon\n    selection_counts[selection_counts == 0] = 1e-6\n    \n    # Calculate the exploration probability (epsilon) based on selection history\n    epsilon = epsilon_base * (1 - current_time_slot / total_time_slots)  # Linearly decrease epsilon\n    \n    # Decide whether to explore or exploit\n    if np.random.rand() < epsilon:\n        # Explore: Randomly select an action\n        action_index = np.random.randint(num_actions)\n    else:\n        # Exploit: Select the action with the highest average score\n        action_index = np.argmax(average_scores)\n\n    return action_index",
          "objective": 2027835185.0582168,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n\n    # Initialize average scores and counts\n    average_scores = np.zeros(num_actions)\n    counts = np.zeros(num_actions)\n\n    for action in range(num_actions):\n        if action in score_set and score_set[action]:\n            average_scores[action] = np.mean(score_set[action])\n            counts[action] = len(score_set[action])\n\n    # Use UCB to select an action\n    if total_selection_count == 0:\n        # If no selections have been made yet, choose a random action\n        action_index = np.random.randint(0, num_actions)\n    else:\n        ucb_values = average_scores + np.sqrt(2 * np.log(total_selection_count + 1) / (counts + 1e-5))\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 2049544440.5144467,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    \n    # Define epsilon dynamically based on total selection count\n    epsilon = max(0.05, 1 - (total_selection_count / (total_time_slots * 0.5 + 1)))  # Minimum epsilon\n\n    # Initialize average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores for each action while handling empty score lists\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        if scores:\n            average_scores[action_index] = np.mean(scores)\n            selection_counts[action_index] = len(scores)\n\n    # Implement softmax normalization of the average scores\n    adjusted_scores = average_scores + (epsilon * np.random.rand(num_actions))\n\n    # Select the action based on exploration or average score\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(np.arange(num_actions), p=(selection_counts / (selection_counts.sum() + 1e-6)))\n    else:\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 2166587680.693474,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    # Set epsilon value that decreases over time for reduced exploration as more selections are made\n    epsilon = max(0.1, 1 - (total_selection_count / (total_time_slots + 1)))  # Minimum exploration rate\n    \n    # Randomly select an action with probability epsilon to encourage exploration\n    if np.random.rand() < epsilon:\n        return np.random.randint(0, num_actions)\n    \n    # Calculate average scores for each action\n    average_scores = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        if scores:\n            average_scores[action_index] = np.mean(scores)\n        else:\n            average_scores[action_index] = 0.0  # Handle zero scores case\n    \n    # Select the action with the maximum average score\n    action_index = np.argmax(average_scores)\n    \n    return action_index",
          "objective": 2278089755.3772264,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n\n    # Define epsilon using a decay based on total selections\n    epsilon = max(0.1, 1 - (total_selection_count / (total_time_slots + 1)))\n\n    # Explore or exploit decision\n    if np.random.rand() < epsilon:\n        return np.random.randint(num_actions)\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Populate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        average_scores[action_index] = np.mean(scores) if scores else 0.0\n\n    # Compute adjusted score using UCB technique for exploration\n    adjusted_scores = average_scores + np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1))\n\n    # Select the action with the maximum adjusted score\n    action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 2398882344.2462015,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon_initial = 0.2  # Initial exploration rate\n    epsilon_minimum = 0.05  # Minimum exploration rate\n    decay_rate = 0.999  # Decay rate for epsilon over time\n    \n    # Initialize average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Calculate dynamic exploration rate based on time slot progression\n    epsilon = max(epsilon_minimum, epsilon_initial * (decay_rate ** current_time_slot))\n\n    # Calculate the upper confidence bounds for each action\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-6))\n    upper_bounds = average_scores + exploration_bonus\n\n    # Action selection\n    if np.random.rand() < epsilon:\n        # Explore: Select a random action, favoring less selected actions\n        unexplored_actions = np.where(selection_counts == 0)[0]\n        if unexplored_actions.size > 0:\n            action_index = np.random.choice(unexplored_actions)\n        else:\n            action_index = np.random.choice(num_actions)\n    else:\n        # Exploit: Select the action with the highest upper bound\n        action_index = np.argmax(upper_bounds)\n    \n    return action_index",
          "objective": 2423500434.450109,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    base_epsilon = 0.2  # Base exploration rate\n    min_epsilon = 0.05   # Minimum exploration rate\n    decay_factor = 0.98  # Decay factor for exploration rate\n\n    # Initialize selection counts and average scores\n    selection_counts = np.zeros(num_actions)\n    average_scores = np.zeros(num_actions)\n\n    # Calculate average scores and counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        average_scores[action_index] = np.mean(scores) if scores else 0.0\n\n    # Epsilon decay based on selection count\n    epsilon = max(min_epsilon, base_epsilon * (decay_factor ** current_time_slot))\n    \n    # Calculate upper confidence bounds\n    confidence_intervals = np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1e-6))\n    upper_bound_scores = average_scores + confidence_intervals\n    \n    # Exploration with probability epsilon\n    if np.random.rand() < epsilon:\n        # Explore: select a random action\n        action_index = np.random.randint(num_actions)\n    else:\n        # Exploit: select action with the highest upper confidence bound\n        action_index = np.argmax(upper_bound_scores)\n\n    return action_index",
          "objective": 2428068554.671296,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n\n    # Calculate epsilon linearly decaying with an adaptive factor\n    epsilon = max(0.1, 1.0 - (total_selection_count / (total_time_slots + 1)))\n\n    # Decide whether to explore (random choice) or exploit (best-known action)\n    if np.random.rand() < epsilon:\n        return np.random.randint(num_actions)\n\n    # Initialize arrays to track average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        average_scores[action_index] = np.mean(scores) if scores else 0.0\n    \n    # Use safe division to avoid division by zero, adding a small constant to selection counts\n    safe_selection_counts = selection_counts + 1e-5\n\n    # Compute adjusted scores considering exploration factor (like UCB)\n    adjusted_scores = average_scores + np.sqrt(np.log(total_selection_count + 1) / safe_selection_counts)\n\n    # Select the action with the maximum adjusted score\n    action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 2746011652.524268,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon_minimum = 0.05  # Minimum exploration rate\n    \n    # Initialize average scores and selection counts\n    average_scores = np.empty(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    # Calculate average scores and counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        average_scores[action_index] = np.mean(scores) if scores else 0.0\n\n    # Dynamic exploration rate based on total selections\n    epsilon = max(epsilon_minimum, 1.0 - (total_selection_count / total_time_slots))\n    \n    # Calculate exploration bonus: utilize selection counts for less explored actions\n    exploration_bonus = np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1e-6))\n    \n    # Combine average scores with exploration bonus for upper confidence bound\n    upper_bounds = average_scores + exploration_bonus\n\n    # Action selection: choose whether to explore or exploit\n    if np.random.rand() < epsilon:\n        # Explore: select a random action while considering selection counts\n        low_count_actions = np.where(selection_counts < 3)[0]  # Threshold to consider less explored\n        if len(low_count_actions) > 0:\n            action_index = np.random.choice(low_count_actions)\n        else:\n            action_index = np.random.choice(num_actions)\n    else:\n        # Exploit: select the action with the highest upper bound\n        action_index = np.argmax(upper_bounds)\n\n    return action_index",
          "objective": 2885831281.5667434,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    \n    # Initialize average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        if scores:\n            average_scores[action_index] = np.mean(scores)\n            selection_counts[action_index] = len(scores)\n\n    # Avoid division by zero for unselected actions\n    selection_counts[selection_counts == 0] = 1e-6  # Small value to prevent division by zero\n    \n    # Use Upper Confidence Bound (UCB) for action selection\n    total_count = total_selection_count + 1  # To avoid zero division when calculating UCB\n    ucb_values = average_scores + np.sqrt((2 * np.log(total_count)) / selection_counts)\n\n    # Select action with the highest UCB value\n    action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 2886439959.850394,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    initial_epsilon = 0.2  # Initial exploration rate\n    min_epsilon = 0.05  # Minimum exploration rate\n\n    # Initialize average scores, selection counts, and variances\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    variances = np.zeros(num_actions)\n\n    # Calculate average scores, counts, and variances for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if scores:\n            average_scores[action_index] = np.mean(scores)\n            variances[action_index] = np.var(scores, ddof=1)  # Sample variance\n        else:\n            average_scores[action_index] = 0.0\n            variances[action_index] = 0.0\n\n    # Adjust exploration rate based on total selections\n    epsilon = max(min_epsilon, initial_epsilon * (1 - total_selection_count / (total_time_slots * num_actions)))\n\n    # Calculate the upper confidence bound (UCB) with variance to encourage exploration\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-6))\n    \n    # Calculate a weighted score using variance\n    weighted_scores = average_scores + exploration_bonus - np.sqrt(variances + 1e-6)\n\n    # Action selection\n    if np.random.rand() < epsilon:\n        # Explore: Randomly select an action among those that have been explored before\n        explored_actions = np.where(selection_counts > 0)[0]\n        if explored_actions.size > 0:\n            action_index = np.random.choice(explored_actions)\n        else:\n            action_index = np.random.randint(num_actions)\n    else:\n        # Exploit: Select the action with the highest weighted score\n        action_index = np.argmax(weighted_scores)\n    \n    return action_index",
          "objective": 3410676509.4192395,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    # Define epsilon dynamically, with a decay pattern based on selection count\n    epsilon = max(0.05, 1 - (total_selection_count / (total_time_slots + 1)))  # Minimum exploration rate\n\n    # Initialize average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts for each action\n    for action_index, scores in score_set.items():\n        if scores:\n            average_scores[action_index] = np.mean(scores)\n            selection_counts[action_index] = len(scores)\n\n    # Normalize selection counts to avoid division by zero\n    total_counts = selection_counts.sum() + 1e-6\n    normalized_counts = selection_counts / total_counts\n\n    # Compute adjusted scores using normalized counts for exploration adjustments\n    adjusted_scores = average_scores * (1 - normalized_counts) + epsilon * np.random.rand(num_actions)\n\n    # Explore with the configured probability epsilon or exploit the best action\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(num_actions)  # Select a random action for exploration\n    else:\n        action_index = np.argmax(adjusted_scores)  # Select the action with the highest adjusted score\n\n    return action_index",
          "objective": 3531769219.735592,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    if total_selection_count == 0:\n        # Randomly select an action if no selections have been made yet\n        return np.random.randint(0, 8)\n    \n    # Calculate average scores for each action\n    average_scores = {}\n    for action_index, scores in score_set.items():\n        if scores:\n            average_scores[action_index] = np.mean(scores)\n        else:\n            average_scores[action_index] = 0.0  # Handle zero scores case\n    \n    # Determine the exploration factor\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n    \n    # Calculate selection probabilities based on historical performance and exploration\n    selection_probabilities = {}\n    for action_index in range(8):\n        avg_score = average_scores.get(action_index, 0.0)\n        selection_count = score_set.get(action_index, [])\n        normalized_count = len(selection_count) / total_selection_count\n        \n        # Combine exploitation (avg_score) and exploration (1 - normalized_count) weighted by exploration factor\n        selection_probabilities[action_index] = (avg_score * (1 - normalized_count) + \n                                                  exploration_factor * (1 - normalized_count))\n    \n    # Normalize selection probabilities to obtain a distribution\n    total_probability = sum(selection_probabilities.values())\n    selection_probabilities = {k: v / total_probability for k, v in selection_probabilities.items()}\n    \n    # Use the probabilities to select an action\n    actions = list(selection_probabilities.keys())\n    probabilities = list(selection_probabilities.values())\n    \n    action_index = np.random.choice(actions, p=probabilities)\n    \n    return action_index",
          "objective": 3698563767.1413016,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    \n    # Initialize average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        if scores:\n            average_scores[action_index] = np.mean(scores)\n            selection_counts[action_index] = len(scores)\n\n    # Handle cases where an action has never been selected\n    for action_index in range(num_actions):\n        if selection_counts[action_index] == 0:\n            # Assign a high value to unselected actions to encourage exploration\n            average_scores[action_index] = 1.0  # Highest possible score\n\n    # Calculate UCB values\n    total_selections = total_selection_count + 1  # Adding 1 to avoid division by zero\n    ucb_values = average_scores + np.sqrt((2 * np.log(total_selections)) / (selection_counts + 1e-6))\n\n    # Select the action with the highest UCB value\n    action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 4049613565.8479853,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    \n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    # Calculate average scores and counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n    \n    # Calculate a confidence metric to prioritize exploration vs exploitation\n    confidence_metric = np.zeros(num_actions)\n    exploration_factor = np.sqrt(total_selection_count + 1)  # Smooth over number of selections\n    \n    for action_index in range(num_actions):\n        if selection_counts[action_index] == 0:\n            confidence_metric[action_index] = np.inf  # Unexplored actions\n        else:\n            # Confidence = average score + exploration term decreasing with selection count\n            confidence_metric[action_index] = average_scores[action_index] + (\n                1.0 / np.sqrt(selection_counts[action_index])\n            )\n\n    # Selection probability based on exploration\n    exploration_probability = max(0.05, min(1.0, (total_time_slots - current_time_slot) / total_time_slots))\n\n    # Random exploration vs greedy exploitation\n    if np.random.rand() < exploration_probability:\n        action_index = np.random.choice(np.arange(num_actions))  # Random action\n    else:\n        action_index = np.argmin(confidence_metric)  # Choose action with highest confidence\n\n    return action_index",
          "objective": 4518357774.510015,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon = max(0.1 * (total_time_slots - current_time_slot) / total_time_slots, 0.01)  # Decay epsilon over time\n\n    # Initialize average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    counts = np.zeros(num_actions)\n\n    # Fill average_scores and counts\n    for action in range(num_actions):\n        if action in score_set and score_set[action]:\n            average_scores[action] = np.mean(score_set[action])\n            counts[action] = len(score_set[action])\n    \n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon or total_selection_count == 0:\n        action_index = np.random.randint(0, num_actions)  # Explore\n    else:\n        # Calculate exploration bonuses\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (counts + 1e-5))  # Add small value to avoid division by zero\n        total_scores = average_scores + exploration_bonus * (1 / (counts + 1e-5))  # Adjust for fewer selections\n        action_index = np.argmax(total_scores)  # Exploit\n\n    return action_index",
          "objective": 4727032889.967783,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon_base = 0.2  # Base exploration rate\n    epsilon_minimum = 0.05  # Minimum exploration rate\n\n    # Initialize average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Adjust exploration rate based on total selections and time slot\n    exploration_factor = epsilon_base * (1 - current_time_slot / total_time_slots)\n    exploration_factor = max(epsilon_minimum, exploration_factor * (1 + np.log(total_selection_count + 1)))\n    epsilon = min(exploration_factor, 1.0)  # Ensure epsilon is at most 1.0\n\n    # Calculate upper confidence bounds\n    confidence_intervals = np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1e-6))\n    upper_bounds = average_scores + confidence_intervals\n\n    # Decide whether to explore or exploit\n    if np.random.rand() < epsilon:\n        # Explore: Randomly select an action\n        action_index = np.random.randint(num_actions)\n    else:\n        # Exploit: Select the action with the highest upper confidence bound\n        action_index = np.argmax(upper_bounds)\n\n    return action_index",
          "objective": 5103186091.522921,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    \n    # Calculate average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Calculate an exploration factor based on the number of times actions have been selected\n    exploration_factor = np.sqrt((np.log(total_selection_count + 1) + 1) / (selection_counts + 1e-6))\n    \n    # Weighted score\n    weighted_scores = average_scores + exploration_factor\n    \n    # Softmax to ensure probabilistic selection\n    exp_scores = np.exp(weighted_scores - np.max(weighted_scores))  # Stability\n    softmax_probs = exp_scores / np.sum(exp_scores)\n\n    # Select action based on softmax probabilities\n    action_index = np.random.choice(np.arange(num_actions), p=softmax_probs)\n\n    return action_index",
          "objective": 5126503209.743253,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon_initial = 0.3  # Initial exploration rate\n    epsilon_minimum = 0.05  # Minimum exploration rate\n    decay_factor = 0.99  # Decay factor for exploration rate\n\n    # Initialize average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if scores:\n            average_scores[action_index] = np.mean(scores)\n\n    # Adjust the exploration rate\n    epsilon = max(epsilon_minimum, epsilon_initial * (decay_factor ** (current_time_slot / total_time_slots)))\n\n    # Calculate the upper confidence bounds\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-6))\n    upper_bounds = average_scores + exploration_bonus\n\n    # Action selection\n    if np.random.rand() < epsilon:\n        # Explore: Randomly select an action\n        if np.any(selection_counts == 0):  # If some actions have not been selected\n            unexplored_actions = np.where(selection_counts == 0)[0]\n            action_index = np.random.choice(unexplored_actions)\n        else:\n            action_index = np.random.randint(num_actions)\n    else:\n        # Exploit: Select the action with the highest upper bound\n        action_index = np.argmax(upper_bounds)\n\n    return action_index",
          "objective": 5460894424.887528,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate the average score for each action\n    avg_scores = np.array([np.mean(scores) if len(scores) > 0 else 0.0 for scores in score_set.values()])\n    \n    # Calculate exploration factor\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))  # Decrease exploration over time\n    \n    # Generate random value for exploration\n    if np.random.rand() < epsilon:\n        # Exploration: Choose an action randomly\n        action_index = np.random.randint(0, 8)\n    else:\n        # Exploitation: Choose the action with the highest average score\n        action_index = np.argmax(avg_scores)\n\n    return action_index",
          "objective": 5951588455.303448,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    base_epsilon = 0.1  # Base exploration rate\n    min_epsilon = 0.01   # Minimum exploration rate\n\n    # Initialize arrays for scores, selection counts and variance\n    selection_counts = np.zeros(num_actions)\n    average_scores = np.zeros(num_actions)\n    score_variances = np.zeros(num_actions)\n\n    # Calculate average scores and counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if scores:\n            average_scores[action_index] = np.mean(scores)\n            score_variances[action_index] = np.var(scores)  # Calculate variance\n\n    # Epsilon decay based on current time slot\n    epsilon = max(min_epsilon, base_epsilon * (1 - current_time_slot / total_time_slots))\n\n    # Confidence intervals based on selection counts\n    confidence_intervals = np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1e-6))\n    \n    # Calculate upper confidence bounds using both mean and variance\n    upper_bound_scores = average_scores + confidence_intervals + np.sqrt(score_variances)\n\n    # Select action based on the epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        # Explore: select a random action\n        action_index = np.random.randint(num_actions)\n    else:\n        # Exploit: select action with the highest upper confidence bound\n        action_index = np.argmax(upper_bound_scores)\n\n    return action_index",
          "objective": 6047834056.885417,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\nimport random\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration probability\n\n    # Determine if we will explore or exploit\n    if random.random() < epsilon:\n        # Exploration: randomly select an action\n        action_index = random.randint(0, 7)\n    else:\n        # Exploitation: calculate average scores and select the best action\n        avg_scores = []\n        for action_index in range(8):\n            scores = score_set.get(action_index, [])\n            if len(scores) > 0:\n                avg_score = np.mean(scores)\n            else:\n                avg_score = 0  # Default to 0 if no scores are available\n            \n            avg_scores.append(avg_score)\n\n        # Select the action with the highest average score\n        action_index = int(np.argmax(avg_scores))\n    \n    return action_index",
          "objective": 6196238702.039829,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon_initial = 1.0\n    epsilon_minimum = 0.1\n    epsilon_decay_rate = 0.95\n\n    # Initialize arrays to store average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        average_scores[action_index] = np.mean(scores) if scores else 0.0\n\n    # Dynamic epsilon value for exploration\n    epsilon = max(epsilon_minimum, epsilon_initial * (epsilon_decay_rate ** total_selection_count))\n\n    # Calculate exploration bonus based on selection frequency\n    exploration_bonus = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        if selection_counts[action_index] > 0:\n            confidence_interval = np.sqrt(np.log(total_selection_count + 1) / (selection_counts[action_index] + 1))\n            exploration_bonus[action_index] = confidence_interval\n\n    # Combine average scores and exploration bonus\n    adjusted_scores = average_scores + exploration_bonus\n\n    # Epsilon-greedy action selection using adjusted scores\n    if np.random.rand() < epsilon:\n        weighted_probabilities = selection_counts / (selection_counts.sum() + 1e-5)  # Prevent division by zero\n        action_index = np.random.choice(np.arange(num_actions), p=weighted_probabilities)\n    else:\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 6290829809.46476,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    exploration_factor = 2.0  # UCB coefficient\n    decay_factor = (current_time_slot + 1) / total_time_slots  # Decay factor based on time slot\n\n    # Initialize average scores and counts\n    average_scores = np.zeros(num_actions)\n    counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action in range(num_actions):\n        if action in score_set and score_set[action]:\n            average_scores[action] = np.mean(score_set[action])\n            counts[action] = len(score_set[action])\n\n    # Modified UCB selection with decay\n    ucb_values = np.zeros(num_actions)\n\n    for action in range(num_actions):\n        if counts[action] > 0:\n            ucb_values[action] = (average_scores[action] * decay_factor) + \\\n                                 exploration_factor * np.sqrt(np.log(total_selection_count + 1) / counts[action])\n        else:\n            ucb_values[action] = float('inf')  # Prioritize actions not selected yet\n\n    action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 6330867477.791445,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_rate = max(0.1, 1 - (total_selection_count / (total_time_slots * 0.5)))\n    \n    avg_scores = []\n    for action_index in range(8):\n        if action_index in score_set and score_set[action_index]:\n            avg_score = np.mean(score_set[action_index])\n        else:\n            avg_score = 0\n        avg_scores.append(avg_score)\n\n    scores_with_exploration = np.array(avg_scores) * (1 - exploration_rate) + np.random.rand(8) * exploration_rate\n     \n    action_index = np.argmax(scores_with_exploration)\n    \n    return action_index",
          "objective": 6450676318.873311,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Define a dynamic epsilon based on the exploration-exploitation trade-off\n    epsilon = max(0.05, 0.5 * (1 - total_selection_count / total_time_slots))\n    \n    # Calculate average scores for each action, handling cases with no history\n    avg_scores = np.zeros(8)\n    for action_index in range(8):\n        if action_index in score_set and score_set[action_index]:\n            avg_scores[action_index] = np.mean(score_set[action_index])\n    \n    # Decide whether to explore or exploit\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(range(8))  # Explore: randomly select an action\n    else:\n        action_index = np.argmax(avg_scores)  # Exploit: select the action with the highest average score\n    \n    return action_index",
          "objective": 6721349464.044484,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize the average scores and selection counts\n    avg_scores = np.array([np.mean(scores) if len(scores) > 0 else 0.0 for scores in score_set.values()])\n    action_counts = np.array([len(scores) for scores in score_set.values()])\n\n    # Epsilon for exploration\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n    \n    # Calculate exploration bonuses based on how many times the action has been selected\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1e-6))\n    \n    # Combined score with exploration bonus\n    combined_scores = avg_scores + exploration_bonus\n    \n    # Generate random value for epsilon-greedy choice\n    if np.random.rand() < epsilon:\n        # Exploration: Choose an action randomly\n        action_index = np.random.randint(0, 8)\n    else:\n        # Exploitation: Choose the action with the highest combined score\n        action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": 7201099205.104615,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate the average score for each action, handling cases with no scores\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    \n    # Handling division by zero for calculating action confidence\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n\n    # Calculate the exploration factor (epsilon)\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n\n    # Initialize the exploration mask: Array to indicate if we should explore\n    should_explore = np.random.rand() < epsilon\n\n    if should_explore:\n        # Random exploration: select an action uniformly at random\n        action_index = np.random.randint(0, 8)\n    else:\n        # Exploitation: Select based on average score and selection confidence\n        # Use a confidence measure: avg_scores + sqrt(2 * log(total_selection_count) / selection_counts)\n        confidence_scores = avg_scores + np.sqrt((2 * np.log(total_selection_count + 1) / (selection_counts + 1e-5)))\n\n        # Select the action with the highest confidence score\n        action_index = np.argmax(confidence_scores)\n\n    return action_index",
          "objective": 7434808029.377623,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores and counts for actions\n    average_scores = {}\n    action_counts = {}\n    \n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        average_scores[action_index] = np.mean(scores) if scores else 0.0\n    \n    # Calculate exploration factor\n    exploration_factor = [(total_selection_count / (action_counts[action_index] + 1)) for action_index in range(8)]\n    \n    # Calculate selection scores: combine average scores and exploration factor\n    selection_scores = {}\n    for action_index in range(8):\n        selection_scores[action_index] = average_scores[action_index] + exploration_factor[action_index] * (1 - (current_time_slot / total_time_slots))\n    \n    # Select action with the highest selection score\n    action_index = max(selection_scores, key=selection_scores.get)\n    \n    return action_index",
          "objective": 7598838662.848671,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    \n    # Adaptive epsilon calculation to encourage exploration initially and stabilize as time goes on\n    if total_selection_count == 0:\n        epsilon = 1.0  # Initial full exploration\n    else:\n        epsilon = max(0.1, 1 - (total_selection_count / (total_time_slots + 1))) \n\n    # Probability decision for exploration vs exploitation\n    if np.random.rand() < epsilon:\n        return np.random.randint(0, num_actions)\n\n    # Calculate average scores and selection counts with safeguards against empty lists\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        average_scores[action_index] = np.mean(scores) if scores else 0.0\n\n    # Adjust scores based on the number of times selected, encouraging exploration\n    adjusted_scores = average_scores * (1 / (selection_counts + 1))  # Normalize by selection counts\n\n    # Select the action with the maximum adjusted score\n    action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 7626820797.982684,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    if total_selection_count == 0:\n        # Randomly select an action if no selections have been made yet\n        return np.random.randint(0, 8)\n    \n    # Calculate average scores for each action\n    average_scores = np.array([\n        np.mean(score_set.get(i, [0.0])) for i in range(8)\n    ])\n\n    # Account for number of selections for each action to avoid division by zero\n    selection_counts = np.array([len(score_set.get(i, [])) for i in range(8)])\n    \n    # Calculate exploration bonus based on the remaining time slots\n    exploration_bonus = (total_time_slots - current_time_slot) / total_time_slots\n\n    # Normalize the selection counts to prevent bias towards lesser-selected actions\n    normalized_counts = selection_counts / total_selection_count\n\n    # Utility function for combined selection strategy\n    combined_scores = (average_scores * (1 - normalized_counts) +\n                       exploration_bonus * (1 - normalized_counts))\n\n    # Apply softmax to the combined scores to derive a probability distribution\n    exp_scores = np.exp(combined_scores - np.max(combined_scores))  # Stability improvement\n    selection_probabilities = exp_scores / np.sum(exp_scores)\n\n    # Use the probabilities to select an action\n    action_index = np.random.choice(range(8), p=selection_probabilities)\n    \n    return action_index",
          "objective": 7952812159.182256,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters\n    exploration_rate = 0.1  # 10% of the time explore\n    \n    # Calculate the average scores for each action\n    average_scores = []\n    for action_idx in range(8):  # Actions 0 to 7\n        scores = score_set.get(action_idx, [])\n        if scores:\n            average_score = np.mean(scores)\n        else:\n            average_score = 0.0  # Handle cases with no scores\n        \n        average_scores.append(average_score)\n    \n    # Decision making: exploration vs. exploitation\n    if np.random.rand() < exploration_rate:\n        # Explore: randomly select an action\n        action_index = np.random.randint(0, 8)\n    else:\n        # Exploit: select the action with the maximum average score\n        action_index = np.argmax(average_scores)\n    \n    return action_index",
          "objective": 7964363777.612345,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    exploration_factor = 2  # UCB coefficient\n   \n    # Initialize average scores and counts\n    average_scores = np.zeros(num_actions)\n    counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action in range(num_actions):\n        if action in score_set and score_set[action]:\n            average_scores[action] = np.mean(score_set[action])\n            counts[action] = len(score_set[action])\n\n    # UCB selection\n    ucb_values = np.zeros(num_actions)\n\n    for action in range(num_actions):\n        if counts[action] > 0:\n            ucb_values[action] = average_scores[action] + \\\n                                 exploration_factor * np.sqrt(np.log(total_selection_count) / counts[action])\n        else:\n            ucb_values[action] = float('inf')  # Prioritize actions not selected yet\n\n    action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 8002102768.180278,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon = max(0.1, 1 - (total_selection_count / (total_time_slots + 1)))  # Minimum exploration rate\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        average_scores[action_index] = np.mean(scores) if selection_counts[action_index] > 0 else 0\n\n    # Use weighted scores for exploration\n    beta = 1.0 / (selection_counts + 1e-6)  # Weight inversely by selection counts\n    adjusted_scores = average_scores * (1 - epsilon) + epsilon * np.random.rand(num_actions) * beta\n\n    # Select the action with the highest adjusted score\n    action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 8203674591.463642,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    \n    # Epsilon-decay strategy for exploration\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n\n    # Decide to explore or exploit\n    if np.random.rand() < epsilon:\n        return np.random.randint(0, num_actions)\n    \n    # Compute average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        average_scores[action_index] = np.mean(scores) if scores else 0.0\n\n    # Bonus for underexplored actions\n    exploration_bonus = (1 / (selection_counts + 1)) * 0.5  # Weigh exploration slightly\n    \n    # Combine scores and exploration\n    total_scores = average_scores + exploration_bonus\n\n    # Select the action with the maximum total score\n    action_index = np.argmax(total_scores)\n\n    return action_index",
          "objective": 8405194279.617317,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Adjusting the epsilon based on the current time slot to reduce exploration over time\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)  # Ensure a minimum epsilon\n\n    # Determine if we will explore or exploit\n    if np.random.rand() < epsilon:\n        # Exploration: select an action randomly\n        action_index = np.random.randint(0, 8)\n    else:\n        # Exploitation: calculate average scores and selection counts\n        avg_scores = np.zeros(8)\n        selection_counts = np.zeros(8)\n        \n        for action_index in range(8):\n            scores = score_set.get(action_index, [])\n            selection_counts[action_index] = len(scores)\n            if selection_counts[action_index] > 0:\n                avg_scores[action_index] = np.mean(scores)\n\n        # Adjust scores with exploration bonus based on selection counts\n        exploration_bonus = np.sqrt((np.log(total_selection_count + 1) / (selection_counts + 1e-5)))  # Add small value to avoid division by zero\n        adjusted_scores = avg_scores + exploration_bonus\n\n        # Select the action with the highest adjusted score\n        action_index = int(np.argmax(adjusted_scores))\n\n    return action_index",
          "objective": 8568224838.862551,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate the average scores and count selections for each action\n    avg_scores = np.zeros(8)\n    counts = np.zeros(8)\n    \n    for action_index, scores in score_set.items():\n        if scores:\n            avg_scores[action_index] = np.mean(scores)\n            counts[action_index] = len(scores)\n    \n    # Calculate exploration factor\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))  # Decrease exploration over time\n\n    # Compute a 'confidence' score for each action by considering both average scores and selection counts\n    if total_selection_count > 0:\n        confidence_scores = avg_scores + (1 / (1 + counts)) * (1.0 - avg_scores) * epsilon\n    else:\n        confidence_scores = avg_scores  # If there are no selections, fallback to average scores\n\n    # Choose action based on exploration-exploitation strategy\n    if np.random.rand() < epsilon:\n        # Exploration: Choose an action randomly, but prefer actions with fewer selections\n        unselected_actions = np.where(counts == 0)[0]\n        if len(unselected_actions) > 0:\n            action_index = np.random.choice(unselected_actions)\n        else:\n            action_index = np.random.randint(0, 8)\n    else:\n        # Exploitation: Choose the action with the highest confidence score\n        action_index = np.argmax(confidence_scores)\n\n    return action_index",
          "objective": 8806654315.873705,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = len(score_set)\n    averages = []\n    for action_index in range(action_count):\n        scores = score_set[action_index]\n        average = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n        probabilities = (selection_count / (total_selection_count + 1)) if total_selection_count > 0 else 1 / action_count\n        exploitation_score = average + np.sqrt((2 * np.log(total_time_slots + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        averages.append(exploitation_score * probabilities)\n\n    epsilon = 0.1  # exploration factor\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_count)\n    else:\n        action_index = np.argmax(averages)\n\n    return action_index",
          "objective": 9940507710.371412,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize variables\n    n_actions = 8\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    \n    # Calculate average scores and selection counts\n    for action_idx in range(n_actions):\n        scores = score_set.get(action_idx, [])\n        selection_counts[action_idx] = len(scores)\n        if selection_counts[action_idx] > 0:\n            average_scores[action_idx] = np.mean(scores)\n    \n    # Initial exploration: If an action has never been selected, prefer it\n    for action_idx in range(n_actions):\n        if selection_counts[action_idx] == 0:\n            return action_idx  # Return action that has never been selected\n    \n    # Calculate UCB values for each action\n    ucb_values = average_scores + np.sqrt((2 * np.log(total_selection_count)) / selection_counts)\n    \n    # Select the action with the maximum UCB value\n    action_index = np.argmax(ucb_values)\n    \n    return action_index",
          "objective": 10188373630.093204,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    # Set epsilon value that decreases over time for more focused selection\n    epsilon = max(0.1, 1 - (total_selection_count / (total_time_slots + 1)))  # Minimum exploration rate\n    \n    # Randomly select an action with probability epsilon for exploration\n    if np.random.rand() < epsilon:\n        return np.random.randint(0, num_actions)\n    \n    # Calculate average scores for each action\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if scores:\n            average_scores[action_index] = np.mean(scores)\n    \n    # Use a weighted average score that considers the number of selections\n    weights = np.where(selection_counts > 0, selection_counts / (total_selection_count + 1), 0)\n    weighted_average_scores = average_scores * (1 - weights)  # Favor less selected actions slightly\n\n    # Select the action with the maximum weighted average score\n    action_index = np.argmax(weighted_average_scores)\n    \n    return action_index",
          "objective": 10955135525.905828,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    exploration_bonus = 2  # Tunable parameter to control the exploration factor\n\n    # Initialize average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Apply a small value for unselected actions to avoid division by zero\n    selection_counts[selection_counts == 0] = 1e-6\n\n    # Calculate UCB values for each action\n    ucb_values = average_scores + exploration_bonus * np.sqrt(np.log(total_selection_count) / selection_counts)\n\n    # Select action based on maximum UCB value\n    action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": 11431410366.932379,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Dynamic epsilon to balance exploration and exploitation\n    epsilon = max(0.05, 0.5 * (1 - current_time_slot / total_time_slots))\n    \n    # Calculate average scores and selection counts\n    avg_scores = np.zeros(8)\n    counts = np.zeros(8)\n    \n    for action_index in range(8):\n        if action_index in score_set:\n            counts[action_index] = len(score_set[action_index])\n            if counts[action_index] > 0:\n                avg_scores[action_index] = np.mean(score_set[action_index])\n\n    # Handle case where all actions have zero counts\n    if total_selection_count == 0:\n        action_index = np.random.choice(range(8))  # Exploration only in the first round\n    else:\n        # Decide whether to explore or exploit\n        if np.random.rand() < epsilon:\n            action_index = np.random.choice(range(8))  # Explore\n        else:\n            action_index = np.argmax(avg_scores)  # Exploit\n\n    return action_index",
          "objective": 11658619061.065899,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores for each action\n    average_scores = {}\n    for action_index, scores in score_set.items():\n        if len(scores) > 0:\n            average_scores[action_index] = np.mean(scores)\n        else:\n            average_scores[action_index] = 0  # No scores yet\n\n    # Calculate exploration factor\n    exploration_factor = np.sqrt(np.log(total_selection_count + 1) / (np.array([len(score_set[i]) for i in range(8)]) + 1))\n    \n    # Combine average scores and exploration factors\n    action_values = np.array([average_scores[i] + exploration_factor[i] for i in range(8)])\n\n    # Select action based on highest value\n    action_index = np.argmax(action_values)\n    \n    return action_index",
          "objective": 14695850595.929653,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    average_scores = []\n    \n    for action in action_indices:\n        scores = score_set[action]\n        if len(scores) > 0:\n            average_scores.append(np.mean(scores))\n        else:\n            average_scores.append(0.0)\n\n    exploration_bonus = np.array((len(action_indices) - np.array([len(score_set[action]) for action in action_indices])) + 1) / (total_selection_count + 1e-5)\n    total_scores = np.array(average_scores) + exploration_bonus\n    \n    if total_time_slots > 0 and current_time_slot < total_time_slots:\n        exploration_factor = 1.0 - (current_time_slot / total_time_slots)\n        exploration_bonus = exploration_factor * (1.0 / (1.0 + total_selection_count))\n    else:\n        exploration_bonus = 0\n\n    total_scores = total_scores + exploration_bonus\n    action_index = np.argmax(total_scores)\n    \n    return action_index",
          "objective": 15153410382.337059,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Define epsilon for exploration\n    epsilon = 0.1\n    \n    # Calculate average scores for actions\n    average_scores = np.zeros(8)\n    action_counts = np.zeros(8)\n    \n    for action_index, scores in score_set.items():\n        action_counts[action_index] = len(scores)\n        average_scores[action_index] = np.mean(scores) if scores else 0.0\n    \n    # Calculate exploration bonuses\n    exploration_bonus = np.where(action_counts == 0, total_selection_count + 1, total_selection_count / action_counts)\n    \n    # Calculate selection scores, prioritizing average scores and adjusting with exploration bonuses\n    selection_scores = average_scores + (exploration_bonus * (1 - (current_time_slot / total_time_slots)))\n\n    # Epsilon-greedy strategy for exploration\n    if np.random.rand() < epsilon:\n        # Randomly select an action for exploration\n        action_index = np.random.randint(0, 8)\n    else:\n        # Select the action with the highest selection score\n        action_index = np.argmax(selection_scores)\n    \n    return action_index",
          "objective": 15645713915.312822,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate the average scores for each action\n    avg_scores = np.array([np.mean(scores) if len(scores) > 0 else 0.0 for scores in score_set.values()])\n    \n    # Handle zero total selection count to avoid division by zero\n    counts = np.array([len(scores) for scores in score_set.values()])\n    action_counts = np.maximum(counts, 1)  # Ensure at least counts of 1 for safe division\n    \n    # Calculate a penalty for actions that have lower average scores\n    penalties = np.clip(1 - avg_scores, 0, None)  # Non-negative penalties\n    exploration_bonus = penalties / action_counts  # Reduce penalty impact based on selection count\n    adjusted_scores = avg_scores + exploration_bonus\n    \n    # Calculate exploration factor\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))  # Decrease exploration over time\n    \n    # Generate random value for exploration\n    if np.random.rand() < epsilon:\n        # Exploration: Choose an action randomly from those underutilized\n        action_index = np.random.choice(np.flatnonzero(action_counts < np.median(action_counts)))\n    else:\n        # Exploitation: Choose the action with the highest adjusted score\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 1463650583998.2144,
          "other_inf": null
     }
]