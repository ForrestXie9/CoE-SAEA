[
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    \n    # Initialize variables to store average scores and selection counts\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    \n    # Calculate average scores and selection counts\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Early exploration phase: select untried actions\n    if total_selection_count < n_actions:\n        unselected_actions = np.where(selection_counts == 0)[0]\n        return int(np.random.choice(unselected_actions))\n\n    # Calculate exploration bonus (UCB)\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n\n    # Adjust exploration-exploitation balance\n    exploration_decay = np.clip((total_time_slots - current_time_slot) / total_time_slots, 0.1, 1.0)\n    epsilon = 0.15 * exploration_decay\n\n    # Apply \u03b5-greedy strategy\n    if np.random.rand() < epsilon:\n        # Explore: select action inversely proportional to selection counts\n        probabilities = 1 / (selection_counts + 1e-5)\n        probabilities /= probabilities.sum()  # Normalize probabilities\n        action_index = np.random.choice(n_actions, p=probabilities)\n    else:\n        # Exploit: select the action with the highest adjusted score\n        adjusted_scores = average_scores + exploration_bonus\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 9077328.02066505,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        average_scores[action_index] = np.mean(scores) if selection_counts[action_index] > 0 else 0\n\n    # Early exploration of unselected actions\n    if total_selection_count < n_actions:\n        unselected_actions = np.where(selection_counts == 0)[0]\n        return int(np.random.choice(unselected_actions))\n\n    # Compute exploration bonus based on UCB\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n\n    # Decay factor for balancing exploration and exploitation\n    decay = np.clip((total_time_slots - current_time_slot) / total_time_slots, 0.1, 1.0)\n\n    # Calculate adjusted scores\n    adjusted_scores = average_scores + decay * exploration_bonus\n\n    # Select the action\n    action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 9088220.148850262,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    \n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Early exploration\n    if total_selection_count < n_actions:\n        unselected_actions = np.where(selection_counts == 0)[0]\n        return int(np.random.choice(unselected_actions))\n\n    # Calculate exploration bonus (UCB-inspired)\n    exploration_bonus = np.sqrt(np.log(total_selection_count) / (selection_counts + 1e-5))\n\n    # Dynamic epsilon based on time slot\n    exploration_ratio = (total_time_slots - current_time_slot) / total_time_slots\n    epsilon = 0.15 * (1 + exploration_ratio)  # Adjust exploration factor as time progresses\n\n    # Use \u03b5-greedy strategy for exploration vs exploitation\n    if np.random.rand() < epsilon:\n        # Stochastic exploration\n        probabilities = 1 / (selection_counts + 1e-5)  # Inverse counts for probability\n        probabilities /= probabilities.sum()  # Normalize probabilities\n        action_index = np.random.choice(n_actions, p=probabilities)\n    else:\n        # Select action with the highest adjusted score (average score + exploration bonus)\n        adjusted_scores = average_scores + exploration_bonus\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 9157047.046267435,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        average_scores[action_index] = np.mean(scores) if selection_counts[action_index] > 0 else 0\n\n    # Early exploration of unselected actions\n    if total_selection_count < n_actions:\n        unselected_actions = np.where(selection_counts == 0)[0]\n        return int(np.random.choice(unselected_actions))\n\n    # Compute exploration bonus (UCB)\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n\n    # Dynamic exploration-exploitation balance\n    epsilon = np.clip(0.2 * ((total_time_slots - current_time_slot) / total_time_slots), 0.05, 0.2)\n\n    # Apply the \u03b5-greedy strategy\n    if np.random.rand() < epsilon:\n        # Explore: Select actions with a probability inversely proportional to selection counts\n        probabilities = np.maximum(1 / (selection_counts + 1e-5), 0)\n        probabilities /= probabilities.sum()  # Normalize to sum to 1\n        action_index = np.random.choice(n_actions, p=probabilities)\n    else:\n        # Exploit: Select the action with the highest adjusted score\n        adjusted_scores = average_scores + exploration_bonus\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 9161888.662910884,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n\n    # Initialize variables for average scores and selection counts\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    average_scores = np.array([np.mean(scores) if scores else 0 for scores in score_set.values()])\n\n    # Early exploration phase: select untried actions\n    if total_selection_count < n_actions:\n        unselected_actions = np.where(selection_counts == 0)[0]\n        return int(np.random.choice(unselected_actions))\n\n    # Calculate exploration bonus using UCB\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n\n    # Set exploration decay\n    exploration_decay = np.clip((total_time_slots - current_time_slot) / total_time_slots, 0.1, 1.0)\n    epsilon = 0.1 * exploration_decay  # Epsilon for epsilon-greedy\n\n    # Epsilon-greedy exploration vs. exploitation decision\n    if np.random.rand() < epsilon:\n        # Explore: select an action inversely proportional to selection counts\n        probabilities = 1 / (selection_counts + 1e-5)\n        probabilities /= probabilities.sum()  # Normalize probabilities\n        action_index = np.random.choice(n_actions, p=probabilities)\n    else:\n        # Exploit: select the action with highest adjusted score\n        adjusted_scores = average_scores + exploration_bonus\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 9234094.18310789,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    \n    # Initialize variables for average scores and selection counts\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    \n    # Calculate average scores and selection counts\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # If exploration phase: select from untried actions\n    if total_selection_count < n_actions:\n        unselected_actions = np.where(selection_counts == 0)[0]\n        return int(np.random.choice(unselected_actions))\n    \n    # Calculate Upper Confidence Bound (UCB)\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n    \n    # Adjust the exploration factor linearly based on time\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n    exploration_factor = np.clip(exploration_factor, 0.1, 1.0)\n    \n    # Calculate scores adjusted with exploration factors\n    adjusted_scores = average_scores + exploration_factor * exploration_bonus\n\n    # Select action with the highest adjusted score\n    action_index = np.argmax(adjusted_scores)\n    \n    return action_index",
          "objective": 9237196.91006097,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    \n    # Initialize variables for average scores and selection counts\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    \n    # Calculate average scores and selection counts\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Early exploration phase: select untried actions\n    if total_selection_count < n_actions:\n        unselected_actions = np.where(selection_counts == 0)[0]\n        return int(np.random.choice(unselected_actions))\n\n    # Calculate exploration bonus (UCB)\n    exploration_bonus = np.sqrt(np.log(total_selection_count) / (selection_counts + 1e-5))\n\n    # Decay exploration factor based on time left\n    exploration_factor = np.clip((total_time_slots - current_time_slot) / total_time_slots, 0.1, 1.0)\n    \n    # Calculate total scores including exploration factor\n    total_scores = average_scores + (exploration_bonus * exploration_factor)\n    \n    # Epsilon for epsilon-greedy strategy\n    epsilon = 0.1 * exploration_factor\n\n    # Apply \u03b5-greedy strategy\n    if np.random.rand() < epsilon:\n        # Explore: select action inversely proportional to selection counts (more uniform)\n        probabilities = 1 / (selection_counts + 1e-5)\n        probabilities /= probabilities.sum()  # Normalize probabilities\n        action_index = np.random.choice(n_actions, p=probabilities)\n    else:\n        # Exploit: select the action with the highest total score\n        action_index = np.argmax(total_scores)\n\n    return action_index",
          "objective": 9244459.47602577,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n\n    # Initialize average scores and selection counts\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Early exploration phase: select untried actions\n    if total_selection_count < n_actions:\n        unselected_actions = np.where(selection_counts == 0)[0]\n        return int(np.random.choice(unselected_actions))\n\n    # Calculate exploration bonus (UCB)\n    exploration_bonus = np.sqrt(np.log(total_selection_count) / (selection_counts + 1e-5))\n\n    # Adjust exploration based on the progress of time\n    exploration_exponent = -10 * (current_time_slot / total_time_slots)\n    exploration_decay = np.exp(exploration_exponent)\n\n    # Combine average scores with exploration bonus while balancing them\n    adjusted_scores = average_scores * (1 - exploration_decay) + exploration_decay * exploration_bonus\n\n    # Select action based on highest adjusted scores\n    action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 9440384.448825372,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Exploration strategy\n    exploration_weight = np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n    \n    # Dynamic exploration-exploitation balance\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n    # Prefer more explored actions over time\n    adjusted_scores = average_scores + (exploration_factor * exploration_weight)\n\n    # Handle never selected actions\n    adjusted_scores[selection_counts == 0] = np.inf\n\n    # Select action with the highest adjusted score\n    action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 9463043.426890407,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Determine adaptive exploration rate\n    epsilon = (total_time_slots - current_time_slot) / total_time_slots\n\n    # Decide whether to explore or exploit\n    if np.random.rand() < epsilon:\n        # Explore: select an action uniformly at random\n        action_index = np.random.choice(np.arange(n_actions))\n    else:\n        # Exploit: select the action with the highest average score\n        # Handle actions that have never been selected by assigning a very low score\n        adjusted_scores = np.copy(average_scores)\n        adjusted_scores[selection_counts == 0] = -np.inf  # Assign negative infinity to unselected actions\n\n        # Select the action with the highest adjusted average score\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 9596046.23359981,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    \n    # Initialize average scores and selection counts\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts, handling division by zero\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Early exploration of unselected actions\n    if total_selection_count < n_actions:\n        unselected_actions = np.where(selection_counts == 0)[0]\n        return int(np.random.choice(unselected_actions))\n\n    # Compute exploration bonus (UCB)\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n    \n    # Dynamic adjustment of exploration-exploitation balance using an adaptive \u03b5-greedy strategy\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n    epsilon = 0.15 * exploration_factor  # Start with a higher epsilon\n\n    # Apply the \u03b5-greedy strategy\n    if np.random.rand() < epsilon:\n        # Explore: Select action inversely proportional to selection counts\n        probabilities = np.maximum(1 / (selection_counts + 1e-5), 0)  # Avoid division by zero\n        probabilities /= probabilities.sum()  # Normalize to sum to 1\n        action_index = np.random.choice(n_actions, p=probabilities)\n    else:\n        # Exploit: Select the action with the highest adjusted score\n        adjusted_scores = average_scores + exploration_bonus\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 9846284.045561189,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    \n    # Initialize average scores and selection counts\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if scores:\n            average_scores[action_index] = np.mean(scores)\n    \n    # Early exploration phase: select untried actions\n    if total_selection_count < n_actions:\n        unselected_actions = np.where(selection_counts == 0)[0]\n        return int(np.random.choice(unselected_actions))\n\n    # Calculate exploration bonus (UCB)\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n\n    # Adjust exploration based on the remaining time\n    exploration_decay = (total_time_slots - current_time_slot) / total_time_slots\n    exploration_decay = np.clip(exploration_decay, 0.1, 1.0)\n    \n    # Combine average scores with exploration bonus\n    adjusted_scores = average_scores + exploration_decay * exploration_bonus\n    \n    # Use a new strategy: \u03b5-greedy for exploration-exploitation balance\n    epsilon = 0.1  # Probability of choosing a random action\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(n_actions)\n    else:\n        action_index = np.argmax(adjusted_scores)\n    \n    return action_index",
          "objective": 9883391.533977646,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    \n    # Initialize average scores and selection counts\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if scores:\n            average_scores[action_index] = np.mean(scores)\n    \n    # Early exploration phase: select untried actions\n    if total_selection_count < n_actions:\n        unselected_actions = np.where(selection_counts == 0)[0]\n        return int(np.random.choice(unselected_actions))\n    \n    # Calculate exploration bonus (UCB)\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n\n    # Adjust exploration based on the remaining time\n    exploration_decay = (total_time_slots - current_time_slot) / total_time_slots\n    exploration_decay = np.clip(exploration_decay, 0.1, 1.0)\n    \n    # Combine average scores with exploration bonus\n    adjusted_scores = average_scores + exploration_decay * exploration_bonus\n    \n    # Apply a softmax approach for action selection\n    exp_scores = np.exp(adjusted_scores)\n    probabilities = exp_scores / np.sum(exp_scores)  # Normalize probabilities\n    \n    # Select action based on calculated probabilities\n    action_index = np.random.choice(n_actions, p=probabilities)\n    \n    return action_index",
          "objective": 10462301.203692336,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Early exploration phase: select unselected actions\n    if total_selection_count < n_actions:\n        unselected_actions = np.where(selection_counts == 0)[0]\n        return int(np.random.choice(unselected_actions))\n\n    # Calculate UCB values to encourage exploration\n    ucb_values = average_scores + np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n\n    # Dynamically adjust exploration factor based on the current time slot\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = (exploration_weight * average_scores) + ((1 - exploration_weight) * ucb_values)\n\n    # Select the action with the highest adjusted score\n    action_index = np.argmax(adjusted_scores)\n    \n    return action_index",
          "objective": 10511738.963354243,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    \n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Early exploration phase - prefer unselected actions\n    if total_selection_count < n_actions:\n        unselected_actions = np.where(selection_counts == 0)[0]\n        return int(np.random.choice(unselected_actions))\n\n    # Calculate exploration bonus using UCB\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n\n    # Compute adjusted scores for exploitation\n    adjusted_scores = average_scores + exploration_bonus\n    \n    # Dynamic epsilon for exploration-exploitation balance\n    epsilon = max(0.1 * (total_time_slots - current_time_slot) / total_time_slots, 0.01)\n\n    # Select action based on epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(np.arange(n_actions), p=(1 / (selection_counts + 1e-5)) / np.sum(1 / (selection_counts + 1e-5)))\n    else:\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 10657477.438920386,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    \n    # Initialize average scores and selection counts\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(n_actions):\n        if action_index in score_set and len(score_set[action_index]) > 0:\n            selection_counts[action_index] = len(score_set[action_index])\n            average_scores[action_index] = np.mean(score_set[action_index])\n\n    # Early exploration of unselected actions\n    if total_selection_count < n_actions:\n        unselected_actions = np.where(selection_counts == 0)[0]\n        return int(np.random.choice(unselected_actions))\n\n    # Adjust exploration bonus using UCB\n    exploration_bonus = np.sqrt(np.log(total_selection_count) / (selection_counts + 1e-10))\n    \n    # Define exploration factor\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n    epsilon = 0.1 * exploration_factor  # Gradually decrease exploration\n\n    # Implement epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        # Explore: Select an action uniformly from those less frequently selected\n        scaled_probabilities = (1 / (selection_counts + 1e-10)) / np.sum(1 / (selection_counts + 1e-10))\n        action_index = np.random.choice(np.arange(n_actions), p=scaled_probabilities)\n    else:\n        # Exploit: Select the action with the highest adjusted score\n        adjusted_scores = average_scores + exploration_bonus\n        if np.all(selection_counts == 0):  # Fallback if all selections are zero\n            action_index = np.random.choice(np.arange(n_actions))\n        else:\n            action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 11056903.71086773,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    \n    # Initialize average scores and selection counts\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Handle early exploration of unselected actions\n    if total_selection_count < n_actions:\n        unselected_actions = np.where(selection_counts == 0)[0]\n        return int(np.random.choice(unselected_actions))\n\n    # Calculate exploration bonus (UCB)\n    exploration_bonus = np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n    \n    # Combine average scores and exploration bonuses for final score\n    ucb_scores = average_scores + exploration_bonus\n    \n    # Define epsilon for adaptive exploration\n    epsilon = 1 - (current_time_slot / total_time_slots)\n    \n    # Select action\n    if np.random.rand() < epsilon:\n        # Explore: Select with probability inversely proportional to selection counts\n        probabilities = (1 / (selection_counts + 1e-5))\n        probabilities /= probabilities.sum()  # Normalize to sum to 1\n        action_index = np.random.choice(n_actions, p=probabilities)\n    else:\n        # Exploit: Select action with the highest UCB score\n        action_index = np.argmax(ucb_scores)\n\n    return action_index",
          "objective": 11126525.644210115,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    \n    # Initialize average scores and selection counts\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Early exploration of unselected actions\n    if total_selection_count < n_actions:\n        unselected_actions = np.where(selection_counts == 0)[0]\n        return int(np.random.choice(unselected_actions))\n\n    # Calculate exploration bonus using UCB\n    exploration_bonus = np.sqrt(np.log(total_selection_count) / (selection_counts + 1e-5))\n    \n    # Dynamic adjustment of exploration-exploitation balance using \u03b5-greedy approach\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n    epsilon = 0.1 * exploration_factor  # Decrease \u03b5 as time progresses\n\n    # Apply the \u03b5-greedy strategy\n    if np.random.rand() < epsilon:\n        # Explore: Randomly select an action favoring less selected actions\n        action_index = np.random.choice(np.arange(n_actions), p=(1/selection_counts) / np.sum(1/selection_counts))\n    else:\n        # Exploit: Select the action with the highest adjusted score\n        adjusted_scores = average_scores + exploration_bonus\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 11235975.62081731,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Early exploration phase\n    if total_selection_count < n_actions:\n        unselected_actions = np.where(selection_counts == 0)[0]\n        return int(np.random.choice(unselected_actions))\n\n    # Calculate exploration factor using UCB\n    exploration_factors = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n    \n    # Combine scores and exploration factors\n    ucb_values = average_scores + exploration_factors\n\n    # Adjust exploration-exploitation balance based on the current time slot\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = (exploration_weight * average_scores) + ((1 - exploration_weight) * ucb_values)\n\n    # Select the action with the highest adjusted score\n    action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 11926902.253279675,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    \n    # Calculate the average scores and selection counts\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n    \n    # Exploration factor based on time slot\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n\n    # Confidence intervals for actions\n    confidence_intervals = np.zeros(n_actions)\n    with np.errstate(divide='ignore', invalid='ignore'):\n        confidence_intervals = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1))\n\n    # Weighting the scores; gradually favor exploitation\n    exploitation_weight = (current_time_slot / total_time_slots) ** 2  # Quadratic increase to favor exploration less over time\n    adjusted_scores = (1 - exploitation_weight) * (average_scores + exploration_factor * confidence_intervals) + \\\n                      exploitation_weight * average_scores\n\n    # Prioritize actions that have never been selected\n    adjusted_scores[selection_counts == 0] = np.inf \n\n    # Select the action with the highest adjusted score\n    action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 12403468.466609357,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Early exploration for unselected actions\n    if total_selection_count < n_actions:\n        return np.argmin(selection_counts)  # Select the least selected action\n\n    # Define exploration parameters\n    exploration_weight = 1.5  # Weight for exploration\n    exploitation_weight = 1.0  # Weight for exploitation\n\n    # Calculate exploration term (UCB-like)\n    upper_confidence_bounds = np.sqrt(exploration_weight * np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n\n    # Calculate adjusted scores with dynamic tweaking based on current time slot\n    time_factor = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = (average_scores * exploitation_weight) + (upper_confidence_bounds * time_factor)\n\n    # Handle actions that have never been selected by assigning a high adjusted score\n    adjusted_scores[selection_counts == 0] = np.inf \n\n    # Select the action with the highest adjusted score\n    action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 12597046.254642267,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    \n    # Initialize average scores and selection counts\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Handle cases of early exploration\n    if total_selection_count < n_actions:\n        unselected_actions = np.where(selection_counts == 0)[0]\n        return int(np.random.choice(unselected_actions))\n\n    # Calculate UCB exploration bonus\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n    \n    # Dynamic epsilon for \u03b5-greedy strategy\n    epsilon = 0.5 * (1 - current_time_slot / total_time_slots)  # Higher at the start, tapering off\n    \n    # Admit randomness based on \u03b5\n    if np.random.rand() < epsilon:\n        # Explore: Randomly select one among less-selected actions\n        prob_weights = (1 / (selection_counts + 1e-5))  # Avoiding division by zero\n        action_index = np.random.choice(n_actions, p=prob_weights / np.sum(prob_weights))\n    else:\n        # Exploit: Select action with highest adjusted scores\n        adjusted_scores = average_scores + exploration_bonus\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 12788453.674689712,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(n_actions):\n        if action_index in score_set:\n            scores = score_set[action_index]\n            selection_counts[action_index] = len(scores)\n            if selection_counts[action_index] > 0:\n                average_scores[action_index] = np.mean(scores)\n\n    # Early exploration for unselected actions\n    if total_selection_count < n_actions:\n        return np.argmin(selection_counts)  # Select the least selected action\n\n    # Calculate exploration bonus\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n\n    # Adjust the exploration-exploitation balance\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = average_scores + exploration_factor * exploration_bonus\n\n    # Handle actions that have never been selected\n    adjusted_scores[selection_counts == 0] = np.inf  # Prioritize unselected actions\n\n    # Select action with the highest adjusted score\n    action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 14421932.236988727,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    \n    # Initialize average scores and selection counts\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        if scores:\n            average_scores[action_index] = np.mean(scores)\n\n    # Early exploration of unselected actions\n    if total_selection_count < n_actions:\n        unselected_actions = np.where(selection_counts == 0)[0]\n        return int(np.random.choice(unselected_actions))\n\n    # Calculate exploration bonus using UCB\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n    \n    # Dynamic adjustment of exploration-exploitation balance using a linear decay\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n    epsilon = 0.1 * exploration_factor  # Decrease \u03b5 as time progresses\n\n    # Apply the \u03b5-greedy strategy\n    if np.random.rand() < epsilon:\n        # Explore: Select a random action with uniform probability\n        action_index = np.random.choice(np.arange(n_actions))\n    else:\n        # Exploit: Select the action with the highest adjusted score\n        adjusted_scores = average_scores + exploration_bonus\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 14523472.814497394,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Early exploration phase\n    if total_selection_count < n_actions:\n        unselected_actions = np.where(selection_counts == 0)[0]\n        return int(np.random.choice(unselected_actions))\n\n    # Calculate exploration factor\n    exploration_factors = np.sqrt((np.log(total_selection_count + 1) / (selection_counts + 1e-5)))\n\n    # Thompson Sampling: sample from the posterior distribution\n    thompson_samples = average_scores + exploration_factors * np.random.randn(n_actions)\n\n    # Log the Thomspon Samples for debugging (optional)\n    # print(f\"Thompson Samples: {thompson_samples}\")\n\n    # Select the action with the highest sampled value\n    action_index = np.argmax(thompson_samples)\n\n    return action_index",
          "objective": 14835568.476246554,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Initialize exploration parameters\n    confidence_interval = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-6))\n    \n    # Calculate exploration weight based on time\n    exploration_weight = 1 / (1 + current_time_slot)\n\n    # Balanced score calculation\n    final_scores = average_scores + exploration_weight * confidence_interval\n\n    # Introduce random exploration in the early stages\n    if current_time_slot < total_time_slots / 4:\n        action_index = np.random.choice(num_actions)\n    else:\n        action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": 16974151.235665124,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Handle zero-selection cases to avoid division errors\n    exploration_bonus = np.zeros(n_actions)\n    for action_index in range(n_actions):\n        if selection_counts[action_index] == 0:\n            exploration_bonus[action_index] = np.inf  # Favor unselected actions\n        else:\n            # Calculate exploration factor (UCB-inspired)\n            exploration_bonus[action_index] = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[action_index])\n\n    # Combine average scores with exploration bonus\n    adjusted_scores = average_scores + exploration_bonus\n\n    # Select action with the highest adjusted score\n    action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 20110995.552139193,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Early exploration for unselected actions\n    if total_selection_count < n_actions:\n        return np.argmin(selection_counts)  # Select the least selected action\n\n    # Calculate exploration bonus using UCB-like strategy\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n\n    # Dynamic adjustment of exploration-exploitation balance\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = average_scores + exploration_factor * exploration_bonus\n\n    # Handle actions that have never been selected\n    adjusted_scores[selection_counts == 0] = np.inf  # Prioritize unselected actions\n\n    # Select action with the highest adjusted score\n    action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 20937762.87376348,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Adjust exploration factor based on time slot progression\n    exploration_factor = np.log(total_selection_count + 1) / (1 + selection_counts)\n\n    # Calculate adjusted scores\n    adjusted_scores = average_scores + exploration_factor\n\n    # Define exploration bonus for unselected actions\n    adjusted_scores[selection_counts == 0] = np.inf  # Unselected actions are prioritized\n\n    # Select action with the highest adjusted score\n    action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 22248227.999424987,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Calculate exploration bonuses for UCB\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-6))\n\n    # Combine average scores and exploration bonuses\n    adjusted_scores = average_scores + exploration_bonus\n\n    # Dynamic adjustment for exploration-exploitation balance\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n    final_scores = (\n        (1 - exploration_factor) * average_scores + \n        exploration_factor * adjusted_scores\n    )\n\n    # Select action based on the highest final score\n    action_index = np.argmax(final_scores)\n\n    return action_index",
          "objective": 22419812.70277937,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate the average scores and selection counts\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Handle unselected actions by assigning them a high exploratory score\n    unexplored_actions = selection_counts == 0\n    if np.any(unexplored_actions):\n        # Select one of the unexplored actions randomly\n        return np.random.choice(np.where(unexplored_actions)[0])\n\n    # Calculate the exploration factor (UCB approach)\n    ucb_scores = average_scores + np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts)\n\n    # Dynamic epsilon-greedy adjustment based on time slot\n    exploration_epsilon = np.maximum(0.1, 1 - (current_time_slot / total_time_slots))\n    \n    # Randomly explore with probability epsilon\n    if np.random.random() < exploration_epsilon:\n        return np.random.choice(n_actions)\n\n    # Select the action with the highest UCB score\n    action_index = np.argmax(ucb_scores)\n\n    return action_index",
          "objective": 22421783.091498177,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and the number of times each action has been chosen\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Set epsilon based on current time slot, favoring exploration in the early phase\n    epsilon = 1 - (current_time_slot / total_time_slots) if total_time_slots > 0 else 0\n    epsilon = max(0.1, epsilon)  # Ensure a minimum exploration rate\n\n    # Decide whether to explore or exploit\n    if np.random.rand() < epsilon:\n        # Exploration: select randomly from unselected actions or all actions\n        unselected_actions = [i for i in range(n_actions) if selection_counts[i] == 0]\n        if unselected_actions:\n            action_index = np.random.choice(unselected_actions)\n        else:\n            action_index = np.random.randint(0, n_actions)  # Randomly select from all\n    else:\n        # Exploitation: select action with the highest average score\n        action_index = np.argmax(average_scores)\n\n    return action_index",
          "objective": 22873502.857720114,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Calculate exploration factor (UCB-inspired)\n    exploration_bonus = np.zeros(n_actions)\n    for action_index in range(n_actions):\n        if selection_counts[action_index] == 0:\n            exploration_bonus[action_index] = np.inf  # Prioritize unselected actions\n        else:\n            exploration_bonus[action_index] = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[action_index])\n\n    # Define epsilon for exploration\n    epsilon = max(0.1, (total_time_slots - current_time_slot) / total_time_slots)  # Decay epsilon over time\n\n    # Randomly explore with probability epsilon\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(n_actions)\n    else:\n        # Combine average scores with exploration bonus\n        adjusted_scores = average_scores + exploration_bonus\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 22919992.216761984,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate the average scores and selection counts\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Base case for unselected actions (to ensure exploration)\n    unexplored_actions = selection_counts == 0\n    if np.any(unexplored_actions):\n        # If there are unexplored actions, select one randomly\n        return np.random.choice(np.where(unexplored_actions)[0])\n\n    # Calculate the exploration factor using the UCB formula\n    confidence_intervals = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts)\n    \n    # Adjust scores with UCB\n    ucb_scores = average_scores + confidence_intervals\n    \n    # Exponentially decay the exploration impact over time\n    exploration_decay = np.exp(-current_time_slot / total_time_slots)\n    adjusted_scores = (1 - exploration_decay) * ucb_scores + exploration_decay * average_scores\n\n    # Select the action with the highest adjusted score\n    action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 22929955.71033628,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    \n    # Calculate average scores and selection counts\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Define exploration parameters\n    exploration_probability = 0.1  # Probability for exploration\n    exploitation_weight = (total_time_slots - current_time_slot) / total_time_slots\n    \n    # Calculate adjusted scores with an exploration term\n    adjusted_scores = average_scores * exploitation_weight\n\n    # Early exploration for unselected actions\n    unexplored_actions = selection_counts == 0\n    if total_selection_count < n_actions:\n        return np.argmin(selection_counts)  # Select the least selected action\n    \n    # Calculate exploration term using a modified UCB\n    confidence_bounds = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n    adjusted_scores += confidence_bounds\n    \n    # Introduce \u03b5-greedy exploration\n    if np.random.rand() < exploration_probability:\n        return np.random.choice(np.where(selection_counts > 0)[0])  # Randomly select from explored actions\n\n    # Select the action with the highest adjusted score\n    action_index = np.argmax(adjusted_scores)\n    \n    return action_index",
          "objective": 35363880.042810805,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate the average scores and selection counts\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Epsilon-greedy exploration factor\n    epsilon = max(1 - (current_time_slot / total_time_slots), 0.1)  # Keep a minimum exploration factor\n    if np.random.rand() < epsilon:\n        # Select a random action for exploration\n        action_index = np.random.choice(np.flatnonzero(selection_counts == 0)) if np.any(selection_counts == 0) else np.random.randint(n_actions)\n        return action_index\n    \n    # Confidence interval component\n    confidence_intervals = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n\n    # Weighted average scores with confidence intervals\n    adjusted_scores = average_scores + confidence_intervals\n\n    # Prioritize actions that have never been selected\n    adjusted_scores[selection_counts == 0] = np.inf \n\n    # Select the action with the highest adjusted score\n    action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 36501652.51560488,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = len(score_set)\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Avoid division by zero for exploration factor\n    exploration_factor = np.where(selection_counts > 0, 1 / selection_counts, np.inf)\n\n    # Dynamic weight for exploration vs exploitation\n    exploration_weight = (1 - (current_time_slot / total_time_slots)) * (1 / (total_selection_count + 1))\n    \n    # Calculate adjusted scores with exploration factor included\n    adjusted_scores = average_scores + exploration_weight * exploration_factor\n\n    # Handle actions never selected and ensure they have a high exploration score\n    adjusted_scores[selection_counts == 0] = np.inf\n\n    # Select action with the highest adjusted score\n    action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 37576747.68518998,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = len(score_set)  # Number of actions based on score_set\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for action_index, scores in score_set.items():\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Compute exploration factor using inverse of selection counts\n    exploration_factor = 1 / (selection_counts + 1)  # Add 1 to avoid division by zero\n\n    # Use a dynamic exploration term that increases with time\n    exploration_weight = 1 - (current_time_slot / total_time_slots)  # Decreases as time progresses\n    exploration_term = exploration_weight * exploration_factor\n\n    # Calculate final adjusted scores\n    adjusted_scores = average_scores + exploration_term\n\n    # Handle actions never selected by assigning them a higher exploration score\n    adjusted_scores[selection_counts == 0] = np.inf  # Prioritize actions that have never been selected\n\n    # Select action with the highest adjusted score\n    action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 38000945.156294756,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Handle cases where selection_counts are zero\n    exploration_bonus = np.where(selection_counts > 0, \n                                  np.log(total_selection_count + 1) / selection_counts, \n                                  np.inf)  # Assign high exploration value to unselected actions\n\n    # Normalized adjusted scores\n    adjusted_scores = average_scores + exploration_bonus\n    adjusted_scores -= np.min(adjusted_scores)  # Shift to non-negative\n    adjusted_scores /= (np.max(adjusted_scores) + 1e-10)  # Avoid division by zero\n\n    # Epsilon-greedy strategy\n    epsilon = 0.1  # Exploration probability\n    if np.random.rand() < epsilon:  # Explore\n        action_index = np.random.choice(num_actions)\n    else:  # Exploit\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 40486011.90669935,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate the average scores and selection counts\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Initialize a base score for unexplored actions\n    unexplored_base_score = 0.5  # arbitrary baseline for unexplored actions\n\n    # Calculate exploration factor\n    exploration_factor = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-6))\n\n    # Dynamic exploration strategy (epsilon-greedy)\n    epsilon = max(1 - (current_time_slot / total_time_slots), 0.1)  # Decrease exploration over time\n    if np.random.rand() < epsilon:\n        # Randomly explore\n        unexplored_actions = np.where(selection_counts == 0)[0]\n        if unexplored_actions.size > 0:\n            return np.random.choice(unexplored_actions)\n\n    # Adjust scores with UCB and base score for unexplored actions\n    adjusted_scores = average_scores + exploration_factor\n    \n    # Fallback for actions that remain unexplored\n    adjusted_scores[selection_counts == 0] = unexplored_base_score\n\n    # Select the action with the highest adjusted score\n    action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 42520666.72024174,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Compute average scores and selection counts\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Define exploration parameter to facilitate exploration vs exploitation\n    exploration_strength = np.log(total_selection_count + 1) / (selection_counts + 1)\n\n    # Calculate confidence intervals to balance exploration and exploitation\n    confidence_intervals = np.sqrt(exploration_strength) * (1 - np.sqrt(current_time_slot / total_time_slots))\n\n    # Apply confidence intervals to adjusted scores\n    adjusted_scores = average_scores + confidence_intervals\n\n    # Ensure unselected actions are prioritized\n    adjusted_scores[selection_counts == 0] = np.inf \n\n    # Select the action with the highest adjusted score\n    action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 43651967.32554703,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Calculate exploration factor (Epsilon-greedy approach)\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (1 + selection_counts))\n\n    # Dynamic adjustment of exploration-exploitation balance\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n\n    # Combine average scores and exploration factors\n    adjusted_scores = average_scores + exploration_factor * exploration_bonus\n\n    # Handle actions that have never been selected\n    adjusted_scores[selection_counts == 0] = np.inf  # Prioritize unselected actions\n\n    # Select action with the highest adjusted score\n    action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 45194864.32848086,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    \n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Early exploration\n    if total_selection_count < n_actions:\n        unselected_actions = np.where(selection_counts == 0)[0]\n        return int(np.random.choice(unselected_actions))\n\n    # Calculate the exploration bonus using UCB\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n\n    # Adjust epsilon based on current time slot\n    exploration_ratio = (total_time_slots - current_time_slot) / total_time_slots\n    epsilon = 0.1 * (1 + exploration_ratio)  # Epsilon reduces over time\n\n    # Use \u03b5-greedy strategy\n    if np.random.rand() < epsilon:\n        # Stochastic exploration: Select actions inversely to their selection counts\n        probabilities = np.maximum(1 / (selection_counts + 1e-5), 0)  # Avoid division by zero\n        probabilities /= probabilities.sum()  # Normalize for probabilities\n        action_index = np.random.choice(n_actions, p=probabilities)\n    else:\n        # Select action with the highest adjusted score (average score + exploration bonus)\n        adjusted_scores = average_scores + exploration_bonus\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 48882252.766245715,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Define exploration factor that decays over time\n    exploration_factor = max(0, (total_time_slots - current_time_slot) / total_time_slots)\n\n    # Calculate UCB-based scores\n    ucb_scores = average_scores + exploration_factor * np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1))\n\n    # Handle actions never selected\n    ucb_scores[selection_counts == 0] = np.inf  # Promote unselected actions\n\n    # Select action based on the highest UCB score\n    action_index = np.argmax(ucb_scores)\n\n    return action_index",
          "objective": 61081298.155015096,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Define exploration parameter with logistic decay to focus on exploration in initial time slots\n    exploration_factor = 1 / (1 + np.exp(-10 * (current_time_slot / total_time_slots - 0.5)))\n\n    # Calculate adjusted scores\n    adjusted_scores = average_scores + exploration_factor * np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1))\n\n    # Handle actions never selected\n    adjusted_scores[selection_counts == 0] = np.inf  # Ensure unselected actions are prioritized\n\n    # Select action with the highest adjusted score\n    action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 67268372.99455608,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    \n    # Calculate average scores and selection counts\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        # Calculate the average score for selected actions\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Exploration factor that encourages exploration based on the time slot\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n\n    # Calculate a confidence interval for each action based on selection counts\n    confidence_intervals = np.zeros(n_actions)\n    with np.errstate(divide='ignore', invalid='ignore'):\n        confidence_intervals = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1))  # Avoid division by zero\n\n    # Adjusted scores combining average scores and exploration\n    adjusted_scores = average_scores + exploration_factor * confidence_intervals\n\n    # Prioritize actions that have never been selected\n    adjusted_scores[selection_counts == 0] = np.inf  # Ensure unselected actions are prioritized\n\n    # Select the action with the highest adjusted score\n    action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 67881431.37091415,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    for action in range(num_actions):\n        scores = score_set.get(action, [])\n        selection_counts[action] = len(scores)\n        \n        if selection_counts[action] > 0:\n            average_scores[action] = np.mean(scores)\n\n    # Determine exploration factor\n    exploration_rate = max(0.1, 1 - (current_time_slot / total_time_slots))\n    \n    # Calculate exploration bonus based on selection counts\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n    \n    # Compute selection values with a mix of average scores and exploration bonus\n    selection_values = average_scores + exploration_rate * exploration_bonus\n\n    # Ensure actions with no selections are encouraged in the beginning\n    if current_time_slot < total_time_slots * 0.5:\n        unexplored_actions = np.flatnonzero(selection_counts == 0)\n        if unexplored_actions.size > 0:\n            action_index = np.random.choice(unexplored_actions)\n        else:\n            action_index = np.argmax(selection_values)\n    else:\n        action_index = np.argmax(selection_values)\n\n    return action_index",
          "objective": 72107976.36425403,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Prevent division by zero and ensure robustness\n    exploration_bonus = np.zeros(num_actions)\n    for i in range(num_actions):\n        if selection_counts[i] > 0:\n            exploration_bonus[i] = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[i])\n        else:\n            exploration_bonus[i] = np.inf  # High bonus for unselected actions\n\n    # Incorporate time slot progressive exploration factor\n    time_factor = (total_time_slots - current_time_slot) / total_time_slots\n    exploration_factor = 1 - time_factor\n\n    # Adjust scores with exploration bonus, scaled by exploration factor\n    adjusted_scores = average_scores + exploration_factor * exploration_bonus\n\n    # Select action based on the highest adjusted score\n    action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 73781316.2528062,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    \n    # Calculate average scores and selection counts\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        # Calculate the average score for selected actions\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Epsilon for exploration; adjusts as we progress through time\n    epsilon = max(0.1, (total_time_slots - current_time_slot) / total_time_slots)\n\n    # Select a random action based on epsilon-greedy approach\n    if np.random.rand() < epsilon:\n        # Explore: select a random action\n        action_index = np.random.randint(n_actions)\n    else:\n        # Exploit: select the action with the highest average score\n        # Prioritize unselected actions (avoid zero selections)\n        adjusted_scores = np.copy(average_scores)\n        adjusted_scores[selection_counts == 0] = np.inf  # High value for actions never selected\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 75735381.47465041,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Calculate exploration adjustment\n    exploration_factor = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1))\n    \n    # Adjust scores to balance exploration and exploitation\n    # Shift towards average scores as current_time_slot progresses\n    decay = current_time_slot / total_time_slots if total_time_slots > 0 else 1\n    adjusted_scores = (1 - decay) * exploration_factor + decay * average_scores\n\n    # Handle actions never selected\n    adjusted_scores[selection_counts == 0] = np.inf\n\n    # Select action with the highest adjusted score\n    action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 77380872.87601346,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = len(score_set)  # Number of actions\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for action_index, scores in score_set.items():\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Prioritize exploration based on selection counts\n    exploration_weights = np.log((total_selection_count + 1) / (selection_counts + 1))  # Logarithmic exploration factor\n    exploration_weights[selection_counts == 0] = np.inf  # Give infinite weight to actions never selected\n\n    # Dynamic exploration adjustment\n    exploration_adjustment = (total_time_slots - current_time_slot) / total_time_slots  # Decrease over time\n    exploration_scores = exploration_weights * exploration_adjustment\n\n    # Calculate final selection scores\n    selection_scores = average_scores + exploration_scores\n\n    # Select action with the highest score\n    action_index = np.argmax(selection_scores)\n\n    return action_index",
          "objective": 78827858.52030067,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Calculate average scores\n    average_scores = np.array([np.mean(scores) if scores else 0 for scores in score_set.values()])\n    \n    # Calculate UCB scores\n    ucb_scores = np.zeros(num_actions)\n    \n    for action_index in range(num_actions):\n        if counts[action_index] > 0:\n            ucb_scores[action_index] = average_scores[action_index] + np.sqrt((2 * np.log(total_selection_count)) / counts[action_index])\n        else:\n            ucb_scores[action_index] = float('inf')  # Prioritize unexplored actions\n    \n    # Exploration vs. Exploitation strategy\n    epsilon = np.sqrt(np.log(1 + current_time_slot) / (current_time_slot + 1))\n    \n    if np.random.rand() < epsilon:  # Explore\n        action_index = np.random.choice(np.where(counts == 0)[0]) if np.any(counts == 0) else np.random.choice(num_actions)\n    else:  # Exploit\n        action_index = np.argmax(ucb_scores)\n    \n    return action_index",
          "objective": 95330058.83953859,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Avoid division by zero by using a small epsilon\n    epsilon = 1e-10\n    exploration_bonus = np.where(selection_counts > 0,\n                                  np.log(total_selection_count + 1) / (selection_counts + 1), \n                                  np.inf)  # High for unselected actions\n\n    # Calculate exploration-exploitation balance\n    balance_factor = current_time_slot / total_time_slots\n    adjusted_scores = (1 - balance_factor) * exploration_bonus + balance_factor * average_scores\n\n    # Normalize adjusted scores\n    adjusted_scores -= np.min(adjusted_scores)  # Shift to non-negative\n    adjusted_scores /= (np.max(adjusted_scores) + epsilon)  # Avoid division by zero\n\n    # Epsilon-greedy strategy with dynamic exploration probability\n    epsilon_dynamic = max(0.1 * (1 - balance_factor), 0.01)  # Decrease exploration probability as time progresses\n    if np.random.rand() < epsilon_dynamic:  # Explore\n        action_index = np.random.choice(num_actions)\n    else:  # Exploit\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 115845412.47295727,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Calculate exploration bonus\n    # Avoid division by zero for actions that have never been selected\n    exploration_bonus = np.where(selection_counts > 0,\n                                  np.log(total_selection_count + 1) / (selection_counts + 1),\n                                  np.inf)  # Encourage exploration for never selected actions\n\n    # Normalize scores: scale average scores to range [0, 1]\n    if np.max(average_scores) - np.min(average_scores) > 1e-10:\n        normalized_scores = (average_scores - np.min(average_scores)) / (np.max(average_scores) - np.min(average_scores))\n    else:\n        normalized_scores = np.zeros(num_actions)  # If all scores are the same\n\n    # Combine exploration and exploitation\n    balance_factor = current_time_slot / total_time_slots\n    combined_scores = (1 - balance_factor) * exploration_bonus + balance_factor * normalized_scores\n\n    # Select action using an epsilon-greedy strategy\n    epsilon_dynamic = max(0.1 * (1 - balance_factor), 0.01)\n    if np.random.rand() < epsilon_dynamic:  # Explore\n        action_index = np.random.choice(num_actions)\n    else:  # Exploit\n        action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": 121021644.99298203,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Avoid division by zero by using a small epsilon\n    epsilon = 1e-10\n    total_selections_plus_one = total_selection_count + 1\n\n    # Exploration component: favor actions with fewer selections\n    exploration_bonus = np.log(total_selections_plus_one) / (selection_counts + 1)\n    exploration_bonus[selection_counts == 0] = np.inf  # Maximize for unselected actions\n\n    # Calculate exploration-exploitation balance\n    balance_factor = current_time_slot / total_time_slots\n    adjusted_scores = (1 - balance_factor) * exploration_bonus + balance_factor * average_scores\n\n    # Normalize adjusted scores for consistent scaling\n    adjusted_scores -= adjusted_scores.min()  # Shift to non-negative\n    adjusted_scores /= (adjusted_scores.max() + epsilon)  # Avoid division by zero\n\n    # Epsilon-greedy strategy with dynamic exploration probability\n    epsilon_dynamic = max(0.1 * (1 - balance_factor), 0.01)  # Controlled exploration decrease\n    if np.random.rand() < epsilon_dynamic:  # Explore\n        action_index = np.random.choice(num_actions)\n    else:  # Exploit\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 137131114.06045893,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Safeguard against division by zero by adding a small epsilon\n    epsilon = 1e-10\n    total_counts = counts + epsilon\n    \n    # Calculate average scores\n    average_scores = np.array([np.mean(scores) if scores else 0 for scores in score_set.values()])\n    \n    # Calculate UCB scores\n    ucb_scores = average_scores + np.sqrt((2 * np.log(total_selection_count + 1)) / total_counts)\n    \n    # Exploration parameter: Epsilon decays over time\n    epsilon_exploration = max(0.1, 1 - current_time_slot / total_time_slots)\n    \n    # Explore or exploit based on current time slot\n    if np.random.rand() < epsilon_exploration:  # Explore\n        # Prefer unexplored actions initially, else choose randomly from all\n        unexplored_actions = np.where(counts == 0)[0]\n        if len(unexplored_actions) > 0:\n            action_index = np.random.choice(unexplored_actions)\n        else:\n            action_index = np.random.choice(num_actions)\n    else:  # Exploit\n        action_index = np.argmax(ucb_scores)\n    \n    return action_index",
          "objective": 242721519.5908218,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores for each action\n    average_scores = []\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        if scores:\n            average_scores.append(np.mean(scores))\n        else:\n            average_scores.append(0)  # Use 0 if no score history\n\n    # Normalize average scores to range [0, 1]\n    max_score = max(average_scores)\n    min_score = min(average_scores)\n    if max_score > min_score:\n        normalized_scores = [(score - min_score) / (max_score - min_score) for score in average_scores]\n    else:\n        normalized_scores = [0.5] * 8  # If all scores are the same, give equal chance\n\n    # Exploration vs. Exploitation strategy\n    epsilon = 1 / (current_time_slot + 1)  # Decaying exploration rate\n    if np.random.rand() < epsilon:\n        # Explore: choose an action randomly\n        action_index = np.random.choice(range(8))\n    else:\n        # Exploit: choose the action based on normalized average scores\n        action_index = np.random.choice(np.where(normalized_scores == np.max(normalized_scores))[0])\n\n    return action_index",
          "objective": 296678861.7404869,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Early exploration phase\n    if total_selection_count < n_actions:\n        unselected_actions = np.where(selection_counts == 0)[0]\n        return int(np.random.choice(unselected_actions))\n\n    # Calculate UCB-based exploration bonus\n    exploration_bonus = np.sqrt(np.log(total_selection_count) / (selection_counts + 1e-5))\n\n    # Gradually decreasing epsilon based on the current time slot\n    epsilon = max(0.1 * (total_time_slots - current_time_slot) / total_time_slots, 0.01)\n\n    # \u03b5-greedy strategy with dynamic exploration-exploitation balance\n    if np.random.rand() < epsilon:\n        # Explore: favor actions based on inverse selection counts\n        action_index = np.random.choice(np.arange(n_actions), p=(1 / (selection_counts + 1e-5)) / np.sum(1 / (selection_counts + 1e-5)))\n    else:\n        # Exploit: select action with highest adjusted score\n        adjusted_scores = average_scores + exploration_bonus\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 368717591.59281415,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Handle cases where selection_counts are zero\n    # Initialize exploration bonuses to infinity for actions never selected\n    exploration_bonus = np.zeros(num_actions)\n    exploration_bonus[selection_counts == 0] = np.inf  # Assign high exploration value to unselected actions\n\n    # Calculate UCB values\n    exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-10))\n    exploration_bonus[selection_counts > 0] = exploration_value[selection_counts > 0]\n\n    # Calculate adjusted scores with UCB\n    adjusted_scores = average_scores + exploration_bonus\n\n    # Epsilon decay based on current time slot to favor exploration earlier\n    exploration_factor = max(0, 1 - (current_time_slot / total_time_slots))\n    epsilon = 0.1 * exploration_factor  # Decrease epsilon over time\n\n    # Epsilon-greedy strategy\n    if np.random.rand() < epsilon:  # Explore\n        action_index = np.random.choice(num_actions)\n    else:  # Exploit\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 369899881.8457066,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = np.array(score_set.get(action_index, []))\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Initialize exploration bonuses\n    exploration_bonus = np.zeros(num_actions)\n    exploration_weight = min(1, (total_time_slots - current_time_slot) / total_time_slots)\n\n    for i in range(num_actions):\n        if selection_counts[i] > 0:\n            exploration_bonus[i] = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[i])\n        else:\n            exploration_bonus[i] = np.inf  # Assign high exploration value to unselected actions\n\n    # Adjust scores with exploration bonus with dynamic weighting\n    adjusted_scores = (1 - exploration_weight) * average_scores + exploration_weight * exploration_bonus\n\n    # Select action based on the highest adjusted score\n    action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 395564444.65215033,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Initialize exploration factor\n    epsilon = 0.1  # Exploration rate for epsilon-greedy approach\n    action_index = None\n\n    # Determine if we should explore or exploit\n    if np.random.rand() < epsilon:\n        # Explore: Choose a random action, favoring less-selected actions\n        action_index = np.random.choice(np.flatnonzero(selection_counts == 0)) if np.any(selection_counts == 0) else np.random.randint(num_actions)\n    else:\n        # Exploit: Calculate UCB for each action\n        ucb_values = np.zeros(num_actions)\n        \n        for action_index in range(num_actions):\n            if selection_counts[action_index] > 0:\n                ucb_values[action_index] = average_scores[action_index] + np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[action_index])\n            else:\n                # Assign a very high value to actions that have never been selected\n                ucb_values[action_index] = np.inf\n        \n        action_index = np.argmax(ucb_values)  # Select action with highest UCB value\n\n    return action_index",
          "objective": 489998018.7315877,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Dynamic exploration factor based on time slot\n    exploration_rate = np.clip(1 - (current_time_slot / total_time_slots), 0.1, 0.5)\n    action_index = None\n\n    # Determine if we should explore or exploit\n    if np.random.rand() < exploration_rate:\n        # Explore: Choose an action that has been selected the least or randomly\n        unexplored_actions = np.flatnonzero(selection_counts == 0)\n        if unexplored_actions.size > 0:\n            action_index = np.random.choice(unexplored_actions)\n        else:\n            action_index = np.random.randint(num_actions)\n    else:\n        # Exploit: Calculate UCB for each action to select the best option\n        ucb_values = np.zeros(num_actions)\n        for action_index in range(num_actions):\n            if selection_counts[action_index] > 0:\n                ucb_values[action_index] = average_scores[action_index] + \\\n                    np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[action_index])\n            else:\n                ucb_values[action_index] = np.inf\n        \n        action_index = np.argmax(ucb_values)  # Select action with highest UCB value\n\n    return action_index",
          "objective": 513341008.18165016,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores and selection counts\n    average_scores = np.zeros(8)\n    selection_counts = np.zeros(8)\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Normalize average scores to range [0, 1]\n    max_score = np.max(average_scores)\n    min_score = np.min(average_scores)\n    if max_score > min_score:\n        normalized_scores = (average_scores - min_score) / (max_score - min_score)\n    else:\n        normalized_scores = np.zeros(8)\n\n    # Calculate exploration terms based on selection counts\n    exploration_bonus = (1 / (selection_counts + 1)) * (1 - (current_time_slot / total_time_slots))\n    exploitation_scores = normalized_scores + exploration_bonus\n\n    # Softmax for final score distribution to balance exploration and exploitation\n    exp_scores = np.exp(exploitation_scores - np.max(exploitation_scores))  # Numerical stability\n    softmax_probs = exp_scores / np.sum(exp_scores)\n\n    # Select action based on softmax probabilities\n    action_index = np.random.choice(range(8), p=softmax_probs)\n\n    return action_index",
          "objective": 542084116.5784013,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Handle the case where no actions have been selected yet\n    if total_selection_count == 0:\n        return np.random.choice(num_actions)\n\n    # Normalize average scores to a [0, 1] range\n    max_score = np.max(average_scores)\n    min_score = np.min(average_scores)\n    range_score = max_score - min_score\n    normalized_scores = (average_scores - min_score) / (range_score if range_score > 0 else 1)\n\n    # Epsilon-greedy strategy for exploration vs exploitation\n    explore_exploit_factor = (total_time_slots - current_time_slot) / total_time_slots\n    epsilon = 0.1 * (1 - explore_exploit_factor)  # Dynamic epsilon\n\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(num_actions)  # Explore\n    else:\n        action_index = np.argmax(normalized_scores)  # Exploit\n    \n    return action_index",
          "objective": 607213535.901753,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n    \n    # Dynamic exploration factor that decreases with time\n    exploration_rate = max(0.1, min((total_time_slots - current_time_slot) / total_time_slots, 0.5))\n    \n    if np.random.rand() < exploration_rate:\n        # Explore: Favor actions with fewer selections\n        weighted_indices = np.random.choice(num_actions, num_actions, replace=False)\n        action_index = weighted_indices[np.argmin(selection_counts[weighted_indices])]\n    else:\n        # Exploit: Calculate a modified score that incorporates exploration\n        modified_scores = average_scores + np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n        action_index = np.argmax(modified_scores)  # Select action with highest modified score\n\n    return action_index",
          "objective": 610564857.3940433,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Calculate average scores considering only the actions that have been selected\n    average_scores = np.array([np.mean(scores) if scores else 0 for scores in score_set.values()])\n    \n    # Calculate base exploration factor inversely related to the counts of actions\n    exploration_bonus = 1 / (counts + 1)  # Add 1 to avoid division by zero\n    \n    # Confidence bounds to encourage exploration\n    confidence_bounds = np.sqrt((2 * np.log(total_selection_count + 1)) / (counts + 1))\n    \n    # Combining average scores with exploration bonus and confidence bounds\n    ucb_scores = average_scores + exploration_bonus * confidence_bounds\n    \n    # Epsilon decay strategy for exploration-exploitation trade-off\n    epsilon = 1 - (current_time_slot / total_time_slots)\n    \n    if np.random.rand() < epsilon:  # Explore\n        unexplored_actions = np.where(counts == 0)[0]\n        if unexplored_actions.size > 0:\n            action_index = np.random.choice(unexplored_actions)\n        else:\n            action_index = np.random.choice(num_actions)\n    else:  # Exploit\n        action_index = np.argmax(ucb_scores)\n    \n    return action_index",
          "objective": 682865861.9629025,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    average_scores = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n    \n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        if action_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Calculate exploration factor based on selection counts\n    exploration_factor = 1 / (1 + action_counts / (1 + total_selection_count / (num_actions * 0.5)))\n\n    # Combine average scores and exploration factor\n    combined_scores = average_scores * exploration_factor\n    \n    # Normalize combined scores to range [0, 1]\n    if np.any(combined_scores):\n        normalized_combined_scores = (combined_scores - np.min(combined_scores)) / (np.max(combined_scores) - np.min(combined_scores))\n    else:\n        normalized_combined_scores = np.ones(num_actions) / num_actions  # Equal chance if all scores are the same\n\n    # Exploration vs. Exploitation strategy\n    epsilon = 1 / (current_time_slot + 1)  # Decaying exploration rate\n    if np.random.rand() < epsilon:\n        # Explore: Choose an action randomly\n        action_index = np.random.choice(range(num_actions))\n    else:\n        # Exploit: Choose the action based on normalized combined scores\n        action_index = np.random.choice(np.where(normalized_combined_scores == np.max(normalized_combined_scores))[0])\n\n    return action_index",
          "objective": 708160273.2685454,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    average_scores = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        if action_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Handle the case where all actions have a count of zero\n    if total_selection_count == 0:\n        return np.random.choice(num_actions)\n\n    # Calculate exploration factor\n    exploration_factor = 1 / (action_counts + 1)\n    \n    # Combine scores with exploration factor\n    combined_scores = average_scores + exploration_factor\n\n    # Softmax for action selection\n    exp_scores = np.exp(combined_scores - np.max(combined_scores))  # for numerical stability\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Epsilon decay for exploration\n    epsilon = 1 / (current_time_slot + 1)\n    if np.random.rand() < epsilon:\n        # Explore: Choose an action randomly\n        action_index = np.random.choice(range(num_actions))\n    else:\n        # Exploit: Choose action based on softmax probabilities\n        action_index = np.random.choice(range(num_actions), p=probabilities)\n\n    return action_index",
          "objective": 807973692.4013216,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores and selection counts for each action\n    average_scores = []\n    selection_counts = []\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        if scores:\n            average_scores.append(np.mean(scores))\n            selection_counts.append(len(scores))\n        else:\n            average_scores.append(0)  # No scores for this action\n            selection_counts.append(0)  # Not selected yet\n    \n    # Avoid division by zero for average scores\n    total_scores = np.sum(average_scores)\n    total_counts = np.sum(selection_counts)\n\n    # Calculate a confidence score for each action\n    confidence_scores = []\n    for i in range(8):\n        if selection_counts[i] > 0:\n            confidence = average_scores[i] * (1 + (total_selection_count / selection_counts[i]))\n        else:\n            confidence = 0  # This action has never been selected\n        confidence_scores.append(confidence)\n    \n    # Dynamic exploration rate\n    exploration_factor = np.sqrt(np.log(total_selection_count + 1) / (np.array(selection_counts) + 1))\n    \n    # Combine scores and exploration factor\n    combined_scores = np.array(confidence_scores) + exploration_factor\n\n    # Normalize combined scores to range [0, 1]\n    max_score = np.max(combined_scores)\n    min_score = np.min(combined_scores)\n    if max_score > min_score:\n        normalized_scores = (combined_scores - min_score) / (max_score - min_score)\n    else:\n        normalized_scores = np.ones(8) * 0.5  # Equal chance if all scores are the same\n\n    # Determine action selection based on exploration vs exploitation\n    epsilon = 1 / (current_time_slot + 1)  # Decaying exploration rate\n    if np.random.rand() < epsilon:\n        # Explore: choose an action randomly\n        action_index = np.random.choice(range(8))\n    else:\n        # Exploit: choose the action based on weighted normalized scores\n        action_index = np.random.choice(np.flatnonzero(normalized_scores == np.max(normalized_scores)))\n\n    return action_index",
          "objective": 826438779.4964143,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = np.array(score_set.get(action_index, []))\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Prevent division by zero and ensure robustness\n    exploration_bonus = np.zeros(num_actions)\n    for i in range(num_actions):\n        if selection_counts[i] > 0:\n            exploration_bonus[i] = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[i])\n        else:\n            exploration_bonus[i] = np.inf  # Assign high exploration value to unselected actions\n\n    # Adjust scores with exploration bonus\n    adjusted_scores = average_scores + exploration_bonus\n\n    # Select action based on the highest adjusted score\n    action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 837338632.1289537,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    \n    # Calculate average scores and selection counts\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    \n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Early exploration phase - prefer actions that have been rarely selected\n    if total_selection_count < n_actions:\n        unselected_actions = np.where(selection_counts == 0)[0]\n        return int(np.random.choice(unselected_actions))\n\n    # Calculate exploration bonus using UCB\n    exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n    \n    # Combine adjusted scores for exploitation and exploration\n    adjusted_scores = average_scores + exploration_bonus\n    \n    # Dynamic epsilon for exploration-exploitation balance\n    epsilon = max(0.1 * (total_time_slots - current_time_slot) / total_time_slots, 0.01)\n\n    # Epsilon-greedy action selection\n    if np.random.rand() < epsilon:\n        # Randomly selecting an action from the available actions\n        action_index = np.random.choice(n_actions)\n    else:\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 852390594.6726172,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    average_scores = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        if action_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Handle case when total_selection_count is 0\n    if total_selection_count == 0:\n        return np.random.choice(num_actions)\n\n    # Normalize average scores to range [0, 1]\n    max_score = np.max(average_scores)\n    min_score = np.min(average_scores)\n    \n    if max_score > min_score:\n        normalized_scores = (average_scores - min_score) / (max_score - min_score)\n    else:\n        normalized_scores.fill(0.5)  # All scores are the same\n\n    # Dynamic exploration-exploitation trade-off\n    exploration_probability = 1 - (current_time_slot / total_time_slots)\n\n    # Explore or exploit\n    if np.random.rand() < exploration_probability:\n        # Explore: Choose action based on selection counts\n        # Actions with fewer selections are favored\n        exploration_scores = (1 / (1 + action_counts))  # Avoid division by 0\n        selection_probabilities = exploration_scores / np.sum(exploration_scores)\n        action_index = np.random.choice(num_actions, p=selection_probabilities)\n    else:\n        # Exploit: Choose action based on normalized scores\n        action_index = np.random.choice(np.where(normalized_scores == np.max(normalized_scores))[0])\n\n    return action_index",
          "objective": 855978026.0437337,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if scores:\n            average_scores[action_index] = np.mean(scores)\n    \n    # Handle the exploration using UCB\n    total_counts = total_selection_count + 1  # To avoid division by zero\n    exploration_bonus = np.sqrt((np.log(total_counts) / (selection_counts + 1e-5)))  # Avoid division by zero in UCB\n    \n    # Calculate combined scores\n    combined_scores = average_scores + exploration_bonus\n\n    # Normalize combined scores to prevent negative values\n    normalized_scores = (combined_scores - np.min(combined_scores)) / (np.max(combined_scores) - np.min(combined_scores) + 1e-5)\n\n    # Epsilon-greedy strategy for exploration\n    epsilon = 1 - (current_time_slot / total_time_slots)  # Linear decay from 1 to 0\n    if np.random.rand() < epsilon:\n        # Explore: select a random action\n        action_index = np.random.choice(num_actions)\n    else:\n        # Exploit: select based on the highest normalized score\n        action_index = np.random.choice(np.flatnonzero(normalized_scores == np.max(normalized_scores)))\n\n    return action_index",
          "objective": 887607929.3336439,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Early exploration phase\n    if total_selection_count < n_actions:\n        unselected_actions = np.where(selection_counts == 0)[0]\n        return int(np.random.choice(unselected_actions))\n\n    # Calculate exploration factor using UCB\n    exploration_factors = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n    \n    # Combine UCB values with average scores\n    ucb_values = average_scores + exploration_factors\n\n    # Dynamic exploration-exploitation balance based on current time slot\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    score_combination = (exploration_weight * average_scores) + ((1 - exploration_weight) * ucb_values)\n\n    # Introduce a randomness factor to avoid local optima\n    randomness = np.random.rand(n_actions) * (1 - exploration_weight)\n    adjusted_scores = score_combination + randomness\n\n    # Select the action with the highest adjusted score\n    action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 938401036.9516126,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Compute average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Compute exploration bonuses\n    exploration_bonus = np.where(selection_counts > 0, 1 / selection_counts, 1)\n\n    # Scale adjusted scores to avoid negative values and to normalize\n    adjusted_scores = average_scores + exploration_bonus\n    \n    # Dynamic exploration vs. exploitation factor\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n    combined_scores = exploration_factor * adjusted_scores + (1 - exploration_factor) * exploration_bonus\n\n    # Normalize combined scores for selection probabilities\n    exp_scores = np.exp(combined_scores - np.max(combined_scores))  # Stability with softmax\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Select action based on computed probabilities\n    action_index = np.random.choice(range(num_actions), p=probabilities)\n\n    return action_index",
          "objective": 949393216.0666382,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Dynamic epsilon for exploration-exploitation trade-off\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n    \n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        # Explore: select a random action with preference to less selected actions\n        action_index = np.argmin(selection_counts) if np.any(selection_counts == 0) else np.random.choice(num_actions)\n    else:\n        # Exploit: select the action with the highest average score\n        action_index = np.argmax(average_scores)\n\n    return action_index",
          "objective": 972987228.9877857,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Avoid division by zero and assign a baseline for exploration\n    adjusted_counts = selection_counts + 1  # Ensure no count is zero\n\n    # Calculate UCB-based adjusted scores\n    confidence_intervals = np.sqrt(2 * np.log(total_selection_count + 1) / adjusted_counts)\n    ucb_scores = average_scores + confidence_intervals\n    \n    # Dynamic exploration vs. exploitation factor\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n    \n    # Weight the scores for exploration and exploitation\n    weighted_scores = (exploration_factor * ucb_scores) + ((1 - exploration_factor) * average_scores)\n\n    # Softmax for action probabilities to maintain continuity\n    exp_scores = np.exp(weighted_scores - np.max(weighted_scores))  # Stability in softmax\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Select action based on probabilities\n    action_index = np.random.choice(range(num_actions), p=probabilities)\n    \n    return action_index",
          "objective": 1077985232.8028674,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Handle exploration bonus\n    exploration_bonus = np.log(total_selection_count + 1) / (selection_counts + 1)  # Logarithmic exploration factor\n    adjusted_scores = average_scores + exploration_bonus\n\n    # Normalize scores\n    if adjusted_scores.size > 0:\n        adjusted_scores -= np.min(adjusted_scores)  # Shift to non-negative\n        adjusted_scores /= (np.max(adjusted_scores) + 1e-10)  # Avoid division by zero\n\n    # Dynamic exploration vs. exploitation factor\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n    weighted_scores = (exploration_factor * adjusted_scores) + ((1 - exploration_factor) * (1 / (selection_counts + 1)))\n\n    # Softmax for action probabilities\n    exp_scores = np.exp(weighted_scores - np.max(weighted_scores))  # Stability in softmax\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Select action based on probabilities\n    action_index = np.random.choice(range(num_actions), p=probabilities)\n\n    return action_index",
          "objective": 1080450454.1650503,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Avoid division by zero in reward to selection ratio calculation\n    with np.errstate(divide='ignore', invalid='ignore'):\n        reward_to_selection_ratio = np.where(selection_counts > 0, average_scores / selection_counts, 0)\n\n    # Compute exploration bonus based on selection counts\n    exploration_bonus = (1.0 / (1 + selection_counts))\n\n    # Combine scores with exploration bonus\n    combined_scores = reward_to_selection_ratio + exploration_bonus\n\n    # Normalize combined scores for better comparative analysis\n    normalized_scores = (combined_scores - np.min(combined_scores)) / (np.max(combined_scores) - np.min(combined_scores) + 1e-5)\n\n    # Calculate dynamic epsilon for exploration-exploitation balance\n    epsilon = min(1.0, 1.0 - (current_time_slot / total_time_slots))\n\n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(range(num_actions))  # Explore\n    else:\n        action_index = np.random.choice(np.where(normalized_scores == np.max(normalized_scores))[0])  # Exploit\n\n    return action_index",
          "objective": 1100089344.2749805,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    # Calculate average scores and counts\n    for action_index in range(num_actions):\n        if action_index in score_set and score_set[action_index]:\n            average_scores[action_index] = np.mean(score_set[action_index])\n            selection_counts[action_index] = len(score_set[action_index])\n\n    total_counts = np.sum(selection_counts)\n\n    # Normalize scores if at least one action has been selected\n    if total_counts > 0:\n        average_scores = np.nan_to_num(average_scores)\n        normalized_scores = (average_scores - np.min(average_scores)) / (np.max(average_scores) - np.min(average_scores)) if np.max(average_scores) - np.min(average_scores) > 0 else np.zeros(num_actions)\n    else:\n        normalized_scores = np.full(num_actions, 0.5)\n\n    # Exploration factor with a slight decay over time based on selection counts\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (1 + selection_counts))\n    \n    # Combine exploitation (normalized scores) and exploration bonuses\n    combined_scores = normalized_scores + exploration_bonus\n\n    # Decay factor to shift focus to exploitation as time progresses\n    decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n    combined_scores *= decay_factor\n\n    # Epsilon-greedy strategy for controlled randomness\n    epsilon = 1 - (current_time_slot / total_time_slots)  # Decreasing epsilon over time\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(range(num_actions), p=combined_scores / np.sum(combined_scores))\n    else:\n        action_index = np.random.choice(range(num_actions))\n\n    return action_index",
          "objective": 1104341598.9972858,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Compute average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if scores:\n            average_scores[action_index] = np.mean(scores)\n\n    # Handle exploration\n    beta = 1  # Exploration constant, can be tuned\n    exploration_bonus = beta / (selection_counts + 1)  # Bonus inversely proportional to selections\n    adjusted_scores = average_scores + exploration_bonus\n\n    # Normalize to range [0, 1]\n    if total_selection_count > 0:\n        adjusted_scores -= np.min(adjusted_scores)  # Shift to non-negative\n        adjusted_scores /= (np.max(adjusted_scores) + 1e-10)  # Prevent division by zero\n    else:\n        adjusted_scores = np.ones(num_actions) / num_actions  # Uniform selection if no history\n\n    # Dynamic exploration vs. exploitation factor\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n    combined_scores = exploration_factor * adjusted_scores + (1 - exploration_factor) * (1 / (selection_counts + 1))\n\n    # Softmax for action selection\n    exp_scores = np.exp(combined_scores - np.max(combined_scores))  # Stability in softmax\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Select action based on probabilities\n    action_index = np.random.choice(range(num_actions), p=probabilities)\n\n    return action_index",
          "objective": 1141396752.8463295,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    average_scores = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n    \n    # Compute average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        if action_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n    \n    # Protect against zero division errors\n    max_count = np.max(action_counts) + 1  # Avoid division by zero\n    \n    # Calculate exploration bonus based on action counts\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (action_counts + 1))\n    \n    # Combine average scores and exploration bonuses\n    combined_scores = average_scores + exploration_bonus\n    \n    # Normalize combined scores for stability\n    if np.max(combined_scores) > np.min(combined_scores):\n        normalized_combined_scores = (combined_scores - np.min(combined_scores)) / (np.max(combined_scores) - np.min(combined_scores))\n    else:\n        normalized_combined_scores = np.ones(num_actions) / num_actions  # All scores are the same\n    \n    # Epsilon decay for exploration vs. exploitation\n    epsilon = np.exp(-current_time_slot / total_time_slots)  # Exponential decay based on time\n    if np.random.rand() < epsilon:\n        # Explore: Choose an action randomly\n        action_index = np.random.choice(range(num_actions))\n    else:\n        # Exploit: Choose the best action based on normalized combined scores\n        action_index = np.random.choice(np.flatnonzero(normalized_combined_scores == np.max(normalized_combined_scores)))\n\n    return action_index",
          "objective": 1145777875.7705772,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Handle the case where no actions have been selected yet\n    if total_selection_count == 0:\n        return np.random.choice(num_actions)\n\n    # Normalize average scores\n    max_score = np.max(average_scores) if np.any(average_scores) else 1.0\n    min_score = np.min(average_scores) if np.any(average_scores) else 0.0\n    range_score = max_score - min_score\n    normalized_scores = (average_scores - min_score) / (range_score if range_score > 0 else 1)\n\n    # Exploration strategy: manage exploration vs exploitation\n    exploration_term = np.sqrt(total_selection_count / (1 + selection_counts))\n    \n    # Blend exploration and weighted scores\n    weighted_scores = normalized_scores * exploration_term\n\n    # Dynamic adjustment of exploration level\n    explore_exploit_factor = (total_time_slots - current_time_slot) / total_time_slots\n    exploration_probability = 0.2 * (1 - explore_exploit_factor)\n\n    # Calculate probabilities\n    action_probabilities = (1 - exploration_probability) * (weighted_scores / np.sum(weighted_scores)) + \\\n                           (exploration_probability / num_actions)\n\n    # Select action based on the calculated probabilities\n    action_index = np.random.choice(num_actions, p=action_probabilities)\n    \n    return action_index",
          "objective": 1180537277.5932813,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        if action_index in score_set and score_set[action_index]:  # Check if action has scores\n            average_scores[action_index] = np.mean(score_set[action_index])\n            selection_counts[action_index] = len(score_set[action_index])\n    \n    # Use different logic if all actions have zero selection counts\n    total_counts = np.sum(selection_counts)\n    if total_counts > 0:\n        average_scores = np.nan_to_num(average_scores)  # Handle any NaNs\n        normalized_scores = (average_scores - np.min(average_scores)) / (np.max(average_scores) - np.min(average_scores)) if np.max(average_scores) - np.min(average_scores) > 0 else np.full(num_actions, 0.5)  # Avoid division by zero\n    else:\n        normalized_scores = np.full(num_actions, 0.5)  # Equal opportunity if no actions have been taken\n\n    # Exploration factor favors less selected actions\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (1 + selection_counts))\n\n    # Weighted scores combining exploitation and exploration\n    combined_scores = normalized_scores + exploration_bonus\n\n    # Apply decay on exploration factor over time \n    decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n    combined_scores *= decay_factor\n\n    # Softmax normalization to compute selection probabilities\n    exp_scores = np.exp(combined_scores - np.max(combined_scores))  # Stability in exponential\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Select action based on computed probabilities\n    action_index = np.random.choice(range(num_actions), p=probabilities)\n\n    return action_index",
          "objective": 1188386454.5848637,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate the average scores and counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        average_scores[action_index] = np.mean(scores) if selection_counts[action_index] > 0 else 0\n\n    # Calculate exploration bonus\n    exploration_bonus = np.where(selection_counts > 0, 1 / selection_counts, 1)\n\n    # Dynamic exploration-exploitation balance\n    remaining_slots = total_time_slots - current_time_slot\n    exploration_factor = remaining_slots / total_time_slots\n\n    # Enhanced scoring with exploration factor\n    adjusted_scores = average_scores + exploration_bonus\n    combined_scores = (1 - exploration_factor) * adjusted_scores + exploration_factor * exploration_bonus\n\n    # Softmax for improved probability distribution\n    exp_scores = np.exp(combined_scores - np.max(combined_scores))  # For numerical stability\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Select an action based on the probability distribution\n    action_index = np.random.choice(range(num_actions), p=probabilities)\n\n    return action_index",
          "objective": 1262483338.4189959,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if scores:\n            average_scores[action_index] = np.mean(scores)\n    \n    # Calculate the exploration bonus\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))  # avoid division by zero\n    scaling_factor = 0.5  # scaling exploration impact\n\n    # Combined score calculation with exploration\n    combined_scores = average_scores + scaling_factor * exploration_bonus\n    \n    # Normalize combined scores to range [0, 1]\n    normalized_scores = (combined_scores - np.min(combined_scores)) / (np.max(combined_scores) - np.min(combined_scores) + 1e-5)\n    \n    # Decaying exploration chance\n    epsilon = 1 - (current_time_slot / total_time_slots)  # starts high, decreases over time\n    if np.random.rand() < epsilon:\n        # Explore: randomly select an action\n        action_index = np.random.choice(range(num_actions))\n    else:\n        # Exploit: select based on weighted normalized scores\n        action_index = np.random.choice(np.flatnonzero(normalized_scores == np.max(normalized_scores)))\n\n    return action_index",
          "objective": 1397298477.841416,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n\n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    \n    # Calculate average scores and selection counts\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Early exploration phase: select unselected actions\n    if total_selection_count < n_actions:\n        unselected_actions = np.where(selection_counts == 0)[0]\n        return int(np.random.choice(unselected_actions))\n\n    # Calculate UCB values to encourage exploration\n    ucb_values = average_scores + np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n\n    # Exploration-exploitation balance computation\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = exploration_weight * average_scores + (1 - exploration_weight) * ucb_values\n\n    # Introduce a stochastic element to ensure continued exploration\n    stochastic_factor = np.random.uniform(0, 1, n_actions)\n    adjusted_scores += 0.1 * stochastic_factor  # Adjust weight to introduce randomness\n\n    # Select the action with the highest adjusted score\n    action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": 1554650725.6145468,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Handle cases with no selections\n    if total_selection_count == 0:\n        return np.random.choice(range(num_actions))\n    \n    # Calculate reward-to-selection ratio to encourage exploration of less-selected actions\n    reward_to_selection_ratio = np.where(selection_counts > 0, average_scores / selection_counts, 0)\n\n    # Normalize scores for exploitation and exploration\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n    normalized_scores = exploration_factor * (reward_to_selection_ratio - np.min(reward_to_selection_ratio) + 1e-5)  # Avoid division by zero\n    normalized_scores /= np.max(normalized_scores)\n\n    # Combine exploration and exploitation\n    epsilon = 1 / (current_time_slot + 1)\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(range(num_actions))  # Explore\n    else:\n        action_index = np.random.choice(np.where(normalized_scores == np.max(normalized_scores))[0])  # Exploit\n\n    return action_index",
          "objective": 1556138101.7502942,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    # Calculate the average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Dynamic epsilon for epsilon-greedy strategy\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n    \n    # Randomly select an action based on epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        # Exploration: select randomly from unselected actions\n        unselected_actions = [i for i in range(num_actions) if selection_counts[i] == 0]\n        if unselected_actions:\n            action_index = np.random.choice(unselected_actions)\n        else:\n            action_index = np.random.choice(num_actions)\n    else:\n        # Exploitation: select action with the highest average score\n        best_avg_score = np.max(average_scores)\n        best_actions = [i for i in range(num_actions) if average_scores[i] == best_avg_score]\n        action_index = np.random.choice(best_actions)\n\n    return action_index",
          "objective": 1692870913.711633,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate the exploration factor that decays with time\n    exploration_factor = max(0.1, 1 - current_time_slot / total_time_slots)\n    \n    # Initialize arrays to store average scores and selection counts\n    avg_scores = np.zeros(8)\n    selection_counts = np.zeros(8)\n    \n    # Calculate average scores and counting selections\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n    \n    # Apply score adjustment based on counts and exploration\n    exploration_bonus = exploration_factor / (selection_counts + 1e-6)  # Avoid division by zero\n    adjusted_scores = avg_scores + exploration_bonus\n    \n    # Discover unexplored actions\n    unexplored_actions = selection_counts == 0\n    if np.any(unexplored_actions):\n        adjusted_scores[unexplored_actions] = np.inf  # Mark unexplored for selection\n    \n    # Select action with the highest adjusted score\n    action_index = np.argmax(adjusted_scores)\n    \n    return action_index",
          "objective": 1993538522.6029208,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    average_scores = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        if action_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Handle cases with zero selections to prevent division by zero\n    with np.errstate(divide='ignore', invalid='ignore'):\n        ucb_values = average_scores + np.sqrt((2 * np.log(total_selection_count + 1)) / (action_counts + 1))\n        ucb_values[action_counts == 0] = np.inf  # Assign infinity if the action hasn't been selected\n\n    # Determine exploration probability\n    epsilon = np.clip(1 - current_time_slot / total_time_slots, 0.1, 1)  # Decaying epsilon\n    if np.random.rand() < epsilon:\n        # Explore: select actions uniformly to promote exploration\n        action_index = np.random.choice(range(num_actions))\n    else:\n        # Exploit: select based on UCB values\n        action_index = np.random.choice(np.flatnonzero(ucb_values == np.max(ucb_values)))\n\n    return action_index",
          "objective": 2000427134.0783203,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and counts of selections for each action\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if scores:\n            average_scores[action_index] = np.mean(scores)\n\n    # Prevent division by zero for exploration bonus calculation\n    adjusted_counts = selection_counts + 1e-10  # Adding a small value to avoid division by zero\n    exploration_bonus = 1.0 / adjusted_counts  # Higher reward for less selected actions\n\n    # Combine the average scores with exploration bonus\n    combined_scores = average_scores + exploration_bonus\n\n    # Normalization\n    if total_selection_count > 0:\n        combined_scores -= np.min(combined_scores)  # Shift to non-negative\n        combined_scores /= (np.max(combined_scores) + 1e-10)  # Normalize to [0, 1]\n    else:\n        combined_scores = np.ones(num_actions) / num_actions  # Uniform selection if no history\n\n    # Dynamic exploration vs. exploitation factor\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n    dynamic_scores = exploration_factor * combined_scores + (1 - exploration_factor) * (1 / (adjusted_counts))\n\n    # Softmax for action selection\n    exp_scores = np.exp(dynamic_scores - np.max(dynamic_scores))  # Stability in softmax\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Select action based on probabilities\n    action_index = np.random.choice(range(num_actions), p=probabilities)\n\n    return action_index",
          "objective": 2050168229.3147254,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    \n    # Initialize arrays to hold average scores and counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Handle cases where no actions have been selected yet\n    if total_selection_count == 0:\n        return np.random.choice(num_actions)\n\n    # Calculate normalized scores to avoid bias\n    normalized_scores = (average_scores - np.min(average_scores)) / (\n                          np.max(average_scores) - np.min(average_scores) \n                          if np.max(average_scores) > np.min(average_scores) else 1)\n\n    # Introduce an exploration term based on selection frequency\n    exploration_bonus = np.sqrt(total_selection_count / (1 + selection_counts))\n\n    # Calculate the selection scores for each action\n    selection_scores = normalized_scores * exploration_bonus\n\n    # Balance exploration and exploitation based on time remaining\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    exploitation_weight = 1 - exploration_weight\n\n    # Create probabilities with added exploration\n    action_probabilities = (selection_scores * exploitation_weight + \n                            (1 / num_actions) * exploration_weight)\n\n    # Normalize action probabilities\n    action_probabilities /= np.sum(action_probabilities)\n\n    # Select action based on computed probabilities\n    action_index = np.random.choice(num_actions, p=action_probabilities)\n    \n    return action_index",
          "objective": 2127227938.6414807,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action in range(num_actions):\n        scores = score_set.get(action, [])\n        selection_counts[action] = len(scores)\n        \n        if selection_counts[action] > 0:\n            average_scores[action] = np.mean(scores)\n\n    # Define exploration parameters\n    exploration_rate = max(0.1, 1 - (current_time_slot / total_time_slots))\n    epsilon = 0.1 * exploration_rate\n\n    # Compute UCB for each action\n    ucb_values = np.zeros(num_actions)\n    for action in range(num_actions):\n        if selection_counts[action] > 0:\n            ucb_values[action] = average_scores[action] + np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts[action] + 1e-5))\n        else:\n            ucb_values[action] = float('inf')  # Encourage selection of unexplored actions\n\n    # Choose an action based on epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        # Explore: Select a random action\n        unexplored_actions = np.flatnonzero(selection_counts == 0)\n        if unexplored_actions.size > 0:\n            action_index = np.random.choice(unexplored_actions)\n        else:\n            action_index = np.random.randint(num_actions)\n    else:\n        # Exploit: Select the action with the highest UCB value\n        action_index = np.argmax(ucb_values)\n    \n    return action_index",
          "objective": 2140530239.9993567,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores and selection counts for each action\n    average_scores = []\n    selection_counts = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate average score or assign 0 if no selections have been made\n        if selection_count > 0:\n            avg_score = np.mean(scores)\n        else:\n            avg_score = 0\n        \n        average_scores.append(avg_score)\n        selection_counts.append(selection_count)\n\n    # Normalize average scores and handle edge cases\n    max_score = max(average_scores)\n    if max_score > 0: \n        normalized_scores = [score / max_score for score in average_scores]\n    else:\n        normalized_scores = [0.5] * 8  # All scores are zero\n\n    # Explore vs. Exploit factor\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots  # Gradual shift towards exploitation\n    adjusted_scores = [(normalized_scores[i] * (1 - exploration_factor) + (exploration_factor / 8)) for i in range(8)]\n    \n    # Select action based on adjusted scores\n    action_index = np.random.choice(np.arange(8), p=adjusted_scores / np.sum(adjusted_scores))\n\n    return action_index",
          "objective": 2227427928.01632,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Handle cases where all actions have been selected zero times\n    if total_selection_count == 0:\n        return np.random.choice(num_actions)\n\n    # Normalize average scores to range [0, 1]\n    max_score = np.max(average_scores) if np.any(average_scores) else 1.0  # Prevent division by zero\n    min_score = np.min(average_scores) if np.any(average_scores) else 0.0\n    range_score = max_score - min_score\n    normalized_scores = (average_scores - min_score) / (range_score if range_score > 0 else 1)\n\n    # Modify exploration factor based on selection counts\n    exploration_factor = np.sqrt(total_selection_count / (1 + selection_counts))\n    weighted_scores = normalized_scores * exploration_factor\n\n    # Dynamic exploration based on time slot progression\n    decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n    explore_exploit_ratio = 0.2 + (0.8 * decay_factor)  # Gradually shift from exploration to exploitation\n\n    # Create probabilities for selecting actions\n    action_probabilities = (1 - explore_exploit_ratio) * (weighted_scores / np.sum(weighted_scores)) + \\\n                           (explore_exploit_ratio / num_actions)\n\n    # Select action based on computed probabilities\n    action_index = np.random.choice(num_actions, p=action_probabilities)\n    \n    return action_index",
          "objective": 2409561827.191992,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Define constants\n    exploration_factor = max(0.1, 1 - current_time_slot / total_time_slots)  # Exploration decays with time\n    \n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(8)\n    selection_counts = np.zeros(8)\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)  # Count selections\n        \n        if selection_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)  # Calculate average score\n    \n    # Calculate a score that combines average scores and exploration factor\n    combined_scores = avg_scores + exploration_factor / (selection_counts + 1e-6)\n    \n    # Handle the case where any action has never been selected\n    unexplored_actions = selection_counts == 0\n    if np.any(unexplored_actions):\n        combined_scores[unexplored_actions] = np.inf  # Give infinite value to unexplored actions\n    \n    # Select action with the highest score\n    action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": 2527664938.816339,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    \n    # Initialize average scores and selection counts\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        if scores:\n            average_scores[action_index] = np.mean(scores)\n\n    # Early exploration of unselected actions\n    if total_selection_count < n_actions:\n        unselected_actions = np.where(selection_counts == 0)[0]\n        return int(np.random.choice(unselected_actions))\n\n    # Softmax exploration factor\n    softmax_temperature = 0.5 * (total_time_slots - current_time_slot) / total_time_slots + 0.1\n    adjusted_scores = average_scores + np.sqrt(np.log(total_selection_count) / (selection_counts + 1e-5))\n    exp_scores = np.exp(adjusted_scores / softmax_temperature)\n    softmax_probabilities = exp_scores / np.sum(exp_scores)\n\n    # Select action based on softmax probabilities\n    action_index = np.random.choice(np.arange(n_actions), p=softmax_probabilities)\n\n    return action_index",
          "objective": 2614999446.658867,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    \n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Calculate exploration factor\n    exploration_factor = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n    normalized_scores = np.clip(average_scores, 0, 1)  # Ensure scores are in [0, 1]\n    \n    # Dynamic epsilon based on time slot\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n\n    # Mixed strategy\n    if np.random.rand() < epsilon:\n        # Exploration: Choose an action randomly\n        action_index = np.random.choice(range(n_actions))\n    else:\n        # Exploitation: Combine average scores and exploration bonus\n        exploitation_scores = normalized_scores + exploration_factor\n        action_index = np.argmax(exploitation_scores)\n\n    return action_index",
          "objective": 2636065487.712907,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and track selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Avoid division by zero in normalization by handling the scenario where total_count is zero\n    if total_selection_count == 0:\n        return np.random.choice(num_actions)\n\n    # Normalize average scores\n    max_score = np.max(average_scores) if np.any(average_scores) else 1\n    min_score = np.min(average_scores) if np.any(average_scores) else 0\n    normalized_scores = (average_scores - min_score) / (max_score - min_score) if max_score > min_score else np.zeros(num_actions)\n\n    # Compute exploration bonus\n    exploration_bonus = np.log(total_selection_count + 1) / (selection_counts + 1)  # Add 1 to avoid division by zero\n\n    # Calculate a dynamic exploration vs exploitation ratio\n    decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n    explore_exploit_ratio = 0.2 + 0.8 * decay_factor\n\n    # Weighted scores combining normalized scores and exploration bonus\n    weighted_scores = normalized_scores + exploration_bonus\n    weighted_scores = np.clip(weighted_scores, 0, None)  # Ensure no negative scores\n\n    # Create action selection probabilities\n    action_probabilities = (1 - explore_exploit_ratio) * (weighted_scores / np.sum(weighted_scores)) + \\\n                           (explore_exploit_ratio / num_actions)\n\n    # Stochastic element for exploration\n    randomness = np.random.rand()\n    if randomness < 0.1:  # 10% chance for random selection to maintain exploration\n        action_index = np.random.choice(num_actions)\n    else:\n        # Select action based on computed probabilities\n        action_index = np.random.choice(num_actions, p=action_probabilities)\n\n    return action_index",
          "objective": 2767029798.6032786,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration rate\n    exploration = np.random.rand() < epsilon\n\n    if exploration:\n        # Randomly select an action (0 to 7)\n        action_index = np.random.randint(0, 8)\n    else:\n        avg_scores = []\n        \n        for action_index in range(8):\n            scores = score_set.get(action_index, [])\n            if scores:\n                avg_score = np.mean(scores)\n            else:\n                avg_score = 0  # Default to 0 if no scores are available\n            avg_scores.append(avg_score)\n        \n        # Normalize average scores based on the total selection count\n        if total_selection_count > 0:\n            normalized_scores = np.array(avg_scores) / total_selection_count\n        else:\n            normalized_scores = np.array(avg_scores)  # If no selections have been made yet\n            \n        # Select action with highest normalized score\n        action_index = np.argmax(normalized_scores)\n    \n    return action_index",
          "objective": 2819007567.848013,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        if scores:\n            average_scores[action_index] = np.mean(scores)\n            selection_counts[action_index] = len(scores)\n    \n    # Handle cases where selection_counts are zero\n    epsilon = 1e-5\n    adjusted_counts = selection_counts + epsilon\n    exploration_factor = np.sqrt(total_selection_count / adjusted_counts)\n    \n    # Normalize average scores to the [0, 1] range\n    max_score = np.max(average_scores)\n    min_score = np.min(average_scores)\n    range_score = max_score - min_score\n\n    if range_score > 0:\n        normalized_scores = (average_scores - min_score) / range_score\n    else:\n        normalized_scores = np.full(num_actions, 0.5)  # Default to equal chances if all scores are the same\n\n    # Combine normalized scores with exploration factors\n    combined_scores = normalized_scores * exploration_factor\n\n    # Decay factor based on current time slot\n    decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n    exploration_prob = 1 - decay_factor\n    \n    # Choose action based on combined scores or exploration\n    if np.random.rand() < exploration_prob:\n        action_index = np.random.choice(num_actions, p=combined_scores / np.sum(combined_scores))\n    else:\n        action_index = np.random.choice(range(num_actions))\n\n    return action_index",
          "objective": 2930610563.6719103,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Normalizing parameters\n    min_exploration_threshold = 0.1\n    exploration_factor = max(min_exploration_threshold, (total_time_slots - current_time_slot) / total_time_slots)\n    exploitation_weight = 0.9  # Balance towards past performance\n\n    # Initialize averages and counts\n    avg_scores = np.zeros(8)\n    action_counts = np.zeros(8)\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        avg_scores[action_index] = np.mean(scores) if action_counts[action_index] > 0 else 0\n\n    # Avoid division by zero and evaluate normalized scores\n    adjusted_scores = np.zeros(8)\n    for i in range(8):\n        count_adjustment = 1 / (action_counts[i] + 1)  # Smoothing adjustment\n        adjusted_scores[i] = (exploitation_weight * avg_scores[i]) + ((1 - exploitation_weight) * count_adjustment)\n\n    # Score normalization\n    if total_selection_count > 0:\n        normalized_scores = adjusted_scores / total_selection_count\n    else:\n        normalized_scores = adjusted_scores\n\n    # Dynamic exploration-exploitation balance\n    random_choice_threshold = np.random.rand()\n    if random_choice_threshold < exploration_factor:\n        # Random selection for exploration\n        action_index = np.random.choice([i for i in range(8) if action_counts[i] == 0], \n                                         size=1)[0] if any(action_counts == 0) else np.random.randint(8)\n    else:\n        # Select action with highest normalized score\n        action_index = np.argmax(normalized_scores)\n\n    return action_index",
          "objective": 2960391398.8420486,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Compute average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if scores:\n            average_scores[action_index] = np.mean(scores)\n\n    # Handle cases where actions have not been selected\n    for action_index in range(num_actions):\n        if selection_counts[action_index] == 0:\n            average_scores[action_index] = 0  # Assign a zero score to unselected actions\n\n    # Normalize scores\n    max_score = np.max(average_scores) if np.max(average_scores) > 0 else 1\n    normalized_scores = average_scores / max_score  # Normalize to [0, 1]\n\n    # Epsilon-greedy selection\n    epsilon = 1 - (current_time_slot / total_time_slots)  # Decay epsilon over time\n    if np.random.rand() < epsilon:\n        # Exploration: randomly select an action\n        action_index = np.random.choice(range(num_actions))\n    else:\n        # Exploitation: select the best action\n        action_index = np.argmax(normalized_scores)\n\n    return action_index",
          "objective": 3037612505.34788,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Define exploration probability and exploration weight\n    epsilon = 0.1 * (1 - current_time_slot / total_time_slots)\n    \n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(8)\n    selection_counts = np.zeros(8)\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)  # Count selections\n        \n        if selection_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)  # Calculate average score\n\n    # Calculate UCB values for each action and incorporate exploration\n    upper_confidence_bounds = (\n        avg_scores + \n        np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-6))\n    )\n    \n    # If an action has never been selected, give it a high score to encourage exploration\n    unexplored_actions = selection_counts == 0\n    if np.any(unexplored_actions):\n        upper_confidence_bounds[unexplored_actions] = np.inf  # Assign infinite value to unexplored actions\n    \n    # Select action based on epsilon-greedy method\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(np.flatnonzero(selection_counts == 0) if np.any(unexplored_actions) else range(8))\n    else:\n        action_index = np.argmax(upper_confidence_bounds)\n\n    return action_index",
          "objective": 3133262154.063756,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores for each action\n    average_scores = np.zeros(8)\n    selection_counts = np.zeros(8)\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if scores:\n            average_scores[action_index] = np.mean(scores)\n\n    # Handle cases where no actions have been selected\n    if total_selection_count == 0:\n        return np.random.choice(range(8))  # Select randomly if no data\n\n    # Adjust average scores with selection counts\n    exploration_factor = (1 - (current_time_slot / total_time_slots)) * 0.1\n    adjusted_scores = average_scores + (exploration_factor * (1 / (selection_counts + 1)))\n\n    # Normalize adjusted scores to range [0, 1]\n    max_score = np.max(adjusted_scores)\n    if max_score > 0:\n        normalized_scores = adjusted_scores / max_score\n    else:\n        normalized_scores = np.zeros(8)\n    \n    # Exploration vs. Exploitation strategy using a softmax approach\n    temperature = max(0.1, 1 - (current_time_slot / total_time_slots))  # Decaying temperature\n    exp_scores = np.exp(normalized_scores / temperature)\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Select action based on probabilities\n    action_index = np.random.choice(range(8), p=probabilities)\n\n    return action_index",
          "objective": 3141548241.801644,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        # Compute the average score for each action if it has been selected\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Calculate UCB for each action\n    exploration_factor = 2  # Can be adjusted for more or less exploration\n    ucb_scores = np.zeros(num_actions)\n    \n    for action_index in range(num_actions):\n        if selection_counts[action_index] == 0:\n            ucb_scores[action_index] = float('inf')  # Prefer unselected actions\n        else:\n            ucb_scores[action_index] = average_scores[action_index] + \\\n                                        exploration_factor * np.sqrt(np.log(total_selection_count) / selection_counts[action_index])\n\n    # Normalize UCB scores for softmax selection\n    exp_scores = np.exp(ucb_scores - np.max(ucb_scores))  # Numerical stability\n    softmax_probs = exp_scores / np.sum(exp_scores)\n\n    # Select action based on softmax probabilities\n    action_index = np.random.choice(range(num_actions), p=softmax_probs)\n\n    return action_index",
          "objective": 3201031427.180463,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        if action_index in score_set and len(score_set[action_index]) > 0:\n            average_scores[action_index] = np.mean(score_set[action_index])\n            selection_counts[action_index] = len(score_set[action_index])\n\n    # Normalize scores\n    normalized_scores = np.zeros(num_actions)\n    if total_selection_count > 0:\n        max_avg_score = average_scores.max()\n        if max_avg_score > 0:\n            normalized_scores = average_scores / max_avg_score\n\n    # Exploration factor\n    exploration_factor = 1 / (1 + selection_counts)\n\n    # Adaptive weight for exploration vs exploitation\n    exploration_weight = (1 - current_time_slot / total_time_slots) + (0.2 * exploration_factor)\n\n    # Combined score\n    combined_scores = (1 - exploration_weight) * normalized_scores + exploration_weight * exploration_factor\n\n    # Softmax normalization\n    exp_scores = np.exp(combined_scores - np.max(combined_scores))\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Select action based on computed probabilities\n    action_index = np.random.choice(range(num_actions), p=probabilities)\n\n    return action_index",
          "objective": 3309464707.6512127,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Define constants\n    exploration_factor = 0.2  # Factor for exploring underused actions\n    exploitation_weight = 0.8  # Weight towards average scores\n    epsilon = 0.1  # Exploration rate for random selection\n    \n    # Calculate average scores for actions based on historical data\n    avg_scores = []\n    action_counts = []\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_count = len(scores)\n        action_counts.append(action_count)\n\n        if action_count > 0:\n            avg_score = np.mean(scores)\n        else:\n            avg_score = 0  # Default to 0 if no scores are available\n\n        avg_scores.append(avg_score)\n\n    # Calculate a balanced score that incorporates both exploration and exploitation\n    balanced_scores = np.zeros(8)\n\n    for i in range(8):\n        balanced_scores[i] = (exploitation_weight * avg_scores[i]) + \\\n                             ((1 - exploitation_weight) * (1 / (action_counts[i] + 1)))\n\n    # Normalize the balanced scores\n    if total_selection_count > 0:\n        normalized_balanced_scores = balanced_scores / total_selection_count\n    else:\n        normalized_balanced_scores = balanced_scores\n\n    # Determine whether to explore or exploit\n    if np.random.rand() < epsilon:\n        # Randomly select an action to explore\n        action_index = np.random.randint(0, 8)\n    else:\n        # Select action with highest balanced score\n        action_index = np.argmax(normalized_balanced_scores)\n\n    return action_index",
          "objective": 3596383240.346912,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for index in range(n_actions):\n        if index in score_set:\n            action_scores = score_set[index]\n            if action_scores:\n                avg_scores[index] = np.mean(action_scores)\n                selection_counts[index] = len(action_scores)\n\n    # Normalize selection counts to avoid division by zero; add a small epsilon for stability\n    epsilon = 1e-10\n    selection_counts = selection_counts + epsilon\n\n    # Calculate normalized scores\n    normalized_scores = avg_scores / selection_counts\n\n    # Dynamic temperature for softmax to control exploration vs exploitation\n    exploration_temp = (total_time_slots - current_time_slot) / total_time_slots\n    scaled_scores = normalized_scores / (1 + exploration_temp)\n\n    # Applying softmax to selects actions while taking into account the exploration term\n    exp_scores = np.exp(scaled_scores - np.max(scaled_scores))  # for numerical stability\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Using probabilities to choose an action based on softmax\n    action_index = np.random.choice(range(n_actions), p=probabilities)\n    \n    return action_index",
          "objective": 3921335240.6716514,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots, exploration_parameter=0.2):\n    num_actions = 8\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        if score_set.get(action_index):\n            average_scores[action_index] = np.mean(score_set[action_index])\n            selection_counts[action_index] = len(score_set[action_index])\n\n    # Handle the case where all actions have zero selection counts\n    total_counts = np.sum(selection_counts)\n    if total_counts > 0:\n        # Normalize average scores\n        normalized_scores = (average_scores - np.min(average_scores)) / (np.max(average_scores) - np.min(average_scores))\n    else:\n        normalized_scores = np.full(num_actions, 0.5)  # Equal chance if all scores are the same\n\n    # Exploration factor (favoring less selected actions)\n    exploration_factor = (total_selection_count / (1 + selection_counts)) ** exploration_parameter\n\n    # Combined score with exploration\n    combined_scores = normalized_scores + exploration_factor\n\n    # Apply decay on exploration factor over time \n    decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n    combined_scores *= decay_factor\n\n    # Softmax normalization to get probabilities\n    exp_scores = np.exp(combined_scores - np.max(combined_scores))  # Stability in exponential\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Select action based on computed probabilities\n    action_index = np.random.choice(range(num_actions), p=probabilities)\n\n    return action_index",
          "objective": 4020251103.4204407,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    n_actions = 8\n    avg_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    for index in range(n_actions):\n        if index in score_set:\n            action_scores = score_set[index]\n            if action_scores:  # Check if there are any scores for this action\n                avg_scores[index] = np.mean(action_scores)\n                selection_counts[index] = len(action_scores)\n\n    # Ensure we do not divide by zero; set selections to 1 for unselected actions\n    selection_counts = np.maximum(selection_counts, 1)\n\n    # UCB Calculation\n    ucb_values = avg_scores + np.sqrt(2 * np.log(total_selection_count) / selection_counts)\n\n    # Select the action with the highest UCB value\n    action_index = int(np.argmax(ucb_values))\n    \n    return action_index",
          "objective": 4060614787.3143687,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores for each action\n    average_scores = []\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        if scores:\n            average_scores.append(np.mean(scores))\n        else:\n            average_scores.append(0.0)  # Assign a score of 0 if no historical scores\n\n    # Softmax to obtain probabilities for each action\n    exp_scores = np.exp(average_scores - np.max(average_scores))  # Stability improvement\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Compute exploration factor (more exploration in early slots)\n    exploration_factor = (current_time_slot + 1) / total_time_slots\n\n    # Combine probabilities with exploration factor\n    adjusted_probabilities = probabilities * (1 - exploration_factor) + (exploration_factor / 8)\n\n    # Select action based on adjusted probabilities\n    action_index = np.random.choice(range(8), p=adjusted_probabilities)\n\n    return action_index",
          "objective": 4173831668.1317434,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if scores:\n            average_scores[action_index] = np.mean(scores)\n\n    # Handle exploration and exploitation\n    exploration_bonus = 1 / (selection_counts + 1e-10)  # Add epsilon to avoid division by zero\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n    \n    # Combine scores\n    combined_scores = exploration_factor * average_scores + (1 - exploration_factor) * exploration_bonus\n    \n    # Adding a small random perturbation for diversity in exploration\n    perturbation = np.random.uniform(0, 0.1, num_actions)\n    combined_scores += perturbation\n\n    # Softmax for action selection\n    exp_scores = np.exp(combined_scores - np.max(combined_scores))  # Stability in softmax\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Select action based on probabilities\n    action_index = np.random.choice(range(num_actions), p=probabilities)\n\n    return action_index",
          "objective": 4336602000.533722,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores for each action and selection counts\n    average_scores = np.zeros(8)\n    selection_counts = np.zeros(8)\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Prevent division by zero and compute the exploration factor\n    exploration_factor = np.exp(-current_time_slot / total_time_slots)\n    \n    # Calculate adjusted scores for exploration\n    adjusted_scores = average_scores + (1 - exploration_factor) * (1 / (selection_counts + 1))\n\n    # Softmax for probability distribution over actions\n    exp_adjusted_scores = np.exp(adjusted_scores - np.max(adjusted_scores))  # For numerical stability\n    probabilities = exp_adjusted_scores / np.sum(exp_adjusted_scores)\n\n    # Select action based on the computed probabilities\n    action_index = np.random.choice(range(8), p=probabilities)\n\n    return action_index",
          "objective": 4355316350.622924,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Define exploration factor (epsilon) that reduces over time\n    epsilon = max(0.05, 0.3 * (1 - current_time_slot / total_time_slots))\n    \n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(8)\n    selection_counts = np.zeros(8)\n\n    # Calculate average scores and selection counts for each action\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        if selection_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n\n    # Calculate confidence scores \n    confidence_scores = np.zeros(8)\n    total_selection_count_with_exploration = total_selection_count + 1  # Avoid division by zero\n    with np.errstate(divide='ignore', invalid='ignore'):\n        confidence_scores = (\n            avg_scores + \n            np.sqrt((np.log(total_selection_count_with_exploration) / (selection_counts + 1e-6)))\n        )\n\n    # Assign infinite confidence to unexplored actions\n    unexplored_actions = selection_counts == 0\n    confidence_scores[unexplored_actions] = np.inf\n\n    # Use an epsilon-greedy strategy to balance exploration and exploitation\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(range(8))\n    else:\n        action_index = np.argmax(confidence_scores)\n    \n    return action_index",
          "objective": 4400607457.750512,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Handling case where selection counts are zero\n    for action_index in range(num_actions):\n        if selection_counts[action_index] == 0:\n            average_scores[action_index] = -np.inf  # Assign lowest score to unselected actions\n\n    # Softmax function to calculate selection probabilities\n    exp_scores = np.exp(average_scores - np.max(average_scores))  # Avoid overflow\n    selection_probabilities = exp_scores / np.sum(exp_scores)\n\n    # Exploration factor inversely proportional to the current time slot\n    exploration_factor = 1 - (current_time_slot / total_time_slots)\n    exploration_weights = (1 - selection_probabilities) * exploration_factor\n\n    # Combine exploitation with exploration\n    combined_probabilities = selection_probabilities + exploration_weights\n    combined_probabilities /= np.sum(combined_probabilities)  # Normalize to sum to 1\n\n    action_index = np.random.choice(num_actions, p=combined_probabilities)\n\n    return action_index",
          "objective": 4818469367.408898,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Define exploration rate decay\n    exploration_rate = max(0.1, 1 - (current_time_slot / total_time_slots))\n    \n    # Initialize structures for average scores and selection counts\n    avg_scores = np.zeros(8)\n    selection_counts = np.zeros(8)\n    \n    # Compute average scores and number of selections for each action\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        if selection_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n    \n    # Adjust scores with exploration term, ensuring stability with small constant\n    exploration_bonus = exploration_rate / (selection_counts + 1e-6)\n    combined_scores = avg_scores + exploration_bonus\n    \n    # Use weighted scores to prioritize both exploration and exploitation\n    weighted_combined_scores = combined_scores * (1 - exploration_rate) + (exploration_rate * np.random.rand(8))\n    \n    # Handle unexplored actions\n    unexplored_actions = selection_counts == 0\n    if np.any(unexplored_actions):\n        weighted_combined_scores[unexplored_actions] = np.inf  # Prefer unexplored actions\n    \n    # Select the action with the highest weighted score\n    action_index = np.argmax(weighted_combined_scores)\n\n    return action_index",
          "objective": 4955944183.192958,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Exploration factor (epsilon) decreases over time\n    epsilon = max(0.05, 0.3 * (1 - current_time_slot / total_time_slots))\n    \n    # Initialize arrays to store average scores and selection counts\n    avg_scores = np.zeros(8)\n    selection_counts = np.zeros(8)\n\n    # Calculate average scores and selection counts\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        avg_scores[action_index] = np.mean(scores) if scores else 0  # Handle empty lists\n\n    # Calculate confidence scores with exploration adjustment\n    confidence_scores = avg_scores.copy()\n    term = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-6))\n    confidence_scores += term  # Encourages exploration based on selection counts\n\n    # Assign infinite confidence to unexplored actions\n    confidence_scores[selection_counts == 0] = np.inf\n\n    # Epsilon-greedy action selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(range(8))  # Explore randomly\n    else:\n        action_index = np.argmax(confidence_scores)  # Exploit based on confidence scores\n    \n    return action_index",
          "objective": 4973065848.926496,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Define exploration factor\n    exploration_factor = 1 / (1 + total_selection_count)\n    \n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(8)\n    selection_counts = np.zeros(8)\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)  # Count selections\n        \n        if selection_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)  # Calculate average score\n\n    # Calculate adjusted scores considering exploration and exploitation\n    exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-6))\n    adjusted_scores = avg_scores + exploration_bonus\n    \n    # Select action: exploit higher average scores but maintain a minimum level of exploration\n    probability_exploitation = 1 - (current_time_slot / total_time_slots)\n    if np.random.rand() < probability_exploitation:\n        action_index = np.argmax(adjusted_scores)\n    else:\n        unexplored_actions = np.flatnonzero(selection_counts == 0)\n        action_index = np.random.choice(unexplored_actions) if len(unexplored_actions) > 0 else np.random.choice(range(8))\n\n    return action_index",
          "objective": 5172232670.671788,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Define constants\n    epsilon = max(0.05, 0.3 * (1 - current_time_slot / total_time_slots))  # Decay epsilon over time\n    \n    # Random exploration\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(range(8))\n    else:\n        # Calculate average scores and selection counts\n        avg_scores = np.zeros(8)\n        selection_counts = np.zeros(8)\n        \n        for action_index in range(8):\n            scores = score_set.get(action_index, [])\n            selection_counts[action_index] = len(scores)  # Count selections\n            \n            if selection_counts[action_index] > 0:\n                avg_scores[action_index] = np.mean(scores)  # Calculate average score\n        \n        # Calculate confidence scores using the upper confidence bound formula\n        confidence_scores = (avg_scores +\n                             np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-6)))\n        \n        # Handle the case where any action has never been selected\n        unexplored_actions = selection_counts == 0\n        if np.any(unexplored_actions):\n            confidence_scores[unexplored_actions] = np.inf  # Give infinite confidence to unexplored\n        \n        # Select action with the highest confidence score\n        action_index = np.argmax(confidence_scores)\n\n    return action_index",
          "objective": 5394595046.180127,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Compute average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if scores:\n            average_scores[action_index] = np.mean(scores)\n\n    # Handle exploration rate\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))  # Decreasing epsilon\n    take_random_action = np.random.rand() < epsilon\n\n    if take_random_action:\n        # Choose a random action with uniform distribution\n        action_index = np.random.choice(range(num_actions))\n    else:\n        # Calculate a score to select based on average scores and exploration\n        adjusted_scores = average_scores + (1 / (selection_counts + 1))  # Add exploration term\n    \n        # Normalize adjusted scores to range [0, 1]\n        adjusted_scores -= np.min(adjusted_scores)  # Ensure all scores are non-negative\n        if np.max(adjusted_scores) > 0:\n            adjusted_scores /= np.max(adjusted_scores)  # Normalize to avoid division by zero\n\n        # Use Softmax for action selection\n        exp_scores = np.exp(adjusted_scores - np.max(adjusted_scores))  # Stability in softmax\n        probabilities = exp_scores / np.sum(exp_scores)\n\n        # Select action based on probabilities\n        action_index = np.random.choice(range(num_actions), p=probabilities)\n\n    return action_index",
          "objective": 5527480187.313488,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores for each action\n    avg_scores = []\n    for index in range(8):\n        if index in score_set and score_set[index]:\n            avg_score = np.mean(score_set[index])\n        else:\n            avg_score = 0.0  # No scores for this action.\n\n        # Adding exploration component based on selection counts\n        exploration_bonus = (1 - total_selection_count / (total_time_slots * 10)) if total_selection_count < total_time_slots * 10 else 0\n        avg_scores.append(avg_score + exploration_bonus)\n\n    # Select action based on max average scores\n    action_index = int(np.argmax(avg_scores))\n    \n    return action_index",
          "objective": 5550485604.791744,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Define constants\n    tau = 0.5  # Temperature parameter for softmax\n    \n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(8)\n    selection_counts = np.zeros(8)\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)  # Count selections\n        \n        if selection_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)  # Calculate average score\n    \n    # Compute adjusted scores for softmax, using selection counts to minimize bias\n    adjusted_scores = avg_scores + np.log(1 + selection_counts) / (total_selection_count + 1e-6)\n\n    # Calculate softmax probabilities\n    exp_scores = np.exp(adjusted_scores / tau)\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Sample an action index based on the calculated probabilities\n    action_index = np.random.choice(range(8), p=probabilities)\n\n    return action_index",
          "objective": 5553960198.925202,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and track selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            average_scores[action_index] = np.mean(scores)\n\n    # Handle the case where all actions have been selected zero times\n    if total_selection_count == 0:\n        return np.random.choice(num_actions)\n\n    # Normalize average scores\n    max_score = np.max(average_scores)\n    min_score = np.min(average_scores)\n    normalized_scores = (average_scores - min_score) / (max_score - min_score) if max_score - min_score > 0 else np.zeros(num_actions)\n\n    # Calculate a base for selection probabilities\n    exploration_bonus = np.log(total_selection_count + 1) / (selection_counts + 1)  # Logarithmic exploration bonus\n    weighted_scores = normalized_scores + exploration_bonus * (1.0 / (selection_counts + 1))  # Adding exploration bonus\n\n    # Dynamic exploration vs exploitation strategy\n    decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n    explore_exploit_ratio = 0.2 + 0.8 * decay_factor\n\n    # Create action selection probabilities\n    action_probabilities = (1 - explore_exploit_ratio) * (weighted_scores / np.sum(weighted_scores)) + \\\n                           (explore_exploit_ratio / num_actions)\n\n    # Select action based on computed probabilities\n    action_index = np.random.choice(num_actions, p=action_probabilities)\n    \n    return action_index",
          "objective": 5850111441.003214,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize arrays to hold average scores and counts\n    average_scores = np.zeros(8)\n    selection_counts = np.zeros(8)\n    \n    # Calculate average scores and number of selections for each action\n    for action_index, scores in score_set.items():\n        if scores:\n            average_scores[action_index] = np.mean(scores)\n            selection_counts[action_index] = len(scores)\n\n    # Compute total counts to avoid division by zero\n    total_counts = max(total_selection_count, 1)\n    \n    # Calculate normalized scores using the formula\n    normalized_scores = average_scores / (1 + selection_counts / total_counts)\n\n    # Epsilon-decaying exploration rate based on the time slot\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n    epsilon = 0.1 * exploration_factor\n    \n    # Randomly decide whether to explore or exploit\n    if np.random.rand() < epsilon:\n        # Explore: select an action less frequently selected\n        unexplored_indices = np.where(selection_counts == 0)[0]\n        if unexplored_indices.size > 0:\n            action_index = np.random.choice(unexplored_indices)\n        else:\n            action_index = np.random.randint(0, 8)\n    else:\n        # Exploit: select the action with highest normalized scores\n        action_index = np.argmax(normalized_scores)\n\n    return action_index",
          "objective": 5858262809.183469,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores for each action\n    average_scores = []\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        if scores:\n            average_scores.append(np.mean(scores))\n        else:\n            average_scores.append(0)  # Use 0 if no score history\n\n    # Calculate selection counts for each action\n    action_counts = [len(score_set.get(action_index, [])) for action_index in range(8)]\n    \n    # Avoid division by zero by adding a small constant\n    selection_counts_adjusted = np.array(action_counts) + 1e-5\n    average_scores = np.array(average_scores)\n\n    # Normalize the average scores by selection counts to favor less-explored actions\n    normalized_scores = average_scores / selection_counts_adjusted\n\n    # Exploration vs. Exploitation strategy\n    epsilon = 1 - (current_time_slot / total_time_slots)  # Decaying exploration rate\n    if np.random.rand() < epsilon:\n        # Explore: choose an action randomly with respect to their normalized scores\n        action_index = np.random.choice(range(8))\n    else:\n        # Exploit: choose the action based on normalized scores\n        action_index = np.random.choice(np.flatnonzero(normalized_scores == normalized_scores.max()))\n\n    return action_index",
          "objective": 5876436834.305179,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Compute average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        average_scores[action_index] = np.mean(scores) if scores else 0\n\n    # Handle exploration\n    exploration_factor = 1 - (current_time_slot / total_time_slots)  # Gradually reduce exploration\n    exploration_bonus = (1 / (selection_counts + 1)) * exploration_factor\n\n    # Adjust scores based on exploration\n    adjusted_scores = average_scores + exploration_bonus\n\n    # If no selections have been made, distribute uniformly\n    if total_selection_count == 0:\n        probabilities = np.ones(num_actions) / num_actions\n    else:\n        adjusted_scores -= np.min(adjusted_scores)  # Shift to non-negative\n        adjusted_scores /= (np.max(adjusted_scores) + 1e-10)  # Prevent division by zero\n        exp_scores = np.exp(adjusted_scores - np.max(adjusted_scores))  # Stability in softmax\n        probabilities = exp_scores / np.sum(exp_scores)\n\n    # Select action based on probabilities\n    action_index = np.random.choice(range(num_actions), p=probabilities)\n\n    return action_index",
          "objective": 5910332340.260965,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Define constants\n    epsilon_start = 0.3  # Starting exploration rate\n    epsilon_end = 0.05    # Ending exploration rate\n    exploration_decay = (epsilon_start - epsilon_end) / total_time_slots\n    \n    # Calculate the current exploration rate\n    epsilon = max(epsilon_end, epsilon_start - (exploration_decay * current_time_slot))\n    \n    # Determine whether to explore or exploit\n    if np.random.rand() < epsilon:\n        # Randomly select an action (0 to 7)\n        action_index = np.random.randint(0, 8)\n    else:\n        avg_scores = np.zeros(8)\n        selection_counts = np.zeros(8)\n        \n        # Calculate average scores and counts\n        for action_index in range(8):\n            scores = score_set.get(action_index, [])\n            if scores:\n                avg_scores[action_index] = np.mean(scores)\n                selection_counts[action_index] = len(scores)\n        \n        # Calculate confidence scores\n        with np.errstate(divide='ignore', invalid='ignore'):\n            confidence_scores = avg_scores + np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-6))\n        \n        # Select action with the highest confidence score, where exploration is balanced\n        action_index = np.argmax(confidence_scores)\n\n    return action_index",
          "objective": 6128515766.260461,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    temp = 1.0  # Temperature parameter for Softmax, can be adjusted for exploration vs exploitation\n    action_indices = list(range(8))\n    \n    # Calculate average scores for each action\n    avg_scores = np.zeros(8)\n    counts = np.array([len(score_set.get(i, [])) for i in action_indices])\n    \n    for action_index in action_indices:\n        scores = score_set.get(action_index, [])\n        if scores:\n            avg_scores[action_index] = np.mean(scores)\n        else:\n            avg_scores[action_index] = 0  # No historical scores\n    \n    # Normalize average scores to mitigate bias\n    normalized_scores = avg_scores / (counts + 1e-6)  # Adding a small epsilon to avoid division by zero\n    \n    # Apply Softmax to the normalized scores\n    exp_scores = np.exp(normalized_scores / temp)\n    softmax_probs = exp_scores / np.sum(exp_scores)\n\n    # Increase exploration for earlier time slots\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    weighted_probs = softmax_probs * (1 - exploration_weight) + (1/8) * exploration_weight\n\n    # Select action based on weighted probabilities\n    action_index = np.random.choice(action_indices, p=weighted_probs)\n    \n    return action_index",
          "objective": 6164350326.936443,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize the average scores array\n    average_scores = np.zeros(8)\n    selection_counts = np.zeros(8)\n\n    # Calculate average scores and selection counts\n    for action_index, scores in score_set.items():\n        if scores:\n            average_scores[action_index] = np.mean(scores)\n            selection_counts[action_index] = len(scores)\n\n    # Handle cases where no action has been selected (avoid division by zero)\n    total_counts = max(total_selection_count, 1)  # Ensure non-zero to avoid division by zero\n    normalized_scores = average_scores * (selection_counts / total_counts)\n\n    # Epsilon-greedy exploration vs exploitation\n    epsilon = 1 / (current_time_slot + 1)  # Decaying exploration rate\n    if np.random.rand() < epsilon:\n        # Explore: choose an action randomly\n        action_index = np.random.randint(0, 8)\n    else:\n        # Exploit: choose based on weighted normalized scores\n        action_index = np.argmax(normalized_scores)\n\n    return action_index",
          "objective": 6174784143.686087,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Define constants\n    epsilon_start = 0.3  # Starting exploration rate\n    epsilon_end = 0.05   # Ending exploration rate\n    exploration_decay = (epsilon_start - epsilon_end) / total_time_slots\n    \n    # Calculate the current exploration rate based on time slot\n    epsilon = max(epsilon_end, epsilon_start - (exploration_decay * current_time_slot))\n    \n    # Determine whether to explore or exploit\n    if np.random.rand() < epsilon:\n        # Randomly select an action (0 to 7)\n        action_index = np.random.randint(0, 8)\n    else:\n        avg_scores = []\n        selection_counts = []\n        \n        for action_index in range(8):\n            scores = score_set.get(action_index, [])\n            if scores:\n                avg_score = np.mean(scores)  # Calculate average score\n                count = len(scores)           # Number of times this action has been selected\n            else:\n                avg_score = 0\n                count = 0\n            \n            avg_scores.append(avg_score)\n            selection_counts.append(count)\n        \n        # Calculate action confidence which incorporates selection count\n        confidence_scores = np.array(avg_scores) + (np.sqrt(np.log(total_selection_count + 1) / (np.array(selection_counts) + 1e-6)) if total_selection_count > 0 else np.zeros(8))\n        \n        # Select action with highest confidence score\n        action_index = np.argmax(confidence_scores)\n\n    return action_index",
          "objective": 6816407558.671327,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        if scores:\n            average_scores[action_index] = np.mean(scores)\n            selection_counts[action_index] = len(scores)\n\n    # Normalize average scores to range [0, 1]\n    max_score = np.max(average_scores)\n    min_score = np.min(average_scores)\n    range_score = max_score - min_score\n    if range_score > 0:\n        normalized_scores = (average_scores - min_score) / range_score\n    else:\n        normalized_scores = np.full(num_actions, 0.5)  # Equal chance if all scores are the same\n\n    # Incorporate selection counts into the strategy\n    exploration_factor = (total_selection_count / (1 + selection_counts)) ** 0.5\n    weighted_scores = normalized_scores * exploration_factor\n\n    # Apply exploration strategy\n    decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n    if np.random.rand() < (1 - decay_factor):\n        action_index = np.random.choice(num_actions, p=weighted_scores / np.sum(weighted_scores))\n    else:\n        action_index = np.random.choice(range(num_actions))\n\n    return action_index",
          "objective": 6905156259.765505,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate the average scores for each action\n    average_scores = np.zeros(8)\n    \n    for action_idx in range(8):\n        if score_set[action_idx]:  # Check if there are any scores for this action\n            average_scores[action_idx] = np.mean(score_set[action_idx])\n    \n    # A simple exploration factor \n    # Higher exploration in the early time slots\n    exploration_prob = max(0.1, (total_time_slots - current_time_slot) / total_time_slots)\n    \n    # Selecting action based on exploration vs exploitation\n    if np.random.rand() < exploration_prob:\n        # Explore: select a random action\n        action_index = np.random.choice(range(8))\n    else:\n        # Exploit: select the action with the highest average score\n        action_index = np.argmax(average_scores)\n    \n    return action_index",
          "objective": 7012896655.298992,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores for each action\n    average_scores = []\n    selection_counts = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        selection_counts.append(selection_count)\n        \n        # Calculate average score, avoiding division by zero\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        average_scores.append(average_score)\n\n    # Normalize the average scores\n    normalized_scores = np.array(average_scores)\n    if total_selection_count > 0:\n        normalized_scores = (normalized_scores - np.min(normalized_scores)) / (np.max(normalized_scores) - np.min(normalized_scores)) if np.max(normalized_scores) > np.min(normalized_scores) else np.full(8, 0.5)\n\n    # Exploration vs. Exploitation strategy\n    exploration_factor = max(0.1, 1 - (current_time_slot / total_time_slots))  # Decaying exploration factor\n    exploration_principle = normalized_scores * (1 - exploration_factor) + (1 - np.array(selection_counts) / (max(1, total_selection_count))) * exploration_factor\n\n    # Select action based on adjusted scores\n    action_index = np.random.choice(np.arange(8), p=exploration_principle / np.sum(exploration_principle))\n\n    return action_index",
          "objective": 8442391290.327172,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Define constants\n    epsilon = max(0.05, 0.3 * (1 - current_time_slot / total_time_slots))  # Decay epsilon over time\n\n    # Initialize variables\n    action_count = len(score_set)  # Number of actions\n    avg_scores = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n\n    # Calculate average scores and count selections\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n\n        if selection_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n    \n    # Random exploration\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(range(action_count))\n    else:\n        # Calculate confidence scores\n        confidence_scores = (avg_scores +\n                             np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-6))) \n        \n        # Handle unexplored actions\n        unexplored_actions = selection_counts == 0\n        confidence_scores[unexplored_actions] = np.inf  # Assign infinite confidence\n\n        # Select action with the highest confidence score\n        action_index = np.argmax(confidence_scores)\n\n    return action_index",
          "objective": 8778588271.834724,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if scores:\n            average_scores[action_index] = np.mean(scores)\n\n    # Calculate exploration bonuses\n    exploration_bonus = np.where(selection_counts > 0, 1 / selection_counts, 1)\n    \n    # Normalize scores to account for exploration\n    if total_selection_count > 0:\n        adjusted_scores = average_scores + exploration_bonus\n    else:\n        adjusted_scores = np.ones(num_actions) / num_actions  # Uniform selection if no history\n    \n    # Scale adjusted scores\n    adjusted_scores -= np.min(adjusted_scores)  # Shift to non-negative\n    adjusted_scores /= (np.max(adjusted_scores) + 1e-10)  # Prevent division by zero\n\n    # Dynamic exploration vs. exploitation factor\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n    weighted_scores = exploration_factor * adjusted_scores + (1 - exploration_factor) / (selection_counts + 1)\n\n    # Softmax selection\n    exp_scores = np.exp(weighted_scores - np.max(weighted_scores))  # Stability in softmax\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Select action based on the computed probabilities\n    action_index = np.random.choice(range(num_actions), p=probabilities)\n\n    return action_index",
          "objective": 9033144395.106287,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if scores:\n            average_scores[action_index] = np.mean(scores)\n\n    # Dynamic weighting based on time and selection counts\n    weight_exploration = 1 / (current_time_slot + 1)  # Decaying exploration weight\n    weight_exploitation = (current_time_slot / total_time_slots) if total_time_slots > 0 else 0\n\n    # Effective scores to consider\n    effective_scores = average_scores * (1 - weight_exploration) + (np.random.rand(num_actions) * weight_exploration)\n\n    # Choose an action based on effective scores considering both exploration and exploitation\n    action_index = np.random.choice(np.arange(num_actions), p=effective_scores / effective_scores.sum())\n\n    return action_index",
          "objective": 9164374564.839916,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    # Calculate average scores and counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if scores:\n            average_scores[action_index] = np.mean(scores)\n    \n    # Handle division by zero for scores that have never been selected\n    adjusted_scores = np.zeros(num_actions)\n    with np.errstate(divide='ignore', invalid='ignore'):\n        adjusted_scores = np.where(selection_counts > 0, average_scores, 0)\n    \n    # Exploration bonus\n    exploration_bonus = 1 / (selection_counts + 1)  # Encourages exploration for less selected actions\n    adjusted_scores += exploration_bonus\n\n    # Dynamic exploration factor based on time\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n    combined_scores = exploration_factor * adjusted_scores + (1 - exploration_factor) * (1 / (selection_counts + 1))\n\n    # Softmax for action selection\n    exp_scores = np.exp(combined_scores - np.max(combined_scores))  # For numerical stability\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Select action based on probabilities\n    action_index = np.random.choice(range(num_actions), p=probabilities)\n\n    return action_index",
          "objective": 9433080293.689487,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Define a temperature parameter\n    temperature = 1.0 + (total_time_slots - current_time_slot) / total_time_slots\n    \n    # Calculate average scores for each action\n    avg_scores = []\n    for action_index in range(8):\n        if action_index in score_set and score_set[action_index]:\n            avg_score = np.mean(score_set[action_index])\n        else:\n            avg_score = 0.0  # Default to 0 if no score\n        avg_scores.append(avg_score)\n\n    # Make selection probabilities based on temperature\n    scores = np.array(avg_scores)\n    scores = np.exp(scores / temperature)  # Softmax operation\n    probabilities = scores / np.sum(scores)\n\n    # Select an action based on computed probabilities\n    action_index = np.random.choice(range(8), p=probabilities)\n\n    return action_index",
          "objective": 9557806930.795544,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_scores = []\n    action_counts = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        if scores:\n            mean_score = np.mean(scores)\n            selection_count = len(scores)\n        else:\n            mean_score = 0\n            selection_count = 0\n        \n        # Use the formula to balance exploitation (mean_score) and exploration (1/(selection_count + 1))\n        exploration_bonus = np.log(total_selection_count + 1) / (selection_count + 1) if selection_count > 0 else np.log(total_selection_count + 1)\n        action_scores.append(mean_score + exploration_bonus)\n        action_counts.append(selection_count)\n\n    # Normalize and score based on current time slot to give more importance to future selections\n    time_based_scores = [score * (total_time_slots - current_time_slot) / total_time_slots for score in action_scores]\n\n    action_index = np.argmax(time_based_scores)\n    return action_index",
          "objective": 9644761619.668362,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Define constants\n    exploration_probability = max(0.1, 1 - current_time_slot / total_time_slots)  # Dynamic exploration\n    \n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(8)\n    selection_counts = np.zeros(8)\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)  # Count selections\n        \n        if selection_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)  # Calculate average score\n    \n    # Calculate a modified score that incorporates exploration\n    score_adjustment = exploration_probability * (1 / (selection_counts + 1e-6))\n    combined_scores = avg_scores + score_adjustment\n    \n    # Handle unexplored actions\n    unexplored_actions = selection_counts == 0\n    if np.any(unexplored_actions):\n        combined_scores[unexplored_actions] = np.inf  # Infinite score for unexplored actions\n    \n    # Epsilon-greedy selection\n    if np.random.rand() < exploration_probability:\n        action_index = np.random.choice(np.where(unexplored_actions)[0]) if np.any(unexplored_actions) else np.random.randint(0, 8)\n    else:\n        action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": 10136948712.23717,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize average scores and selection counts\n    average_scores = np.zeros(8)\n    selection_counts = np.zeros(8)\n    \n    for action_idx in range(8):\n        selection_counts[action_idx] = len(score_set[action_idx])\n        if selection_counts[action_idx] > 0:\n            average_scores[action_idx] = np.mean(score_set[action_idx])\n    \n    # Prevent division by zero for normalization\n    normalized_scores = np.zeros(8)\n    with np.errstate(divide='ignore', invalid='ignore'):\n        normalized_scores = average_scores / (selection_counts + 1e-5)  # Add a small number to avoid division by zero\n    \n    # Compute exploration factor using an adjusted epsilon-greedy approach\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n    \n    # Randomly select an action with probability epsilon (exploration)\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(range(8))\n    else:\n        # Select action based on normalized scores (exploitation)\n        action_index = np.argmax(normalized_scores)\n        \n    return action_index",
          "objective": 11518879807.751204,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize variables for average scores and selection counts\n    average_scores = []\n    selection_counts = []\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        average_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n        \n        average_scores.append(average_score)\n        selection_counts.append(selection_count)\n\n    # Normalize selection counts for softmax\n    selection_counts = np.array(selection_counts)\n    epsilon = 0.1\n    \n    # Calculate the probabilities for each action with an exploration factor using softmax\n    # Add a small value to avoid division by zero and encourage defining probabilities\n    adjusted_counts = selection_counts + 1e-5\n    normalized_counts = adjusted_counts / np.sum(adjusted_counts)\n    \n    # Incorporate average scores weighted by normalized counts\n    scores_adjusted = np.array(average_scores) * normalized_counts\n\n    # Apply epsilon-greedy strategy to select action\n    if np.random.rand() < epsilon or current_time_slot < total_time_slots / 4:\n        action_index = np.random.choice(range(8), p=normalized_counts)\n    else:\n        action_index = np.argmax(scores_adjusted)\n\n    return action_index",
          "objective": 12266212654.964325,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index, scores in score_set.items():\n        if scores:\n            average_scores[action_index] = np.mean(scores)\n            selection_counts[action_index] = len(scores)\n\n    # Handle zero selection counts by using a small constant to avoid division by zero\n    selection_threshold = selection_counts + 1e-5\n    normalized_scores = average_scores / selection_threshold\n\n    # Exploration vs. Exploitation strategy\n    exploration_bias = (1 - selection_counts / (total_selection_count + 1e-5)) / (current_time_slot + 1)\n    adjusted_scores = normalized_scores * (1 + exploration_bias)\n\n    # Softmax action selection\n    exp_scores = np.exp(adjusted_scores - np.max(adjusted_scores))  # Stability improvement\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Select action based on probabilities\n    action_index = np.random.choice(num_actions, p=probabilities)\n\n    return action_index",
          "objective": 14948023208.988796,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    epsilon = max(0.05, 0.3 * (1 - current_time_slot / total_time_slots))\n    \n    # Random exploration\n    if np.random.rand() < epsilon:\n        return np.random.choice(num_actions)\n    \n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        \n        if selection_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n    \n    # Prevent division by zero and define a small constant for log terms\n    log_term = np.log(total_selection_count + 1)\n    adjusted_counts = selection_counts + 1e-6  # Avoid dividing by zero\n\n    # Compute UCB scores\n    confidence_scores = avg_scores + np.sqrt(log_term / adjusted_counts)\n    \n    # Assign high exploration values to never-selected actions\n    unexplored_actions = selection_counts == 0\n    confidence_scores[unexplored_actions] = np.inf\n    \n    return action_index",
          "objective": 15255314040.335663,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_factor = 1.0\n    delta_t = total_time_slots - current_time_slot\n    \n    action_values = []\n    \n    for action_index, scores in score_set.items():\n        if len(scores) == 0:\n            average_score = 0\n        else:\n            average_score = np.mean(scores)\n        \n        if total_selection_count > 0:\n            selection_count = len(scores)\n            bonus = exploration_factor * np.sqrt(np.log(total_selection_count) / (selection_count + 1))\n        else:\n            bonus = exploration_factor\n        \n        action_value = average_score + bonus\n        action_values.append((action_value, action_index))\n    \n    action_values.sort(reverse=True, key=lambda x: x[0])\n    action_index = action_values[0][1]\n    \n    return action_index",
          "objective": 16243407986.14112,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate the average scores for each action\n    average_scores = {}\n    for action_index, scores in score_set.items():\n        if len(scores) > 0:\n            average_scores[action_index] = np.mean(scores)\n        else:\n            average_scores[action_index] = 0  # No scores yet, assign zero\n    \n    # Implementing epsilon-greedy strategy\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))  # Epsilon decreases over time\n    if np.random.rand() < epsilon:  # Exploration\n        action_index = np.random.choice(list(score_set.keys()))\n    else:  # Exploitation\n        action_index = max(average_scores, key=average_scores.get)\n    \n    return action_index",
          "objective": 26209908363.678963,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = max(1 - current_time_slot / total_time_slots, 0.1)  # Decay epsilon based on time slot\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        if len(scores) > 0:\n            average_score = np.mean(scores)\n        else:\n            average_score = 0.0\n        \n        action_scores.append(average_score)\n        \n    if total_selection_count == 0 or np.random.rand() < epsilon:\n        # Explore: select random action\n        action_index = np.random.randint(0, 8)\n    else:\n        # Exploit: select action with highest average score\n        action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": 28600472115.391216,
          "other_inf": null
     }
]