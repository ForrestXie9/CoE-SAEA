[
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Configuration for the exploration-exploitation balance\n    initial_epsilon = 1.0  # Start with exploration\n    min_epsilon = 0.01\n    decay_rate = 0.99  # How quickly to decrease epsilon\n    \n    # Epsilon decay based on selection count\n    if total_selection_count < 100:\n        epsilon = initial_epsilon  # Full exploration in initial phase\n    else:\n        epsilon = max(initial_epsilon * (decay_rate ** (total_selection_count - 99)), min_epsilon)\n\n    action_indices = list(score_set.keys())\n    scores = []\n    \n    for action in action_indices:\n        historical_scores = score_set[action]\n        average_score = np.mean(historical_scores) if historical_scores else 0\n        selection_count = len(historical_scores)\n\n        # UCB term\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n\n        # Combined score using Thompson Sampling approach and UCB\n        scores.append(average_score + exploration_value)\n\n    # Selection based on epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = np.argmax(scores)\n\n    return action_index",
          "objective": -321.04537352397944,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Configuration for the exploration-exploitation balance\n    initial_epsilon = 1.0  # Start with exploration\n    min_epsilon = 0.01\n    decay_rate = 0.99  # How quickly to decrease epsilon\n\n    # Epsilon decay based on selection count\n    epsilon = max(initial_epsilon * (decay_rate ** total_selection_count), min_epsilon)\n    \n    action_indices = list(score_set.keys())\n    scores = []\n    \n    for action in action_indices:\n        historical_scores = score_set[action]\n        average_score = np.mean(historical_scores) if historical_scores else 0\n        selection_count = len(historical_scores)\n        \n        # UCB term\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n\n        # Combined score\n        scores.append(average_score + exploration_value)\n\n    # Selection based on epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = np.argmax(scores)\n\n    return action_index",
          "objective": -321.04537352397944,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Configuration for dynamic exploration-exploitation balance\n    initial_epsilon = 1.0  \n    min_epsilon = 0.01\n    decay_factor = 0.95  # Weight for reducing epsilon\n    exploration_threshold = 0.1  # Threshold for switching to hybrid method\n    \n    # Adjust epsilon dynamically based on the exploration progress\n    epsilon = max(initial_epsilon * (decay_factor ** (total_selection_count / (total_time_slots + 1))), min_epsilon)\n    \n    action_indices = list(score_set.keys())\n    scores = []\n    selection_counts = []\n    \n    for action in action_indices:\n        historical_scores = score_set[action]\n        average_score = np.mean(historical_scores) if historical_scores else 0\n        selection_count = len(historical_scores)\n        selection_counts.append(selection_count)\n        \n        # UCB term\n        ucb_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n\n        # Combined score\n        scores.append((average_score, ucb_value))\n\n    # Softmax for action selection\n    if total_selection_count > 0 and np.random.rand() >= epsilon and np.var([avg for avg, _ in scores]) <= exploration_threshold:\n        max_average = max(scores, key=lambda x: x[0])[0]\n        scores = [avg for avg, _ in scores]\n        exp_scores = np.exp(np.array(scores) - max_average)\n        action_index = np.random.choice(action_indices, p=exp_scores / exp_scores.sum())\n    else:\n        action_index = np.argmax([avg + ucb for avg, ucb in scores])\n\n    return action_index",
          "objective": -321.04537352397944,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Constants for exploration-exploitation strategy\n    initial_epsilon = 1.0\n    min_epsilon = 0.05\n    decay_factor = 0.997\n    \n    # Dynamic epsilon decay based on total_selection_count\n    if total_selection_count < 50:\n        epsilon = initial_epsilon\n    else:\n        epsilon = max(initial_epsilon * (decay_factor ** (total_selection_count - 49)), min_epsilon)\n\n    action_indices = list(score_set.keys())\n    scores = []\n    \n    for action in action_indices:\n        historical_scores = score_set[action]\n        average_score = np.mean(historical_scores) if historical_scores else 0\n        selection_count = len(historical_scores)\n\n        # Upper Confidence Bound (UCB) calculation\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n\n        # Combining average score with exploration value\n        scores.append(average_score + exploration_value)\n\n    # Action selection based on epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = np.argmax(scores)\n\n    return action_index",
          "objective": -321.04537352397944,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    scores = []\n    num_actions = len(action_indices)\n    \n    # Dynamic epsilon calculation\n    initial_epsilon = 1.0\n    min_epsilon = 0.05\n    decay_factor = 0.99\n    epsilon = max(min_epsilon, initial_epsilon * (decay_factor ** (total_selection_count / 100)))\n\n    if total_selection_count < 20:  # Early exploration phase\n        action_index = np.random.choice(action_indices)\n        return action_index\n    \n    for action in action_indices:\n        historical_scores = score_set[action]\n        selection_count = len(historical_scores) + 1  # Avoid division by zero\n        average_score = np.mean(historical_scores) if historical_scores else 0\n        \n        # Upper Confidence Bound calculation\n        ucb_value = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_count)\n\n        # Softmax calculation for better exploration/exploitation balance\n        temperature = 0.1\n        softmax_value = np.exp(average_score / temperature)\n        \n        # Combine scores\n        total_score = (1 - epsilon) * average_score + epsilon * (ucb_value + softmax_value)\n        scores.append(total_score)\n\n    action_index = np.argmax(scores)  # Select action with highest score\n    return action_index",
          "objective": -321.04537352397944,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Constants\n    initial_epsilon = 1.0\n    min_epsilon = 0.05\n    decay_factor = 0.99\n    exploration_weight = 0.5  # weight for UCB in the hybrid approach\n\n    action_indices = list(score_set.keys())\n    scores = []\n    \n    # Initialize historical data\n    historical_data = {i: score_set[i] for i in action_indices}\n    \n    # Calculate average scores and exploration values\n    for action in action_indices:\n        historical_scores = historical_data[action]\n        average_score = np.mean(historical_scores) if historical_scores else 0\n        selection_count = len(historical_scores)\n\n        # UCB calculation\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n\n        scores.append((average_score, exploration_value, selection_count))\n    \n    # Calculate softmax probabilities\n    avg_scores = np.array([score[0] for score in scores])\n    exp_scores = np.exp(avg_scores - np.max(avg_scores))  # for numerical stability\n    softmax_probs = exp_scores / np.sum(exp_scores)\n\n    # Adaptive epsilon decay\n    if total_selection_count < 50 or current_time_slot < total_time_slots * 0.1:\n        epsilon = initial_epsilon\n    else:\n        epsilon = max(initial_epsilon * (decay_factor ** (total_selection_count - 49)), min_epsilon)\n    \n    # Action selection based on epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        combined_scores = (1 - exploration_weight) * avg_scores + exploration_weight * np.array([score[1] for score in scores])\n        action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -321.04537352397944,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Configuration for the exploration-exploitation balance\n    min_epsilon = 0.01\n    initial_epsilon = 1.0\n    min_variance_threshold = 0.01  # Minimum variance to consider for exploitation\n    exploration_weight = 1.0  # Weighting for exploration part of UCB\n    total_actions = len(score_set)\n    \n    # Calculate average scores and selection counts\n    average_scores = np.zeros(total_actions)\n    selection_counts = np.zeros(total_actions)\n    \n    for action, scores in score_set.items():\n        if scores:\n            average_scores[action] = np.mean(scores)\n            selection_counts[action] = len(scores)\n    \n    # Epsilon decay based on selection count and variance of average scores\n    if total_selection_count > 0:\n        variance = np.var(average_scores)\n        epsilon = max(initial_epsilon * np.exp(-total_selection_count / total_time_slots), min_epsilon)\n    else:\n        epsilon = initial_epsilon\n    \n    # UCB calculation\n    exploration_values = np.zeros(total_actions)\n    for action in range(total_actions):\n        if selection_counts[action] > 0:\n            exploration_values[action] = exploration_weight * np.sqrt(np.log(total_selection_count + 1) / (selection_counts[action] + 1))\n        else:\n            exploration_values[action] = float('inf')  # Explore unselected actions\n    \n    # Combined score for UCB approach\n    ucb_scores = average_scores + exploration_values\n    \n    # Selection based on contextual epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(range(total_actions))\n    else:\n        action_index = np.argmax(ucb_scores)\n\n    return action_index",
          "objective": -321.04537352397944,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    initial_epsilon = 1.0\n    min_epsilon = 0.01\n    decay_rate = 0.99\n    ucb_weight = 0.5  # Weight for the UCB term in the final score\n\n    # Determine epsilon based on the total selection count\n    if total_selection_count < 100:\n        epsilon = initial_epsilon\n    else:\n        epsilon = max(initial_epsilon * (decay_rate ** (total_selection_count - 99)), min_epsilon)\n\n    action_indices = list(score_set.keys())\n    scores = []\n    \n    for action in action_indices:\n        historical_scores = score_set[action]\n        average_score = np.mean(historical_scores) if historical_scores else 0\n        selection_count = len(historical_scores)\n\n        # UCB term calculation\n        ucb = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n\n        # Combine UCB and average score with weights\n        combined_score = (1 - ucb_weight) * average_score + ucb_weight * ucb\n        scores.append(combined_score)\n\n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = np.argmax(scores)\n\n    return action_index",
          "objective": -321.04537352397944,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Adaptive parameters\n    min_epsilon = 0.01\n    exploration_threshold = 50  # Threshold for transitioning from exploration to exploitation\n    softmax_temperature = 0.5  # Temperature parameter for softmax\n\n    action_indices = list(score_set.keys())\n    scores = []\n    \n    for action in action_indices:\n        historical_scores = score_set[action]\n        average_score = np.mean(historical_scores) if historical_scores else 0\n        selection_count = len(historical_scores)\n        \n        # UCB term\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combined Score for UCB\n        ucb_score = average_score + exploration_value\n        scores.append(ucb_score)\n\n    # Exploration phase\n    if total_selection_count < exploration_threshold:\n        action_index = np.random.choice(action_indices)\n    else:\n        # Softmax action selection based on scores\n        exp_scores = np.exp(np.array(scores) / softmax_temperature)\n        probabilities = exp_scores / np.sum(exp_scores)\n        action_index = np.random.choice(action_indices, p=probabilities)\n    \n    return action_index",
          "objective": -321.04537352397944,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Configuration for exploration-exploitation balance\n    initial_epsilon = 1.0  # Start with full exploration\n    min_epsilon = 0.05     # Set a minimum epsilon for exploration\n    decay_rate = 0.99      # Decay factor for epsilon\n    exploration_threshold = 0.1  # Threshold for UCB exploration\n\n    # Calculate epsilon based on the total selection count\n    if total_selection_count < 100:\n        epsilon = initial_epsilon\n    else:\n        epsilon = max(initial_epsilon * (decay_rate ** (total_selection_count - 99)), min_epsilon)\n\n    action_indices = list(score_set.keys())\n    scores = []\n    \n    for action in action_indices:\n        historical_scores = score_set[action]\n        average_score = np.mean(historical_scores) if historical_scores else 0\n        selection_count = len(historical_scores)\n\n        # UCB term to favor under-explored actions\n        ucb_exploration = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n\n        # Combined score with UCB\n        scores.append(average_score + ucb_exploration)\n\n    # Determine selected action based on epsilon-greedy approach\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = np.argmax(scores)\n\n    return action_index",
          "objective": -321.04537352397944,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Constants\n    initial_epsilon = 1.0\n    min_epsilon = 0.05\n    decay_factor = 0.95\n    exploration_weight = 0.3  # Adjusted weight for UCB or Thompson Sampling\n    action_indices = list(score_set.keys())\n    \n    # Initialize historical data\n    historical_data = {i: score_set[i] for i in action_indices}\n    scores = np.zeros(len(action_indices))\n    counts = np.zeros(len(action_indices))\n    \n    # Calculate average scores and counts\n    for i, action in enumerate(action_indices):\n        historical_scores = historical_data[action]\n        counts[i] = len(historical_scores)\n        if counts[i] > 0:\n            scores[i] = np.mean(historical_scores)\n    \n    # Adaptive epsilon decay\n    if total_selection_count < 30 or current_time_slot < total_time_slots * 0.2:\n        epsilon = initial_epsilon\n    else:\n        epsilon = max(initial_epsilon * (decay_factor ** (total_selection_count - 29)), min_epsilon)\n\n    # Action selection based on epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        # UCB for actions\n        total_count = total_selection_count + 1  # to avoid division by zero\n        ucb_values = scores + np.sqrt((2 * np.log(total_count)) / (counts + 1))  # UCB formula\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -321.04537352397944,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Configuration for exploration-exploitation balance\n    initial_epsilon = 1.0\n    min_epsilon = 0.01\n    decay_rate = 0.99\n    \n    # Epsilon decay calculation\n    if total_selection_count < 100:\n        epsilon = initial_epsilon\n    else:\n        epsilon = max(initial_epsilon * (decay_rate ** (total_selection_count - 99)), min_epsilon)\n\n    action_indices = list(score_set.keys())\n    scores = np.zeros(len(action_indices))\n    \n    for i, action in enumerate(action_indices):\n        historical_scores = score_set[action]\n        average_score = np.mean(historical_scores) if historical_scores else 0\n        selection_count = len(historical_scores)\n\n        # UCB term calculation\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combine average score and exploration term\n        scores[i] = average_score + exploration_value\n\n    # Selection logic\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = np.argmax(scores)\n\n    return action_index",
          "objective": -321.04537352397944,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Constants\n    initial_epsilon = 1.0\n    min_epsilon = 0.05\n    decay_factor = 0.97\n    ucb_weight = 0.5  # weight for UCB in the hybrid approach\n    softmax_weight = 0.5  # weight for softmax component\n\n    action_indices = list(score_set.keys())\n    scores = []\n    \n    # Initialize historical data\n    historical_data = {i: score_set[i] for i in action_indices}\n    \n    # Calculate average scores and exploration values\n    for action in action_indices:\n        historical_scores = historical_data[action]\n        average_score = np.mean(historical_scores) if historical_scores else 0\n        selection_count = len(historical_scores)\n\n        # UCB calculation\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n\n        scores.append((average_score, exploration_value, selection_count))\n    \n    # Extract scores for easy manipulation\n    avg_scores = np.array([score[0] for score in scores])\n    exploration_values = np.array([score[1] for score in scores])\n    \n    # Adaptive epsilon decay based on standard deviation\n    if total_selection_count < 50 or current_time_slot < total_time_slots * 0.1:\n        epsilon = initial_epsilon\n    else:\n        std_dev = np.std(avg_scores) if len(avg_scores) > 1 else 1.0\n        epsilon = max(min_epsilon, std_dev / (std_dev + (total_selection_count / 1000)))\n\n    # Epsilon-greedy action selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        combined_scores = (1 - ucb_weight) * avg_scores + ucb_weight * exploration_values\n        action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": -321.04537352397944,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    \n    if total_selection_count < num_actions:  # Uniformly explore initially\n        return total_selection_count\n    \n    scores = []\n    for action in action_indices:\n        historical_scores = score_set[action]\n        selection_count = len(historical_scores)\n        average_score = np.mean(historical_scores) if historical_scores else 0\n        \n        # Calculate variance for adaptive epsilon\n        variance = np.var(historical_scores) if historical_scores else 1\n        epsilon = max(0.1, min(1.0, np.sqrt(variance) / (1 + average_score)))\n        \n        # UCB calculation\n        ucb_value = np.sqrt((2 * np.log(total_selection_count)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Softmax for action probability\n        softmax_scores = np.exp([(average_score + ucb_value) for average_score in historical_scores])\n        softmax_probabilities = softmax_scores / np.sum(softmax_scores)\n        \n        # Combine exploration and exploitation\n        total_score = (1 - epsilon) * (average_score + ucb_value) + epsilon * np.random.choice([0, 1], p=[1 - np.mean(softmax_probabilities), np.mean(softmax_probabilities)])\n        scores.append(total_score)\n\n    action_index = np.argmax(scores)  # Select the action with the highest total score\n    return action_index",
          "objective": -321.04537352397944,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Configuration for exploration-exploitation balance\n    initial_epsilon = 1.0\n    min_epsilon = 0.01\n    decay_rate = 0.995  # More gradual decay to allow more exploration\n\n    # Epsilon decay based on selection count\n    if total_selection_count < 100:\n        epsilon = initial_epsilon\n    else:\n        epsilon = max(initial_epsilon * (decay_rate ** (total_selection_count - 99)), min_epsilon)\n\n    action_indices = list(score_set.keys())\n    scores = []\n    selection_counts = []\n    \n    for action in action_indices:\n        historical_scores = score_set[action]\n        average_score = np.mean(historical_scores) if historical_scores else 0\n        selection_count = len(historical_scores)\n        \n        # Store selection counts\n        selection_counts.append(selection_count)\n        \n        # UCB term\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combined score\n        scores.append(average_score + exploration_value)\n\n    # Normalize scores for softmax\n    max_score = np.max(scores)\n    exp_scores = np.exp(scores - max_score)  # Shift for numerical stability\n    softmax_probs = exp_scores / np.sum(exp_scores)\n\n    # Select action using epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = np.random.choice(action_indices, p=softmax_probs)\n\n    return action_index",
          "objective": -321.04537352397944,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Adaptive epsilon strategy\n    initial_epsilon = 1.0\n    min_epsilon = 0.05\n    epsilon_decay_rate = 0.99\n\n    # Calculate epsilon based on selection count and average scores\n    average_scores = {action: np.mean(scores) if scores else 0 for action, scores in score_set.items()}\n    epsilon = max(initial_epsilon * (epsilon_decay_rate ** (total_selection_count // 10)), min_epsilon)\n\n    # Number of actions\n    action_indices = list(score_set.keys())\n    scores = []\n    \n    for action in action_indices:\n        historical_scores = score_set[action]\n        average_score = average_scores[action]\n        selection_count = len(historical_scores)\n\n        # Upper Confidence Bound (UCB) calculation\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n\n        # Combining average score with exploration value\n        scores.append(average_score + exploration_value)\n\n    # Action selection based on the adaptive epsilon-greedy strategy\n    if np.random.rand() < epsilon and total_selection_count < 50:  # Explore during the initial phase\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = np.argmax(scores)\n\n    return action_index",
          "objective": -321.04537352397944,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters\n    initial_epsilon = 1.0\n    min_epsilon = 0.01\n    decay_rate = 0.99\n    ucb_weight = 0.5\n    softmax_temperature = 0.1\n\n    # Epsilon calculation based on the total selection count\n    if total_selection_count < 100:\n        epsilon = initial_epsilon\n    else:\n        avg_variance = np.mean([np.var(scores) if scores else 0 for scores in score_set.values()])\n        epsilon = min(max(min_epsilon, 1 / (1 + avg_variance)), initial_epsilon)\n\n    action_indices = list(score_set.keys())\n    scores = []\n    \n    for action in action_indices:\n        historical_scores = score_set[action]\n        average_score = np.mean(historical_scores) if historical_scores else 0\n        selection_count = len(historical_scores)\n\n        # UCB term calculation\n        ucb = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n\n        # Combined score using UCB\n        combined_score = (1 - ucb_weight) * average_score + ucb_weight * ucb\n        scores.append(combined_score)\n\n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        # Softmax selection for better exploration\n        exp_scores = np.exp(scores / softmax_temperature)\n        probabilities = exp_scores / np.sum(exp_scores)\n        action_index = np.random.choice(action_indices, p=probabilities)\n\n    return action_index",
          "objective": -321.04537352397944,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Configuration for exploration-exploitation balance\n    initial_epsilon = 1.0  # Start fully exploring\n    min_epsilon = 0.01\n    decay_rate = 0.99\n    exploration_threshold = 100\n\n    # Epsilon calculation\n    if total_selection_count < exploration_threshold:\n        epsilon = initial_epsilon\n    else:\n        epsilon = max(initial_epsilon * (decay_rate ** (total_selection_count - exploration_threshold + 1)), min_epsilon)\n\n    action_indices = list(score_set.keys())\n    scores = []\n    \n    for action in action_indices:\n        historical_scores = score_set[action]\n        average_score = np.mean(historical_scores) if historical_scores else 0.0\n        selection_count = len(historical_scores)\n\n        # Upper Confidence Bound (UCB) calculation\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Adjust scores for selection\n        scores.append(average_score + exploration_value)\n\n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = np.argmax(scores)\n\n    return action_index",
          "objective": -321.04537352397944,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    initial_epsilon = 1.0\n    min_epsilon = 0.01\n    decay_rate = 0.995\n    ucb_weight = 0.5\n\n    # Compute epsilon based on the total selection count\n    if total_selection_count < 100:\n        epsilon = initial_epsilon\n    else:\n        epsilon = max(initial_epsilon * (decay_rate ** (total_selection_count - 99)), min_epsilon)\n\n    action_indices = list(score_set.keys())\n    scores = []\n    average_scores = []\n    selection_counts = []\n\n    for action in action_indices:\n        historical_scores = score_set[action]\n        average_score = np.mean(historical_scores) if historical_scores else 0\n        selection_count = len(historical_scores)\n        \n        average_scores.append(average_score)\n        selection_counts.append(selection_count)\n\n        # UCB term calculation\n        ucb = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combined score\n        combined_score = (1 - ucb_weight) * average_score + ucb_weight * ucb\n        scores.append(combined_score)\n\n    # Epsilon-greedy and hybrid strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        max_score = np.max(scores)\n        # Softmax exploration for higher-scored actions\n        exp_scores = np.exp(scores - max_score)  # for numerical stability\n        probabilities = exp_scores / np.sum(exp_scores)\n        action_index = np.random.choice(action_indices, p=probabilities)\n\n    return action_index",
          "objective": -321.04537352397944,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Configuration for exploration-exploitation balance\n    initial_epsilon = 1.0  # Start with full exploration\n    min_epsilon = 0.01\n    decay_rate = 0.95  # Epsilon decay rate\n\n    # Calculate epsilon based on total selections\n    if total_selection_count < 100:\n        epsilon = initial_epsilon\n    else:\n        epsilon = max(initial_epsilon * (decay_rate ** (total_selection_count - 99)), min_epsilon)\n\n    action_indices = list(score_set.keys())\n    scores = []\n    selection_counts = []\n\n    for action in action_indices:\n        historical_scores = score_set[action]\n        average_score = np.mean(historical_scores) if historical_scores else 0\n        selection_count = len(historical_scores)\n\n        # UCB component\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Variance for softmax component\n        variance = np.var(historical_scores) if selection_count > 0 else 0\n        softmax_score = np.exp(average_score) / (np.sum(np.exp(np.array([np.mean(score_set[a]) for a in action_indices if score_set[a]])) + 1e-10) if selection_count > 0 else 1)\n\n        # Combined score using UCB and softmax\n        scores.append((average_score + exploration_value, softmax_score))\n        selection_counts.append(selection_count)\n\n    # Selection with epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        combined_scores = np.array([score[0] * (1 + score[1]) for score in scores])\n        action_index = action_indices[np.argmax(combined_scores)]\n\n    return action_index",
          "objective": -321.04537352397944,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    scores = []\n    num_actions = len(action_indices)\n    \n    # Adaptive exploration factor\n    epsilon = max(0.1, min(1.0, 1.0 - (total_selection_count / (total_time_slots * 0.5))))\n    \n    for action in action_indices:\n        historical_scores = score_set[action]\n        selection_count = len(historical_scores)\n        average_score = np.mean(historical_scores) if historical_scores else 0\n        \n        # Upper Confidence Bound calculation\n        ucb_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Thompson Sampling component\n        beta_param = np.random.beta(1 + sum(historical_scores), 1 + selection_count - sum(historical_scores))\n        \n        # Combining scores\n        total_score = (1 - epsilon) * (average_score + ucb_value) + epsilon * beta_param\n        scores.append(total_score)\n\n    action_index = np.argmax(scores)  # Select the action with the highest total score\n    return action_index",
          "objective": -321.0453735239794,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    total_scores = []\n    selection_counts = np.array([len(score_set[action]) for action in action_indices])\n    averages = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in action_indices])\n    \n    # Dynamic epsilon for exploration\n    epsilon = max(1 - (total_selection_count / (total_selection_count + 25)), 0.01)\n    \n    # Hybrid approach based on selection count\n    if total_selection_count < 100:  # Exploration phase\n        action_index = np.random.choice(action_indices) if np.random.rand() < epsilon else np.argmax(averages)\n    else:\n        # UCB values calculation\n        ucb_values = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1))\n        \n        # Combine the average scores with UCB values\n        hybrid_scores = (1 - epsilon) * averages + epsilon * ucb_values\n        action_index = np.argmax(hybrid_scores)\n\n    return action_index",
          "objective": -321.0453735239794,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    min_epsilon = 0.01\n    initial_epsilon = 1.0\n    decay_rate = 0.99\n\n    # Epsilon decay based on selection count\n    if total_selection_count < 100:\n        epsilon = initial_epsilon\n    else:\n        epsilon = max(initial_epsilon * (decay_rate ** (total_selection_count - 99)), min_epsilon)\n\n    action_indices = list(score_set.keys())\n    scores = []\n    \n    for action in action_indices:\n        historical_scores = score_set[action]\n        average_score = np.mean(historical_scores) if historical_scores else 0\n        selection_count = len(historical_scores)\n\n        # UCB term computation\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combine exploration and exploitation\n        adjusted_score = average_score + exploration_value\n        scores.append(adjusted_score)\n\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        # Softmax selection to diversify among high-scoring actions\n        exp_scores = np.exp(scores - np.max(scores))  # for numerical stability\n        probabilities = exp_scores / np.sum(exp_scores)\n        action_index = np.random.choice(action_indices, p=probabilities)\n\n    return action_index",
          "objective": -321.0453735239794,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    scores = []\n    num_actions = len(action_indices)\n    \n    # Use an adaptive epsilon-greedy approach\n    initial_epsilon = 1.0\n    min_epsilon = 0.1\n    decay_factor = 0.995\n    epsilon = max(min_epsilon, initial_epsilon * (decay_factor ** total_selection_count))\n    \n    for action in action_indices:\n        historical_scores = score_set[action]\n        selection_count = len(historical_scores) + 1  # Add 1 to avoid zero division\n        average_score = np.mean(historical_scores) if historical_scores else 0\n        \n        # Upper Confidence Bound calculation\n        ucb_value = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_count)\n        \n        # Combine average score with exploration weighting\n        total_score = (1 - epsilon) * average_score + epsilon * ucb_value\n        scores.append(total_score)\n\n    action_index = np.argmax(scores)  # Select the action with the highest total score\n    return action_index",
          "objective": -321.0453735239794,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    \n    # Parameters for epsilon-greedy and UCB\n    epsilon_start = 1.0\n    epsilon_min = 0.1\n    epsilon_decay = 0.99\n    ucb_exploration_factor = 2.0\n    ucb_threshold = 10  # Threshold for transitioning strategies\n\n    # Epsilon for exploration\n    epsilon = max(epsilon_min, epsilon_start * (epsilon_decay ** total_selection_count))\n    \n    selection_counts = np.array([len(score_set[action]) for action in action_indices])\n    \n    # Calculate average scores and UCB values\n    average_scores = np.zeros(num_actions)\n    for i, action in enumerate(action_indices):\n        historical_scores = score_set[action]        \n        average_scores[i] = np.mean(historical_scores) if historical_scores else 0\n    \n    # UCB values\n    ucb_values = np.zeros(num_actions)\n    for i in range(num_actions):\n        if selection_counts[i] > 0:\n            ucb_values[i] = np.sqrt((ucb_exploration_factor * np.log(total_selection_count + 1)) / selection_counts[i])\n        else:\n            ucb_values[i] = float('inf')  # Encourage exploration of untried actions\n            \n    # Selection logic\n    if total_selection_count < ucb_threshold:\n        # Apply epsilon-greedy strategy\n        if np.random.rand() < epsilon:\n            action_index = np.random.choice(action_indices)\n        else:\n            action_index = action_indices[np.argmax(average_scores)]\n    else:\n        # Apply a combined approach of Average Scores and UCB\n        scores = average_scores + ucb_values\n        action_index = action_indices[np.argmax(scores)]\n    \n    return action_index",
          "objective": -321.0453735239794,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    average_scores = np.zeros(len(action_indices))\n    selection_counts = np.zeros(len(action_indices))\n    \n    # Calculate average scores and selection counts\n    for i, action in enumerate(action_indices):\n        historical_scores = score_set[action]\n        selection_counts[i] = len(historical_scores)\n        average_scores[i] = np.mean(historical_scores) if historical_scores else 0\n    \n    # Dynamic exploration-exploitation balancing using Softmax\n    temperature = max(1 - (total_selection_count / (total_selection_count + 100)), 0.1)\n    exp_scores = np.exp(average_scores / temperature)\n    probabilities = exp_scores / np.sum(exp_scores)\n    \n    # Implement UCB to address uncertainty in selection\n    ucb_values = np.zeros(len(action_indices))\n    with np.errstate(divide='ignore', invalid='ignore'):\n        ucb_values = average_scores + np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n    \n    # Combine softmax probabilities and UCB values\n    combined_scores = probabilities + 0.5 * ucb_values\n    action_index = action_indices[np.argmax(combined_scores)]\n    \n    return action_index",
          "objective": -321.0453735239794,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    \n    # Initialize arrays for average scores and selection counts\n    average_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for i, action in enumerate(action_indices):\n        historical_scores = score_set[action]\n        selection_counts[i] = len(historical_scores)\n        average_scores[i] = np.mean(historical_scores) if historical_scores else 0.0\n\n    # Exploration with epsilon-greedy in early slots\n    epsilon = 0.1 * (1 - (total_selection_count / (total_selection_count + 100)))\n    epsilon = max(epsilon, 0.05)  # Keep a minimum exploration rate\n    if np.random.rand() < epsilon:\n        return np.random.choice(action_indices)\n\n    # Upper Confidence Bound (UCB) for balancing exploration-exploitation\n    ucb_values = np.zeros(num_actions)\n    with np.errstate(divide='ignore', invalid='ignore'):\n        ucb_values = average_scores + np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n    \n    # Combine UCB values to get final selection probabilities\n    combined_scores = ucb_values\n    action_index = action_indices[np.argmax(combined_scores)]\n    \n    return action_index",
          "objective": -321.0453735239794,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    \n    # Dynamic epsilon calculation\n    initial_epsilon = 1.0\n    min_epsilon = 0.05\n    decay_factor = 0.99\n    epsilon = max(min_epsilon, initial_epsilon * (decay_factor ** (total_selection_count / 100)))\n\n    # Early exploration phase\n    if total_selection_count < 20:\n        return np.random.choice(action_indices)\n\n    scores = []\n    for action in action_indices:\n        historical_scores = score_set[action]\n        selection_count = len(historical_scores) + 1  # Avoid division by zero\n        average_score = np.mean(historical_scores) if historical_scores else 0\n        \n        # Upper Confidence Bound calculation\n        ucb_value = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_count)\n\n        # Softmax calculation for better exploration/exploitation balance\n        softmax_temp = 0.1\n        softmax_value = np.exp(average_score / softmax_temp)\n        \n        # Combine scores with a weight towards exploration (epsilon)\n        total_score = (1 - epsilon) * average_score + epsilon * (ucb_value + softmax_value)\n        scores.append(total_score)\n\n    # Choose the action with the highest score\n    action_index = np.argmax(scores)\n    return action_index",
          "objective": -321.0453735239794,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Constants\n    initial_epsilon = 1.0\n    min_epsilon = 0.05\n    decay_factor = 0.99\n    exploration_weight = 0.5  # weight for diversity\n    ucb_weight = 0.5         # weight for UCB in the hybrid approach\n\n    action_indices = list(score_set.keys())\n    \n    # Initialize historical data\n    historical_scores = {index: score_set[index] for index in action_indices}\n    \n    # Calculate average scores, selection counts, and UCB values\n    avg_scores = []\n    selection_counts = []\n    ucb_values = []\n    \n    for action in action_indices:\n        historical_score = historical_scores[action]\n        avg_score = np.mean(historical_score) if historical_score else 0\n        selection_count = len(historical_score)\n\n        avg_scores.append(avg_score)\n        selection_counts.append(selection_count)\n        \n        # UCB calculation\n        ucb_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        ucb_values.append(ucb_value)\n\n    # Convert lists to NumPy arrays for efficient computation\n    avg_scores = np.array(avg_scores)\n    selection_counts = np.array(selection_counts)\n    ucb_values = np.array(ucb_values)\n\n    # Adaptive epsilon-decay strategy\n    if total_selection_count < 50 or current_time_slot < total_time_slots * 0.1:\n        epsilon = initial_epsilon\n    else:\n        epsilon = max(initial_epsilon * (decay_factor ** (total_selection_count - 49)), min_epsilon)\n    \n    # Action selection using an epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        combined_scores = (1 - exploration_weight) * avg_scores + exploration_weight * ucb_values\n        action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": -321.0453735239794,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    scores = []\n    num_actions = len(action_indices)\n    \n    # Adaptive exploration factor\n    epsilon = max(0.1, min(1.0, (1.0 - (total_selection_count / (total_time_slots * 0.5)))))\n    \n    # Calculate average scores and variances\n    average_scores = []\n    variances = []\n    \n    for action in action_indices:\n        historical_scores = score_set[action]\n        selection_count = len(historical_scores)\n        average_score = np.mean(historical_scores) if historical_scores else 0\n        variance = np.var(historical_scores) if historical_scores else 0\n        \n        average_scores.append(average_score)\n        variances.append(variance)\n\n        # UCB calculation\n        ucb_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Store (UCB+Average) score and adjustment for exploration\n        total_score = average_score + ucb_value\n        scores.append(total_score)\n\n    # Softmax-based adjustment using average scores and variances\n    exp_scores = np.exp(np.array(average_scores) / (1 + np.array(variances)))\n    softmax_scores = exp_scores / np.sum(exp_scores)\n\n    # Combine scores with exploration probability\n    adjusted_scores = ((1 - epsilon) * np.array(scores)) + (epsilon * softmax_scores)\n    \n    action_index = np.argmax(adjusted_scores)  # Select the action with the highest adjusted score\n    return action_index",
          "objective": -321.0453735239794,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Configuration for exploration-exploitation balance\n    initial_epsilon = 1.0  # Initial exploration\n    min_epsilon = 0.05\n    epsilon_decay_start = 10  # After which count to start decaying epsilon\n    decay_rate = 0.99  # Epsilon decay rate\n    threshold_selection_count = 100  # After which switch to hybrid strategy\n\n    # Dynamic epsilon based on the total selection count\n    if total_selection_count < epsilon_decay_start:\n        epsilon = initial_epsilon\n    else:\n        epsilon = max(initial_epsilon * (decay_rate ** (total_selection_count - epsilon_decay_start)), min_epsilon)\n\n    action_indices = list(score_set.keys())\n    scores = []\n    exploration_values = []\n    \n    for action in action_indices:\n        historical_scores = score_set[action]\n        average_score = np.mean(historical_scores) if historical_scores else 0\n        selection_count = len(historical_scores)\n        \n        # Upper Confidence Bound (UCB) term\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        exploration_values.append(exploration_value)\n        \n        # Combined value considering UCB and average score\n        scores.append(average_score + exploration_value)\n\n    # Implementing the hybrid strategy with Thompson Sampling after threshold_selection_count\n    if total_selection_count >= threshold_selection_count:\n        sampled_scores = np.random.beta([score + 1 for score in scores], [selection_count + 1 for selection_count in map(len, score_set.values())])\n        action_index = np.argmax(sampled_scores)\n    else:\n        # Epsilon-greedy selection strategy\n        if np.random.rand() < epsilon:\n            action_index = np.random.choice(action_indices)\n        else:\n            action_index = np.argmax(scores)\n    \n    return action_index",
          "objective": -321.0453735239794,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Configuration parameters\n    initial_epsilon = 1.0  # Exploration weight\n    min_epsilon = 0.01\n    epsilon_decay = 0.95  # Epsilon decay rate\n    ucb_coefficient = 1.5  # The balance term for UCB exploration\n    \n    # Calculate epsilon based on selection count\n    if total_selection_count < 100:\n        epsilon = initial_epsilon\n    else:\n        epsilon = max(initial_epsilon * (epsilon_decay ** (total_selection_count - 99)), min_epsilon)\n    \n    action_indices = list(score_set.keys())\n    scores = []\n    \n    # Calculate average scores and UCB values\n    for action in action_indices:\n        historical_scores = score_set[action]\n        average_score = np.mean(historical_scores) if historical_scores else 0\n        selection_count = len(historical_scores)\n\n        # UCB term calculation\n        ucb_value = ucb_coefficient * np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n\n        # Combined score\n        scores.append(average_score + ucb_value)\n\n    # Epsilon-greedy action selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = np.argmax(scores)\n\n    return action_index",
          "objective": -321.0453735239794,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    initial_epsilon = 1.0\n    min_epsilon = 0.01\n    decay_rate = 0.99\n\n    if total_selection_count < 100:\n        epsilon = initial_epsilon\n    else:\n        epsilon = max(initial_epsilon * (decay_rate ** (total_selection_count - 99)), min_epsilon)\n\n    action_indices = list(score_set.keys())\n    scores = []\n    \n    for action in action_indices:\n        historical_scores = score_set[action]\n        selection_count = len(historical_scores)\n        average_score = np.mean(historical_scores) if historical_scores else 0\n        \n        # UCB calculation\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        scores.append(average_score + exploration_value)\n\n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = np.argmax(scores)\n\n    return action_index",
          "objective": -321.0453735239794,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Configuration for exploration-exploitation balance\n    initial_epsilon = 1.0\n    min_epsilon = 0.01\n    decay_rate = 0.995\n    \n    # Dynamically adjust epsilon based on selection count\n    if total_selection_count < 50:\n        epsilon = initial_epsilon\n    else:\n        epsilon = max(min_epsilon, initial_epsilon * (decay_rate ** (total_selection_count - 49)))\n    \n    action_indices = list(score_set.keys())\n    scores = []\n    selection_counts = []\n    \n    for action in action_indices:\n        historical_scores = score_set[action]\n        average_score = np.mean(historical_scores) if historical_scores else 0\n        selection_count = len(historical_scores)\n        selection_counts.append(selection_count)\n        \n        # Softmax exploration-exploitation calculation\n        if selection_count > 0:\n            ucb_value = np.sqrt(np.log(total_selection_count) / selection_count)\n        else:\n            ucb_value = float('inf')\n        \n        scores.append((average_score, ucb_value))\n\n    # Normalizing scores for softmax\n    avg_scores = np.array([s[0] for s in scores])\n    ucb_values = np.array([s[1] for s in scores])\n    \n    # Hybrid approach for combining UCB with softmax\n    combined_scores = avg_scores + ucb_values\n    softmax_probabilities = np.exp(combined_scores - np.max(combined_scores))\n    softmax_probabilities /= softmax_probabilities.sum()\n    \n    # Epsilon-greedy selection mechanism\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = np.random.choice(action_indices, p=softmax_probabilities)\n\n    return action_index",
          "objective": -321.0453735239794,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Constants\n    initial_epsilon = 1.0\n    min_epsilon = 0.05\n    decay_factor = 0.98\n    exploration_weight = 0.4  # weight for UCB in the hybrid approach\n    exploration_threshold = 50  # time steps before focusing on exploitation\n    \n    action_indices = list(score_set.keys())\n    \n    # Initialize historical data\n    historical_data = {i: score_set[i] for i in action_indices}\n    \n    # Calculate average scores, exploration values, and standard deviations\n    avg_scores = []\n    exploration_values = []\n    selection_counts = []\n    \n    for action in action_indices:\n        historical_scores = historical_data[action]\n        selection_count = len(historical_scores)\n        \n        avg_score = np.mean(historical_scores) if selection_count > 0 else 0\n        std_dev = np.std(historical_scores) if selection_count > 0 else 0\n        \n        avg_scores.append(avg_score)\n        selection_counts.append(selection_count)\n        \n        # UCB calculation\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        exploration_values.append(exploration_value)\n    \n    avg_scores = np.array(avg_scores)\n    exp_values = np.array(exploration_values)\n    \n    # Dynamic epsilon based on the standard deviation of scores\n    if total_selection_count < exploration_threshold:\n        epsilon = initial_epsilon\n    else:\n        epsilon = min(initial_epsilon * (decay_factor ** (total_selection_count - exploration_threshold)), min_epsilon)\n    \n    # Action selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        combined_scores = (1 - exploration_weight) * avg_scores + exploration_weight * exp_values\n        action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": -321.0453735239794,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Configuration for the exploration-exploitation balance\n    initial_epsilon = 1.0  # Start fully exploring\n    min_epsilon = 0.01\n    decay_rate = 0.99  # Decay rate for epsilon\n    \n    # Dynamic epsilon based on exploration needs\n    epsilon = max(initial_epsilon * (decay_rate ** (total_selection_count // 10)), min_epsilon)\n\n    action_indices = list(score_set.keys())\n    scores = []\n    \n    for action in action_indices:\n        historical_scores = score_set[action]\n        selection_count = len(historical_scores)\n        \n        # Calculate average score with safe handling of empty actions\n        average_score = np.mean(historical_scores) if selection_count > 0 else 0\n        \n        # Standard deviation as a measure of score variability\n        score_std = np.std(historical_scores) if selection_count > 1 else 0\n        \n        # UCB component - exploring less frequently tried actions\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n\n        # Combined score with weighted exploration\n        combined_score = average_score + exploration_value + score_std / (selection_count + 1)\n        scores.append(combined_score)\n\n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = np.argmax(scores)\n\n    return action_index",
          "objective": -321.0453735239794,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters for exploration-exploitation balance\n    initial_epsilon = 1.0\n    min_epsilon = 0.01\n    decay_rate = 0.995  # Smoother decay for epsilon\n    ucb_weight = 1.5     # Weight to encourage exploration via UCB\n\n    # Determine epsilon based on the total selection count\n    epsilon = max(initial_epsilon * (decay_rate ** total_selection_count), min_epsilon)\n\n    action_indices = list(score_set.keys())\n    scores = []\n\n    # Calculate UCB scores for each action\n    for action in action_indices:\n        historical_scores = np.array(score_set[action])\n        average_score = np.mean(historical_scores) if historical_scores.size > 0 else 0\n        selection_count = historical_scores.size\n\n        # UCB term\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1) if selection_count > 0 else float('inf'))\n        \n        # Combined score with UCB weight\n        scores.append(average_score + ucb_weight * exploration_value)\n\n    # Selection based on epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = np.argmax(scores)\n\n    return action_index",
          "objective": -321.0453735239794,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Configuration\n    min_epsilon = 0.01\n    initial_epsilon = 1.0\n    exploration_weight = 1.5\n    total_actions = len(score_set)\n    \n    # Initialize variables\n    average_scores = np.zeros(total_actions)\n    selection_counts = np.zeros(total_actions)\n\n    # Calculate average scores and selection counts\n    for action, scores in score_set.items():\n        if scores:\n            average_scores[action] = np.mean(scores)\n            selection_counts[action] = len(scores)\n\n    # Calculate epsilon based on selection counts\n    if total_selection_count > 0:\n        epsilon = min(initial_epsilon * (1 - total_selection_count / total_time_slots), min_epsilon)\n    else:\n        epsilon = initial_epsilon\n\n    # Calculate exploration values for UCB\n    exploration_values = np.where(selection_counts > 0, \n                                   exploration_weight * np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1)), \n                                   float('inf'))\n\n    # Combined score for the selection strategy\n    combined_scores = average_scores + exploration_values\n    \n    # Choose action based on the epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(range(total_actions), p=(1/total_actions) * np.ones(total_actions))\n    else:\n        action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": -321.0453735239794,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    scores = []\n    \n    # Dynamic epsilon calculation\n    epsilon = max(0.1, min(1.0, 1.0 - (total_selection_count / total_time_slots)))\n\n    # Initialize variables for Thompson Sampling\n    alpha = np.array([1] * num_actions)  # Successful counts\n    beta = np.array([1] * num_actions)   # Failure counts\n\n    # Gather historical scores and counts\n    for action in action_indices:\n        historical_scores = score_set[action]\n        selection_count = len(historical_scores)\n        \n        if selection_count > 0:\n            average_score = np.mean(historical_scores)\n            total_score = (1 - epsilon) * average_score + epsilon * np.random.rand()\n        else:\n            total_score = np.random.rand()  # Full exploration for untried actions\n        \n        # Update alpha and beta for Thompson Sampling\n        success_count = np.sum(historical_scores)\n        alpha[action] += success_count\n        beta[action] += selection_count - success_count\n        \n        scores.append(total_score)\n\n    # Thompson Sampling method to select action\n    sampled_scores = np.random.beta(alpha, beta)\n    scores = np.array(scores)\n    combined_scores = (1 - epsilon) * sampled_scores + epsilon * scores\n    \n    action_index = np.argmax(combined_scores)  # Select the action with the highest combined score\n    return action_index",
          "objective": -321.0453735239793,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    \n    # Parameters for adaptive exploration and exploitation\n    epsilon_start = 1.0\n    epsilon_min = 0.1\n    epsilon_decay = 0.99\n    ucb_exploration_factor = 2.0\n    exploration_threshold = 5  # Minimum selections before focusing on exploitation\n\n    # Adaptive epsilon for exploration\n    epsilon = max(epsilon_min, epsilon_start * (epsilon_decay ** total_selection_count))\n    \n    # Selection counts and historical scores\n    selection_counts = np.array([len(score_set[action]) for action in action_indices])\n    average_scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in action_indices])\n    \n    # UCB values calculation\n    ucb_values = np.zeros(num_actions)\n    for i in range(num_actions):\n        if selection_counts[i] > 0:\n            ucb_values[i] = np.sqrt((ucb_exploration_factor * np.log(total_selection_count + 1)) / selection_counts[i])\n        else:\n            ucb_values[i] = float('inf')  # Encourage exploration of untried actions\n    \n    # Dynamic strategy choice based on selection count\n    if total_selection_count < exploration_threshold or current_time_slot < num_actions:\n        action_index = np.random.choice(action_indices) if np.random.rand() < epsilon else action_indices[np.argmax(average_scores)]\n    else:\n        scores = average_scores + ucb_values\n        action_index = action_indices[np.argmax(scores)]\n\n    return action_index",
          "objective": -321.0453735239793,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    initial_epsilon = 1.0\n    min_epsilon = 0.01\n    decay_factor = 0.95\n    softmax_temperature = 0.5\n    \n    # Determine epsilon based on total selections\n    if total_selection_count < 10:\n        epsilon = 1.0  # Explore fully in early stages\n    else:\n        epsilon = max(initial_epsilon * (decay_factor ** total_selection_count), min_epsilon)\n\n    action_indices = list(score_set.keys())\n    scores = []\n    selection_counts = []\n\n    for action in action_indices:\n        historical_scores = score_set[action]\n        average_score = np.mean(historical_scores) if historical_scores else 0\n        selection_count = len(historical_scores)\n        selection_counts.append(selection_count)\n        \n        # UCB term\n        ucb_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n\n        scores.append((average_score, selection_count, ucb_value))\n\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        # UCB + softmax approach\n        actions = [(avg + ucb, count) for (avg, _, ucb), count in zip(scores, selection_counts)]\n        \n        # Normalize UCB scores for softmax selection\n        max_score = max(action[0] for action in actions)\n        exp_scores = np.exp((np.array([action[0] for action in actions]) - max_score) / softmax_temperature)\n        \n        # Softmax probabilities\n        probabilities = exp_scores / exp_scores.sum()\n        action_index = np.random.choice(action_indices, p=probabilities)\n\n    return action_index",
          "objective": -321.0453735239793,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Configuration for the exploration-exploitation balance\n    initial_epsilon = 1.0  # Start with exploration\n    min_epsilon = 0.01\n    decay_rate = 0.99  # How quickly to decrease epsilon\n    \n    action_indices = list(score_set.keys())\n    scores = []\n    selection_counts = []\n    \n    for action in action_indices:\n        historical_scores = score_set[action]\n        selection_count = len(historical_scores)\n        selection_counts.append(selection_count)\n        \n        average_score = np.mean(historical_scores) if historical_scores else 0\n        variance = np.var(historical_scores) if historical_scores else 0\n        \n        # UCB term\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combine average score with exploration and variance\n        combined_score = average_score + exploration_value * (1 + np.sqrt(min_epsilon + variance))\n        scores.append(combined_score)\n    \n    # Calculate dynamic epsilon based on time slot and total selection counts\n    epsilon = max(initial_epsilon * (decay_rate ** total_selection_count), min_epsilon)\n\n    # Selection based on adaptive exploration-exploitation strategy\n    if np.random.rand() < epsilon and total_selection_count < 10:  # Prioritize exploration in early stages\n        action_index = np.random.choice(action_indices)\n    else:\n        selected_action = np.argmax(scores)\n        action_index = action_indices[selected_action]\n    \n    return action_index",
          "objective": -321.0453735239793,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters for exploration-exploitation balance\n    initial_epsilon = 1.0\n    min_epsilon = 0.01\n    decay_factor = 0.995\n    action_indices = list(score_set.keys())\n    \n    # Calculate average scores and selection counts\n    averages = []\n    selection_counts = []\n    \n    for action in action_indices:\n        historical_scores = score_set[action]\n        average_score = np.mean(historical_scores) if historical_scores else 0\n        averages.append(average_score)\n        selection_counts.append(len(historical_scores))\n    \n    # Compute epsilon based on variance in scores\n    score_variance = np.std(averages) if len(averages) > 1 else 0\n    epsilon = max(min_epsilon, initial_epsilon * (decay_factor ** total_selection_count))\n    \n    # Compute UCB scores\n    ucb_scores = [\n        avg + np.sqrt(2 * np.log(total_selection_count + 1) / (count + 1)) if count > 0 else float('inf') \n        for avg, count in zip(averages, selection_counts)\n    ]\n    \n    # Combine UCB with Softmax probabilities for selection\n    exp_scores = np.exp(ucb_scores - np.max(ucb_scores))  # for numerical stability\n    softmax_probs = exp_scores / np.sum(exp_scores)\n    \n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)  # exploration\n    else:\n        action_index = np.random.choice(action_indices, p=softmax_probs)  # exploitation with softmax\n    \n    return action_index",
          "objective": -321.04537352397926,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    scores = []\n    n_actions = len(action_indices)\n    \n    # Dynamic exploration factor, starting high and decaying over time\n    epsilon = max(1 - (total_selection_count / (total_selection_count + 100)), 0.05)\n    \n    # Calculate average scores and selection counts for each action\n    for action in action_indices:\n        historical_scores = score_set[action]\n        selection_count = len(historical_scores)\n        average_score = np.mean(historical_scores) if historical_scores else 0\n        \n        # Calculate UCB component with a small adjustment to avoid division by zero\n        ucb_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1e-5)) if selection_count > 0 else float('inf')\n        \n        # Softmax component for exploration\n        softmax_factor = np.exp(average_score) / np.sum(np.exp(np.array([np.mean(score_set[a]) if score_set[a] else 0 for a in action_indices])))\n\n        # Combine exploration and exploitation\n        total_score = (1 - epsilon) * average_score + epsilon * (ucb_value + softmax_factor)\n        scores.append(total_score)\n\n    # Normalize scores to avoid extreme values affecting selection\n    normalized_scores = scores - np.min(scores) + 1e-5\n    action_index = np.argmax(normalized_scores / np.sum(normalized_scores))\n    \n    return action_index",
          "objective": -321.0453735239792,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Configuration for exploration-exploitation balance\n    exploration_factor = 1.0\n    exploit_weight = 0.8\n    exploration_weight = 0.2\n    threshold_selection_count = 100\n\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    \n    # Initialize scores for actions\n    scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and counts\n    for idx, action in enumerate(action_indices):\n        historical_scores = score_set[action]\n        selection_counts[idx] = len(historical_scores)\n        if selection_counts[idx] > 0:\n            average_score = np.mean(historical_scores)\n        else:\n            average_score = 0\n        scores[idx] = average_score\n\n    # If total selections are low, focus on exploration\n    if total_selection_count < threshold_selection_count:\n        action_index = np.random.choice(action_indices)\n    else:\n        # Compute UCB values\n        ucb_values = scores + np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n        \n        # Hybrid approach for selection\n        if np.random.rand() < exploration_factor:\n            action_index = np.random.choice(action_indices)\n        else:\n            action_index = action_indices[np.argmax(ucb_values)]\n\n    return action_index",
          "objective": -321.0453735239792,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Configuration for the exploration-exploitation balance\n    initial_epsilon = 1.0\n    min_epsilon = 0.01\n    decay_rate = 0.98\n    \n    # Epsilon decay based on selection count\n    epsilon = max(initial_epsilon * (decay_rate ** (total_selection_count // 100)), min_epsilon)\n\n    action_indices = list(score_set.keys())\n    scores = []\n    \n    for action in action_indices:\n        historical_scores = score_set[action]\n        selection_count = len(historical_scores)\n        average_score = np.mean(historical_scores) if historical_scores else 0\n        \n        # UCB term - only considers actions that have been selected\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combine average score and exploration term\n        total_score = average_score + exploration_value\n        scores.append(total_score)\n\n    # Selection based on epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = np.argmax(scores)\n\n    return action_index",
          "objective": -321.0453735239792,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    scores = []\n    \n    # Dynamic epsilon calculation for exploration\n    exploration_threshold = 50\n    epsilon = max(1 - (total_selection_count / (total_selection_count + exploration_threshold)), 0.01)\n    \n    # Calculate average scores and selection counts\n    average_scores = np.zeros(len(action_indices))\n    selection_counts = np.zeros(len(action_indices))\n\n    for i, action in enumerate(action_indices):\n        historical_scores = score_set[action]\n        selection_counts[i] = len(historical_scores)\n        average_scores[i] = np.mean(historical_scores) if historical_scores else 0\n    \n    # Choose either exploration or exploitation based on epsilon\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        # Implement hybrid score calculation\n        ucb_values = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1))\n        ucb_values[selection_counts == 0] = float('inf')  # Give infinite value to unselected actions\n        hybrid_scores = 0.75 * average_scores + 0.25 * ucb_values  # Weighted combination\n        action_index = action_indices[np.argmax(hybrid_scores)]  # Exploit\n    \n    return action_index",
          "objective": -321.04537352397915,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    \n    # Parameters for exploration-exploitation\n    initial_temperature = 1.0\n    min_temperature = 0.1\n    decay_factor = 0.95\n    ucb_exploration_factor = 2.0\n\n    # Dynamic softmax temperature\n    temperature = max(min_temperature, initial_temperature * (decay_factor ** total_selection_count))\n    \n    scores = np.zeros(num_actions)\n    selection_counts = np.array([len(score_set[action]) for action in action_indices])\n    \n    # Calculate average scores and UCB values\n    for i, action in enumerate(action_indices):\n        historical_scores = score_set[action]\n        average_score = np.mean(historical_scores) if historical_scores else 0\n        \n        if selection_counts[i] > 0:\n            ucb_value = np.sqrt((ucb_exploration_factor * np.log(total_selection_count + 1)) / selection_counts[i])\n        else:\n            ucb_value = float('inf')  # Prioritize exploring unselected actions\n        \n        scores[i] = average_score + ucb_value\n\n    # Softmax probability calculation for action selection\n    exp_scores = np.exp(scores / temperature)\n    probabilities = exp_scores / np.sum(exp_scores)\n    \n    # Select an action based on probabilities\n    action_index = np.random.choice(action_indices, p=probabilities)\n    \n    return action_index",
          "objective": -321.0453735239791,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Configuration for exploration-exploitation balance\n    min_epsilon = 0.01\n    decay_steps = 100  # steps to transition to exploitation\n    exploration_weight = 0.1  # weight for exploration\n    softmax_temperature = 1.0  # initial softmax temperature\n\n    action_indices = list(score_set.keys())\n    scores = []\n    \n    for action in action_indices:\n        historical_scores = score_set[action]\n        average_score = np.mean(historical_scores) if historical_scores else 0\n        selection_count = len(historical_scores)\n        \n        # Encourage exploration for lesser-used actions using Gaussian exploration\n        exploration_value = exploration_weight * (1 / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combined score for softmax\n        scores.append(average_score + exploration_value)\n\n    # Softmax function for selecting action based on scores\n    scores_exp = np.exp(np.array(scores) / softmax_temperature)\n    probabilities = scores_exp / np.sum(scores_exp)\n    \n    # Determine exploration-exploitation threshold\n    total_time_weight = (current_time_slot + 1) / total_time_slots\n    exploration_probability = max(min_epsilon, 1 - total_time_weight)\n    \n    # Selection\n    if np.random.rand() < exploration_probability:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = np.random.choice(action_indices, p=probabilities)\n\n    return action_index",
          "objective": -321.0453735239791,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Configuration for dynamic exploration-exploitation balance\n    initial_epsilon = 1.0\n    min_epsilon = 0.01\n    decay_factor = 0.99\n    thompson_probability = 0.7  # Probability for Thompson Sampling adoption\n\n    action_indices = list(score_set.keys())\n    scores = []\n    selection_counts = []\n\n    for action in action_indices:\n        historical_scores = score_set[action]\n        average_score = np.mean(historical_scores) if historical_scores else 0\n        selection_count = len(historical_scores)\n        selection_counts.append(selection_count)\n\n        # UCB term\n        ucb_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        scores.append((average_score, selection_count, ucb_value))\n\n    # Calculate dynamic epsilon\n    epsilon = max(initial_epsilon * (decay_factor ** (total_selection_count / (total_time_slots + 1))), min_epsilon)\n\n    if total_selection_count < total_time_slots or np.random.rand() < epsilon:\n        # Broad exploration\n        action_index = np.random.choice(action_indices)\n    else:\n        # Hybrid strategy: Combination of Thompson Sampling and UCB\n        thompson_scores = np.random.beta([avg + 1 for avg, _, _ in scores], \n                                          [max(0, selection_count - avg) + 1 for _, selection_count, _ in scores])\n        combined_scores = [avg + ucb for (avg, _, ucb), thompson in zip(scores, thompson_scores)]\n        \n        action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": -321.04537352397904,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    scores = []\n    \n    # Initialize eccentric parameters.\n    epsilon_initial = 1.0\n    epsilon_decay_base = 50\n    epsilon_min = 0.01\n    exploration_factor = max(\n        epsilon_min, \n        epsilon_initial * np.exp(-total_selection_count / epsilon_decay_base)\n    )\n\n    softmax_temp = 0.1  # Temperature parameter for softmax\n    cumulative_average = []\n\n    for action in action_indices:\n        historical_scores = score_set[action]\n        selection_count = len(historical_scores)\n        average_score = np.mean(historical_scores) if historical_scores else 0\n        \n        # UCB component\n        ucb_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n\n        # Combined score\n        total_score = (1 - exploration_factor) * average_score + exploration_factor * ucb_value\n        scores.append(total_score)\n\n    # Softmax adjustment\n    scores_array = np.array(scores)\n    exp_scores = np.exp(scores_array / softmax_temp)\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Select action based on probabilities\n    action_index = np.random.choice(action_indices, p=probabilities)\n    return action_index",
          "objective": -321.0453735239786,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Configuration for exploration-exploitation balance\n    initial_epsilon = 1.0\n    min_epsilon = 0.01\n    decay_rate = 0.99\n    softmax_temperature = 0.1\n\n    action_indices = list(score_set.keys())\n    scores = []\n    action_counts = []\n\n    for action in action_indices:\n        historical_scores = score_set[action]\n        average_score = np.mean(historical_scores) if historical_scores else 0\n        selection_count = len(historical_scores)\n        action_counts.append(selection_count)\n\n        # UCB term\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        scores.append((average_score, exploration_value))\n\n    if total_selection_count < 100:\n        # Full exploration in initial phase\n        action_index = np.random.choice(action_indices)\n    else:\n        # Epsilon-greedy strategy\n        epsilon = max(initial_epsilon * (decay_rate ** (total_selection_count - 99)), min_epsilon)\n        if np.random.rand() < epsilon:\n            action_index = np.random.choice(action_indices)\n        else:\n            # Hybrid approach: using UCB combined with softmax\n            ucb_scores = [avg + exp for avg, exp in scores]\n            max_ucb_score = max(ucb_scores)\n            exp_scores = np.exp(ucb_scores - max_ucb_score) / softmax_temperature\n            probabilities = exp_scores / np.sum(exp_scores)\n            action_index = np.random.choice(action_indices, p=probabilities)\n\n    return action_index",
          "objective": -321.0453735239784,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Gradually decreasing exploration factor based on the total selection count\n    epsilon = max(0.1 * (1 - total_selection_count / (total_time_slots * 0.5)), 0.01)\n    action_indices = list(score_set.keys())\n    scores = []\n\n    for action in action_indices:\n        historical_scores = score_set[action]\n        average_score = np.mean(historical_scores) if historical_scores else 0\n        selection_count = len(historical_scores)\n        \n        # Calculate the exploration term\n        exploration_value = (np.sqrt(total_selection_count) / (1 + selection_count)) if selection_count > 0 else float('inf')\n        \n        # Score considering both average score and exploration\n        scores.append(average_score + epsilon * exploration_value)\n\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = np.argmax(scores)\n\n    return action_index",
          "objective": -321.04537352397836,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    \n    # Initialize arrays for scores and counts\n    selection_counts = np.array([len(score_set[action]) for action in action_indices])\n    averages = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in action_indices])\n    \n    # Handle cases where selection counts are zero for UCB calculations\n    selection_counts = np.where(selection_counts > 0, selection_counts, 1)\n    \n    # Dynamic exploration-exploitation balance\n    if total_selection_count < 100:  # Initial exploration phase\n        probabilities = np.ones(num_actions) / num_actions\n    else:\n        ucb_values = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts)\n        hybrid_scores = averages + ucb_values\n        probabilities = np.exp(hybrid_scores - np.max(hybrid_scores))  # Softmax for probabilities\n        probabilities /= np.sum(probabilities)\n        \n    action_index = np.random.choice(action_indices, p=probabilities)\n\n    return action_index",
          "objective": -321.04537352397836,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    scores = []\n    num_actions = len(action_indices)\n    \n    # Dynamic epsilon based on the total selection count\n    initial_epsilon = 1.0\n    min_epsilon = 0.05\n    decay_factor = 0.99\n    threshold_count = 5  # Threshold to switch to UCB\n    epsilon = max(min_epsilon, initial_epsilon * (decay_factor ** total_selection_count))\n\n    # Calculate scores for each action\n    for action in action_indices:\n        historical_scores = score_set[action]\n        selection_count = len(historical_scores)\n\n        # Handling cases with zero historical scores\n        average_score = np.mean(historical_scores) if historical_scores else 0\n\n        if selection_count > 0:\n            # UCB calculation\n            ucb_value = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_count)\n        else:\n            ucb_value = float('inf')  # Explore unselected actions\n\n        # Weighted score calculation\n        total_score = (1 - epsilon) * average_score + epsilon * ucb_value\n        scores.append(total_score)\n\n    # Select the action with the maximum score\n    action_index = np.argmax(scores)\n    return action_index",
          "objective": -321.0453735239782,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    scores = []\n    num_actions = len(action_indices)\n    \n    # Dynamic epsilon based on the total selection count\n    initial_epsilon = 1.0\n    min_epsilon = 0.05\n    epsilon_decay_rate = 0.99\n    epsilon = max(min_epsilon, initial_epsilon * (epsilon_decay_rate ** total_selection_count))\n\n    # Softmax temperature parameter\n    temperature = 1.0 \n    threshold_count = 10  # Point to shift focus from exploration\n\n    for action in action_indices:\n        historical_scores = score_set[action]\n        selection_count = len(historical_scores)\n\n        # Average score calculation with zero handling\n        average_score = np.mean(historical_scores) if historical_scores else 0\n\n        # UCB calculation with zero handling\n        ucb_value = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_count) if selection_count > 0 else float('inf')\n\n        # Combine scores\n        combined_score = (1 - epsilon) * average_score + epsilon * ucb_value\n        \n        scores.append(combined_score)\n\n    # Softmax selection to encourage exploration while respecting scores\n    exp_scores = np.exp(np.array(scores) / temperature)\n    probabilities = exp_scores / np.sum(exp_scores)\n    \n    # Select action based on probabilities\n    action_index = np.random.choice(action_indices, p=probabilities)\n    \n    return action_index",
          "objective": -321.0453735239781,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    scores = []\n    \n    # Dynamically decreasing exploration factor based on selection count\n    epsilon = max(1 - (total_selection_count / (total_selection_count + 50)), 0.01)\n\n    for action in action_indices:\n        historical_scores = score_set[action]\n        selection_count = len(historical_scores)\n        average_score = np.mean(historical_scores) if historical_scores else 0\n        \n        # Calculate UCB component\n        ucb_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n\n        # Combine exploration and exploitation\n        total_score = (1 - epsilon) * average_score + epsilon * ucb_value\n        scores.append(total_score)\n\n    # Select action with the highest score\n    action_index = np.argmax(scores)  \n    return action_index",
          "objective": -321.0453735239778,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    \n    # Parameters for exploration-exploitation\n    initial_temperature = 1.0\n    min_temperature = 0.1\n    decay_factor = 0.90\n    ucb_exploration_factor = 2.0\n    exploration_limit = 5  # Initial exploration limit\n\n    # Dynamic softmax temperature\n    temperature = max(min_temperature, initial_temperature * (decay_factor ** total_selection_count))\n    \n    scores = np.zeros(num_actions)\n    selection_counts = np.array([len(score_set[action]) for action in action_indices])\n    \n    # Calculate average scores and UCB values\n    for i, action in enumerate(action_indices):\n        historical_scores = score_set[action]\n        average_score = np.mean(historical_scores) if historical_scores else 0\n        \n        ucb_value = (\n            np.sqrt((ucb_exploration_factor * np.log(total_selection_count + 1)) / (selection_counts[i] + 1e-6))\n            if selection_counts[i] > 0 else float('inf')\n        )\n        \n        scores[i] = average_score + ucb_value\n    \n    # Softmax probability calculation for action selection\n    exp_scores = np.exp(scores / temperature)\n    probabilities = exp_scores / np.sum(exp_scores)\n    \n    # Select an action based on probabilities\n    action_index = np.random.choice(action_indices, p=probabilities)\n    \n    return action_index",
          "objective": -321.0453735239778,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    scores = np.zeros(n_actions)\n\n    # Epsilon-greedy initial strategy\n    epsilon = min(1.0, 0.5 * (1 - (total_selection_count / (total_selection_count + 100))))\n    \n    for idx, action in enumerate(action_indices):\n        historical_scores = score_set[action]\n        selection_count = len(historical_scores)\n        average_score = np.mean(historical_scores) if historical_scores else 0\n\n        # UCB calculation\n        ucb_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Softmax component for exploration\n        exp_scores = np.exp(np.array(historical_scores) - np.max(historical_scores))  # for numerical stability\n        softmax_score = np.sum(exp_scores) / (np.sum(np.exp(average_score)) + 1e-5)\n        \n        # Total score balancing exploration and exploitation\n        total_score = (1 - epsilon) * average_score + epsilon * ucb_value * softmax_score\n        scores[idx] = total_score\n\n    # Select action with the highest calculated score\n    action_index = np.argmax(scores)\n    return action_index",
          "objective": -321.04537352397773,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    \n    # Dynamic exploration rate\n    if total_selection_count < 100:\n        epsilon = 1.0  # High exploration in early stages\n    else:\n        epsilon = max(0.1, 1.0 - (total_selection_count / (total_selection_count + 100)))\n\n    # Initialize average scores and selection counts\n    average_scores = np.zeros(len(action_indices))\n    selection_counts = np.zeros(len(action_indices))\n\n    for i, action in enumerate(action_indices):\n        historical_scores = score_set[action]\n        selection_counts[i] = len(historical_scores)\n        average_scores[i] = np.mean(historical_scores) if historical_scores else 0.0\n\n    # Exploration vs Exploitation decision\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        # Calculate UCB values, handling division by zero\n        ucb_values = np.where(\n            selection_counts == 0,\n            np.inf,  # Assign infinity to actions that have never been selected to ensure exploration\n            np.sqrt((2 * np.log(total_selection_count)) / selection_counts)\n        )\n        \n        # Softmax scores\n        softmax_denominator = np.sum(np.exp(average_scores))\n        softmax_scores = np.exp(average_scores) / softmax_denominator if softmax_denominator > 0 else np.zeros(len(action_indices))\n        \n        # Combine strategies\n        combined_scores = 0.5 * average_scores + 0.25 * ucb_values + 0.25 * softmax_scores\n        action_index = action_indices[np.argmax(combined_scores)]  # Exploit\n\n    return action_index",
          "objective": -321.0453735239195,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    \n    # Calculate averages and selection counts\n    selection_counts = np.array([len(score_set[action]) for action in action_indices])\n    averages = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in action_indices])\n    \n    # Softmax function for probability distribution\n    def softmax(values):\n        e_x = np.exp(values - np.max(values))  # Stability improvement\n        return e_x / e_x.sum()\n\n    # Exploration factor\n    exploration_factor = max(0.1, 1 - (total_selection_count / (total_selection_count + 30)))\n    \n    # UCB values calculation\n    ucb_values = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1))\n    \n    # Hybrid scoring combining averages and UCB\n    hybrid_scores = averages + exploration_factor * ucb_values\n    \n    # Probability distribution using softmax\n    action_probabilities = softmax(hybrid_scores)\n    \n    # Select action based on the calculated probabilities\n    action_index = np.random.choice(action_indices, p=action_probabilities)\n    \n    return action_index",
          "objective": -321.04537352391657,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    \n    # Calculate averages and selection counts\n    averages = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in action_indices])\n    selection_counts = np.array([len(score_set[action]) for action in action_indices])\n    \n    # Dynamic epsilon decay based on selection count\n    epsilon = max(0.1 * (1 - (total_selection_count / 200)), 0.01)  # Reduce exploration over time\n    \n    # Exploration phase\n    if total_selection_count < 50:  # Initial exploration phase\n        action_index = np.random.choice(action_indices)\n    else:\n        # UCB calculations\n        ucb_values = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1))\n        \n        # Softmax probabilities\n        exp_scores = np.exp(averages / (1 + np.max(averages)))\n        softmax_probs = exp_scores / np.sum(exp_scores)\n        \n        # Hybrid scores combining exploration (softmax) and exploitation (UCB)\n        combined_scores = (1 - epsilon) * ucb_values + epsilon * softmax_probs\n        \n        action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": -321.04537352391446,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    scores = []\n    num_actions = len(action_indices)\n\n    # Dynamic epsilon decay\n    initial_epsilon = 1.0\n    min_epsilon = 0.1\n    decay_factor = 0.999\n    epsilon = max(min_epsilon, initial_epsilon * (decay_factor ** total_selection_count))\n    \n    # Softmax temperature scaling\n    temp = 1.0 if total_selection_count < 10 else np.log(total_selection_count) / 5.0\n    \n    for action in action_indices:\n        historical_scores = score_set[action]\n        selection_count = len(historical_scores)\n        \n        # Calculate average and handle division by zero\n        average_score = np.mean(historical_scores) if historical_scores else 0\n        \n        # Upper Confidence Bound calculation\n        ucb_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1e-5))\n        \n        # Combine average score with exploration weighting\n        total_score = (1 - epsilon) * average_score + epsilon * ucb_value\n        \n        # Softmax scaling\n        scores.append(np.exp(total_score / temp))\n\n    probabilities = scores / np.sum(scores) if np.sum(scores) > 0 else np.ones(num_actions) / num_actions\n    action_index = np.random.choice(action_indices, p=probabilities)\n\n    return action_index",
          "objective": -321.04537352391014,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    average_scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    \n    # Calculate average scores and selection counts\n    for i, action in enumerate(action_indices):\n        historical_scores = score_set[action]\n        selection_counts[i] = len(historical_scores)\n        average_scores[i] = np.mean(historical_scores) if historical_scores else 0\n\n    # Dynamic epsilon-greedy strategy parameters\n    epsilon = max(1 - (total_selection_count / (total_selection_count + 50)), 0.01)\n    \n    # Choose action based on exploration/exploitation balance\n    if np.random.rand() < epsilon:\n        # Exploration: Randomly select an action with equal probability\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploitation: Using UCB combined with temperature-based softmax\n        ucb_values = np.zeros(n_actions)\n        for i in range(n_actions):\n            if selection_counts[i] > 0:\n                ucb_values[i] = average_scores[i] + np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts[i] + 1))\n            else:\n                ucb_values[i] = float('inf')  # Prioritize actions with no prior selections\n        \n        # Softmax selection\n        exp_scores = np.exp(ucb_values - np.max(ucb_values))\n        probabilities = exp_scores / np.sum(exp_scores)\n\n        action_index = np.random.choice(action_indices, p=probabilities)\n        \n    return action_index",
          "objective": -321.0453735239083,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    scores = []\n    \n    # Calculate average and selection count for each action\n    for action in action_indices:\n        historical_scores = score_set[action]\n        selection_count = len(historical_scores)\n        average_score = np.mean(historical_scores) if historical_scores else 0\n        \n        # Softmax score adjustment\n        exploration_factor = max(1 - (total_selection_count / (total_selection_count + 100)), 0.1)\n        \n        # Calculate UCB component\n        ucb_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combine exploration and exploitation\n        adjusted_score = (1 - exploration_factor) * average_score + exploration_factor * ucb_value\n        scores.append(adjusted_score)\n\n    # Normalize scores to retain comparative values and introduce temperature parameter\n    max_score = max(scores)\n    exp_scores = np.exp((scores - max_score) / 0.5)  # Softmax with temperature parameter\n    probabilities = exp_scores / np.sum(exp_scores)\n    \n    # Select action based on calculated probabilities\n    action_index = np.random.choice(action_indices, p=probabilities)\n    return action_index",
          "objective": -321.04537352387985,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    \n    # Dynamic exploration rate\n    epsilon = max(0.05, min(1.0, 1 - (total_selection_count / (total_selection_count + 50))))\n\n    # Average scores and selection counts\n    average_scores = np.zeros(len(action_indices))\n    selection_counts = np.zeros(len(action_indices))\n\n    for i, action in enumerate(action_indices):\n        historical_scores = score_set[action]\n        selection_counts[i] = len(historical_scores)\n        average_scores[i] = np.mean(historical_scores) if historical_scores else 0.0\n\n    # Exploration vs Exploitation\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        # Calculate UCB values\n        ucb_values = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n        \n        # Softmax scores\n        softmax_scores = np.exp(average_scores - np.max(average_scores))  # Stability improvement\n        softmax_scores /= np.sum(softmax_scores) if np.sum(softmax_scores) > 0 else 1\n\n        # Combine strategies with weighted factors\n        combined_scores = 0.4 * average_scores + 0.3 * ucb_values + 0.3 * softmax_scores\n        action_index = action_indices[np.argmax(combined_scores)]  # Exploit\n\n    return action_index",
          "objective": -321.04537352386484,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    scores = np.zeros(num_actions)\n    selection_counts = np.array([len(score_set[action]) for action in action_indices])\n    \n    # Initialize epsilon and parameters\n    initial_epsilon = 1.0\n    min_epsilon = 0.1\n    decay_factor = 0.995\n    epsilon = max(min_epsilon, initial_epsilon * (decay_factor ** total_selection_count))\n    \n    # Calculate average scores and UCBs\n    for i, action in enumerate(action_indices):\n        historical_scores = score_set[action]\n        average_score = np.mean(historical_scores) if historical_scores else 0\n        \n        if selection_counts[i] > 0:\n            ucb_value = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[i])\n        else:\n            ucb_value = np.inf  # If the action was never selected, give it maximum exploration weight\n        \n        # Combine average score with exploration weighting\n        scores[i] = (1 - epsilon) * average_score + epsilon * ucb_value\n\n    # Softmax implementation for action selection\n    exp_scores = np.exp(scores - np.max(scores))  # Stable softmax\n    probabilities = exp_scores / np.sum(exp_scores)\n    \n    # Select action based on softmax probabilities\n    action_index = np.random.choice(action_indices, p=probabilities)\n    \n    return action_index",
          "objective": -321.0453735237023,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    \n    # Dynamic epsilon-greedy approach\n    initial_epsilon = 1.0\n    min_epsilon = 0.1\n    decay_factor = 0.99\n    epsilon = max(min_epsilon, initial_epsilon * (decay_factor ** total_selection_count))\n    \n    # Calculate average scores and selection counts\n    averages = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    for i, action in enumerate(action_indices):\n        historical_scores = score_set[action]\n        selection_counts[i] = len(historical_scores)\n        averages[i] = np.mean(historical_scores) if historical_scores else 0\n\n    # Use Upper Confidence Bound for exploration weighting\n    ucb_values = np.zeros(num_actions)\n    for i in range(num_actions):\n        if selection_counts[i] > 0:\n            ucb_values[i] = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts[i])\n        else:\n            ucb_values[i] = float('inf')  # Prioritize unchosen actions\n    \n    # Combine exploration and exploitation\n    total_scores = (1 - epsilon) * averages + epsilon * ucb_values\n    \n    # Hybrid selection: incorporate softmax for stable selection\n    temperature = max(0.1, (1 - total_selection_count / total_time_slots) * 10)  # Adjust temperature with time\n    softmax_scores = np.exp(total_scores / temperature) / np.sum(np.exp(total_scores / temperature))\n    \n    # Sample an action based on softmax probabilities\n    action_index = np.random.choice(action_indices, p=softmax_scores)\n    \n    return action_index",
          "objective": -321.045373523588,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    scores = np.zeros(n_actions)\n    \n    # Decay exploration factor (\u03b5) over time\n    epsilon = max(0.1 * (1 - (total_selection_count / (total_selection_count + 100))), 0.01)\n    \n    for idx, action in enumerate(action_indices):\n        historical_scores = score_set[action]\n        selection_count = len(historical_scores)\n        average_score = np.mean(historical_scores) if historical_scores else 0\n        \n        # UCB calculation\n        ucb_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Softmax component for exploration\n        softmax_score = np.exp(average_score / (epsilon + 1e-5))\n        \n        # Combine exploration and exploitation strategies\n        total_score = (1 - epsilon) * average_score + epsilon * ucb_value + (1 - epsilon) * softmax_score / np.sum(np.exp(np.array(scores) / (epsilon + 1e-5)))\n        scores[idx] = total_score\n\n    # Select action with the highest calculated score\n    action_index = np.argmax(scores)\n    return action_index",
          "objective": -321.04537352356783,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    \n    # Dynamic exploration rate\n    epsilon = max(0.1, min(1.0, 1 - (total_selection_count / (total_selection_count + 100))))\n    \n    # Average scores and selection counts\n    average_scores = np.zeros(len(action_indices))\n    selection_counts = np.zeros(len(action_indices))\n\n    for i, action in enumerate(action_indices):\n        historical_scores = score_set[action]\n        selection_counts[i] = len(historical_scores)\n        average_scores[i] = np.mean(historical_scores) if historical_scores else 0.0\n\n    # Exploration vs Exploitation\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        # Calculate UCB values\n        ucb_values = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1))\n        \n        # Softmax scores\n        softmax_scores = np.exp(average_scores) / np.sum(np.exp(average_scores)) if np.sum(np.exp(average_scores)) > 0 else np.zeros(len(action_indices))\n        \n        # Combine strategies with weighted factors\n        combined_scores = 0.5 * average_scores + 0.25 * ucb_values + 0.25 * softmax_scores\n        action_index = action_indices[np.argmax(combined_scores)]  # Exploit\n\n    return action_index",
          "objective": -321.0453735235176,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    \n    # Calculate average scores and selection counts\n    for i, action in enumerate(action_indices):\n        historical_scores = score_set[action]\n        average_score = np.mean(historical_scores) if historical_scores else 0\n        selection_counts[i] = len(historical_scores)\n        scores[i] = average_score\n    \n    # Adaptive epsilon for exploration\n    epsilon = max(0.1 * (1 - total_selection_count / (total_time_slots * 0.5)), 0.01)\n    \n    # Calculate exploration values\n    exploration_values = np.sqrt(total_selection_count) / (1 + selection_counts)\n    \n    # UCB combined with average scores\n    ucb_scores = scores + epsilon * exploration_values\n    \n    if total_selection_count < n_actions * 10:\n        action_index = np.random.choice(action_indices) if np.random.rand() < epsilon else action_indices[np.argmax(ucb_scores)]\n    else:\n        # Softmax selection for final balancing\n        exp_scores = np.exp(ucb_scores - np.max(ucb_scores))  # softmax trick for stability\n        probabilities = exp_scores / np.sum(exp_scores)\n        action_index = np.random.choice(action_indices, p=probabilities)\n    \n    return action_index",
          "objective": -321.04537352282455,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate the average scores and counts per action\n    action_indices = list(score_set.keys())\n    average_scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in action_indices])\n    selection_counts = np.array([len(score_set[action]) for action in action_indices])\n    \n    # Initialize parameters for exploration-exploitation trade-off\n    epsilon_start = 1.0\n    epsilon_end = 0.1\n    epsilon_decay_rate = 0.01\n    epsilon = max(epsilon_end, epsilon_start - (epsilon_decay_rate * total_selection_count))\n\n    # Calculate UCB values for each action\n    ucb_values = np.zeros(len(action_indices))\n    total_count = total_selection_count if total_selection_count > 0 else 1  # Avoid division by zero\n    \n    for i, count in enumerate(selection_counts):\n        if count > 0:\n            ucb_values[i] = average_scores[i] + np.sqrt(np.log(total_count) / count)\n        else:\n            ucb_values[i] = float('inf')  # Prioritize unexplored actions\n\n    # Softmax selection based on scores and UCB\n    softmax_weights = np.exp(average_scores) / np.sum(np.exp(average_scores))\n    ucb_weights = np.exp(ucb_values) / np.sum(np.exp(ucb_values))\n\n    # Combine the two strategies\n    combined_scores = (1 - epsilon) * softmax_weights + epsilon * ucb_weights\n    action_index = np.random.choice(action_indices, p=combined_scores)\n\n    return action_index",
          "objective": -321.0453735222228,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Exploration factor that decreases over time\n    exploration_factor = 1.0 / (1 + current_time_slot)\n    action_indices = list(score_set.keys())\n    scores = []\n\n    for action in action_indices:\n        historical_scores = score_set[action]\n        average_score = np.mean(historical_scores) if historical_scores else 0\n        selection_count = len(historical_scores)\n\n        # UCB calculation\n        if selection_count > 0:\n            exploration_value = np.sqrt(np.log(total_selection_count) / selection_count)\n        else:\n            exploration_value = float('inf')  # Ensure under-explored actions are chosen\n            \n        total_score = average_score + exploration_factor * exploration_value\n        scores.append(total_score)\n\n    action_index = np.argmax(scores)  # Select the action with the highest score\n\n    return action_index",
          "objective": -321.0453735221922,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n\n    if total_selection_count == 0:\n        return np.random.choice(action_indices)\n    \n    averages = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in action_indices])\n    selection_counts = np.array([len(score_set[action]) for action in action_indices])\n\n    # Dynamic epsilon for exploration\n    epsilon = max(0.1 * (1 - (total_selection_count / (total_selection_count + 50))), 0.01)\n\n    if total_selection_count < 100:  # Early exploration phase\n        if np.random.rand() < epsilon:\n            return np.random.choice(action_indices)\n        else:\n            return np.argmax(averages)\n\n    # UCB values calculation\n    total_count = total_selection_count + 1\n    ucb_values = np.sqrt((2 * np.log(total_count)) / (selection_counts + 1))\n\n    # Softmax for action selection\n    temperature = 0.5 / (1 + total_selection_count / 500)  # gradually decrease temperature\n    ucb_adjusted = averages + ucb_values\n    exp_values = np.exp(ucb_adjusted / temperature)\n    probabilities = exp_values / np.sum(exp_values)\n\n    action_index = np.random.choice(action_indices, p=probabilities)\n\n    return action_index",
          "objective": -321.0453735206791,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    scores = []\n    \n    # Define the exploration threshold\n    exploration_threshold = 20\n    epsilon = max(1 - (total_selection_count / (total_selection_count + exploration_threshold)), 0.01)\n    \n    for action in action_indices:\n        historical_scores = score_set[action]\n        selection_count = len(historical_scores)\n        average_score = np.mean(historical_scores) if historical_scores else 0\n        \n        # Upper Confidence Bound (UCB) part\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n\n        # Total score is a combination of exploitation and exploration\n        total_score = (1 - epsilon) * average_score + epsilon * exploration_value\n        scores.append(total_score)\n    \n    action_index = np.argmax(scores)  # Select the action with the highest score\n    return action_index",
          "objective": -321.0453735178445,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    \n    # Dynamic epsilon calculation for exploration\n    exploration_threshold = 100\n    epsilon = max(1 - (total_selection_count / (total_selection_count + exploration_threshold)), 0.1)\n    \n    # Average scores and selection counts\n    average_scores = np.zeros(len(action_indices))\n    selection_counts = np.zeros(len(action_indices))\n\n    for i, action in enumerate(action_indices):\n        historical_scores = score_set[action]\n        selection_counts[i] = len(historical_scores)\n        average_scores[i] = np.mean(historical_scores) if historical_scores else 0\n    \n    # Choose either exploration or exploitation based on epsilon\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        # Implement Hybrid approach\n        total_counts_adjusted = selection_counts + 1  # To avoid division by zero\n        ucb_values = np.sqrt((2 * np.log(total_selection_count + 1)) / total_counts_adjusted)\n        \n        # Softmax probabilities\n        softmax_scores = np.exp(average_scores) / np.sum(np.exp(average_scores))\n        \n        hybrid_scores = 0.6 * average_scores + 0.2 * ucb_values + 0.2 * softmax_scores\n        action_index = action_indices[np.argmax(hybrid_scores)]  # Exploit\n    \n    return action_index",
          "objective": -321.04537351535873,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    scores = []\n\n    # Define exploration and exploitation parameters\n    exploration_weight = 1 / (1 + total_selection_count)\n    \n    # Threshold for switching strategy\n    threshold = 100  # Define when to switch to hybrid method\n\n    for action in action_indices:\n        historical_scores = score_set[action]\n        selection_count = len(historical_scores)\n\n        # Calculate average score safely\n        average_score = np.mean(historical_scores) if historical_scores else 0\n        \n        if total_selection_count < threshold:\n            # Epsilon-Greedy Strategy for early selections\n            total_score = (1 - exploration_weight) * average_score + exploration_weight * np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1))\n        else:\n            # Hybrid UCB and softmax\n            ucb_value = average_score + np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n            scores.append((ucb_value, average_score))\n\n    if total_selection_count < threshold:\n        # Use purely exploration exploitation strategy\n        scores = [(mean_exp) for mean_exp in scores]\n    else:\n        # Softmax distribution for UCB scores\n        ucb_values = np.array([score[0] for score in scores])\n        exp_scores = np.exp(ucb_values - np.max(ucb_values))  # Subtract max for numerical stability\n        softmax_scores = exp_scores / np.sum(exp_scores)\n\n        # Random choice weighted by softmax\n        action_index = np.random.choice(action_indices, p=softmax_scores)\n        return action_index\n\n    # Selecting action by maximum score in early phase\n    action_index = np.argmax(scores)\n    return action_index",
          "objective": -321.0453735102002,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    \n    # Dynamic epsilon adjustments\n    initial_epsilon = 1.0\n    min_epsilon = 0.1\n    decay_factor = 0.99\n    epsilon = max(min_epsilon, initial_epsilon * (decay_factor ** total_selection_count))\n    \n    # Average scores and selection counts\n    average_scores = np.zeros(len(action_indices))\n    selection_counts = np.zeros(len(action_indices))\n\n    for i, action in enumerate(action_indices):\n        historical_scores = score_set[action]\n        selection_counts[i] = len(historical_scores)\n        average_scores[i] = np.mean(historical_scores) if historical_scores else 0\n    \n    # Selection based on exploration versus exploitation\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        # Hybrid approach combining UCB and softmax\n        total_counts_adjusted = selection_counts + 1  # Prevent division by zero\n        ucb_values = np.sqrt((2 * np.log(total_selection_count + 1)) / total_counts_adjusted)\n        \n        softmax_probs = np.exp(average_scores - np.max(average_scores))  # For numerical stability\n        softmax_probs /= np.sum(softmax_probs)\n\n        hybrid_scores = average_scores + 0.5 * ucb_values + 0.5 * softmax_probs\n        action_index = action_indices[np.argmax(hybrid_scores)]  # Exploit\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    k = len(score_set)  # Number of actions\n    epsilon_base = 0.1  # Base exploration factor\n    epsilon_decay = 0.99  # Decay rate for epsilon\n    min_epsilon = 0.01  # Minimum exploration probability\n    \n    # Calculate average scores and selection counts\n    average_scores = np.zeros(k)\n    selection_counts = np.zeros(k)\n\n    for idx in score_set.keys():\n        scores = score_set[idx]\n        selection_counts[idx] = len(scores)\n        if selection_counts[idx] > 0:\n            average_scores[idx] = np.mean(scores)\n\n    # Calculate dynamic epsilon\n    epsilon = max(min_epsilon, epsilon_base * (epsilon_decay ** total_selection_count))\n\n    # Explore or exploit\n    if np.random.rand() > epsilon:\n        action_index = np.argmax(average_scores)\n    else:\n        # Softmax exploration\n        exp_scores = np.exp(average_scores - np.max(average_scores))  # Stability for large values\n        probabilities = exp_scores / np.sum(exp_scores)\n        action_index = np.random.choice(k, p=probabilities)\n\n    return action_index",
          "objective": -321.0453734988279,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    k = len(score_set)  # Number of actions\n    epsilon_base = 0.1  # Base exploration factor\n    epsilon_decay = 0.99  # Decay factor for exploration parameter\n    min_epsilon = 0.01  # Minimum exploration probability\n    \n    # Calculate average scores and selection counts\n    average_scores = np.zeros(k)\n    selection_counts = np.zeros(k)\n    \n    for idx in score_set.keys():\n        scores = score_set[idx]\n        selection_counts[idx] = len(scores)\n        if selection_counts[idx] > 0:\n            average_scores[idx] = np.mean(scores)\n    \n    # Calculate dynamic epsilon\n    if total_selection_count > 0:\n        epsilon = max(min_epsilon, epsilon_base * (epsilon_decay ** (total_selection_count - 1)))\n    else:\n        epsilon = 1.0  # Explore fully in the first round\n    \n    # Exploration or exploitation decision\n    if np.random.rand() > epsilon:\n        # Exploitation: Select action with the highest known average score\n        action_index = np.argmax(average_scores)\n    else:\n        # Exploration: Randomly select less chosen actions or any action\n        unselected_actions = np.where(selection_counts == 0)[0]\n        if len(unselected_actions) > 0:\n            action_index = np.random.choice(unselected_actions)\n        else:\n            action_index = np.random.choice(k)\n    \n    return action_index",
          "objective": -321.0453734895412,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    scores = []\n    epsilon = 1.0 / (1 + total_selection_count)  # Epsilon decreases with more selections\n\n    for action in action_indices:\n        historical_scores = score_set[action]\n        selection_count = len(historical_scores) + 1  # Avoid division by zero\n        average_score = np.mean(historical_scores) if historical_scores else 0\n        \n        # Calculate exploration and exploitation balance\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / selection_count)\n\n        # Combine average score with exploration weight adjusted by epsilon\n        total_score = (1 - epsilon) * average_score + epsilon * exploration_value\n        scores.append(total_score)\n\n    action_index = np.argmax(scores)  # Select the action with the highest score\n    return action_index",
          "objective": -321.0453734845421,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    scores = []\n    selection_counts = np.array([len(score_set[action]) for action in action_indices])\n    \n    # Calculate average scores\n    average_scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in action_indices])\n    \n    # Dynamic exploration parameter based on selection count\n    epsilon = max(1 - (total_selection_count / 100), 0.1)  # Decrease exploration over time\n    \n    if np.random.rand() < epsilon:\n        # Epsilon-Greedy: explore\n        action_index = np.random.choice(action_indices)\n    else:\n        # UCB calculation\n        ucb_values = average_scores + np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n        # Softmax based on UCB values\n        exp_scores = np.exp(ucb_values - np.max(ucb_values))  # For numerical stability\n        softmax_scores = exp_scores / np.sum(exp_scores)\n        action_index = np.random.choice(action_indices, p=softmax_scores)\n    \n    return action_index",
          "objective": -321.0453734464003,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Epsilon-greedy settings\n    epsilon = 1.0 - (total_selection_count / (total_time_slots * 0.1))\n    epsilon = max(0.1, min(epsilon, 1.0))  # Clamp epsilon between 0.1 and 1.0\n\n    action_indices = list(score_set.keys())\n    scores = []\n    softmax_scores = []\n    \n    for action in action_indices:\n        historical_scores = score_set[action]\n        selection_count = len(historical_scores)\n\n        # Calculate average score\n        average_score = np.mean(historical_scores) if selection_count > 0 else 0\n\n        # UCB calculation\n        if selection_count > 0:\n            exploration_value = np.sqrt(np.log(total_selection_count) / selection_count)\n        else:\n            exploration_value = float('inf')  # Ensure under-explored actions are chosen\n        \n        contribution = average_score + exploration_value\n        \n        scores.append(contribution)\n        softmax_scores.append(np.exp(average_score))  # Softmax for potential selection\n\n    # Decision based on epsilon-greedy approach\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        # Hybrid approach of UCB and softmax\n        normalized_softmax_scores = np.array(softmax_scores) / np.sum(softmax_scores)\n        action_index = np.random.choice(action_indices, p=normalized_softmax_scores)\n\n    return action_index",
          "objective": -321.0453734449188,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    k = len(score_set)  # Number of actions\n    initial_epsilon = 0.1  # Initial exploration factor\n    min_epsilon = 0.01  # Minimum exploration probability\n    epsilon_decay_threshold = 50  # Threshold after which to use UCB/softmax\n    exploration_factor = 0.1  # Softmax temperature\n\n    average_scores = np.zeros(k)\n    selection_counts = np.zeros(k)\n\n    for idx in score_set.keys():\n        scores = score_set[idx]\n        selection_counts[idx] = len(scores)\n        if selection_counts[idx] > 0:\n            average_scores[idx] = np.mean(scores)\n    \n    # Dynamic epsilon\n    if total_selection_count < epsilon_decay_threshold:\n        epsilon = max(min_epsilon, initial_epsilon * (1 - total_selection_count / epsilon_decay_threshold))\n        if np.random.rand() < epsilon:\n            action_index = np.random.choice(k)\n        else:\n            action_index = np.argmax(average_scores)\n    else:\n        # UCB strategy\n        ucb_values = average_scores + np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -321.045373430976,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    k = len(score_set)  # Number of actions\n    epsilon_base = 0.1  # Base exploration factor\n    epsilon_decay = 0.99  # Decay factor for exploration parameter\n    min_epsilon = 0.01  # Minimum exploration probability\n    \n    # Calculate average scores and selection counts\n    average_scores = np.zeros(k)\n    selection_counts = np.zeros(k)\n    \n    for idx in score_set.keys():\n        scores = score_set[idx]\n        selection_counts[idx] = len(scores)\n        if selection_counts[idx] > 0:\n            average_scores[idx] = np.mean(scores)\n    \n    # Calculate dynamic epsilon\n    if total_selection_count > 0:\n        epsilon = max(min_epsilon, epsilon_base * (epsilon_decay ** (total_selection_count - 1)))\n    else:\n        epsilon = 1.0  # Explore fully in the first round\n    \n    # Explore or exploit decision\n    if np.random.rand() > epsilon:\n        # Exploitation using softmax to allow for a probabilistic selection\n        exp_scores = np.exp(average_scores - np.max(average_scores))  # Avoid overflow\n        probabilities = exp_scores / np.sum(exp_scores)\n        action_index = np.random.choice(k, p=probabilities)\n    else:\n        # Exploration: randomly select an action\n        action_index = np.random.choice(k)\n    \n    return action_index",
          "objective": -321.04537315157074,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    \n    # Calculate averages and selection counts\n    selection_counts = np.array([len(score_set[action]) for action in action_indices], dtype=float)\n    averages = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in action_indices])\n    \n    # Adaptive epsilon for exploration\n    epsilon = max(1 - (total_selection_count / (total_selection_count + 20)), 0.05)\n    \n    # Exploration vs. exploitation\n    if total_selection_count < 50:  # Initial exploration phase\n        action_index = np.random.choice(action_indices) if np.random.rand() < epsilon else np.argmax(averages)\n    else:\n        # Calculate UCB values\n        ucb_values = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))  # Adding a small value to avoid division by zero\n        \n        # Softmax values for exploration\n        softmax_scores = np.exp(averages) / np.sum(np.exp(averages))\n        \n        # Combine UCB and softmax to create hybrid scores\n        hybrid_scores = (1 - epsilon) * ucb_values + epsilon * softmax_scores\n        action_index = np.argmax(hybrid_scores)\n    \n    return action_index",
          "objective": -321.0453727590536,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    k = len(score_set)  # Number of actions\n    epsilon_base = 0.5  # Initial exploration probability\n    epsilon_decay = 0.99  # Decay factor for exploration probability\n    min_epsilon = 0.01  # Minimum exploration probability\n    ucb_exploration_weight = 1.5  # Weight for UCB exploration component\n\n    # Calculate average scores and selection counts\n    average_scores = np.zeros(k)\n    selection_counts = np.zeros(k)\n\n    for idx in score_set.keys():\n        scores = score_set[idx]\n        selection_counts[idx] = len(scores)\n        if selection_counts[idx] > 0:\n            average_scores[idx] = np.mean(scores)\n\n    # Calculate dynamic epsilon\n    epsilon = max(min_epsilon, epsilon_base * (epsilon_decay ** total_selection_count))\n\n    if total_selection_count < 10:  # Use epsilon-greedy for initial selections\n        if np.random.rand() > epsilon:\n            # Exploitation: Select action with the highest known average score\n            action_index = np.argmax(average_scores)\n        else:\n            # Exploration: Randomly choose an action\n            action_index = np.random.choice(k)\n    else:\n        # Apply Upper Confidence Bound (UCB) strategy for later selections\n        ucb_values = average_scores + ucb_exploration_weight * np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1))\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -321.04476036595196,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    avg_scores = []\n    exploration_factor = 0.2  # Exploration factor\n    decay_factor = 1 - (current_time_slot / total_time_slots)  # Decay factor\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_selection_count = len(scores)\n\n        # Calculate average score\n        avg_score = np.mean(scores) if action_selection_count > 0 else 0\n        \n        # Add exploration term to encourage exploration of less-selected actions\n        exploration_term = exploration_factor / (1 + action_selection_count) if total_selection_count > 0 else exploration_factor\n\n        # Compute final score combining average score and exploration term\n        final_score = avg_score * decay_factor + exploration_term\n        avg_scores.append(final_score)\n\n    action_index = np.argmax(avg_scores)\n    return action_index",
          "objective": -321.0438602011868,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    scores = []\n    \n    # Variable epsilon-greedy setting\n    epsilon = 1.0 - (total_selection_count / (total_time_slots * 0.5))\n    epsilon = max(0.1, min(epsilon, 1.0))  # Clamp between 0.1 and 1.0\n\n    for action in action_indices:\n        historical_scores = score_set[action]\n        selection_count = len(historical_scores)\n\n        # Calculate average score\n        average_score = np.mean(historical_scores) if selection_count > 0 else 0.0\n        \n        # UCB calculation\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # Combined score: weight between exploration and exploitation\n        total_score = average_score + exploration_value\n        scores.append(total_score)\n\n    # Decision based on epsilon-greedy approach\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        # Probabilistic selection based on normalized scores\n        probabilities = np.array(scores) / np.sum(scores)\n        action_index = np.random.choice(action_indices, p=probabilities)\n\n    return action_index",
          "objective": -321.04338989417585,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    scores = []\n    \n    for action in action_indices:\n        historical_scores = score_set[action]\n        average_score = np.mean(historical_scores) if historical_scores else 0\n        selection_count = len(historical_scores)\n\n        # Calculate a dynamic exploration rate\n        exploration_rate = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        # A combination of average score and exploration value\n        total_score = average_score + exploration_rate\n        scores.append(total_score)\n\n    action_index = np.argmax(scores)  # Select the action with the highest score\n    return action_index",
          "objective": -321.0432550307499,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    k = len(score_set)  # Number of actions\n    epsilon_base = 0.1  # Base exploration factor\n    epsilon_decay = 0.95  # Decay factor for exploration parameter\n    min_epsilon = 0.05  # Minimum exploration probability\n    \n    average_scores = np.zeros(k)\n    selection_counts = np.zeros(k, dtype=int)\n    \n    for idx in score_set.keys():\n        scores = score_set[idx]\n        selection_counts[idx] = len(scores)\n        if selection_counts[idx] > 0:\n            average_scores[idx] = np.mean(scores)\n    \n    # Calculate dynamic epsilon\n    epsilon = max(min_epsilon, epsilon_base * (epsilon_decay ** (total_selection_count)))\n    \n    # Exploration or exploitation decision\n    if np.random.rand() > epsilon:\n        # Exploitation: Select action with the highest average score\n        action_index = np.argmax(average_scores)\n    else:\n        # Exploration: Softmax selection among actions\n        exp_scores = np.exp(average_scores - np.max(average_scores))  # Stability for large values\n        probabilities = exp_scores / np.sum(exp_scores)\n        action_index = np.random.choice(k, p=probabilities)\n    \n    return action_index",
          "objective": -321.04312919482857,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Adaptive epsilon setting\n    epsilon = max(0.1, min(1.0, 1.0 - (total_selection_count / (total_time_slots * 0.1))))\n\n    action_indices = list(score_set.keys())\n    scores = []\n    selection_counts = []\n    \n    for action in action_indices:\n        historical_scores = score_set[action]\n        selection_count = len(historical_scores)\n        selection_counts.append(selection_count)\n\n        # Calculate average score\n        average_score = np.mean(historical_scores) if selection_count > 0 else 0\n\n        # UCB calculation\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        scores.append(average_score + exploration_value)\n\n    scores = np.array(scores)\n    softmax_scores = np.exp(scores) / np.sum(np.exp(scores))  # Softmax scores based on UCB + average\n\n    # Decision based on adaptive epsilon-greedy approach\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = np.random.choice(action_indices, p=softmax_scores)\n\n    return action_index",
          "objective": -321.0394125829152,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    scores = []\n    \n    for action in action_indices:\n        historical_scores = score_set[action]\n        average_score = np.mean(historical_scores) if historical_scores else 0\n        selection_count = len(historical_scores)\n\n        # Epsilon-greedy exploration factor\n        epsilon = max(0.01, 1 - (total_selection_count / (total_time_slots * 10)))  # Reducing exploration as more actions are taken\n        exploration_probability = np.random.rand()\n        \n        if exploration_probability < epsilon:\n            scores.append(float('inf') if selection_count == 0 else average_score)  # Favor unselected actions\n        else:\n            # Upper Confidence Bound (UCB) calculation\n            exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else float('inf')\n            total_score = average_score + exploration_value\n            scores.append(total_score)\n\n    action_index = np.argmax(scores)  # Select the action with the highest score\n    return action_index",
          "objective": -321.0368255400067,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    scores = []\n    \n    epsilon = max(0, 1 - total_selection_count / (total_time_slots * 2))  # Decaying exploration rate\n\n    for action in action_indices:\n        historical_scores = score_set[action]\n        average_score = np.mean(historical_scores) if historical_scores else 0\n        selection_count = len(historical_scores)\n\n        if selection_count > 0:\n            # Upper Confidence Bound (UCB) approach for exploration\n            exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / selection_count)\n        else:\n            exploration_value = float('inf')  # Ensure unselected actions can still be chosen\n\n        total_score = average_score + (epsilon * exploration_value)\n        scores.append(total_score)\n\n    action_index = np.argmax(scores)  # Select the action with the highest score\n    return action_index",
          "objective": -321.03640686193665,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    scores = []\n    \n    # Calculate the epsilon value based on the total selection count\n    epsilon = max(0.1 * (1 - total_selection_count / (total_time_slots * 0.5)), 0.01)\n\n    for action in action_indices:\n        historical_scores = score_set[action]\n        average_score = np.mean(historical_scores) if historical_scores else 0\n        selection_count = len(historical_scores)\n\n        # If selection_count is zero, we assign a high exploration value\n        exploration_value = np.sqrt(total_selection_count / (1 + selection_count)) if selection_count > 0 else float('inf')\n        \n        # Score calculation combining average score and exploration\n        scores.append(average_score + epsilon * exploration_value)\n\n    # Epsilon-greedy choice to balance exploration and exploitation\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = np.argmax(scores)\n\n    return action_index",
          "objective": -321.02846901012066,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    scores = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n    \n    # Collect historical scores and counts\n    for i, action in enumerate(action_indices):\n        historical_scores = score_set[action]\n        selection_count = len(historical_scores)\n        selection_counts[i] = selection_count\n        \n        # Calculate average score\n        average_score = np.mean(historical_scores) if selection_count > 0 else 0.0\n        scores[i] = average_score\n\n    # Dynamic epsilon\n    epsilon = max(0.1, min(1.0 - (total_selection_count / (total_time_slots * 0.5)), 1.0))\n\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        # Upper Confidence Bound (UCB) component\n        ucb_values = np.zeros(n_actions)\n        for i in range(n_actions):\n            if selection_counts[i] > 0:\n                exploration_value = np.sqrt(np.log(total_selection_count) / (selection_counts[i] + 1))\n            else:\n                exploration_value = float('inf')\n            ucb_values[i] = scores[i] + exploration_value\n        \n        # Softmax probabilities for refined selection\n        exp_ucb = np.exp(ucb_values - np.max(ucb_values))  # For numerical stability\n        probabilities = exp_ucb / np.sum(exp_ucb)\n        action_index = np.random.choice(action_indices, p=probabilities)\n\n    return action_index",
          "objective": -320.97511163044226,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    avg_scores = []\n    action_counts = []\n    \n    # Calculate the average scores and counts for each action\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_count = len(scores)\n        avg_score = np.mean(scores) if action_count > 0 else 0\n        action_counts.append(action_count)\n\n        avg_scores.append(avg_score)\n\n    # Dynamic epsilon for exploration\n    epsilon = max(0.1, 1 - (total_selection_count / (total_time_slots * 2)))\n\n    # Action selection using epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        # Exploration - select a random action\n        action_index = np.random.choice(range(8))\n    else:\n        # Exploitation - calculate UCB scores\n        ucb_scores = []\n        for idx in range(8):\n            if action_counts[idx] > 0:\n                ucb = avg_scores[idx] + np.sqrt(2 * np.log(total_selection_count) / action_counts[idx])\n            else:\n                ucb = float('inf')  # Encourage exploration of unselected actions\n            ucb_scores.append(ucb)\n        \n        # Select the action with the highest UCB score\n        action_index = np.argmax(ucb_scores)\n\n    return action_index",
          "objective": -320.9700476649091,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    k = len(score_set)  # Number of actions\n    epsilon_base = 1.0  # Start with full exploration\n    min_epsilon = 0.01  # Minimum exploration probability\n    alpha = 0.5  # Tunable weight for exploration in UCB\n    exploration_factor = 1.0  # Factor to adjust exploration in UCB\n\n    average_scores = np.zeros(k)\n    selection_counts = np.zeros(k)\n\n    for idx in score_set.keys():\n        scores = score_set[idx]\n        selection_counts[idx] = len(scores)\n        if selection_counts[idx] > 0:\n            average_scores[idx] = np.mean(scores)\n\n    # Calculate epsilon dynamically\n    epsilon = max(min_epsilon, epsilon_base * (0.99 ** total_selection_count))\n\n    if np.random.rand() > epsilon:\n        # UCB Strategy\n        ucb_values = average_scores + exploration_factor * np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-6))\n        action_index = np.argmax(ucb_values)\n    else:\n        # Explore: select an action at random\n        action_index = np.random.choice(k)\n\n    return action_index",
          "objective": -320.94301479305364,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    scores = []\n    epsilon = max(0.1, 1 - (total_selection_count / (total_time_slots * 100)))\n    \n    for action in action_indices:\n        historical_scores = score_set[action]\n        selection_count = len(historical_scores)\n        \n        # Calculate average score and handle cases with zero selections\n        average_score = np.mean(historical_scores) if selection_count > 0 else 0\n        \n        if selection_count > 0:\n            exploration_value = np.sqrt(np.log(total_selection_count) / selection_count)\n        else:\n            exploration_value = float('inf')  # Prioritize unexplored actions\n            \n        total_score = average_score + exploration_value\n        \n        # Incorporating epsilon-greedy strategy\n        if np.random.rand() < epsilon:\n            total_score *= np.random.rand()  # Randomize score significantly for exploration\n            \n        scores.append(total_score)\n    \n    action_index = np.argmax(scores)  # Select the action with the highest score\n\n    return action_index",
          "objective": -320.8538962744109,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    k = len(score_set)  # Number of actions (should be 8)\n    \n    # Initialize average scores and selection counts\n    average_scores = np.zeros(k)\n    selection_counts = np.zeros(k, dtype=int)\n    \n    for idx in score_set.keys():\n        scores = score_set[idx]\n        selection_counts[idx] = len(scores)\n        if selection_counts[idx] > 0:\n            average_scores[idx] = np.mean(scores)\n\n    # Calculate exploration parameter: epsilon starts high and decays\n    epsilon_base = 1.0  # Start with high exploration\n    min_epsilon = 0.1   # Minimum exploration probability\n    epsilon_decay = 0.99 # Decay factor for exploration parameter\n    epsilon = max(min_epsilon, epsilon_base * (epsilon_decay ** (total_selection_count)))\n    \n    # Exploration or exploitation decision\n    if np.random.rand() > epsilon:\n        # Exploitation: Softmax selection based on average scores\n        exp_scores = np.exp(average_scores - np.max(average_scores))  # Stability for large values\n        probabilities = exp_scores / np.sum(exp_scores)\n        action_index = np.random.choice(k, p=probabilities)\n    else:\n        # Exploration: Uniform selection of an action\n        action_index = np.random.randint(k)\n\n    return action_index",
          "objective": -304.0011097302152,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    average_scores = []\n    selection_counts = []\n    \n    for action in action_indices:\n        historical_scores = score_set[action]\n        average_score = np.mean(historical_scores) if historical_scores else 0\n        average_scores.append(average_score)\n        selection_counts.append(len(historical_scores))\n    \n    # Calculate exploration probabilities\n    exploration_prob = max(0.1 * (1 - total_selection_count / (total_time_slots * 0.5)), 0.01)\n    \n    # Softmax scores\n    exponentiated_scores = np.exp(average_scores - np.max(average_scores))\n    softmax_probabilities = exponentiated_scores / np.sum(exponentiated_scores)\n\n    # Combine exploration and exploitation\n    combined_probabilities = (1 - exploration_prob) * softmax_probabilities + exploration_prob / len(action_indices)\n\n    # Select action based on combined probabilities\n    action_index = np.random.choice(action_indices, p=combined_probabilities)\n\n    return action_index",
          "objective": -282.31478930559416,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    k = len(score_set)  # Number of actions\n    epsilon = max(0.1, 1 - (total_selection_count / total_time_slots))  # Decreasing epsilon\n    average_scores = np.zeros(k)\n    selection_counts = np.zeros(k)\n\n    for idx in range(k):\n        scores = score_set.get(idx, [])\n        selection_counts[idx] = len(scores)\n        if selection_counts[idx] > 0:\n            average_scores[idx] = np.mean(scores)\n\n    unexplored_actions = np.where(selection_counts == 0)[0]\n\n    if np.random.rand() < epsilon:\n        if len(unexplored_actions) > 0:\n            action_index = np.random.choice(unexplored_actions)\n        else:\n            action_index = np.random.randint(k)\n    else:\n        # Upper Confidence Bound (UCB) strategy\n        total_selections = total_selection_count + 1  # Avoid division by zero\n        ucb_values = average_scores + np.sqrt((2 * np.log(total_selections)) / (selection_counts + 1e-5))\n        action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -277.7811192315676,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = max(0.1, 1.0 - (current_time_slot / total_time_slots))  # Epsilon decreases over time\n    action_indices = list(score_set.keys())\n    average_scores = []\n    \n    for action in action_indices:\n        historical_scores = score_set[action]\n        average_score = np.mean(historical_scores) if historical_scores else 0\n        average_scores.append(average_score)\n    \n    # Epsilon-greedy selection process\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        # Softmax selection based on average scores\n        exp_scores = np.exp(average_scores - np.max(average_scores))  # For numerical stability\n        probabilities = exp_scores / np.sum(exp_scores)\n        action_index = np.random.choice(action_indices, p=probabilities)\n\n    return action_index",
          "objective": -259.3790666136486,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    k = len(score_set)  # Number of actions\n    max_exploration = 1.0  # Maximum exploration probability\n    min_exploration = 0.01  # Minimum exploration probability\n    exploration_decay = max_exploration * (1 - (total_selection_count / total_time_slots))  # Decaying exploration\n    \n    # Calculate average scores and selection counts\n    average_scores = np.zeros(k)\n    selection_counts = np.zeros(k)\n    \n    for idx in score_set.keys():\n        scores = score_set[idx]\n        selection_counts[idx] = len(scores)\n        if selection_counts[idx] > 0:\n            average_scores[idx] = np.mean(scores)\n    \n    # Handle actions with zero selections\n    unexplored_actions = np.where(selection_counts == 0)[0]\n    \n    # Dynamic exploration probability\n    exploration_probability = max(min_exploration, exploration_decay)\n    \n    # Exploration or exploitation decision\n    if np.random.rand() > exploration_probability:\n        # Exploitation: Select action with the highest known average score\n        action_index = np.argmax(average_scores)\n    else:\n        # Exploration: Randomly select an unexplored action or any action\n        if len(unexplored_actions) > 0:\n            action_index = np.random.choice(unexplored_actions)\n        else:\n            # If all actions have been explored, sample probabilities based on average scores\n            selection_probabilities = average_scores / np.sum(average_scores)\n            action_index = np.random.choice(k, p=selection_probabilities)\n\n    return action_index",
          "objective": -250.65651221901624,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    avg_scores = np.zeros(action_count)\n    exploration_bonus = 1.0  # Exploration factor\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_selection_count = len(scores)\n\n        # Calculate average score for the action\n        if action_selection_count > 0:\n            avg_scores[action_index] = np.mean(scores)\n        else:\n            avg_scores[action_index] = 0\n\n        # UCB term to encourage exploration\n        if total_selection_count > 0:\n            exploration_term = exploration_bonus * np.sqrt(np.log(total_selection_count + 1) / (action_selection_count + 1))\n        else:\n            exploration_term = exploration_bonus\n        \n        avg_scores[action_index] += exploration_term\n\n    action_index = np.argmax(avg_scores)\n    return action_index",
          "objective": -248.47427247519556,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    min_exploration_threshold = 10  # Minimum selections before transitioning to exploitation\n    exploration_factor = 0.1  # Initial exploration probability\n    decay_rate = 0.99  # Decay rate for exploration over time\n    \n    action_indices = list(score_set.keys())\n    scores = []\n    selection_counts = []\n\n    for action in action_indices:\n        historical_scores = score_set[action]\n        average_score = np.mean(historical_scores) if historical_scores else 0.0\n        selection_count = len(historical_scores)\n        selection_counts.append(selection_count)\n        \n        # Softmax calculation for the exploration factor\n        exploration_value = (\n            np.sqrt(total_selection_count + 1) / (1 + selection_count) if selection_count > 0 else float('inf')\n        )\n        \n        # Store scores for softmax calculation\n        scores.append(average_score)\n\n    # Update exploration factor based on total selections\n    exploration_probability = exploration_factor * (decay_rate ** (total_selection_count // min_exploration_threshold))\n    \n    if np.random.rand() < exploration_probability or total_selection_count < min_exploration_threshold:\n        # Explore: Select action based on uniform probability\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploit: Select action based on softmax probabilities\n        exp_scores = np.exp(scores - np.max(scores))  # for numerical stability\n        probabilities = exp_scores / np.sum(exp_scores)\n        action_index = np.random.choice(action_indices, p=probabilities)\n\n    return action_index",
          "objective": -246.77107142212006,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    k = len(score_set)  # Number of actions\n    exploration_rate = max(0.01, 1.0 - (total_selection_count / total_time_slots))  # Decaying exploration rate\n\n    # Calculate average scores and selection counts\n    average_scores = np.zeros(k)\n    selection_counts = np.zeros(k)\n\n    for idx in score_set.keys():\n        scores = score_set[idx]\n        selection_counts[idx] = len(scores)\n        if selection_counts[idx] > 0:\n            average_scores[idx] = np.mean(scores)\n    \n    # Handling actions with zero selections\n    unexplored_actions = np.where(selection_counts == 0)[0]\n\n    # Determine action selection\n    if np.random.rand() < exploration_rate and len(unexplored_actions) > 0:\n        action_index = np.random.choice(unexplored_actions)\n    else:\n        # If all actions are explored, use softmax for a weighted selection\n        exp_scores = np.exp(average_scores - np.max(average_scores))  # Stability in softmax\n        selection_probabilities = exp_scores / np.sum(exp_scores)\n        action_index = np.random.choice(k, p=selection_probabilities)\n\n    return action_index",
          "objective": -243.36298695578245,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    avg_scores = []\n    epsilon = 0.1  # Exploration probability\n    decay_factor = 1 - (current_time_slot / total_time_slots)  # Decay factor\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_selection_count = len(scores)\n\n        # Calculate average score\n        avg_score = np.mean(scores) if action_selection_count > 0 else 0\n\n        # Epsilon-greedy strategy: explore with a probability of epsilon\n        if np.random.rand() < epsilon:\n            final_score = -1  # Set a low score to prefer exploration\n        else:\n            final_score = avg_score * decay_factor\n\n        # Adjust final score with a penalty for actions with low selections\n        if action_selection_count > 0:\n            final_score += (1 / (1 + action_selection_count)) * (1 - decay_factor)\n\n        avg_scores.append(final_score)\n\n    action_index = np.argmax(avg_scores)\n    return action_index",
          "objective": -238.7217102454266,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    k = len(score_set)  # Number of actions\n    exploration_factor = 0.1  # Base exploration rate\n\n    # Calculate average scores and selection counts\n    average_scores = np.zeros(k)\n    selection_counts = np.zeros(k)\n\n    for idx in range(k):\n        scores = score_set.get(idx, [])\n        selection_counts[idx] = len(scores)\n        if selection_counts[idx] > 0:\n            average_scores[idx] = np.mean(scores)\n\n    # Dynamic exploration vs exploitation scaling\n    exploration_scale = (total_time_slots - current_time_slot) / total_time_slots\n    exploration_probability = exploration_factor * exploration_scale\n\n    if total_selection_count > 0 and np.random.rand() > exploration_probability:\n        # Exploitation: Select action with the highest average score and exploration bonus\n        exploration_bonus = np.sqrt(np.log(total_selection_count) / (selection_counts + 1e-5))\n        action_scores = average_scores + exploration_bonus\n        action_index = np.argmax(action_scores)\n    else:\n        # Exploration: Select among less chosen actions or random\n        unselected_actions = np.where(selection_counts == 0)[0]\n        if len(unselected_actions) > 0:\n            action_index = np.random.choice(unselected_actions)\n        else:\n            action_index = np.random.choice(k)\n\n    return action_index",
          "objective": -234.59132873383973,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    avg_scores = []\n    num_actions = 8\n    exploration_factor = 0.7  # Explore more initially\n    exploitation_factor = 0.3  # Emphasize exploitation later\n    alpha = 1.0  # Weighting for exploration\n\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_selection_count = len(scores)\n\n        # Calculate average score\n        avg_score = np.mean(scores) if action_selection_count > 0 else 0\n        \n        # Dynamic exploration term\n        exploration_term = (exploration_factor / (1 + action_selection_count)) if total_selection_count > 0 else exploration_factor\n        \n        # Combine exploration and exploitation\n        final_score = alpha * avg_score + exploration_term * (1 - alpha)\n        avg_scores.append(final_score)\n\n    # Softmax function to normalize scores for selection probabilities\n    exp_scores = np.exp(avg_scores - np.max(avg_scores))  # Avoid overflow\n    selection_probabilities = exp_scores / np.sum(exp_scores)\n\n    # Select action based on probabilities\n    action_index = np.random.choice(range(num_actions), p=selection_probabilities)\n    return action_index",
          "objective": -222.04842627946533,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    k = len(score_set)  # Number of actions\n    exploration_factor = 0.1  # Exploration parameter\n    \n    # Calculate average scores and selection counts\n    average_scores = np.zeros(k)\n    selection_counts = np.zeros(k)\n    \n    for idx in score_set.keys():\n        scores = score_set[idx]\n        selection_counts[idx] = len(scores)\n        if selection_counts[idx] > 0:\n            average_scores[idx] = np.mean(scores)\n    \n    # Dynamic exploration vs exploitation scaling\n    exploration_scale = (total_time_slots - current_time_slot) / total_time_slots\n    exploration_probability = exploration_factor * exploration_scale\n\n    if total_selection_count > 0 and np.random.rand() > exploration_probability:\n        # Exploitation: Select action with the highest average score\n        action_index = np.argmax(average_scores)\n    else:\n        # Exploration: Select among less chosen actions or random\n        unselected_actions = np.where(selection_counts == 0)[0]\n        if len(unselected_actions) > 0:\n            action_index = np.random.choice(unselected_actions)\n        else:\n            action_index = np.random.choice(k)\n    \n    return action_index",
          "objective": -220.285989460143,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration factor\n    action_indices = list(score_set.keys())\n    scores = []\n\n    for action in action_indices:\n        historical_scores = score_set[action]\n        if len(historical_scores) > 0:\n            average_score = np.mean(historical_scores)\n        else:\n            average_score = 0  # No score if no history\n        \n        selection_count = len(historical_scores)\n        exploration_value = (np.sqrt(total_selection_count) / (1 + selection_count)) if selection_count > 0 else float('inf')\n        \n        scores.append(average_score + epsilon * exploration_value)\n\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = np.argmax(scores)\n    \n    return action_index",
          "objective": -220.11469312976715,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    k = len(score_set)  # Number of actions\n    min_epsilon = 0.01  # Minimum epsilon value\n    initial_epsilon = 1.0  # Fully explore in the beginning\n    final_epsilon = min_epsilon\n    exploration_weight = 0.1  # Adjust weight for exploration vs. exploitation\n    \n    # Average scores and selection counts\n    average_scores = np.zeros(k)\n    selection_counts = np.zeros(k)\n    \n    for idx, scores in score_set.items():\n        selection_counts[idx] = len(scores)\n        if selection_counts[idx] > 0:\n            average_scores[idx] = np.mean(scores)\n\n    # Dynamic epsilon based on total selections\n    epsilon = max(final_epsilon, initial_epsilon * (1 - (total_selection_count / total_time_slots)))\n\n    # Exploration or exploitation decision\n    if np.random.rand() > epsilon:\n        # Exploitation\n        action_index = np.argmax(average_scores)\n    else:\n        # Exploration: Softmax probability distribution to select an action\n        exploration_scores = average_scores + exploration_weight * np.random.rand(k)\n        probabilities = np.exp(exploration_scores) / np.sum(np.exp(exploration_scores))\n        action_index = np.random.choice(k, p=probabilities)\n    \n    return action_index",
          "objective": -218.3055595754889,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    k = len(score_set)  # Number of actions\n    epsilon_base = 0.1  # Base exploration factor\n    epsilon_decay = 0.95  # Decay factor for exploration parameter\n    min_epsilon = 0.05  # Minimum exploration probability\n    \n    average_scores = np.zeros(k)\n    selection_counts = np.zeros(k, dtype=int)\n    \n    for idx in range(k):\n        scores = score_set.get(idx, [])\n        selection_counts[idx] = len(scores)\n        if selection_counts[idx] > 0:\n            average_scores[idx] = np.mean(scores)\n\n    # Calculate dynamic epsilon\n    epsilon = max(min_epsilon, epsilon_base * (epsilon_decay ** total_selection_count))\n\n    # Decision: exploration or exploitation\n    if np.random.rand() > epsilon:\n        # Exploitation: Select action with the highest average score\n        action_index = np.argmax(average_scores)\n    else:\n        # Exploration: Softmax selection among actions\n        exp_scores = np.exp(average_scores - np.max(average_scores))\n        probabilities = exp_scores / np.sum(exp_scores)\n        action_index = np.random.choice(k, p=probabilities)\n    \n    return action_index",
          "objective": -214.56881929528382,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    total_actions = 8\n    action_scores = np.zeros(total_actions)\n    \n    for action_index in range(total_actions):\n        historical_scores = score_set.get(action_index, [])\n        selection_count = len(historical_scores)\n        average_score = np.mean(historical_scores) if selection_count > 0 else 0\n        \n        # Use a confidence interval to encourage exploration\n        confidence = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1e-5))\n        action_scores[action_index] = average_score + confidence\n    \n    # Dynamic exploration rate decreasing over time\n    epsilon = max(0.1 * (1 - (current_time_slot / total_time_slots)), 0.01)\n\n    # Epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(range(total_actions))\n    else:\n        action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -212.5386006030859,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Define parameters for exploration and exploitation\n    exploration_fraction = 0.1  # Exploration probability\n    epsilon = exploration_fraction * (1 - (current_time_slot / total_time_slots)) + (1 - exploration_fraction) * (current_time_slot / total_time_slots)\n    \n    action_scores = []\n    total_actions = 8\n    \n    for action_index in range(total_actions):\n        historical_scores = score_set.get(action_index, [])\n        average_score = np.mean(historical_scores) if historical_scores else 0\n        selection_count = len(historical_scores)\n        \n        # If selection_count is zero, add a small constant to encourage selection\n        adjusted_score = average_score + (1 / (selection_count + 1e-5))\n        action_scores.append(adjusted_score)\n\n    # Normalize scores to probabilities\n    score_probabilities = np.array(action_scores) / np.sum(action_scores)\n\n    # Select an action based on epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(range(total_actions))\n    else:\n        action_index = np.argmax(score_probabilities)\n    \n    return action_index",
          "objective": -211.93664299662848,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    avg_scores = np.zeros(action_count)\n    epsilon = 1.0 / (1 + total_selection_count)  # Decaying exploration factor\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        action_selection_count = len(scores)\n\n        # Calculate average score for the action\n        avg_scores[action_index] = np.mean(scores) if action_selection_count > 0 else 0\n\n    # Determine action probabilities based on average scores and the exploration factor\n    action_probabilities = (1 - epsilon) * (avg_scores / np.sum(avg_scores)) + (epsilon / action_count)\n\n    # Select action based on calculated probabilities\n    action_index = np.random.choice(range(action_count), p=action_probabilities)\n    return action_index",
          "objective": -210.19470973538142,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    k = len(score_set)  # Number of actions\n    average_scores = np.zeros(k)\n    selection_counts = np.zeros(k)\n\n    for idx in score_set.keys():\n        scores = score_set[idx]\n        selection_counts[idx] = len(scores)\n        if selection_counts[idx] > 0:\n            average_scores[idx] = np.mean(scores)\n\n    # Epsilon-greedy strategy parameters\n    epsilon_initial = 1.0  # Initial exploration probability\n    epsilon_final = 0.1    # Final exploration probability\n    decay_rate = 0.05      # Epsilon decay rate per selection\n    \n    # Calculate current epsilon based on selections\n    epsilon = max(epsilon_final, epsilon_initial * np.exp(-decay_rate * total_selection_count))\n\n    if total_selection_count == 0 or np.random.rand() < epsilon:\n        # Exploration: Pick randomly\n        action_index = np.random.choice(k)\n    else:\n        # Exploitation: Select action with the highest average score\n        action_index = np.argmax(average_scores)\n\n    return action_index",
          "objective": -209.941323307752,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    scores = []\n    epsilon = 0.1  # Exploration factor (10% of the time)\n\n    for action in action_indices:\n        historical_scores = score_set[action]\n        average_score = np.mean(historical_scores) if historical_scores else 0\n        selection_count = len(historical_scores)\n\n        # Softmax exploration mechanism\n        if total_selection_count > 0:\n            scores.append(average_score)\n        else:\n            scores.append(float('inf'))  # Ensure under-explored actions are favored\n            \n    # Convert scores to probabilities with softmax\n    exp_scores = np.exp(scores - np.max(scores))  # For numerical stability\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        action_index = np.random.choice(action_indices, p=probabilities)  # Exploit based on probabilities\n\n    return action_index",
          "objective": -209.78161628425457,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        selection_counts[action_index] = selection_count\n        \n        if selection_count > 0:\n            avg_scores[action_index] = np.mean(scores)\n\n    # Parameters for the selection strategy\n    exploration_prob = 0.1  # Epsilon for exploration strategy\n    decay_factor = (1 - current_time_slot / total_time_slots)\n    \n    # Compute adjusted scores\n    adjusted_scores = avg_scores * decay_factor\n    \n    # Softmax adjustment for scores to convert to probabilities\n    max_score = np.max(adjusted_scores) if num_actions > 0 else 0\n    exp_scores = np.exp(adjusted_scores - max_score)\n    probabilities = exp_scores / np.sum(exp_scores) if np.sum(exp_scores) > 0 else np.zeros(num_actions)\n    \n    # Introduce exploration with epsilon-greedy\n    if total_selection_count > 0 and np.random.rand() < exploration_prob:\n        action_index = np.random.randint(0, num_actions)\n    else:\n        action_index = np.random.choice(range(num_actions), p=probabilities)\n\n    return action_index",
          "objective": -207.9855603547861,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    min_selection = 2  # Minimum number of selections before favoring average scores\n    exploration_factor = 1.0  # Factor for exploration influence\n    temperature = 0.5  # Softmax temperature for randomness\n\n    action_indices = list(score_set.keys())\n    scores = []\n    probabilities = []\n    \n    for action in action_indices:\n        historical_scores = score_set[action]\n        selection_count = len(historical_scores)\n        average_score = np.mean(historical_scores) if selection_count > 0 else 0\n        \n        # Favor exploration for actions that have been selected fewer than min_selection times\n        if selection_count < min_selection:\n            scores.append(float('inf'))  # Infinite score encourages selection\n        else:\n            scores.append(average_score)\n\n    # Apply softmax for probability distribution\n    exp_scores = np.exp(np.array(scores) / temperature)\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    action_index = np.random.choice(action_indices, p=probabilities)\n    \n    return action_index",
          "objective": -204.67011974853676,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize variables\n    avg_scores = np.zeros(8)\n    action_counts = np.zeros(8)\n    \n    # Calculate average scores and action selection counts\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        if action_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n    \n    # Calculate exploration term with a lower exploration factor over time\n    exploration_bonus = 0.5 * (1 - (current_time_slot / total_time_slots))\n    \n    # Avoid division by zero using max function\n    with np.errstate(divide='ignore', invalid='ignore'):\n        exploration_terms = exploration_bonus / (1 + action_counts)\n    \n    # Combine exploitation and exploration\n    adjusted_scores = avg_scores + exploration_terms\n    \n    # Select the action with the highest score\n    action_index = np.argmax(adjusted_scores)\n    return action_index",
          "objective": -204.3953688464949,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon_start = 1.0  # Initial epsilon for exploration\n    epsilon_end = 0.1    # Final epsilon for exploitation\n    exploration_decay = total_time_slots / 10  # Decay factor for epsilon\n    \n    # Calculate the current epsilon value based on the time slot\n    epsilon = max(epsilon_end, epsilon_start - (epsilon_start - epsilon_end) * (current_time_slot / exploration_decay))\n    \n    action_indices = list(score_set.keys())\n    scores = []\n    \n    for action in action_indices:\n        historical_scores = score_set[action]\n        average_score = np.mean(historical_scores) if historical_scores else 0\n        selection_count = len(historical_scores)\n        \n        # Compute exploration factor using logarithmic scale for better handling of less frequent actions\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (1 + selection_count)) if selection_count > 0 else float('inf')\n        \n        # Total score combining average score with exploration value\n        scores.append(average_score + epsilon * exploration_value)\n\n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = np.argmax(scores)\n    \n    return action_index",
          "objective": -203.99581897367082,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    avg_scores = []\n    epsilon_initial = 0.5  # Initial exploration rate\n    epsilon_decay = 0.99    # Decay factor for exploration rate\n    min_epsilon = 0.05      # Minimum exploration rate\n\n    # Calculate exploration rate based on total selections\n    epsilon = max(min_epsilon, epsilon_initial * (epsilon_decay ** current_time_slot))\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_selection_count = len(scores)\n\n        # Calculate average score\n        avg_score = np.mean(scores) if action_selection_count > 0 else 0\n        \n        # Append average score to the list\n        avg_scores.append(avg_score)\n\n    # Apply epsilon-greedy strategy to balance exploration and exploitation\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, 8)  # Explore: random action\n    else:\n        action_index = np.argmax(avg_scores)  # Exploit: best average score action\n\n    return action_index",
          "objective": -203.3483875771018,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # exploration parameter\n    action_indices = list(score_set.keys())\n    \n    # Calculate average scores for each action\n    average_scores = np.zeros(8)\n    for idx in action_indices:\n        scores = score_set[idx]\n        if len(scores) > 0:\n            average_scores[idx] = np.mean(scores)\n    \n    if total_selection_count > 0 and np.random.rand() > epsilon:\n        # Exploitation: Select action with the highest average score\n        action_index = np.argmax(average_scores)\n    else:\n        # Exploration: Randomly select an action\n        action_index = np.random.choice(action_indices)\n    \n    return action_index",
          "objective": -202.27115573883137,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    avg_scores = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate average scores and counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        if action_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n\n    # Epsilon-greedy strategy\n    epsilon = max(0.1, 1 - (total_selection_count / (total_time_slots * 5)))\n    \n    if np.random.rand() < epsilon:\n        # Exploration: Select a random action\n        action_index = np.random.randint(0, num_actions)\n    else:\n        # Exploitation: Select the action with the highest average score\n        action_index = np.argmax(avg_scores)\n\n    return action_index",
          "objective": -201.53790576505284,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    k = len(score_set)  # Number of actions\n    epsilon_base = 0.1  # Base exploration factor\n    min_epsilon = 0.01  # Minimum exploration probability\n    \n    average_scores = np.zeros(k)\n    selection_counts = np.zeros(k)\n\n    for idx in score_set.keys():\n        scores = score_set[idx]\n        selection_counts[idx] = len(scores)\n        if selection_counts[idx] > 0:\n            average_scores[idx] = np.mean(scores)\n\n    # Dynamic epsilon based on exploration and decay\n    epsilon = max(min_epsilon, epsilon_base * (1 - current_time_slot / total_time_slots))\n    \n    # Exploration or exploitation decision\n    if np.random.rand() > epsilon:\n        # Exploitation: Softmax based action selection\n        softmax_scores = np.exp(average_scores - np.max(average_scores))  # Stability in computation\n        action_probabilities = softmax_scores / np.sum(softmax_scores)\n        action_index = np.random.choice(k, p=action_probabilities)\n    else:\n        # Exploration: Give preference to less selected actions\n        unselected_actions = np.where(selection_counts == 0)[0]\n        if len(unselected_actions) > 0:\n            action_index = np.random.choice(unselected_actions)\n        else:\n            action_index = np.random.choice(k)\n\n    return action_index",
          "objective": -197.26309612975206,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    total_actions = 8\n\n    action_indices = np.arange(total_actions)\n    action_scores = np.zeros(total_actions)\n    selection_counts = np.zeros(total_actions)\n\n    for action_index in action_indices:\n        historical_scores = score_set.get(action_index, [])\n        average_score = np.mean(historical_scores) if historical_scores else 0\n        selection_count = len(historical_scores)\n        \n        action_scores[action_index] = average_score\n        selection_counts[action_index] = selection_count\n\n    # Calculate Upper Confidence Bound (UCB) values\n    ucb_values = action_scores + np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n\n    # Select action based on UCB\n    action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -196.209162572151,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    average_scores = []\n    \n    for action in action_indices:\n        scores = score_set[action]\n        if len(scores) > 0:\n            average_score = np.mean(scores)\n        else:\n            average_score = 0  # Default to 0 if no scores\n        average_scores.append(average_score)\n\n    exploration_factor = np.sqrt(np.log(total_selection_count + 1) / (np.array([len(score_set[action]) for action in action_indices]) + 1))\n    adjusted_scores = average_scores + exploration_factor\n\n    action_index = np.argmax(adjusted_scores)\n    return action_index",
          "objective": -196.19191051034892,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    total_actions = 8\n    action_scores = np.zeros(total_actions)\n    selection_counts = np.zeros(total_actions)\n    \n    # Calculating average scores and selection counts\n    for action_index in range(total_actions):\n        historical_scores = score_set.get(action_index, [])\n        action_scores[action_index] = np.mean(historical_scores) if historical_scores else 0\n        selection_counts[action_index] = len(historical_scores)\n    \n    # Calculate the UCB (Upper Confidence Bound)\n    ucb_values = action_scores + np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_counts + 1e-5))\n    \n    # Normalize to probabilities for Softmax to incorporate exploration\n    softmax_probs = np.exp(ucb_values - np.max(ucb_values))  # Stabilizing softmax\n    softmax_probs /= np.sum(softmax_probs)\n    \n    # Epsilon for exploration\n    epsilon = 0.1 * (1 - (current_time_slot / total_time_slots)) + 0.9 * (current_time_slot / total_time_slots)\n    \n    action_index = np.random.choice(total_actions) if np.random.rand() < epsilon else np.argmax(softmax_probs)\n    \n    return action_index",
          "objective": -193.49687364680815,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Decay epsilon over time for exploration-exploitation balance\n    initial_epsilon = 1.0\n    final_epsilon = 0.1\n    decay_rate = (initial_epsilon - final_epsilon) / total_time_slots\n    epsilon = max(initial_epsilon - decay_rate * current_time_slot, final_epsilon)\n    \n    action_indices = list(score_set.keys())\n    \n    # Calculate average scores for each action\n    average_scores = np.zeros(8)\n    selections_count = np.zeros(8)\n    \n    for idx in action_indices:\n        scores = score_set[idx]\n        selections_count[idx] = len(scores)\n        if selections_count[idx] > 0:\n            average_scores[idx] = np.mean(scores)\n\n    if total_selection_count > 0 and np.random.rand() > epsilon:\n        # Exploitation: Softmax over average scores\n        exp_scores = np.exp(average_scores - np.max(average_scores))  # for numerical stability\n        probabilities = exp_scores / np.sum(exp_scores)\n        action_index = np.random.choice(action_indices, p=probabilities)\n    else:\n        # Exploration: Select action with fewer selections with some randomness\n        unexplored_actions = [idx for idx in action_indices if selections_count[idx] == 0]\n        if unexplored_actions and np.random.rand() < 0.5:\n            action_index = np.random.choice(unexplored_actions)\n        else:\n            action_index = np.random.choice(action_indices)\n\n    return action_index",
          "objective": -193.28622815408357,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    avg_scores = []\n    exploration_factor = 0.5  # Increased exploration factor for better exploration\n    decay_factor = 1 - (current_time_slot / total_time_slots)  # Decay based on current time\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_selection_count = len(scores)\n\n        # Calculate average score\n        avg_score = np.mean(scores) if action_selection_count > 0 else 0\n        \n        # Dynamic exploration term\n        exploration_term = exploration_factor * ((1 / (1 + action_selection_count)) if total_selection_count > 0 else 1)\n\n        # Combine exploration and exploitation\n        final_score = avg_score * decay_factor + exploration_term\n        avg_scores.append(final_score)\n\n    # Softmax function to normalize scores for selection probabilities\n    exp_scores = np.exp(avg_scores - np.max(avg_scores))  # Avoid overflow\n    selection_probabilities = exp_scores / np.sum(exp_scores)\n\n    # Select action based on probabilities\n    action_index = np.random.choice(range(8), p=selection_probabilities)\n    return action_index",
          "objective": -192.72597836180188,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    avg_scores = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        if action_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n\n    # Avoid division by zero\n    with np.errstate(divide='ignore', invalid='ignore'):\n        action_selection_frequencies = action_counts / total_selection_count if total_selection_count > 0 else np.zeros(num_actions)\n\n    # Epsilon-greedy exploration factor\n    epsilon = 1.0 / (current_time_slot + 1)\n    \n    # Calculate adjusted scores for exploitation\n    adjusted_scores = avg_scores * (1 - epsilon) + (1 - avg_scores) * epsilon / (1 + action_selection_frequencies)\n\n    # Select the action with the highest score\n    action_index = np.argmax(adjusted_scores)\n    \n    return action_index",
          "objective": -192.3866122066653,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    total_actions = 8\n    exploration_fraction = 0.1\n    epsilon = exploration_fraction * (1 - (current_time_slot / total_time_slots)) + (1 - exploration_fraction) * (current_time_slot / total_time_slots)\n    \n    action_scores = np.zeros(total_actions)\n    selection_counts = np.zeros(total_actions)\n    \n    for action_index in range(total_actions):\n        historical_scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(historical_scores)\n        action_scores[action_index] = np.mean(historical_scores) if historical_scores else 0\n    \n    # Adjust scores based on selection count to provide a bonus for underexplored actions\n    adjusted_scores = action_scores + (1 / (selection_counts + 1e-5))\n    \n    # Normalize scores to probabilities\n    score_probabilities = adjusted_scores / np.sum(adjusted_scores)\n\n    # Epsilon-greedy action selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(range(total_actions))\n    else:\n        action_index = np.argmax(score_probabilities)\n\n    return action_index",
          "objective": -192.31588264949917,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    avg_scores = []\n    exploration_bonus = 0.1  # Exploration factor\n    decay_factor = 1 - (current_time_slot / total_time_slots)  # Decay factor based on time\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        if len(scores) > 0:\n            avg_score = np.mean(scores)\n        else:\n            avg_score = 0\n        \n        # Encourage exploration with a bonus based on how rarely this action has been chosen\n        if total_selection_count > 0:\n            action_selection_count = len(scores)\n            exploration_term = exploration_bonus * (1 / (1 + action_selection_count))\n        else:\n            exploration_term = exploration_bonus\n        \n        avg_scores.append(avg_score * decay_factor + exploration_term)\n    \n    action_index = np.argmax(avg_scores)\n    return action_index",
          "objective": -192.03458231329176,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    scores = []\n    \n    epsilon = max(0.1, 1 - (total_selection_count / (total_time_slots * 2)))  # Epsilon decays to a minimum of 0.1\n    rand_value = np.random.rand()  # Random value for epsilon-greedy approach\n\n    for action in action_indices:\n        historical_scores = score_set[action]\n        selection_count = len(historical_scores)\n\n        # Calculate average score\n        average_score = np.mean(historical_scores) if historical_scores else 0\n        \n        # Safe exploration value using softmax\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else float('inf')\n        \n        adjusted_score = average_score + exploration_value\n\n        scores.append(adjusted_score)\n\n    if rand_value < epsilon:  # Epsilon-greedy exploration\n        action_index = np.random.choice(action_indices)\n    else:  # Exploitation phase (softmax selection)\n        exp_scores = np.exp(scores - np.max(scores))  # For numerical stability\n        selection_probs = exp_scores / np.sum(exp_scores)\n        action_index = np.random.choice(action_indices, p=selection_probs)\n    \n    return action_index",
          "objective": -190.72375049362353,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    k = len(score_set)  # Number of actions\n    exploration_factor = 0.1  # Base exploration rate\n\n    # Calculate average scores and selection counts\n    average_scores = np.zeros(k)\n    selection_counts = np.zeros(k)\n\n    for idx in range(k):\n        scores = score_set.get(idx, [])\n        selection_counts[idx] = len(scores)\n        if selection_counts[idx] > 0:\n            average_scores[idx] = np.mean(scores)\n\n    # Dynamic exploration vs exploitation scaling\n    exploration_scale = (total_time_slots - current_time_slot) / total_time_slots\n    exploration_probability = exploration_factor * exploration_scale\n\n    # If exploration is favored\n    if total_selection_count > 0 and np.random.rand() > exploration_probability:\n        # Exploitation: Select action with the highest average score and UCB\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n        action_scores = average_scores + exploration_bonus\n        action_index = np.argmax(action_scores)\n    else:\n        # Exploration: Select action randomly or prioritize less selected actions\n        unselected_actions = np.where(selection_counts == 0)[0]\n        if len(unselected_actions) > 0:\n            action_index = np.random.choice(unselected_actions)\n        else:\n            action_index = np.random.choice(k)\n\n    return action_index",
          "objective": -190.35273458114628,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    avg_scores = np.zeros(8)\n    exploration_bonus = 0.2  # Exploration factor\n    exploration_strength = 2.0  # Strength of exploration term\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_selection_count = len(scores)\n        \n        if action_selection_count > 0:\n            avg_scores[action_index] = np.mean(scores)\n        else:\n            avg_scores[action_index] = 0\n            \n        # Calculate exploration term\n        if total_selection_count > 0:\n            exploration_term = exploration_bonus * (exploration_strength / (1 + action_selection_count))\n        else:\n            exploration_term = exploration_bonus * exploration_strength\n        \n        avg_scores[action_index] += exploration_term\n        \n    # Epsilon-greedy exploration\n    epsilon = 0.1  # Exploration rate\n    if np.random.rand() < epsilon and total_selection_count > 0:\n        action_index = np.random.choice(np.arange(8))\n    else:\n        action_index = np.argmax(avg_scores)\n    \n    return action_index",
          "objective": -189.81162619995285,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    k = len(score_set)  # Number of actions\n    exploration_factor = 1.0  # Epsilon for exploration\n    exploration_decay = 0.99  # Rate of decay for exploration over time\n\n    # Calculate average scores and selection counts\n    average_scores = np.zeros(k)\n    selection_counts = np.zeros(k)\n\n    for idx in range(k):\n        scores = score_set.get(idx, [])\n        selection_counts[idx] = len(scores)\n        if selection_counts[idx] > 0:\n            average_scores[idx] = np.mean(scores)\n\n    # Dynamic exploration based on total selections and current time\n    exploration_probability = max(exploration_factor * (exploration_decay ** current_time_slot), 0.1)\n\n    if total_selection_count > 0 and np.random.rand() > exploration_probability:\n        # Exploitation: Select action with the highest average score and a confidence bonus\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n        action_scores = average_scores + exploration_bonus\n        action_index = np.argmax(action_scores)\n    else:\n        # Exploration: Select randomly or from less chosen actions\n        unselected_actions = np.where(selection_counts == 0)[0]\n        if len(unselected_actions) > 0:\n            action_index = np.random.choice(unselected_actions)\n        else:\n            action_index = np.random.choice(k)\n\n    return action_index",
          "objective": -188.14888826347723,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_factor = 0.5\n    action_scores = []\n\n    for action_index in range(8):\n        scores = score_set[action_index]\n        if not scores:\n            # If there are no scores, we assume a score of 0\n            average_score = 0\n        else:\n            average_score = np.mean(scores)\n        \n        # Calculate the exploration term\n        selection_count = len(scores)\n        if total_selection_count > 0:\n            exploration_term = exploration_factor * np.log(total_selection_count / (selection_count + 1))\n        else:\n            exploration_term = exploration_factor * (total_time_slots - current_time_slot + 1)\n        \n        # Combine average score with exploration term\n        action_value = average_score + exploration_term\n        action_scores.append(action_value)\n\n    # Select the action with the highest score\n    action_index = np.argmax(action_scores)\n    \n    return action_index",
          "objective": -186.0861541823846,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters\n    exploration_factor = 1.0  # UCB exploration factor\n    min_selection_threshold = 5  # Minimum times an action must be selected for it to be considered\n    \n    action_indices = list(score_set.keys())\n    scores = []\n    \n    for action in action_indices:\n        historical_scores = score_set[action]\n        average_score = np.mean(historical_scores) if historical_scores else 0\n        selection_count = len(historical_scores)\n\n        # Calculate UCB score\n        ucb_value = (exploration_factor * np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1))) if selection_count > 0 else float('inf')\n\n        # Balance exploitation and exploration\n        scores.append(average_score + ucb_value)\n\n    # Ensure actions are explored sufficiently\n    if total_selection_count < min_selection_threshold * len(action_indices):\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = np.argmax(scores)\n    \n    return action_index",
          "objective": -183.04998119549478,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_factor = 0.5  # Exploration factor for dynamic exploration\n    avg_scores = np.zeros(8)   # Initialize average score array\n    exploration_bonus = np.zeros(8)  # Initialize exploration bonus array\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_selection_count = len(scores)\n        \n        # Calculate average score, handle division by zero\n        if action_selection_count > 0:\n            avg_score = np.mean(scores)\n        else:\n            avg_score = 0\n        \n        avg_scores[action_index] = avg_score\n        \n        # Calculate exploration bonus\n        if total_selection_count > 0:\n            exploration_bonus[action_index] = exploration_factor * (1 / (1 + action_selection_count))\n        else:\n            exploration_bonus[action_index] = exploration_factor  # Encourage exploration in the first selections\n    \n    # Combine exploitation (avg scores) and exploration (bonus)\n    combined_scores = avg_scores + exploration_bonus\n    \n    # Decrease exploration influence over time\n    exploration_decay = 1 - (current_time_slot / total_time_slots)\n    final_scores = combined_scores * exploration_decay\n    \n    # Select the action with the highest score\n    action_index = np.argmax(final_scores)\n    \n    return action_index",
          "objective": -182.94942872026309,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    k = len(score_set)  # Number of actions\n    exploration_factor = 0.1  # Base exploration parameter\n    \n    # Calculate average scores and selection counts\n    average_scores = np.zeros(k)\n    selection_counts = np.zeros(k, dtype=int)\n    \n    for idx, scores in score_set.items():\n        selection_counts[idx] = len(scores)\n        if selection_counts[idx] > 0:\n            average_scores[idx] = np.mean(scores)\n    \n    # Epsilon-greedy exploration rate decreasing over time\n    epsilon = exploration_factor * (1 - (total_selection_count / (total_time_slots * exploration_factor)))\n    \n    if np.random.rand() > epsilon:\n        # Exploitation: Upper Confidence Bound (UCB)\n        total_counts = np.sum(selection_counts)\n        if total_counts > 0:\n            ucb_values = average_scores + np.sqrt((2 * np.log(total_counts)) / (selection_counts + 1e-5))\n            action_index = np.argmax(ucb_values)\n        else:\n            action_index = np.random.choice(k)  # explore if no actions selected yet\n    else:\n        # Exploration: Select a random action\n        action_index = np.random.choice(k)\n    \n    return action_index",
          "objective": -181.9238097369937,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Decay factor for exploration\n    # We start with a maximum exploration rate and decay it over time\n    initial_epsilon = 0.5\n    decay_rate = 0.5\n    epsilon = initial_epsilon * (1 - current_time_slot / total_time_slots)\n    \n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        if scores:\n            average_score = np.mean(scores)\n            selection_count = len(scores)\n        else:\n            average_score = 0\n            selection_count = 0\n            \n        # Use the UCB approach\n        if selection_count == 0:\n            action_value = float('inf')  # Favor unselected actions\n        else:\n            action_value = average_score + np.sqrt(2 * np.log(total_selection_count + 1) / selection_count)\n        \n        action_scores.append((action_value, selection_count))\n    \n    if np.random.rand() > epsilon:\n        # Exploitation: Select the action with the highest value\n        action_index = np.argmax([value for value, count in action_scores])\n    else:\n        # Exploration: Select an action randomly\n        action_index = np.random.randint(0, 8)\n    \n    return action_index",
          "objective": -176.51598815880732,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Define parameters for exploration and exploitation\n    exploration_fraction = 0.1\n    epsilon = exploration_fraction * (1 - (current_time_slot / total_time_slots)) + (1 - exploration_fraction) * (current_time_slot / total_time_slots)\n    \n    action_scores = []\n    total_actions = 8\n    \n    for action_index in range(total_actions):\n        historical_scores = score_set.get(action_index, [])\n        average_score = np.mean(historical_scores) if historical_scores else 0\n        selection_count = len(historical_scores)\n\n        # Adjust score based on selection count\n        if total_selection_count > 0:\n            exploration_bonus = (1 - (selection_count / total_selection_count))\n        else:\n            exploration_bonus = 1\n        \n        adjusted_score = average_score + exploration_bonus\n        action_scores.append(adjusted_score)\n\n    action_scores = np.array(action_scores)\n    score_probabilities = action_scores / np.sum(action_scores) if np.sum(action_scores) > 0 else np.ones(total_actions) / total_actions\n\n    # Select an action based on epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(range(total_actions))\n    else:\n        action_index = np.argmax(score_probabilities)\n    \n    return action_index",
          "objective": -173.10428315730522,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        selection_counts[action_index] = selection_count\n        \n        # Calculate average score\n        avg_score = np.mean(scores) if selection_count > 0 else 0\n        avg_scores[action_index] = avg_score\n\n    # Exploration factor decreasing over time\n    exploration_factor = max(0.1, 1 - (current_time_slot / total_time_slots))\n    \n    # Calculate Upper Confidence Bounds\n    ucb_scores = avg_scores + exploration_factor * np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1))\n    \n    # Handle zero selection counts to prevent division by zero\n    ucb_scores[np.isnan(ucb_scores)] = 0\n    \n    # Softmax normalization for selection probabilities\n    exp_scores = np.exp(ucb_scores - np.max(ucb_scores))  \n    selection_probabilities = exp_scores / np.sum(exp_scores)\n\n    # Select action based on probabilities\n    action_index = np.random.choice(range(num_actions), p=selection_probabilities)\n    \n    return action_index",
          "objective": -170.95390134873506,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8\n    avg_scores = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate average scores and counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        if action_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n\n    if total_selection_count == 0:\n        # In the beginning, select randomly to give all options a chance\n        action_index = np.random.randint(0, num_actions)\n    else:\n        # UCB1 strategy for exploration-exploitation balance\n        total_counts = np.sum(action_counts)\n        confidence_bounds = np.sqrt((2 * np.log(total_counts)) / (action_counts + 1e-5))  # Avoid division by zero\n        ucb_scores = avg_scores + confidence_bounds\n\n        # Select the action with the highest UCB\n        action_index = np.argmax(ucb_scores)\n\n    return action_index",
          "objective": -168.65582644081505,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    scores = np.zeros(len(action_indices))\n    selection_counts = np.array([len(score_set[action]) for action in action_indices])\n    historical_scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in action_indices])\n\n    # Epsilon for exploration\n    epsilon = max(0.1, 1 - (total_selection_count / (10 * total_time_slots)))\n\n    # Softmax temperature based on exploration factor\n    exploration_factor = 1.0 / (1 + current_time_slot)\n    \n    # Compute scores for actions\n    for i in range(len(action_indices)):\n        if selection_counts[i] > 0:\n            exploration_value = np.sqrt(np.log(total_selection_count) / selection_counts[i])\n        else:\n            exploration_value = float('inf')  # Prioritize unselected actions\n            \n        # Total score combines average score and exploration\n        scores[i] = historical_scores[i] + exploration_factor * exploration_value\n\n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = action_indices[np.argmax(scores)]\n\n    return action_index",
          "objective": -166.2050417857794,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize average scores and counts\n    num_actions = 8\n    avg_scores = np.zeros(num_actions)\n    action_counts = np.zeros(num_actions)\n\n    # Calculate average scores and action selection counts\n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        action_counts[action_index] = len(scores)\n        if action_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n\n    # Calculate exploration probability\n    exploration_factor = 1 - (total_selection_count / total_time_slots)\n    exploration_prob = np.clip(exploration_factor, 0.1, 1)  # Ensure minimum exploration\n\n    # Generate random values for exploration decision\n    random_values = np.random.rand(num_actions)\n\n    # Epsilon-greedy: select exploitation or exploration\n    if np.random.rand() < exploration_prob:\n        # Exploration: Select a random action\n        action_index = np.random.choice(num_actions)\n    else:\n        # Exploitation: Select action with highest average score\n        action_index = np.argmax(avg_scores)\n\n    return action_index",
          "objective": -149.00661511664114,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration rate\n    action_indices = list(score_set.keys())\n    scores = np.array([np.mean(score_set[i]) if score_set[i] else 0 for i in action_indices])\n    \n    if np.random.rand() < epsilon:\n        # Explore: Select a random action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploit: Select the action with the highest score\n        action_index = action_indices[np.argmax(scores)]\n    \n    return action_index",
          "objective": -143.28157315560455,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    k = len(score_set)  # Number of actions\n    \n    # Calculate average scores and selection counts\n    average_scores = np.zeros(k)\n    selection_counts = np.zeros(k, dtype=int)\n    \n    for idx in range(k):\n        scores = score_set.get(idx, [])\n        selection_counts[idx] = len(scores)\n        if selection_counts[idx] > 0:\n            average_scores[idx] = np.mean(scores)\n    \n    # Epsilon decay parameters\n    max_epsilon = 1.0\n    min_epsilon = 0.01\n    exploration_fraction = 0.1\n\n    # Dynamic epsilon based on total selection count\n    epsilon = max(min_epsilon, max_epsilon * (1 - total_selection_count / (total_time_slots * exploration_fraction)))\n    \n    # Exploration or exploitation decision\n    if np.random.rand() > epsilon:\n        action_index = np.argmax(average_scores)\n    else:\n        # Prioritize less selected actions\n        if total_selection_count < k:  # Explore unselected actions first\n            unselected_actions = np.where(selection_counts == 0)[0]\n            action_index = np.random.choice(unselected_actions) if len(unselected_actions) > 0 else np.random.choice(k)\n        else:\n            action_index = np.random.choice(k)\n    \n    return action_index",
          "objective": -137.78411289693926,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    total_actions = 8\n    action_scores = []\n    exploration_fraction = 0.1\n\n    for action_index in range(total_actions):\n        historical_scores = score_set.get(action_index, [])\n        average_score = np.mean(historical_scores) if historical_scores else 0\n        selection_count = len(historical_scores)\n        \n        # Confidence interval factor: Encourages exploration for less-selected actions\n        confidence_factor = np.sqrt(total_selection_count / (1 + selection_count)) if selection_count > 0 else total_selection_count + 1\n        \n        adjusted_score = average_score + confidence_factor / (selection_count + 1e-5)\n        action_scores.append(adjusted_score)\n    \n    # Softmax probabilities for action selection\n    action_scores = np.array(action_scores)\n    exp_scores = np.exp(action_scores - np.max(action_scores))  # Stability for large values\n    score_probabilities = exp_scores / np.sum(exp_scores)\n\n    # Epsilon-greedy exploration factor\n    epsilon = exploration_fraction * (1 - (current_time_slot / total_time_slots)) + \\\n              (1 - exploration_fraction) * (current_time_slot / total_time_slots)\n\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(range(total_actions))\n    else:\n        action_index = np.random.choice(range(total_actions), p=score_probabilities)\n    \n    return action_index",
          "objective": -134.08816566681432,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Smooth the exploration factor over time based on the total selection count\n    exploration_factor = max(1.0 - (current_time_slot / total_time_slots), 0.1)\n    action_indices = list(score_set.keys())\n    scores = []\n    \n    for action in action_indices:\n        historical_scores = score_set[action]\n        selection_count = len(historical_scores)\n        \n        # Compute average score for each action\n        average_score = np.mean(historical_scores) if selection_count > 0 else 0\n        \n        # Confidence interval for exploration\n        exploration_value = (np.sqrt(total_selection_count) / (1 + selection_count)) if selection_count > 0 else float('inf')\n        \n        # Combine exploitation and exploration to get the final score\n        scores.append(average_score + exploration_factor * exploration_value)\n\n    # Epsilon-greedy approach for exploration\n    epsilon = 0.1  # Fixed exploration probability\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = np.argmax(scores)\n\n    return action_index",
          "objective": -121.98748362417811,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    avg_scores = []\n    exploration_factor = 1.0  # Exploration parameter\n    epsilon = 0.1  # Probability for exploration\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        action_selection_count = len(scores)\n\n        # Calculate average score\n        avg_score = np.mean(scores) if action_selection_count > 0 else 0\n        \n        # Compute exploration term\n        exploration_term = exploration_factor / (1 + action_selection_count)\n\n        # Compute final score with decay\n        decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        final_score = avg_score * decay_factor + exploration_term\n\n        avg_scores.append(final_score)\n\n    # Epsilon-greedy strategy for exploration\n    if np.random.rand() < epsilon and total_selection_count > 0:\n        action_index = np.random.choice(range(8))\n    else:\n        action_index = np.argmax(avg_scores)\n\n    return action_index",
          "objective": -117.930650383759,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    k = len(score_set)  # Number of actions\n    average_scores = np.zeros(k)\n    selection_counts = np.zeros(k)\n\n    for idx in score_set:\n        scores = score_set[idx]\n        selection_counts[idx] = len(scores)\n        if selection_counts[idx] > 0:\n            average_scores[idx] = np.mean(scores)\n\n    # Upper Confidence Bound (UCB) parameters\n    exploration_factor = 2\n    ucb_values = np.zeros(k)\n\n    # Calculate UCB for each action\n    for idx in range(k):\n        if selection_counts[idx] == 0:\n            ucb_values[idx] = float('inf')  # Encourage exploration for unselected actions\n        else:\n            ucb_values[idx] = average_scores[idx] + exploration_factor * np.sqrt(np.log(total_selection_count) / selection_counts[idx])\n\n    # Select action based on UCB values\n    action_index = np.argmax(ucb_values)\n\n    return action_index",
          "objective": -110.26835023474729,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    k = len(score_set)  # Number of actions\n    \n    # Calculate average scores and selection counts\n    average_scores = np.zeros(k)\n    selection_counts = np.zeros(k)\n    \n    for idx in score_set.keys():\n        scores = score_set[idx]\n        selection_counts[idx] = len(scores)\n        if selection_counts[idx] > 0:\n            average_scores[idx] = np.mean(scores)\n    \n    # Softmax temperature for balancing exploration and exploitation\n    temperature = 1.0 / (1 + total_selection_count / (total_time_slots * 0.5))\n    softmax_probs = np.exp(average_scores / temperature) / np.sum(np.exp(average_scores / temperature))\n    \n    # Select action based on computed probabilities\n    action_index = np.random.choice(k, p=softmax_probs)\n    \n    return action_index",
          "objective": -104.45082731917023,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    total_actions = 8\n    action_scores = []\n    exploration_fraction = 0.1\n    \n    for action_index in range(total_actions):\n        historical_scores = score_set.get(action_index, [])\n        average_score = np.mean(historical_scores) if historical_scores else 0\n        selection_count = len(historical_scores)\n\n        # Dynamic confidence based on selection count\n        if selection_count > 0:\n            confidence_factor = np.sqrt(total_selection_count / (1 + selection_count))\n        else:\n            confidence_factor = total_selection_count + 1\n            \n        adjusted_score = average_score + confidence_factor / (selection_count + 1e-5)\n        action_scores.append(adjusted_score)\n    \n    action_scores = np.array(action_scores)\n\n    # Softmax normalization with stability\n    exp_scores = np.exp(action_scores - np.max(action_scores))\n    score_probabilities = exp_scores / np.sum(exp_scores)\n\n    # Epsilon-greedy dynamic exploration factor\n    epsilon = exploration_fraction * (1 - (current_time_slot / total_time_slots)) + \\\n              (1 - exploration_fraction) * (current_time_slot / total_time_slots)\n\n    # Action selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(range(total_actions))\n    else:\n        action_index = np.random.choice(range(total_actions), p=score_probabilities)\n    \n    return action_index",
          "objective": -90.01548690067611,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    avg_scores = np.zeros(action_count)\n    action_selection_counts = np.zeros(action_count)\n\n    # Calculate average scores and selection counts\n    for action_index, scores in score_set.items():\n        action_selection_counts[action_index] = len(scores)\n        if action_selection_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n\n    exploration_factor = 1.0\n    epsilon = max(0.1, 1 - (total_selection_count / (total_time_slots * 0.5)))  # Epsilon decay\n\n    # Epsilon-Greedy Selection\n    if np.random.rand() < epsilon:  # Explore\n        action_index = np.random.choice(action_count)\n    else:  # Exploit\n        # UCB1 Adjustment\n        exploration_terms = np.zeros(action_count)\n        for action_index in range(action_count):\n            if action_selection_counts[action_index] > 0:\n                exploration_terms[action_index] = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / (action_selection_counts[action_index] + 1))\n            else:\n                exploration_terms[action_index] = float('inf')  # Encourage selection of untried actions\n\n        action_index = np.argmax(avg_scores + exploration_terms)\n\n    return action_index",
          "objective": -89.41507240693235,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    k = len(score_set)  # Number of actions\n    num_exploratory_choices = 2  # Allow for multiple exploratory choices\n    \n    # Calculate average scores and selection counts\n    average_scores = np.zeros(k)\n    selection_counts = np.zeros(k)\n    \n    for idx in score_set.keys():\n        scores = score_set[idx]\n        selection_counts[idx] = len(scores)\n        if selection_counts[idx] > 0:\n            average_scores[idx] = np.mean(scores)\n    \n    # Handle actions with zero selections\n    unexplored_actions = np.where(selection_counts == 0)[0]\n    \n    # Calculate exploration probability\n    exploration_probability = np.clip(1 - (total_selection_count / total_time_slots), 0.01, 1.0)\n    \n    # Exploration or exploitation decision\n    if np.random.rand() > exploration_probability:\n        # Exploitation: Softmax probability distribution based on average scores\n        exp_scores = np.exp(average_scores - np.max(average_scores))  # Stabilize exponentials\n        selection_probabilities = exp_scores / np.sum(exp_scores)\n        action_index = np.random.choice(k, p=selection_probabilities)\n    else:\n        # Exploration: If there are unexplored actions, prefer them\n        if len(unexplored_actions) > 0:\n            # Favor unexplored choices to ensure all actions get a chance\n            action_index = np.random.choice(unexplored_actions)\n        else:\n            # Select one of the actions at random for exploration when all are explored\n            action_index = np.random.randint(k)\n    \n    return action_index",
          "objective": -83.45400589576369,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    k = len(score_set)  # Number of actions\n    exploration_factor = 0.1  # Base exploration parameter\n    \n    # Initialize arrays for tracking average scores and selection counts\n    average_scores = np.zeros(k)\n    selection_counts = np.zeros(k)\n    \n    # Calculate average scores and selection counts\n    for idx, scores in score_set.items():\n        selection_counts[idx] = len(scores)\n        if selection_counts[idx] > 0:\n            average_scores[idx] = np.mean(scores)\n    \n    # Calculate a scale for exploration based on time slots\n    exploration_scale = (total_time_slots - current_time_slot) / total_time_slots\n    exploration_probability = exploration_factor * exploration_scale\n\n    # Use a softmax approach for scaling average scores to promote diversity\n    scaled_scores = np.exp(average_scores) / np.sum(np.exp(average_scores))\n    \n    if total_selection_count > 0 and np.random.rand() > exploration_probability:\n        # Exploitation: Select action based on scaled probabilities of average scores\n        action_index = np.random.choice(k, p=scaled_scores)\n    else:\n        # Exploration: Mix of unselected actions or randomly select an action\n        unselected_actions = np.where(selection_counts == 0)[0]\n        if len(unselected_actions) > 0:\n            action_index = np.random.choice(unselected_actions)\n        else:\n            action_index = np.random.choice(k)\n    \n    return action_index",
          "objective": -66.90457423713286,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration factor\n    actions = list(score_set.keys())\n    \n    # Calculate average scores for each action\n    average_scores = []\n    for action in actions:\n        scores = score_set[action]\n        if len(scores) > 0:\n            average_score = np.mean(scores)\n        else:\n            average_score = 0  # If action has never been selected\n        average_scores.append(average_score)\n\n    # Exploration-exploitation strategy\n    if np.random.rand() < epsilon or current_time_slot < total_time_slots / 4:\n        action_index = np.random.choice(actions)  # Explore\n    else:\n        action_index = np.argmax(average_scores)  # Exploit\n        \n    return action_index",
          "objective": -65.16583710526504,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration probability\n    action_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        if scores:\n            average_score = np.mean(scores)\n        else:\n            average_score = 0\n        action_scores.append((average_score, len(scores)))\n    \n    if total_selection_count > 0 and np.random.rand() > epsilon:\n        # Exploitation: Select the action with the highest average score\n        action_index = np.argmax([score for score, count in action_scores])\n    else:\n        # Exploration: Select an action randomly\n        action_index = np.random.randint(0, 8)\n    \n    return action_index",
          "objective": -59.68172179008309,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    k = len(score_set)  # Number of actions\n    baseline_exploration = 0.1  # Base exploration factor\n    min_exploration = 0.01  # Minimum exploration probability\n    exploration_decay = 0.99  # Decay factor for epsilon\n\n    # Calculate average scores and selection counts\n    average_scores = np.zeros(k)\n    selection_counts = np.zeros(k)\n\n    for idx in score_set.keys():\n        scores = score_set[idx]\n        selection_counts[idx] = len(scores)\n        if selection_counts[idx] > 0:\n            average_scores[idx] = np.mean(scores)\n\n    # Calculate dynamic epsilon\n    epsilon = max(min_exploration, baseline_exploration * (exploration_decay ** (total_selection_count - 1)))\n    \n    # Softmax-based action selection probabilities\n    exp_scores = np.exp(average_scores - np.max(average_scores))  # Stability with max subtraction\n    probabilities = exp_scores / np.sum(exp_scores)\n    \n    # Dual selection strategy based on epsilon\n    if np.random.rand() > epsilon:\n        # Exploitation: Select according to softmax probabilities\n        action_index = np.random.choice(k, p=probabilities)\n    else:\n        # Exploration: Select randomly among all actions\n        action_index = np.random.choice(k)\n    \n    return action_index",
          "objective": -44.57783235852804,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    total_actions = 8\n    action_scores = np.zeros(total_actions)\n    \n    for action_index in range(total_actions):\n        historical_scores = score_set.get(action_index, [])\n        average_score = np.mean(historical_scores) if historical_scores else 0\n        selection_count = len(historical_scores)\n\n        # Calculate an adjusted score incorporating selection count\n        exploration_factor = np.sqrt(total_selection_count) / (selection_count + 1) if selection_count > 0 else total_selection_count\n        adjusted_score = average_score + exploration_factor\n        action_scores[action_index] = adjusted_score\n\n    # Normalize scores to obtain probabilities\n    score_probabilities = action_scores / np.sum(action_scores) if np.sum(action_scores) > 0 else np.ones(total_actions) / total_actions\n\n    # Epsilon-greedy strategy for action selection\n    exploration_fraction = 0.1\n    epsilon = exploration_fraction * (1 - (current_time_slot / total_time_slots)) + (1 - exploration_fraction) * (current_time_slot / total_time_slots)\n    \n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(range(total_actions))\n    else:\n        action_index = np.random.choice(np.flatnonzero(score_probabilities == np.max(score_probabilities)))\n\n    return action_index",
          "objective": 2.952272794007172,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores for each action\n    avg_scores = []\n    for i in range(8):\n        scores = score_set.get(i, [])\n        avg_score = np.mean(scores) if scores else 0  # Handle cases with no history\n        avg_scores.append(avg_score)\n\n    exploration_prob = 0.1  # Exploration probability threshold\n    if total_selection_count > 0:\n        exploration_prob = max(0.1, 0.2 * (total_time_slots - current_time_slot) / total_time_slots)\n    \n    # Epsilon-greedy selection\n    if np.random.rand() < exploration_prob:\n        action_index = np.random.choice(range(8))\n    else:\n        action_index = np.argmax(avg_scores)\n\n    return action_index",
          "objective": 20.30989771049269,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    avg_scores = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        selection_counts[action_index] = selection_count\n        \n        # Calculate average score for the action\n        if selection_count > 0:\n            avg_scores[action_index] = np.mean(scores)\n        else:\n            avg_scores[action_index] = 0\n\n    # Dynamic exploration factor based on total selection count\n    if total_selection_count == 0:\n        exploration_factor = np.ones(action_count)  # Equal exploration on first selection\n    else:\n        exploration_values = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1))\n        exploration_factor = np.min(exploration_values) / exploration_values  # Inverse for better exploration\n        exploration_factor[np.isnan(exploration_factor)] = 1  # Handle divisions by zero\n\n    # Combine average scores and exploration factor\n    combined_scores = avg_scores + exploration_factor\n\n    # Epsilon-greedy approach\n    epsilon = max(0.1, 1 - (total_selection_count / total_time_slots))\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_count)  # Explore\n    else:\n        action_index = np.argmax(combined_scores)  # Exploit\n\n    return action_index",
          "objective": 51.38728129906508,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_weight = max(1 - (current_time_slot / total_time_slots), 0)  # Encourage exploration early\n    exploitation_weight = current_time_slot / total_time_slots  # Encourage exploitation later\n    \n    action_scores = []\n    \n    for action_index in range(8):\n        historical_scores = score_set.get(action_index, [])\n        average_score = np.mean(historical_scores) if historical_scores else 0\n        selection_count = len(historical_scores)\n        \n        # Calculate a score that incorporates both exploration and exploitation\n        exploration_term = (1 / (selection_count + 1e-5))  # Add a small constant to avoid division by zero\n        total_score = (exploitation_weight * average_score) + (exploration_weight * exploration_term)\n        \n        action_scores.append(total_score)\n    \n    action_index = np.argmax(action_scores)\n    return action_index",
          "objective": 55.03048701125937,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    total_actions = 8\n    \n    # Calculate epsilon dynamically based on time slot progression\n    exploration_fraction = 0.1\n    epsilon = exploration_fraction * (1 - (current_time_slot / total_time_slots)) + (1 - exploration_fraction) * (current_time_slot / total_time_slots)\n\n    action_values = []\n    \n    for action_index in range(total_actions):\n        historical_scores = score_set.get(action_index, [])\n        average_score = np.mean(historical_scores) if historical_scores else 0\n        selection_count = len(historical_scores)\n\n        # Calculate a score that factors in both average score and selection count\n        if selection_count > 0:\n            exploration_bonus = 1 / np.sqrt(selection_count)  # Inverse square root to decrease bonus with more selections\n        else:\n            exploration_bonus = 1  # Encourage selection for actions not yet taken\n        \n        adjusted_value = average_score + exploration_bonus\n        action_values.append(adjusted_value)\n\n    action_values = np.array(action_values)\n\n    # Normalize to make probabilities\n    adjusted_probabilities = action_values / np.sum(action_values) if np.sum(action_values) > 0 else np.ones(total_actions) / total_actions\n\n    # Epsilon-greedy decision making\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(range(total_actions))\n    else:\n        action_index = np.argmax(adjusted_probabilities)\n    \n    return action_index",
          "objective": 97.00867385113168,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    total_actions = 8\n    exploration_fraction = 0.1\n    epsilon = exploration_fraction * (1 - (current_time_slot / total_time_slots)) + (1 - exploration_fraction) * (current_time_slot / total_time_slots)\n\n    action_scores = np.zeros(total_actions)\n    selection_counts = np.zeros(total_actions)\n\n    for action_index in range(total_actions):\n        historical_scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(historical_scores)\n        \n        if selection_counts[action_index] > 0:\n            average_score = np.mean(historical_scores)\n        else:\n            average_score = 0\n\n        # Adjusting score with a selection count factor\n        confidence = np.sqrt(np.log(total_selection_count + 1) / (selection_counts[action_index] + 1e-5))\n        action_scores[action_index] = average_score + confidence\n\n    # Normalize scores to probabilities\n    score_probabilities = np.exp(action_scores) / np.sum(np.exp(action_scores))\n\n    # Epsilon-greedy action selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(range(total_actions))\n    else:\n        action_index = np.argmax(score_probabilities)\n\n    return action_index",
          "objective": 249.54136758859136,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_factor = 2  # Stronger emphasis on exploration\n    action_indices = list(score_set.keys())\n    scores = []\n\n    for action in action_indices:\n        historical_scores = score_set[action]\n        average_score = np.mean(historical_scores) if len(historical_scores) > 0 else 0\n        selection_count = len(historical_scores)\n\n        # Adjusting exploration based on selection count and total time slots\n        exploration_value = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / (1 + selection_count)) if selection_count > 0 else float('inf')\n        \n        # Expected value = average score + exploration component\n        expected_value = average_score + exploration_value\n        scores.append(expected_value)\n\n    # Epsilon decay based on the time slot to allow more exploration earlier\n    epsilon = max(0.1 * (total_time_slots - current_time_slot) / total_time_slots, 0.01)\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = np.argmax(scores)\n    \n    return action_index",
          "objective": 260.13265870444843,
          "other_inf": null
     }
]