[
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Initialize average scores and selection counts\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Avoid division by zero\n    normalized_counts = np.where(selection_counts > 0, selection_counts, 1)\n\n    # Dynamic epsilon calculation with exploration rate\n    epsilon_min = 0.05\n    epsilon_max = 1.0\n    epsilon = max(epsilon_min, epsilon_max * (1 - current_time_slot / total_time_slots))\n\n    # Calculate uncertainty based on variance\n    variance = np.array([np.var(scores) if scores else 1.0 for scores in score_set.values()])\n    uncertainty = np.sqrt(2 * np.log(total_selection_count + 1) / normalized_counts)\n\n    # Combined scores with variance for exploration\n    combined_scores = avg_scores + uncertainty + (variance / normalized_counts)\n\n    # Epsilon-greedy selection strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, num_actions)  # Explore\n    else:\n        action_index = np.argmax(combined_scores)  # Exploit\n    \n    return action_index",
          "objective": -312.0907571229305,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n\n    # Calculate average scores and ensure at least one score is present to avoid division by zero\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n\n    # Dynamic epsilon calculation\n    epsilon_max = 1.0\n    epsilon_min = 0.05\n    epsilon = epsilon_min + (epsilon_max - epsilon_min) * (1 - (current_time_slot / total_time_slots))\n\n    # Variance for exploration bonus\n    score_variances = np.array([np.var(scores) if len(scores) > 1 else 0.0 for scores in score_set.values()])\n    \n    # Compute exploration bonus\n    explore_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1)) + score_variances\n\n    # Combined scoring for exploration-exploitation balance\n    combined_scores = avg_scores + epsilon * explore_bonus\n\n    # Decision making based on total_selection_count\n    if total_selection_count == 0:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": -312.0907571228597,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores and counts for each action\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Avoid division by zero and ensure at least 1 selection for uncertainty calculation\n    normalized_counts = np.maximum(selection_counts, 1)\n    \n    # Dynamic epsilon for exploration\n    epsilon_min = 0.05\n    epsilon_max = 1.0\n    epsilon = epsilon_max * (1 - current_time_slot / total_time_slots)\n    epsilon = max(epsilon, epsilon_min)\n    \n    # Calculate uncertainty based on counts\n    uncertainty = np.sqrt(2 * np.log(total_selection_count + 1) / normalized_counts)\n    \n    # Calculate variance for exploring less sampled actions\n    variance = np.array([np.var(scores) if scores else 1.0 for scores in score_set.values()])\n    \n    # Combined metric for action selection\n    combined_scores = avg_scores + uncertainty + (variance / normalized_counts)\n    \n    # Epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, num_actions)  # Explore\n    else:\n        action_index = np.argmax(combined_scores)  # Exploit\n    \n    return action_index",
          "objective": -312.0907571228477,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Extract average scores and counts from score_set\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Avoid division by zero\n    normalized_counts = np.maximum(selection_counts, 1)\n\n    # Dynamic epsilon for exploration\n    epsilon_min = 0.05\n    epsilon_max = 1.0\n    epsilon = max(epsilon_min, epsilon_max * (1 - current_time_slot / total_time_slots))\n    \n    # Calculate uncertainty based on counts\n    uncertainty = np.sqrt(2 * np.log(total_selection_count + 1) / normalized_counts)\n    \n    # Calculate variance of scores for exploration\n    variance = np.array([np.var(scores) if scores else 1.0 for scores in score_set.values()])\n    \n    # Combined metric for action selection\n    combined_scores = avg_scores + uncertainty + (variance / normalized_counts)\n    \n    # Epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, num_actions)  # Explore\n    else:\n        action_index = np.argmax(combined_scores)  # Exploit\n    \n    return action_index",
          "objective": -312.09075712281725,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Avoid division by zero and handle empty score lists\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    normalized_counts = np.maximum(selection_counts, 1)\n    \n    # Decaying epsilon for exploration\n    epsilon_min = 0.05\n    epsilon_max = 1.0\n    epsilon = epsilon_max * (1 - current_time_slot / total_time_slots)\n    epsilon = max(epsilon, epsilon_min)\n    \n    # Calculate uncertainty\n    uncertainty = np.sqrt(2 * np.log(total_selection_count + 1) / normalized_counts)\n    \n    # Calculate action variance\n    variance = np.array([np.var(scores) if scores else 1.0 for scores in score_set.values()])\n    \n    # Adjust combined score metrics\n    combined_scores = avg_scores + uncertainty + (variance / normalized_counts)\n\n    # Epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, num_actions)  # Explore\n    else:\n        action_index = np.argmax(combined_scores)  # Exploit\n\n    return action_index",
          "objective": -312.0907571226961,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n\n    # Extract average scores and counts from score_set\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Handle zero counts\n    normalized_counts = np.maximum(selection_counts, 1)\n\n    # Dynamic epsilon for exploration, starting with higher exploration\n    epsilon_min = 0.05\n    epsilon_max = 1.0\n    epsilon = epsilon_max - (epsilon_max - epsilon_min) * (current_time_slot / total_time_slots)\n\n    # Compute uncertainty and variance for improved exploration robustness\n    uncertainty = np.sqrt(2 * np.log(total_selection_count + 1) / normalized_counts)\n    variance = np.array([np.var(scores) if len(scores) > 1 else 1.0 for scores in score_set.values()])\n\n    # Combined scoring metric\n    combined_scores = avg_scores + uncertainty + (variance / normalized_counts)\n\n    # Epsilon-greedy strategy for balanced exploration and exploitation\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, num_actions)  # Explore\n    else:\n        action_index = np.argmax(combined_scores)  # Exploit\n\n    return action_index",
          "objective": -312.09075712216514,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n\n    # Calculate average scores and handle zero selections\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n\n    # Dynamic epsilon-greedy parameters\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    epsilon_decay_rate = 0.99\n    epsilon = max(epsilon_min, epsilon_max * (1 - current_time_slot / total_time_slots) ** epsilon_decay_rate)\n\n    # Exploration bonus using score variance\n    score_variances = np.array([np.var(scores) if len(scores) > 1 else 0.0 for scores in score_set.values()])\n    explore_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1))\n    explore_bonus += np.where(selection_counts == 0, 1.0, 0.0)  # Ensure infinite bonus for unselected actions\n\n    # Calculate combined scores with exploration and variance consideration\n    combined_scores = avg_scores + epsilon * explore_bonus + 0.1 * score_variances\n\n    # Select action based on combined scores\n    action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": -312.0907571221287,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n\n    # Dynamic epsilon-greedy parameters\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    epsilon_decay = 0.99\n    epsilon = max(epsilon_min, epsilon_max * (1 - current_time_slot / total_time_slots) ** epsilon_decay)\n\n    # Exploration bonus using score variance and ensuring exploration of unselected actions\n    explore_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))  # +1e-5 to avoid division by zero\n    explore_bonus += np.where(selection_counts == 0, 1.0, 0.0)\n\n    # Combined score calculation\n    combined_scores = avg_scores + epsilon * explore_bonus\n    \n    # Select action based on combined scores\n    action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": -312.09075712112843,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores and selection counts\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Handle zero counts for normalization\n    normalized_counts = np.where(selection_counts > 0, selection_counts, 1)\n\n    # Dynamic epsilon calculation\n    epsilon_min = 0.1\n    epsilon = max(epsilon_min, (1 - current_time_slot / total_time_slots) * (1 - epsilon_min))\n\n    # Exploration bonus using Bayesian optimization (Beta distribution)\n    uncertainty = np.sqrt(2 * np.log(total_selection_count + 1) / normalized_counts)\n    combined_scores = avg_scores + uncertainty\n\n    # Apply epsilon greedy strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, num_actions)  # Explore\n    else:\n        action_index = np.argmax(combined_scores)  # Exploit\n    \n    return action_index",
          "objective": -312.09075712070074,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores, counts & handle edge cases\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n\n    # Dynamic epsilon decay\n    epsilon_max = 1.0\n    epsilon_min = 0.05\n    epsilon_decay = (epsilon_max - epsilon_min) / total_time_slots\n    epsilon = max(epsilon_min, epsilon_max - epsilon_decay * current_time_slot)\n\n    # Calculate variance & exploration bonus\n    score_variances = np.array([np.var(scores) if len(scores) > 1 else 0.0 for scores in score_set.values()])\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1)) + 0.5 * score_variances\n\n    # Combined scores for action selection\n    combined_scores = avg_scores + epsilon * exploration_bonus\n    \n    # Epsilon-greedy strategy for action selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": -312.0907571197682,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores, handling empty score lists\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n\n    # Epsilon decay strategy\n    epsilon_max = 1.0\n    epsilon_min = 0.05\n    epsilon = epsilon_min + (epsilon_max - epsilon_min) * np.exp(-0.1 * current_time_slot / total_time_slots)\n\n    # Variance-based exploration bonus\n    score_variances = np.array([np.var(scores) if len(scores) > 1 else 0.0 for scores in score_set.values()])\n    explore_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1)) + score_variances\n\n    # Combine scores for exploration-exploitation\n    combined_scores = avg_scores + epsilon * explore_bonus\n\n    # Handle case when no scores have been selected: select randomly\n    if total_selection_count == 0:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": -312.09075711974987,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n\n    # Calculate average scores safely\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n\n    # Epsilon decay parameters\n    epsilon_max = 1.0\n    epsilon_min = 0.05\n    epsilon_decay = (epsilon_max - epsilon_min) / total_time_slots\n    epsilon = max(epsilon_min, epsilon_max - epsilon_decay * current_time_slot)\n\n    # Calculate score variance safely\n    score_variances = np.array([np.var(scores) if len(scores) > 1 else 0.0 for scores in score_set.values()])\n\n    # Exploration bonus calculation\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1)) + 0.5 * score_variances\n\n    # Combined scores for action selection\n    combined_scores = avg_scores + epsilon * exploration_bonus\n\n    # Epsilon-greedy action selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": -312.0907571197478,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Initialize average scores and selection counts\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Avoid division by zero\n    normalized_counts = np.where(selection_counts > 0, selection_counts, 1)\n\n    # Decaying epsilon for exploration\n    epsilon_min = 0.05\n    epsilon_max = 1.0\n    epsilon = max(epsilon_min, epsilon_max * (1 - current_time_slot / total_time_slots))\n    \n    # Calculate uncertainty and combined scores\n    variance = np.array([np.var(scores) if scores else 1.0 for scores in score_set.values()])\n    uncertainty = np.sqrt(2 * np.log(total_selection_count + 1) / normalized_counts)\n    \n    # Use combined metric for action selection\n    combined_scores = avg_scores + uncertainty + (variance / normalized_counts)\n\n    # Epsilon-greedy strategy implementation\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, num_actions)  # Explore\n    else:\n        action_index = np.argmax(combined_scores)  # Exploit\n    \n    return action_index",
          "objective": -312.09075711950595,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n\n    # Ensure no division by zero for normalized counts\n    normalized_counts = np.where(selection_counts > 0, selection_counts, 1)\n\n    # Decay epsilon for exploration\n    epsilon_min = 0.05\n    epsilon_max = 1.0\n    epsilon = max(epsilon_min, epsilon_max * (1 - current_time_slot / total_time_slots))\n\n    # Calculate variance and uncertainty\n    variance = np.array([np.var(scores) if scores else 1.0 for scores in score_set.values()])\n    uncertainty = np.sqrt(2 * np.log(total_selection_count + 1) / normalized_counts)\n\n    # Enhanced combined scores\n    exploration_bonus = np.log(1 + normalized_counts)  # Encourage exploration based on selection frequency\n    combined_scores = avg_scores + uncertainty + (variance / normalized_counts) + exploration_bonus\n\n    # Epsilon-greedy selection strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, num_actions)  # Explore\n    else:\n        action_index = np.argmax(combined_scores)  # Exploit\n\n    return action_index",
          "objective": -312.0907571194547,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Initialize arrays for average scores and selection counts\n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    for action_index, scores in score_set.items():\n        if scores:\n            avg_scores[action_index] = np.mean(scores)\n            selection_counts[action_index] = len(scores)\n\n    # Dynamic epsilon decay\n    epsilon_max = 1.0\n    epsilon_min = 0.05\n    epsilon_decay = (epsilon_max - epsilon_min) / total_time_slots\n    epsilon = max(epsilon_min, epsilon_max - epsilon_decay * current_time_slot)\n\n    # Variance and exploration bonus calculations\n    score_variances = np.array([np.var(scores) if len(scores) > 1 else 0.0 for scores in score_set.values()])\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1)) + 0.5 * score_variances\n\n    # Combined scores for action selection\n    combined_scores = avg_scores + epsilon * exploration_bonus\n\n    # Epsilon-greedy strategy for action selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": -312.0907571192437,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores and selection counts\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Handle division by zero by replacing counts of zero with a small value\n    epsilon = 0.1\n    selection_counts = np.where(selection_counts > 0, selection_counts, 1e-5)  # Avoid division by zero\n\n    # Dynamic exploration factor\n    epsilon_min = 0.05\n    epsilon_max = 1.0\n    epsilon = max(epsilon_min, epsilon_max * (1 - current_time_slot / total_time_slots))\n\n    # Calculate uncertainty based on average scores variance\n    variance = np.array([np.var(scores) if scores else 1.0 for scores in score_set.values()])\n    exploration_bonus = np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts)\n\n    # Combining average scores with exploration bonus and variance influence\n    combined_scores = avg_scores + exploration_bonus + (variance / selection_counts)\n\n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, num_actions)  # Exploration\n    else:\n        action_index = np.argmax(combined_scores)  # Exploitation\n    \n    return action_index",
          "objective": -312.09075711916614,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores for each action\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    \n    # Calculate selection counts\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Dynamic epsilon-greedy parameters\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    epsilon_decay_rate = 0.9  # Tweaked decay rate for smoother transition\n    epsilon = epsilon_min + (epsilon_max - epsilon_min) * (1 - current_time_slot / total_time_slots)\n    \n    # Calculation for selection probability for exploration\n    exploration_bonus = np.where(selection_counts == 0, np.inf, \n                                   np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1)))\n    \n    # Calculate combined scores\n    combined_scores = avg_scores + epsilon * exploration_bonus\n    \n    # Select action based on combined scores\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -312.090757118235,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores and handle edge cases\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n\n    # Dynamic epsilon decay\n    epsilon_max = 1.0\n    epsilon_min = 0.05\n    epsilon_decay = (epsilon_max - epsilon_min) / total_time_slots\n    epsilon = max(epsilon_min, epsilon_max - epsilon_decay * current_time_slot)\n\n    # Calculate the variance and exploration term\n    score_variances = np.array([np.var(scores) if len(scores) > 1 else 0.0 for scores in score_set.values()])\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1)) + 0.5 * score_variances\n\n    # Combine scores for action selection\n    combined_scores = avg_scores + epsilon * exploration_bonus\n    \n    # Epsilon-greedy strategy for action selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": -312.0907571176531,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores and counts while handling empty score lists\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Dynamic epsilon calculation\n    epsilon_max = 1.0\n    epsilon_min = 0.05\n    epsilon = epsilon_min + (epsilon_max - epsilon_min) * (1 - (current_time_slot / total_time_slots))\n\n    # Handle zero selection counts to avoid division by zero\n    explore_bonus = np.zeros(num_actions)\n    non_zero_counts = selection_counts > 0\n    if np.any(non_zero_counts):\n        score_variances = np.array([np.var(scores) if len(scores) > 1 else 0.0 for scores in score_set.values()])\n        explore_bonus[non_zero_counts] = np.sqrt(np.log(total_selection_count + 1) / (selection_counts[non_zero_counts] + 1)) + score_variances[non_zero_counts]\n    \n    # Combined scores for action selection\n    combined_scores = avg_scores + epsilon * explore_bonus\n    \n    # Select action based on combined scores\n    if total_selection_count == 0:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": -312.0907571175517,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Handle empty score sets gracefully\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n\n    # Dynamic epsilon calculation with a smoother decay function\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    epsilon_decay = (epsilon_max - epsilon_min) / total_time_slots\n    epsilon = max(epsilon_min, epsilon_max - epsilon_decay * current_time_slot)\n\n    # Exploration bonus considering both selection counts and variance\n    score_variances = np.array([np.var(scores) if len(scores) > 1 else 0.0 for scores in score_set.values()])\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1)) + score_variances\n\n    # Combined score computation\n    combined_scores = avg_scores + epsilon * exploration_bonus\n\n    # Select action based on the combined scores\n    if total_selection_count == 0:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": -312.09075711692094,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Handle cases where there are no selections\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n\n    # Dynamic epsilon-greedy parameters\n    epsilon_max = 1.0\n    epsilon_min = 0.05\n    epsilon_decay_rate = 0.98\n    epsilon = max(epsilon_min, epsilon_max * (1 - current_time_slot / total_time_slots) ** epsilon_decay_rate)\n\n    # Calculate uncertainty using Bayesian approach\n    alpha = avg_scores * selection_counts + 1\n    beta = (1 - avg_scores) * selection_counts + 1\n    bayesian_scores = alpha / (alpha + beta)\n\n    # Exploration bonus based on selection counts\n    explore_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1))\n    explore_bonus += np.where(selection_counts == 0, 1.0, 0.0)  # Infinite bonus for unselected actions\n\n    # Combined score incorporating exploitation (Bayesian), exploration, and variance\n    combined_scores = bayesian_scores + epsilon * explore_bonus\n\n    # Select action based on the highest combined score\n    action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": -312.0907571165566,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores, selection counts, and initialize variables\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Dynamic epsilon calculation\n    epsilon_max = 1.0\n    epsilon_min = 0.05\n    epsilon_decay = 0.05  # Positive decay for a gradual reduction\n    epsilon = epsilon_min + (epsilon_max - epsilon_min) * np.exp(epsilon_decay * current_time_slot / total_time_slots)\n\n    # Exploration bonus based on selection variance and counts\n    score_variances = np.array([np.var(scores) if len(scores) > 1 else 0.0 for scores in score_set.values()])\n    explore_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1)) + score_variances\n\n    # Combined scores computation\n    combined_scores = avg_scores + epsilon * explore_bonus\n\n    # Select action based on the combined scores\n    if total_selection_count == 0:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": -312.09075711623933,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores and handle cases with no selections\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n\n    # Dynamic epsilon calculation\n    epsilon = max(0.05, 1.0 - (current_time_slot / total_time_slots))\n\n    # Variance for exploration bonus\n    score_variances = np.array([np.var(scores) if len(scores) > 1 else 0.0 for scores in score_set.values()])\n    \n    # Compute exploration bonus\n    explore_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1)) + score_variances \n\n    # Combined scoring for exploration-exploitation balance\n    combined_scores = avg_scores + epsilon * explore_bonus\n\n    # Select action based on the combined scores\n    action_index = np.random.randint(num_actions) if total_selection_count == 0 else np.argmax(combined_scores)\n\n    return action_index",
          "objective": -312.0907571161395,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Dynamic epsilon calculation\n    epsilon_min = 0.05\n    epsilon = max(epsilon_min, 1.0 - (current_time_slot / total_time_slots))\n    \n    # Variance for exploration bonus\n    score_variances = np.array([np.var(scores) if len(scores) > 1 else 0.0 for scores in score_set.values()])\n    \n    # Compute exploration bonus\n    explore_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1)) + score_variances\n    \n    # Combined score calculation\n    combined_scores = avg_scores + epsilon * explore_bonus\n    \n    # Select action based on combined scores\n    action_index = np.random.randint(num_actions) if total_selection_count == 0 else np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -312.0907571148473,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores and selection counts\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Calculate epsilon for exploration\n    epsilon_max = 1.0\n    epsilon_min = 0.05\n    epsilon_decay = -0.1\n    epsilon = epsilon_min + (epsilon_max - epsilon_min) * np.exp(epsilon_decay * current_time_slot / total_time_slots)\n\n    # Incorporate score variances as exploration bonus\n    score_variances = np.array([np.var(scores) if len(scores) > 1 else 0.0 for scores in score_set.values()])\n    explore_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1)) + score_variances\n\n    # Combined scores for exploration-exploitation balance\n    combined_scores = avg_scores + epsilon * explore_bonus\n\n    # If no selections made, pick a random action\n    if total_selection_count == 0:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": -312.09075698161337,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores and handle edge cases for empty actions\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n\n    # Dynamic epsilon decay\n    epsilon_max = 1.0\n    epsilon_min = 0.05\n    epsilon_decay = (epsilon_max - epsilon_min) / total_time_slots\n    epsilon = max(epsilon_min, epsilon_max - epsilon_decay * current_time_slot)\n\n    # Calculate exploration bonus with a modified formula considering variance\n    score_variances = np.array([np.var(scores) if len(scores) > 1 else 0.0 for scores in score_set.values()])\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1)) + 0.5 * score_variances\n\n    # Combined score for action selection\n    combined_scores = avg_scores + epsilon * exploration_bonus\n\n    # Epsilon-greedy strategy for action selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": -312.09075697921446,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n\n    # Calculate average scores, selection counts, and variances\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    score_variances = np.array([np.var(scores) if len(scores) > 1 else 0.0 for scores in score_set.values()])\n\n    # Dynamic epsilon-greedy parameters\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    epsilon_decay_rate = 0.99\n    epsilon = max(epsilon_min, epsilon_max * (1 - current_time_slot / total_time_slots) ** epsilon_decay_rate)\n\n    # Exploration bonus for unselected actions and based on selection counts\n    explore_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1))\n    explore_bonus += np.where(selection_counts == 0, 1.0, 0.0)  # Bonus for unselected actions\n\n    # Calculate combined scores accounting for exploration and variance\n    combined_scores = avg_scores + epsilon * explore_bonus + 0.1 * score_variances\n\n    # Select the action with the highest combined score\n    action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": -312.0907561267735,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n\n    # Calculate average scores and counts, handling empty lists\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n\n    # Epsilon decay strategy\n    epsilon_max = 1.0\n    epsilon_min = 0.05\n    epsilon_decay = np.exp(-0.1 * current_time_slot / total_time_slots)\n    epsilon = epsilon_min + (epsilon_max - epsilon_min) * epsilon_decay\n\n    # Calculate the uncertainty using standard deviation\n    score_stddev = np.array([np.std(scores) if len(scores) > 1 else 0.0 for scores in score_set.values()])\n    \n    # Exploration bonus based on uncertainty and selection counts\n    explore_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1)) + score_stddev\n\n    # Combine scores for exploration-exploitation\n    combined_scores = avg_scores + epsilon * explore_bonus\n\n    # If no actions have been selected yet, select randomly\n    if total_selection_count == 0:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": -312.09075349154773,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores and selection counts\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Epsilon decay strategy\n    epsilon_max = 1.0\n    epsilon_min = 0.05\n    epsilon = epsilon_min + (epsilon_max - epsilon_min) * np.exp(-0.1 * current_time_slot / total_time_slots)\n    \n    # Variance-based exploration bonus\n    score_variances = np.array([np.var(scores) if len(scores) > 1 else 0.0 for scores in score_set.values()])\n    \n    # For handling selection counts, avoid division by zero\n    selection_counts += 1  # Ensure we don't divide by zero\n    explore_bonus = np.sqrt(np.log(total_selection_count + 1) / selection_counts) + score_variances\n    \n    # Combined scores for exploration-exploitation balance\n    combined_scores = avg_scores + epsilon * explore_bonus\n    \n    # Handle case when no selections have been made yet\n    if total_selection_count == 0:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -312.0907498262819,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores and selection counts\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Handle zero counts for normalization and avoid division errors\n    normalized_counts = np.where(selection_counts > 0, selection_counts, 1)\n\n    # Dynamic epsilon calculation\n    epsilon_min = 0.05\n    epsilon = max(epsilon_min, (1 - current_time_slot / total_time_slots) * (1 - epsilon_min))\n    \n    # Calculate score variance and uncertainty\n    variances = np.array([np.var(scores) if len(scores) > 1 else 0.0 for scores in score_set.values()])\n    uncertainty = np.sqrt(2 * np.log(total_selection_count + 1) / normalized_counts) + np.sqrt(variances)\n    \n    # Combine scores with uncertainty\n    combined_scores = avg_scores + uncertainty\n\n    # Apply epsilon greedy strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, num_actions)  # Explore\n    else:\n        action_index = np.argmax(combined_scores)  # Exploit\n    \n    return action_index",
          "objective": -312.09074090827437,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores and selection counts\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Dynamic epsilon-greedy strategy\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    epsilon_decay_rate = 0.9\n    epsilon = max(epsilon_min, epsilon_max * (1 - current_time_slot / total_time_slots))\n\n    # Variance of scores for each action\n    variances = np.array([np.var(scores) if len(scores) > 1 else 0.0 for scores in score_set.values()])\n    \n    # UCB calculation with exploration bonuses\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-6))\n    ucbs = avg_scores + exploration_bonus + variances\n    \n    # Select action using epsilon-greedy approach\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, num_actions)\n    else:\n        action_index = np.argmax(ucbs)\n\n    return action_index",
          "objective": -312.09057831211265,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores and selection counts\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Modified epsilon calculation\n    epsilon_max = 1.0\n    epsilon_min = 0.05\n    epsilon = epsilon_min + (epsilon_max - epsilon_min) * np.exp(-0.1 * current_time_slot / total_time_slots)\n    \n    # Variance-based exploration bonus\n    score_variances = np.array([np.var(scores) if len(scores) > 1 else 0.0 for scores in score_set.values()])\n    explore_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1)) + score_variances\n    \n    # Combined scores: exploitation and exploration balance\n    combined_scores = avg_scores + epsilon * explore_bonus\n\n    # Select action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -312.09016224127515,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores and selection counts\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Handle zero selection counts\n    normalized_counts = np.where(selection_counts > 0, selection_counts, 1)\n\n    # Epsilon decay parameters\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    epsilon_decay_rate = (epsilon_max - epsilon_min) / total_time_slots\n    epsilon = max(epsilon_min, epsilon_max - epsilon_decay_rate * current_time_slot)\n\n    # Variance calculation for score variability\n    score_variances = np.array([np.var(scores) if scores else 0.0 for scores in score_set.values()])\n\n    # Exploration bonus based on selection counts and score variance\n    explore_bonus = np.sqrt(np.log(total_selection_count + 1) / normalized_counts) + np.sqrt(score_variances)\n\n    # Combined score calculation using a softmax approach for smoother exploration\n    combined_scores = avg_scores + epsilon * explore_bonus\n    combined_scores_exp = np.exp(combined_scores - np.max(combined_scores))  # for numerical stability\n    probabilities = combined_scores_exp / np.sum(combined_scores_exp)\n\n    # Select action based on calculated probabilities\n    action_index = np.random.choice(num_actions, p=probabilities)\n    \n    return action_index",
          "objective": -312.0890406634062,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores for each action and handle possible empty lists\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    \n    # Calculate selection counts\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Dynamic epsilon-greedy parameters\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    epsilon_decay_rate = 0.95\n    epsilon = epsilon_max - (epsilon_max - epsilon_min) * (current_time_slot / total_time_slots) ** epsilon_decay_rate\n    \n    # Exploration bonus\n    explore_bonus = np.where(selection_counts == 0, np.inf, np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1)))\n    \n    # Calculate combined scores\n    combined_scores = avg_scores + epsilon * explore_bonus\n\n    # Select action based on combined scores\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -312.08814589956455,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    \n    # Count selections for each action\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Dynamic epsilon-greedy strategy\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    epsilon_decay = 0.9\n    epsilon = max(epsilon_min, epsilon_max * (1 - current_time_slot / total_time_slots))\n    \n    # Variance of scores\n    variances = np.array([np.var(scores) if len(scores) > 1 else 0.0 for scores in score_set.values()])\n    \n    # UCB calculation\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-6))\n    upper_confidence_bounds = avg_scores + exploration_bonus + variances\n\n    # Epsilon-greedy action selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(upper_confidence_bounds)\n    \n    return action_index",
          "objective": -312.08091465929647,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores and selection counts\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Handle zero counts for normalization\n    normalized_counts = np.where(selection_counts > 0, selection_counts, 1)\n\n    # Dynamic epsilon calculation\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    epsilon_decay_rate = (epsilon_max - epsilon_min) / total_time_slots\n    epsilon = max(epsilon_min, epsilon_max - epsilon_decay_rate * current_time_slot)\n\n    # Exploration bonus using Upper Confidence Bound (UCB)\n    uncertainty = np.sqrt(2 * np.log(total_selection_count + 1) / normalized_counts)\n    combined_scores = avg_scores + uncertainty\n\n    # Apply epsilon greedy strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, num_actions)  # Explore\n    else:\n        action_index = np.argmax(combined_scores)  # Exploit\n    \n    return action_index",
          "objective": -312.0735643465982,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    epsilon_max = 1.0\n    epsilon_min = 0.05\n    epsilon_decay = 0.1\n    epsilon = max(epsilon_min, epsilon_max * np.exp(-epsilon_decay * current_time_slot / total_time_slots))\n    \n    std_devs = np.array([np.std(scores) if len(scores) > 1 else 0.0 for scores in score_set.values()])\n    explore_bonus = std_devs * np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1))\n    \n    combined_scores = avg_scores + epsilon * explore_bonus\n\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -312.07286835632385,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = 8  # We have actions indexed from 0 to 7.\n    \n    # Calculate average scores and selection counts\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n\n    # Dynamic epsilon-greedy parameters\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    epsilon_decay = (epsilon_max - epsilon_min) / total_time_slots\n    epsilon = max(epsilon_min, epsilon_max - epsilon_decay * current_time_slot)\n\n    # Calculation for exploration bonus (UCB)\n    exploration_bonus = np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n    combined_scores = avg_scores + epsilon * exploration_bonus\n\n    # Select action based on combined scores\n    action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": -312.0706709518399,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    \n    # Count selections for each action\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Dynamic epsilon-greedy strategy\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    epsilon_decay = 0.95\n    epsilon = max(epsilon_min, epsilon_max * (1 - current_time_slot / total_time_slots))\n    \n    # Variance of scores\n    variances = np.array([np.var(scores) if len(scores) > 1 else 0.0 for scores in score_set.values()])\n    \n    # UCB calculation\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-6))\n    upper_confidence_bounds = avg_scores + exploration_bonus + variances\n    \n    # Epsilon-greedy action selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(num_actions)\n    else:\n        action_index = np.argmax(upper_confidence_bounds)\n    \n    return action_index",
          "objective": -310.4305040447379,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Initialize arrays for average scores, selection counts, and variances\n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    # Calculate average scores and selection counts\n    for action_index, scores in score_set.items():\n        selection_counts[action_index] = len(scores)\n        if selection_counts[action_index] > 0:\n            avg_scores[action_index] = np.mean(scores)\n    \n    # Dynamic epsilon-greedy parameters\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    epsilon_decay_rate = 0.95\n    epsilon = max(epsilon_min, epsilon_max * (epsilon_decay_rate ** current_time_slot))\n\n    # Exploration bonus\n    explore_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1))  \n    explore_bonus[selection_counts == 0] = np.inf  # Ensure untried actions get infinite bonus\n\n    # Calculate confidence intervals\n    confidence_intervals = np.zeros(num_actions)\n    for i in range(num_actions):\n        if selection_counts[i] > 0:\n            confidence_intervals[i] = np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts[i])\n        else:\n            confidence_intervals[i] = np.inf  # Prefer untried actions\n        \n    # Combine scores with exploration bonus and confidence interval\n    combined_scores = avg_scores + epsilon * explore_bonus + confidence_intervals\n\n    # Select action based on combined scores\n    action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": -309.3519043753424,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores and selection counts\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Adaptive epsilon-greedy strategy\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    epsilon_decay_rate = 0.9\n    epsilon = epsilon_min + (epsilon_max - epsilon_min) * (1 - current_time_slot / total_time_slots)\n\n    # Upper confidence bound calculation\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-6))\n    ucbs = avg_scores + epsilon * exploration_bonus\n\n    # Select action based on UCB scores, with exploration probability\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, num_actions)\n    else:\n        action_index = np.argmax(ucbs)\n\n    return action_index",
          "objective": -302.70151029890116,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores for each action, handling possible empty lists\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    \n    # Calculate selection counts\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Calculate exploration bonuses\n    # +1 in selection_counts prevents division by zero\n    explore_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n    \n    # Dynamic epsilon calculation\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    epsilon_decay_rate = 0.95\n    remaining_time_fraction = (total_time_slots - current_time_slot) / total_time_slots\n    epsilon = epsilon_min + (epsilon_max - epsilon_min) * (remaining_time_fraction ** epsilon_decay_rate)\n\n    # Combined score calculation\n    exploration_adjusted_scores = (1 - epsilon) * avg_scores + epsilon * explore_bonus\n    combined_scores = np.nan_to_num(exploration_adjusted_scores)  # Handle any NaN values if they arise\n\n    # Select action based on combined scores\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -299.9114381564152,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores while handling empty score lists\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    \n    # Calculate selection counts\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Epsilon decay parameters\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    epsilon_decay_rate = 0.05  # Adjusted for a slower decay for exploration\n    epsilon = epsilon_min + (epsilon_max - epsilon_min) * np.exp(-epsilon_decay_rate * current_time_slot)\n    \n    # Calculate exploration bonus\n    explore_bonus = np.where(selection_counts > 0, np.sqrt(np.log(total_selection_count) / selection_counts), np.inf)\n    \n    # Combined scores\n    combined_scores = avg_scores + epsilon * explore_bonus\n    \n    # Select action index with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -272.4875357390057,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores and selection counts\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n\n    # Epsilon decay parameters\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    epsilon_decay_rate = 0.98\n    epsilon = epsilon_min + (epsilon_max - epsilon_min) * np.exp(-epsilon_decay_rate * (current_time_slot / total_time_slots))\n\n    # Compute exploration weight\n    explore_bonus = np.where(selection_counts > 0, np.sqrt(np.log(total_selection_count) / selection_counts), np.inf)\n\n    # Calculate combined scores\n    combined_scores = avg_scores + epsilon * explore_bonus\n    \n    # Select action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -266.65175211104474,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n\n    # Calculate average scores and selection counts\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n\n    # Dynamic epsilon settings\n    epsilon_initial = 1.0\n    epsilon_min = 0.1\n    epsilon_decay = 0.99\n    epsilon = max(epsilon_min, epsilon_initial * (epsilon_decay ** current_time_slot))\n\n    # Exploration factor based on selection counts\n    explore_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1))\n\n    # Combined scores for selection\n    combined_scores = avg_scores + epsilon * explore_bonus\n\n    # Select action with the highest combined score\n    action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": -263.2997225703948,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores and update selection counts\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n\n    # Calculate epsilon, which changes over time slots\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    epsilon_decay_rate = 0.2\n    epsilon = epsilon_min + (epsilon_max - epsilon_min) * (1 - current_time_slot / total_time_slots)\n\n    # Exploration term\n    explore_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1))\n    explore_bonus[selection_counts == 0] = np.inf  # Assign infinite bonus for unselected actions\n    \n    # Combined scores\n    combined_scores = avg_scores + epsilon * explore_bonus\n    \n    # Select the action with the highest combined score\n    action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": -261.6416915258962,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores with handling empty score lists\n    avg_scores = {action_index: np.mean(scores) if scores else 0.0 for action_index, scores in score_set.items()}\n    \n    # Calculate action selection counts\n    selection_counts = {action_index: len(scores) for action_index, scores in score_set.items()}\n\n    # Adaptive epsilon based on current time slot\n    min_epsilon = 0.05\n    max_epsilon = 0.8\n    epsilon_decay = (max_epsilon - min_epsilon) * (current_time_slot / total_time_slots)\n    epsilon = max(min_epsilon, max_epsilon - epsilon_decay)\n    \n    # Calculate UCB exploration scores\n    exploration_scores = {\n        action_index: avg_scores[action_index] + np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts[action_index] + 1)) \n        if selection_counts[action_index] > 0 else float('inf') \n        for action_index in score_set\n    }\n\n    # Combine exploitation and exploration scores\n    combined_scores = {\n        action_index: (1 - epsilon) * avg_scores[action_index] + epsilon * exploration_scores[action_index]\n        for action_index in score_set\n    }\n\n    # Select the action with the highest combined score\n    action_index = max(combined_scores, key=combined_scores.get)\n    \n    return action_index",
          "objective": -255.17337693386366,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n\n    # Calculate average scores safely to handle empty score lists\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    \n    # Count selections for each action\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n\n    # Dynamic epsilon-greedy parameters\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    epsilon_decay_rate = 0.95\n    epsilon = epsilon_min + (epsilon_max - epsilon_min) * (1 - (current_time_slot / total_time_slots) ** epsilon_decay_rate)\n    \n    # Exploration bonus to encourage under-selected actions\n    explore_bonus = np.where(selection_counts == 0, np.inf, np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1)))\n    \n    # Combined scores: balancing optimism and average performance\n    combined_scores = avg_scores + epsilon * explore_bonus\n\n    # Select action with the highest combined score\n    action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": -247.17044650722914,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores and selection counts\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Epsilon parameters\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    epsilon_decay_rate = 0.95\n    epsilon = epsilon_min + (epsilon_max - epsilon_min) * (1 - (current_time_slot / total_time_slots) ** epsilon_decay_rate)\n    \n    # Exploration bonus\n    explore_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1)) if total_selection_count > 0 else np.inf\n    explore_bonus[selection_counts == 0] = np.inf  # Assign infinity for actions never selected\n    \n    # Combined scores: weighted balance between exploration and exploitation\n    combined_scores = avg_scores + epsilon * explore_bonus\n    \n    # Select action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -246.8004418882471,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores and selection counts\n    avg_scores = np.array([np.mean(scores) if scores else 0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Handle edge case when total_selection_count is 0\n    if total_selection_count == 0:\n        return np.random.randint(num_actions)\n    \n    # Dynamic epsilon-greedy strategy\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    epsilon = epsilon_min + (epsilon_max - epsilon_min) * (1 - (current_time_slot / total_time_slots))\n    \n    # Exploration term\n    exploration_bonus = np.sqrt(np.log(total_selection_count) / (selection_counts + 1e-5))\n    \n    # Combined scores\n    combined_scores = avg_scores + epsilon * exploration_bonus\n    \n    # Select the action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -246.00171996169257,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores and variances for each action\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    variances = np.array([np.var(scores) if len(scores) > 1 else 0.0 for scores in score_set.values()])\n    \n    # Calculate selection counts\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Dynamic epsilon-greedy parameters\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    epsilon_decay_rate = 0.95\n    epsilon = epsilon_min + (epsilon_max - epsilon_min) * (1 - current_time_slot / total_time_slots)\n    \n    # Calculation for exploration bonus, using variances to identify candidates\n    exploration_bonus = np.where(selection_counts == 0, np.inf, \n                                   np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1)))\n    \n    # Combine scores with exploration and variance adjustment\n    combined_scores = avg_scores + epsilon * exploration_bonus + 0.5 * variances\n    \n    # Select action based on combined scores\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -244.15478389852944,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters for epsilon-greedy strategy\n    min_epsilon = 0.1\n    epsilon = max(min_epsilon, (total_time_slots - current_time_slot) / total_time_slots)\n\n    # Calculate average scores for each action\n    avg_scores = {\n        action_index: (np.mean(scores) if scores else 0.0)\n        for action_index, scores in score_set.items()\n    }\n\n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        # Explore: select an action randomly from those available\n        action_index = np.random.choice(\n            [action_index for action_index in range(8) if action_index in score_set]\n        )\n    else:\n        # Exploit: select the action with the highest average score\n        action_index = max(avg_scores, key=avg_scores.get)\n\n    return action_index",
          "objective": -244.04273819662706,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores for each action\n    avg_scores = {\n        action_index: (np.mean(scores) if scores else 0.0)\n        for action_index, scores in score_set.items()\n    }\n    \n    # Calculate exploration bonuses using Upper Confidence Bound (UCB)\n    exploration_bonuses = {}\n    for action_index in range(8):\n        if action_index in score_set and score_set[action_index]:\n            n_sq = len(score_set[action_index])\n            exploration_bonuses[action_index] = np.sqrt(2 * np.log(total_selection_count + 1) / (n_sq + 1))\n        else:\n            exploration_bonuses[action_index] = np.inf  # Encourage exploration for untested actions\n\n    # Compute combined scores\n    combined_scores = {\n        action_index: avg_scores[action_index] + exploration_bonuses.get(action_index, 0)\n        for action_index in range(8)\n    }\n\n    # Penalize scores based on the remaining time slots\n    time_factor = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = {\n        action_index: combined_scores[action_index] * time_factor\n        for action_index in range(8)\n    }\n\n    # Select the action index with the highest score\n    action_index = max(adjusted_scores, key=adjusted_scores.get)\n    \n    return action_index",
          "objective": -239.67528178044137,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores and count of selections for each action\n    avg_scores = np.array([np.mean(scores) if scores else 0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Handle edge case where no selections have been made\n    if total_selection_count == 0:\n        return np.random.randint(num_actions)\n    \n    # Implement a modified epsilon-greedy strategy\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    epsilon_decay = 0.05\n    epsilon = max(epsilon_min, epsilon_max - epsilon_decay * (current_time_slot / total_time_slots))\n\n    # Exploration bonus scaling with selection counts\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1))\n\n    # Combined scores with exploration factor\n    combined_scores = avg_scores + epsilon * exploration_bonus\n    \n    # Select the action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -238.7384499391759,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n\n    # Calculate average scores and selection counts, handling empty lists\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n\n    # Adaptive epsilon setting\n    epsilon_max = 1.0\n    epsilon_min = 0.05\n    epsilon_decay = 0.9\n    epsilon = max(epsilon_min, epsilon_max * np.exp(-epsilon_decay * current_time_slot / total_time_slots))\n\n    # Exploration bonus\n    explore_bonus = np.where(selection_counts > 0,\n                              np.sqrt(np.log(total_selection_count) / selection_counts),\n                              np.inf)\n\n    # Combined scores: exploitation and exploration balance\n    combined_scores = avg_scores + epsilon * explore_bonus\n\n    # Select action with the highest combined score\n    action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": -236.27301955089905,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores and selection counts\n    avg_scores = np.array([np.mean(scores) if scores else 0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Dynamic epsilon-greedy strategy\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    epsilon = epsilon_min + (epsilon_max - epsilon_min) * (1 - (current_time_slot / total_time_slots))\n    \n    # Exploration term\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1))\n    \n    # Combined scores\n    combined_scores = avg_scores + epsilon * exploration_bonus\n    \n    # Select the action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -236.26116014168815,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores while handling potential empty score lists\n    avg_scores = {action_index: np.mean(scores) if scores else 0.0 for action_index, scores in score_set.items()}\n    \n    # Dynamic exploration rate based on total selections\n    min_epsilon = 0.05\n    max_epsilon = 0.8\n    decay_factor = (total_selection_count / (total_time_slots * 8))  # Normalize selection count\n    epsilon = max(min_epsilon, max_epsilon * (1 - decay_factor))\n    \n    # Count selections for each action\n    selection_counts = {action_index: len(scores) for action_index, scores in score_set.items()}\n    \n    if total_selection_count == 0:\n        return np.random.randint(0, 8)\n    \n    # UCB-based exploration\n    exploration_scores = {\n        action_index: avg_scores[action_index] + np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts[action_index] + 1))\n        if selection_counts[action_index] > 0 else float('inf') for action_index in avg_scores\n    }\n\n    if np.random.rand() < epsilon:\n        action_index = max(exploration_scores, key=exploration_scores.get)\n    else:\n        action_index = max(avg_scores, key=avg_scores.get)\n    \n    return action_index",
          "objective": -233.8294618814835,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores and selection counts\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Set exploration parameters\n    epsilon_initial = 1.0\n    epsilon_min = 0.1\n    epsilon_decay = (epsilon_initial - epsilon_min) / total_time_slots\n    epsilon = max(epsilon_min, epsilon_initial - (epsilon_decay * current_time_slot))\n    \n    # Exploration term\n    explore_bonus = np.where(selection_counts == 0, np.inf, np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1)))\n    \n    # Combined scores considering exploration\n    combined_scores = avg_scores + epsilon * explore_bonus\n    \n    # Select the action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -233.76000615818114,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n\n    # Calculate average scores, handle empty score lists\n    avg_scores = np.array([np.mean(scores) if scores else 0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n\n    # Handle edge case when total_selection_count is 0\n    if total_selection_count == 0:\n        return np.random.randint(num_actions)\n    \n    # Calculate a dynamic exploration factor\n    exploration_factor = 1.0 - (current_time_slot / total_time_slots)\n\n    # Avoid division by zero in selection counts\n    adjusted_counts = selection_counts + 1e-5\n\n    # Calculate exploration bonus using UCB\n    exploration_bonus = np.sqrt(np.log(total_selection_count) / adjusted_counts)\n\n    # Calculate combined scores, prioritizing average score enhanced by exploration\n    combined_scores = avg_scores + exploration_factor * exploration_bonus\n\n    # Select the action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -233.15988918959215,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores and selection counts, handling empty cases\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Smooth selection counts\n    smoothed_counts = selection_counts + 1  # Add 1 for log calculation\n    \n    # Epsilon decay parameters\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    epsilon_decay_rate = (epsilon_max - epsilon_min) / total_time_slots\n    epsilon = max(epsilon_min, epsilon_max - epsilon_decay_rate * current_time_slot)\n\n    # Calculate score variance\n    score_variances = np.array([np.var(scores) if scores else 0.0 for scores in score_set.values()])\n    \n    # Exploration bonus considering selection counts and score variance\n    explore_bonus = (np.sqrt(np.log(total_selection_count + 1) / smoothed_counts) + np.sqrt(score_variances))\n    \n    # Combined scores\n    combined_scores = avg_scores + epsilon * explore_bonus\n    combined_scores_exp = np.exp(combined_scores - np.max(combined_scores))  # for numerical stability\n    probabilities = combined_scores_exp / np.sum(combined_scores_exp)\n\n    # Select action using the calculated probabilities\n    action_index = np.random.choice(num_actions, p=probabilities)\n    \n    return action_index",
          "objective": -229.75102147091732,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores and selection counts\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Adaptive epsilon based on time slot\n    exploration_threshold = 5  # minimum selections required before reducing exploration\n    min_explore_count = np.maximum(exploration_threshold, 1)\n    exploration_bonus = np.where(selection_counts < min_explore_count, \n                                  np.inf, \n                                  np.sqrt(np.log(total_selection_count) / selection_counts))\n    \n    # Epsilon computation\n    epsilon = max(1, total_time_slots - current_time_slot) / total_time_slots\n    combined_scores = avg_scores + epsilon * exploration_bonus\n    \n    # Select action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -229.15207058022855,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n\n    # Normalize selection counts to avoid division by zero\n    normalized_counts = np.where(selection_counts > 0, selection_counts, 1)\n\n    # Dynamic epsilon calculation\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    epsilon_decay_rate = (epsilon_max - epsilon_min) / total_time_slots\n    epsilon = max(epsilon_min, epsilon_max - epsilon_decay_rate * current_time_slot)\n\n    # Exploration bonus based on normalized counts\n    explore_bonus = np.sqrt(np.log(total_selection_count + 1) / normalized_counts)\n\n    # Combined scores calculation\n    combined_scores = avg_scores + epsilon * explore_bonus\n\n    # Select the action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -229.0991570171926,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize parameters\n    action_count = len(score_set)\n    \n    # Calculate average scores and counts\n    avg_scores = np.array([np.mean(scores) if scores else 0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n\n    # Avoid division by zero\n    with np.errstate(divide='ignore', invalid='ignore'):\n        exploration_factor = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1))\n\n    # Define adaptive epsilon\n    min_epsilon = 0.05  # Minimum exploration rate\n    epsilon = max(min_epsilon, 1.0 - (current_time_slot / total_time_slots))\n\n    # Combine exploitation (avg_score) and exploration (exploration_factor)\n    combined_scores = avg_scores + epsilon * exploration_factor\n    \n    # Use softmax to derive selection probabilities\n    exp_scores = np.exp(combined_scores - np.max(combined_scores))  # Stability in softmax\n    selection_probabilities = exp_scores / np.sum(exp_scores)\n\n    # Select action using weighted probabilities\n    action_index = np.random.choice(action_count, p=selection_probabilities)\n    \n    return action_index",
          "objective": -228.85908763057472,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Initialize arrays for average scores and counts\n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index, scores in score_set.items():\n        selection_counts[action_index] = len(scores)\n        avg_scores[action_index] = np.mean(scores) if scores else 0.0\n\n    # Dynamic epsilon-greedy parameters\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    epsilon_decay_rate = 0.9  # Adjusted for a stronger decay rate\n    epsilon = max(epsilon_min, epsilon_max * (1 - current_time_slot / total_time_slots))\n\n    # Exploration bonus with score variance consideration\n    variance = np.array([np.var(scores) if scores else 0.0 for scores in score_set.values()])\n    explore_bonus = np.where(selection_counts > 0, np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1)), np.inf)\n\n    # Combine scores and exploration factors\n    combined_scores = avg_scores + epsilon * explore_bonus + (0.5 * variance)  # Weighting variance for exploration\n\n    # Select action based on combined scores\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -228.35261122569665,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores and handle possible empty lists\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    \n    # Calculate selection counts\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Dynamic epsilon-greedy parameters\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    epsilon_decay_rate = 0.95\n    epsilon = max(epsilon_min, epsilon_max * (1 - current_time_slot / total_time_slots))\n    \n    # Exploration bonus\n    explore_bonus = np.where(selection_counts > 0, \n                              np.sqrt(np.log(total_selection_count) / selection_counts),\n                              np.inf)\n    \n    # Calculate combined scores\n    combined_scores = avg_scores + epsilon * explore_bonus\n\n    # Select action based on combined scores\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -228.08331025820064,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n\n    # Calculate average scores, defaulting to zero if no scores are available\n    avg_scores = np.array([np.mean(scores) if scores else 0 for scores in score_set.values()])\n    \n    # Count of selections for each action\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n\n    # Handle the edge case of no selections made\n    if total_selection_count == 0:\n        return np.random.randint(num_actions)\n    \n    # Dynamic epsilon-greedy strategy\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    epsilon_decay = 0.05\n    epsilon = max(epsilon_min, epsilon_max - epsilon_decay * (current_time_slot / total_time_slots))\n    \n    # Exploration bonus adjusting for selection counts\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1))\n    \n    # Combine average scores with exploration bonus\n    combined_scores = avg_scores + epsilon * exploration_bonus\n    \n    # Select the best action based on the combined scores\n    action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": -227.6602068560979,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores and selection counts\n    avg_scores = np.array([np.mean(scores) if scores else 0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n\n    # Prevent zero division and assign small value for unselected actions\n    with np.errstate(divide='ignore', invalid='ignore'):\n        selection_counts_adjusted = np.where(selection_counts == 0, 1e-10, selection_counts)\n\n    # Flexible epsilon calculation\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    epsilon_decay_rate = 1 - (current_time_slot / total_time_slots) ** 2\n    epsilon = epsilon_min + (epsilon_max - epsilon_min) * epsilon_decay_rate\n\n    # Exploration term (lower exploration for more frequently selected actions)\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / selection_counts_adjusted)\n    \n    # Combined scores\n    combined_scores = avg_scores + epsilon * exploration_bonus\n    \n    # Select the action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -227.5704768071903,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores and selection counts\n    avg_scores = np.array([np.mean(scores) if scores else 0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Prevent division by zero for actions that haven't been selected yet\n    selection_counts = np.where(selection_counts == 0, 1, selection_counts)\n    \n    # Dynamic epsilon-greedy strategy\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    epsilon = epsilon_min + (epsilon_max - epsilon_min) * (1 - (current_time_slot / total_time_slots))\n    \n    # Exploration term\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / selection_counts)\n    \n    # Combined scores\n    combined_scores = avg_scores + epsilon * exploration_bonus\n    \n    # Select the action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -226.7812125269828,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize variables\n    num_actions = 8\n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n\n    # Calculate average scores and selection counts\n    for action_index, scores in score_set.items():\n        if scores:\n            avg_scores[action_index] = np.mean(scores)\n            selection_counts[action_index] = len(scores)\n    \n    # Set a minimum threshold for epsilon\n    min_epsilon = 0.05\n    max_epsilon = 0.8\n    epsilon_decay_factor = total_selection_count / (total_time_slots * 8)\n    epsilon = max(min_epsilon, max_epsilon * (1 - epsilon_decay_factor))\n\n    # UCB-based exploration calculation\n    exploration_scores = np.zeros(num_actions)\n    for action_index in range(num_actions):\n        if selection_counts[action_index] > 0:\n            exploration_scores[action_index] = avg_scores[action_index] + np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts[action_index] + 1))\n        else:\n            exploration_scores[action_index] = float('inf')  # Favor unselected actions\n\n    # Select action based on epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        action_index = np.argmax(exploration_scores)\n    else:\n        action_index = np.argmax(avg_scores)\n    \n    return action_index",
          "objective": -225.9080789728782,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores for each action\n    avg_scores = {\n        action_index: (np.mean(scores) if scores else 0.0)\n        for action_index, scores in score_set.items()\n    }\n    \n    # Calculate selection counts\n    selection_counts = {action_index: len(scores) for action_index, scores in score_set.items()}\n    \n    # Dynamic exploration rate\n    min_epsilon = 0.05\n    max_epsilon = 0.8\n    epsilon = max(min_epsilon, max_epsilon * (1 - (current_time_slot / total_time_slots)))\n\n    # If no actions have been selected, explore randomly\n    if total_selection_count == 0:\n        return np.random.randint(0, 8)\n    \n    # Select action based on exploration vs. exploitation\n    if np.random.rand() < epsilon:\n        # Exploration: Weighted exploration based on selection count\n        exploration_scores = {\n            action_index: avg_scores[action_index] + (1 / (1 + selection_counts[action_index])) \n            for action_index in avg_scores\n        }\n        action_index = max(exploration_scores, key=exploration_scores.get)\n    else:\n        # Exploitation: Select action with highest average score\n        action_index = max(avg_scores, key=avg_scores.get)\n\n    return action_index",
          "objective": -225.59250860195561,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n\n    # Calculate average scores for each action\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    \n    # Selection counts\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Dynamic epsilon-greedy parameters\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    epsilon_decay_rate = 0.95\n    epsilon = max(epsilon_min, epsilon_max * (epsilon_decay_rate ** current_time_slot))\n    \n    # Exploration bonus based on selection count and variance\n    explore_bonus = np.where(selection_counts == 0, np.inf, \n                               np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1)))\n    \n    # Variance adjustment for each action\n    score_variance = np.array([np.var(scores) if len(scores) > 1 else 0 for scores in score_set.values()])\n    variance_bonus = np.sqrt(score_variance * (1 / (selection_counts + 1)))\n    \n    # Calculate combined scores\n    combined_scores = avg_scores + epsilon * (explore_bonus + variance_bonus)\n\n    # Select action based on combined scores\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -225.00598293933598,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Extract action indices and their corresponding scores\n    action_indices = list(score_set.keys())\n    scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Context-aware epsilon for exploration\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    epsilon_decay_rate = 0.9\n    epsilon = epsilon_min + (epsilon_max - epsilon_min) * (1 - np.clip(current_time_slot / total_time_slots, 0, 1))\n    \n    # Exploration bonus\n    explore_bonus = np.where(selection_counts == 0, np.inf, np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1)))\n    \n    # Combined score calculation\n    combined_scores = scores + epsilon * explore_bonus\n    \n    # Selecting action with the maximum combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -222.9615709571256,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores, handling empty lists\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Epsilon for exploration, decaying over time slots\n    epsilon = max(0.05, 1.0 * (total_time_slots - current_time_slot) / total_time_slots)\n    \n    # Calculate the exploration bonus: higher for less selected actions\n    explore_bonus = np.where(selection_counts == 0, np.inf, \n                              np.log(total_selection_count) / (selection_counts + 1))\n    \n    # Combine average scores with exploration factor\n    combined_scores = avg_scores + epsilon * explore_bonus\n    \n    # Select the action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -222.7542991908424,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores and counts, ensuring to handle empty lists\n    avg_scores = {action_index: (np.mean(scores) if scores else 0.0) for action_index, scores in score_set.items()}\n    selection_counts = {action_index: len(scores) for action_index, scores in score_set.items()}\n\n    # Dynamic exploration rate calculation\n    min_epsilon = 0.05\n    max_epsilon = 0.8\n    epsilon = min_epsilon + (max_epsilon - min_epsilon) * (1 - (current_time_slot / total_time_slots))\n    \n    # Select action based on exploration-exploitation strategy\n    if total_selection_count == 0 or np.random.rand() < epsilon:\n        # Exploration: balance scores with selection counts\n        exploration_scores = {\n            action_index: avg_scores[action_index] + (1 / (1 + selection_counts[action_index])) for action_index in avg_scores\n        }\n        # Select action based on maximum exploration score\n        action_index = max(exploration_scores, key=exploration_scores.get)\n    else:\n        # Exploitation: select action with highest average score\n        action_index = max(avg_scores, key=avg_scores.get)\n    \n    return action_index",
          "objective": -222.6480269866551,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores and selection counts\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Handle zero count for normalization\n    normalized_counts = np.where(selection_counts > 0, selection_counts, 1)\n\n    # Dynamic epsilon calculation\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    epsilon_decay_rate = (epsilon_max - epsilon_min) / total_time_slots\n    epsilon = max(epsilon_min, epsilon_max - epsilon_decay_rate * current_time_slot)\n\n    # Variance calculation for score variability\n    score_variances = np.array([np.var(scores) if scores else 0.0 for scores in score_set.values()])\n\n    # Exploration bonus based on selection counts and score variance\n    explore_bonus = np.sqrt(np.log(total_selection_count + 1) / normalized_counts) + np.sqrt(score_variances)\n\n    # Combined score calculation\n    combined_scores = avg_scores + epsilon * explore_bonus\n\n    # Select the action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -222.47060768904043,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate averages and selection counts\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n\n    # Epsilon decay parameters\n    epsilon = max(0.1, 1.0 - (current_time_slot / total_time_slots))\n    \n    # Handle zero counts to avoid division by zero\n    normalized_counts = np.where(selection_counts > 0, selection_counts, 1)\n\n    # Variance calculation for score variability\n    score_variances = np.array([np.var(scores) if scores else 0.0 for scores in score_set.values()])\n\n    # Exploration bonus based on selection counts and score variance\n    explore_bonus = np.sqrt(np.log(total_selection_count + 1) / normalized_counts) + np.sqrt(score_variances)\n\n    # Calculate combined scores\n    combined_scores = avg_scores + epsilon * explore_bonus\n    \n    # Use softmax to create probability distribution\n    combined_scores_exp = np.exp(combined_scores - np.max(combined_scores))  # for numerical stability\n    probabilities = combined_scores_exp / np.sum(combined_scores_exp)\n\n    # Select action based on calculated probabilities\n    action_index = np.random.choice(num_actions, p=probabilities)\n    \n    return action_index",
          "objective": -221.48356126880708,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores and selection counts\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Set exploration parameters\n    epsilon_initial = 1.0\n    epsilon_min = 0.1\n    epsilon_decay = 0.98\n    epsilon = max(epsilon_min, epsilon_initial * (epsilon_decay ** current_time_slot))\n    \n    # Exploration factor: encourage exploration of less-selected actions\n    explore_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1))\n    \n    # Combined scores considering exploration\n    combined_scores = avg_scores + epsilon * explore_bonus\n    \n    # Select action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -220.69712906048832,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n\n    # Calculate average scores and selection counts\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n\n    # Calculate score variances\n    score_variances = np.array([np.var(scores) if len(scores) > 1 else 0.0 for scores in score_set.values()])\n\n    # Epsilon decay parameters\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    epsilon_decay_rate = 0.98\n    epsilon = epsilon_min + (epsilon_max - epsilon_min) * np.exp(-epsilon_decay_rate * (current_time_slot / total_time_slots))\n\n    # Adjust exploration strategy\n    explore_bonus = np.where(selection_counts > 0, \n                              np.sqrt(np.log(total_selection_count) / selection_counts), \n                              np.inf)\n\n    # Combined scores: average scores plus exploration bonus and variance\n    combined_scores = avg_scores + epsilon * explore_bonus - score_variances\n\n    # Avoid negative combined scores by capping at 0\n    combined_scores = np.maximum(combined_scores, 0)\n\n    # Select action with the highest combined score\n    action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": -220.47211252441213,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores for each action\n    avg_scores = {action_index: np.mean(scores) if scores else 0.0 for action_index, scores in score_set.items()}\n\n    # Calculate action selection counts\n    selection_counts = {action_index: len(scores) for action_index, scores in score_set.items()}\n    \n    # Dynamic epsilon based on current time slot\n    min_epsilon = 0.1\n    max_epsilon = 0.9\n    epsilon_decay_steps = total_time_slots\n    epsilon = max(min_epsilon, max_epsilon * (1 - current_time_slot / epsilon_decay_steps))\n    \n    # Exploration scores using a modified UCB approach\n    exploration_scores = {\n        action_index: avg_scores[action_index] + np.sqrt(2 * np.log(total_selection_count + 1) / \n        (selection_counts[action_index] + 1)) if selection_counts[action_index] > 0 else float('inf')\n        for action_index in avg_scores\n    }\n    \n    # Combining exploitation and exploration\n    weighted_avg_scores = {\n        action_index: (1 - epsilon) * avg_scores[action_index] + epsilon * exploration_scores[action_index]\n        for action_index in avg_scores\n    }\n    \n    # Select action based on the calculated scores\n    action_index = max(weighted_avg_scores, key=weighted_avg_scores.get)\n\n    return action_index",
          "objective": -220.32023185085714,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores with handling for empty score lists\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    \n    # Track selection counts\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Dynamic epsilon-greedy strategy for exploration\n    epsilon_initial = 1.0\n    epsilon_min = 0.1\n    epsilon_decay = 0.99\n    epsilon = max(epsilon_min, epsilon_initial * (epsilon_decay ** current_time_slot))\n    \n    # Exploration bonus derived from selection count\n    explore_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1))\n    \n    # Combined scores for exploration and exploitation\n    combined_scores = avg_scores + epsilon * explore_bonus\n    \n    # Select action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -220.13026634297464,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores and counts\n    avg_scores = {action_index: (np.mean(scores) if scores else 0.0) for action_index, scores in score_set.items()}\n    selection_counts = {action_index: len(scores) for action_index, scores in score_set.items()}\n\n    # Dynamic exploration rate with a minimum bound\n    min_epsilon = 0.05\n    max_epsilon = 0.8\n    epsilon = min_epsilon + (max_epsilon - min_epsilon) * (1 - (current_time_slot / total_time_slots))\n    \n    # Select action based on exploration-exploitation strategy\n    if total_selection_count == 0 or np.random.rand() < epsilon:\n        # Exploration: favor actions that have been selected less\n        exploration_scores = {\n            action_index: avg_scores[action_index] + (1 / (1 + selection_counts[action_index])) for action_index in avg_scores\n        }\n        action_index = max(exploration_scores, key=exploration_scores.get)\n    else:\n        # Exploitation: favor actions with the highest average score\n        action_index = max(avg_scores, key=avg_scores.get)\n    \n    return action_index",
          "objective": -220.02020886875982,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores and selection counts\n    avg_scores = np.array([np.mean(scores) if scores else 0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Set exploration parameters\n    epsilon_initial = 1.0\n    epsilon_min = 0.05\n    epsilon_decay = 0.999\n    epsilon = max(epsilon_min, epsilon_initial * (epsilon_decay ** current_time_slot))\n\n    # Ensure at least one selection per action for exploration\n    explore_bonus = (total_selection_count - selection_counts) < 1\n    \n    # Exploration terms: Avoid division by zero and use infinity for unselected actions\n    exploration_bonus = np.where(explore_bonus, np.inf, np.sqrt(np.log(total_selection_count) / (selection_counts + 1)))\n    \n    # Combined scores considering exploration\n    combined_scores = avg_scores + epsilon * exploration_bonus\n    \n    # Select the action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -219.80638446905888,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores and selection counts\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Epsilon schedule for exploration\n    epsilon_max = 1.0\n    epsilon_min = 0.05\n    epsilon_decay = (epsilon_max - epsilon_min) / total_time_slots\n    epsilon = max(epsilon_min, epsilon_max - epsilon_decay * current_time_slot)\n    \n    # Variance-based exploration bonus\n    score_variances = np.array([np.var(scores, ddof=1) if len(scores) > 1 else 0.0 for scores in score_set.values()])\n    explore_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1)) + score_variances\n\n    # Combined scores: exploitation and exploration balance\n    combined_scores = avg_scores + epsilon * explore_bonus\n\n    # Select action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -219.35754741332036,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores and selection counts\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n\n    # Dynamic epsilon-greedy parameters\n    epsilon_initial = 1.0\n    epsilon_min = 0.1\n    epsilon_decay = 0.95\n    epsilon = max(epsilon_min, epsilon_initial * (epsilon_decay ** current_time_slot))\n    \n    # Calculate variance for each action's scores to assess variability\n    variances = np.array([np.var(scores) if len(scores) > 1 else 0.0 for scores in score_set.values()])\n    \n    # Exploration factor: modify exploration by variance\n    explore_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1))\n    \n    # Combined scores: prioritize high average score and variability\n    combined_scores = avg_scores + epsilon * explore_bonus + (0.1 * variances)\n    \n    # Select action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -219.0661216830319,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores (default to 0.0 for empty lists)\n    avg_scores = {action_index: np.mean(scores) if scores else 0.0 for action_index, scores in score_set.items()}\n    \n    # Get selection counts\n    selection_counts = {action_index: len(scores) for action_index, scores in score_set.items()}\n    \n    # Dynamic epsilon calculation\n    min_epsilon = 0.05\n    max_epsilon = 0.8\n    decay_factor = current_time_slot / total_time_slots\n    epsilon = max(min_epsilon, max_epsilon * (1 - decay_factor))\n    \n    # UCB-based exploration scores\n    exploration_scores = {\n        action_index: (avg_scores[action_index] + \n                       np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts[action_index] + 1))\n                       if selection_counts[action_index] > 0 else float('inf'))\n        for action_index in avg_scores\n    }\n    \n    # Combined scores\n    combined_scores = {\n        action_index: ((1 - epsilon) * avg_scores[action_index] +\n                       epsilon * exploration_scores[action_index])\n        for action_index in avg_scores\n    }\n    \n    # Select action based on the calculated combined scores\n    action_index = max(combined_scores, key=combined_scores.get)\n    \n    return action_index",
          "objective": -219.0208015307886,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores and selection counts\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Dynamic epsilon-greedy parameters\n    epsilon_max = 1.0\n    epsilon_min = 0.05\n    epsilon_decay_rate = 0.9\n    epsilon = epsilon_min + (epsilon_max - epsilon_min) * (1 - np.clip(current_time_slot / total_time_slots, 0, 1))\n    \n    # Exploration bonus ensuring all actions are selected at least once\n    explore_bonus = np.where(selection_counts == 0, np.inf, np.sqrt(np.log(total_selection_count) / (selection_counts + 1)))\n    \n    # Combined scores: exploitation and exploration balance\n    combined_scores = avg_scores + epsilon * explore_bonus\n    \n    # Select action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -218.68238843931007,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n\n    # Calculate average scores and selection counts\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n\n    # Handle zero counts for normalization\n    normalized_counts = np.where(selection_counts > 0, selection_counts, 1)\n\n    # Dynamic epsilon calculation\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    epsilon_decay_rate = (epsilon_max - epsilon_min) / total_time_slots\n    epsilon = max(epsilon_min, epsilon_max - epsilon_decay_rate * current_time_slot)\n\n    # Variance calculation for score variability\n    score_variances = np.array([np.var(scores) if scores else 0.0 for scores in score_set.values()])\n\n    # Exploration bonus based on selection counts and score variance\n    explore_bonus = np.sqrt(np.log(total_selection_count + 1) / (normalized_counts + 1e-6)) + np.sqrt(score_variances)\n\n    # Combined score calculation\n    combined_scores = avg_scores + epsilon * explore_bonus\n\n    # Handle case where combined scores are tied\n    combined_scores_tiebreaker = combined_scores + np.random.rand(num_actions) * 1e-6 \n\n    # Select the action with the highest combined score\n    action_index = np.argmax(combined_scores_tiebreaker)\n\n    return action_index",
          "objective": -218.37827003041863,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores while handling empty score lists\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    \n    # Dynamic exploration rate\n    min_epsilon = 0.05\n    max_epsilon = 0.8\n    epsilon = min_epsilon + (max_epsilon - min_epsilon) * (1 - (current_time_slot / total_time_slots))\n    \n    # Count selections for each action\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Ensure action selection\n    if total_selection_count == 0:\n        return np.random.randint(0, 8)\n    \n    # Select between exploration and exploitation\n    if np.random.rand() < epsilon:\n        # Explore: Adjust scores based on selection counts\n        exploration_scores = avg_scores + (1.0 / (1 + selection_counts))\n        action_index = np.argmax(exploration_scores)\n    else:\n        # Exploit: Pick best average score\n        action_index = np.argmax(avg_scores)\n    \n    return action_index",
          "objective": -217.83091251710863,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n\n    # Calculate average scores\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    \n    # Calculate selection counts\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Handle zero counts for normalization\n    normalized_counts = np.maximum(selection_counts, 1)\n\n    # Dynamic epsilon calculation\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    epsilon_decay_rate = (epsilon_max - epsilon_min) / total_time_slots\n    epsilon = max(epsilon_min, epsilon_max - epsilon_decay_rate * current_time_slot)\n\n    # Variance calculation\n    score_variances = np.array([np.var(scores) if scores else 0.0 for scores in score_set.values()])\n\n    # Exploration bonus based on selection counts and score variance\n    explore_bonus = np.sqrt(np.log(total_selection_count + 1) / normalized_counts) + np.sqrt(score_variances)\n\n    # Combined scores calculation\n    combined_scores = avg_scores + epsilon * explore_bonus\n\n    # Select the action with the highest combined score\n    action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": -217.77602280473968,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores and variances for each action\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    variances = np.array([np.var(scores) if len(scores) > 1 else 0.0 for scores in score_set.values()])\n    \n    # Calculate selection counts\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Dynamic epsilon-greedy parameters\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    epsilon_decay_rate = 0.95\n    epsilon = epsilon_max * (epsilon_decay_rate ** current_time_slot)\n    \n    # Explore bonus\n    explore_bonus = np.where(selection_counts == 0, np.inf, np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1)))\n    \n    # Combine scores with exploration bonus and variance consideration\n    combined_scores = avg_scores + epsilon * explore_bonus + (variances / (selection_counts + 1))\n    \n    # Select action based on combined scores\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -217.66429926777323,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores and handle empty score lists\n    avg_scores = {\n        action_index: (np.mean(scores) if scores else 0.0)\n        for action_index, scores in score_set.items()\n    }\n    \n    # Dynamic exploration rate with decay\n    min_epsilon = 0.05\n    max_epsilon = 0.8\n    epsilon_decay = max_epsilon - min_epsilon\n    epsilon = max(min_epsilon, max_epsilon * (1 - current_time_slot / total_time_slots))\n    \n    # Count selection frequency for each action\n    selection_counts = {action_index: len(scores) for action_index, scores in score_set.items()}\n    \n    # Ensure at least one action is selected if total_selection_count is zero\n    if total_selection_count == 0:\n        action_index = np.random.randint(0, 8)\n    else:\n        if np.random.rand() < epsilon:\n            # Explore by selecting actions with lower selection counts\n            exploration_scores = {\n                action_index: avg_scores[action_index] + (1 / (1 + selection_counts[action_index])) \n                for action_index in avg_scores\n            }\n            action_index = max(exploration_scores, key=exploration_scores.get)\n        else:\n            # Exploit by selecting the action with the highest average score\n            action_index = max(avg_scores, key=avg_scores.get)\n    \n    return action_index",
          "objective": -217.2425034151061,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores and selection counts\n    avg_scores = np.array([np.mean(scores) if scores else 0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Dynamic epsilon-greedy strategy\n    epsilon_initial = 1.0\n    epsilon_min = 0.05\n    epsilon_decay = (epsilon_initial - epsilon_min) / total_time_slots\n    epsilon = max(epsilon_min, epsilon_initial - (epsilon_decay * current_time_slot))\n    \n    # Ensure all actions are explored at least once\n    unselected = (total_selection_count - selection_counts) < 1\n    exploration_bonus = np.where(unselected, np.inf, np.sqrt(np.log(total_selection_count) / (selection_counts + 1)))\n    \n    # Combined scores considering both exploitation and exploration\n    combined_scores = avg_scores + epsilon * exploration_bonus\n    \n    # Select the action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -216.71155792855848,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    actions = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in actions])\n    \n    # Calculate exploration factor\n    exploration_factor = np.sqrt(np.log(total_selection_count + 1) / (np.array([len(score_set[action]) for action in actions]) + 1))\n    \n    # Dynamic epsilon-greedy strategy\n    epsilon_max = 0.9\n    epsilon_min = 0.1\n    decay_rate = total_time_slots / 10  # control decay rate\n    epsilon = epsilon_max * (1 - current_time_slot / decay_rate)\n    epsilon = max(epsilon, epsilon_min)  # ensure epsilon does not go below minimum\n    \n    # Random selection with probability epsilon\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(actions)\n    else:\n        combined_scores = scores + exploration_factor\n        action_index = actions[np.argmax(combined_scores)]\n    \n    return action_index",
          "objective": -216.53658631254964,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    \n    # Calculate selection counts\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Handle zero selection counts for exploration bonus calculation\n    normalized_counts = np.where(selection_counts > 0, selection_counts, 1)\n\n    # Dynamic epsilon calculation for exploration\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    epsilon_decay_rate = (epsilon_max - epsilon_min) / total_time_slots\n    epsilon = max(epsilon_min, epsilon_max - epsilon_decay_rate * current_time_slot)\n    \n    # Exploration bonus computation\n    explore_bonus = np.sqrt(np.log(total_selection_count + 1) / normalized_counts)\n\n    # Combined score calculation\n    combined_scores = avg_scores + epsilon * explore_bonus\n\n    # Select the action with the highest score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -216.44623062079285,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores for each action, handling cases with no historical scores\n    avg_scores = {\n        action_index: (np.mean(scores) if scores else 0.0)\n        for action_index, scores in score_set.items()\n    }\n\n    # Calculate exploration coefficients (epsilon-greedy)\n    epsilon = 0.1  # Exploration probability\n    exploration = np.random.rand() < epsilon\n\n    # Base scores for each action considering exploration\n    base_scores = {\n        action_index: avg_scores.get(action_index, 0.0)\n        for action_index in range(8)\n    }\n\n    # Utilize exploration if decided\n    if exploration:\n        # Select a random action for exploration\n        action_index = np.random.choice([action for action in range(8) if action in score_set])\n    else:\n        # Calculate total score with exploration term (UCB)\n        exploration_bonuses = {\n            action_index: np.sqrt(2 * np.log(total_selection_count + 1) / (len(score_set[action_index]) + 1))\n            if action_index in score_set and score_set[action_index] else np.inf\n            for action_index in range(8)\n        }\n        \n        combined_scores = {\n            action_index: base_scores[action_index] + exploration_bonuses[action_index]\n            for action_index in range(8)\n        }\n        \n        # Penalize scores based on the remaining time slots\n        time_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_scores = {\n            action_index: combined_scores[action_index] * time_factor\n            for action_index in range(8)\n        }\n\n        # Select the action with the highest adjusted score\n        action_index = max(adjusted_scores, key=adjusted_scores.get)\n\n    return action_index",
          "objective": -216.3164827125159,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Initialize scores and selection counts\n    avg_scores = np.array([np.mean(scores) if scores else 0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Calculate exploration factor\n    exploration_factor = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-6))  # Avoid division by zero\n    exploration_weight = 0.1  # Tuning parameter for exploration\n\n    # Calculate dynamic epsilon based on time slot\n    epsilon = max(0.1, 1.0 * (1 - (current_time_slot / total_time_slots)))\n    \n    # Combined score: balancing exploration and exploitation\n    combined_scores = avg_scores + (exploration_weight * epsilon * exploration_factor)\n\n    # Select action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -215.99529398861654,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores for each action, handling empty lists\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    \n    # Calculate selection counts\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n\n    # Dynamic epsilon-greedy parameters\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    decay_factor = 0.99\n    epsilon = epsilon_max * (decay_factor ** current_time_slot)\n\n    # Exploration bonus\n    explore_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n    explore_bonus[selection_counts == 0] = np.inf  # Infinite bonus for unselected actions\n\n    # Calculate adjusted scores\n    adjusted_scores = avg_scores + epsilon * explore_bonus\n\n    # Select the action with the highest adjusted score\n    action_index = np.argmax(adjusted_scores)\n    \n    return action_index",
          "objective": -215.72763960357693,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores for each action\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    \n    # Calculate selection counts\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Define exploration factor based on selection counts\n    explore_factor = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n    \n    # Dynamic epsilon calculation\n    epsilon_max = 0.9\n    epsilon_min = 0.05\n    epsilon_decay_rate = 0.99\n    epsilon = epsilon_min + (epsilon_max - epsilon_min) * (1 - current_time_slot / total_time_slots)\n    \n    # Combined score calculation\n    exploration_scores = (1 - epsilon) * avg_scores + epsilon * explore_factor\n    exploration_scores = np.nan_to_num(exploration_scores)  # Handle NaN values\n    \n    # Apply a penalty for actions with low average scores\n    score_penalty = np.clip(1 - avg_scores, 0, None)  # Penalty should not be negative\n    adjusted_scores = exploration_scores - score_penalty * 0.5  # Adjust by penalty factor\n    \n    # Select action based on adjusted scores\n    action_index = np.argmax(adjusted_scores)\n    \n    return action_index",
          "objective": -215.67173464190927,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores and selection counts for each action\n    avg_scores = np.array([np.mean(scores) if scores else 0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Dynamic epsilon calculation\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    epsilon_decay = 0.9\n    epsilon = max(epsilon_min, epsilon_max * (1 - current_time_slot / total_time_slots))\n\n    # Exploration bonus\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1))\n    \n    # Combined scores\n    combined_scores = avg_scores + epsilon * exploration_bonus\n    \n    # Select action index with the highest combined score\n    action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": -215.67082720897815,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Initialize average scores and selection counts\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n\n    # Dynamic epsilon calculation\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    exploration_rate = epsilon_min + (epsilon_max - epsilon_min) * (1 - current_time_slot / total_time_slots)\n\n    # Adjusting scores based on selection counts\n    normalized_avg_scores = avg_scores * (selection_counts / (total_selection_count + 1))\n    \n    # Exploration bonus, avoiding division by zero\n    explore_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n    explore_bonus[selection_counts == 0] = np.inf  # Allow for full exploration of untried actions\n    \n    # Combined scores with exploration factor\n    combined_scores = normalized_avg_scores + exploration_rate * explore_bonus\n    \n    # Action selection\n    action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": -215.61196821156562,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize parameters\n    min_epsilon = 0.1  # Minimum exploration factor\n    max_epsilon = 1.0  # Initial exploration factor\n    decay_rate = (max_epsilon - min_epsilon) / total_time_slots  # Epsilon decay rate\n    \n    # Calculate exploration rate\n    epsilon = max(min_epsilon, max_epsilon - decay_rate * current_time_slot)\n    \n    # Calculate average scores and count of selections\n    avg_scores = np.array([np.mean(scores) if scores else 0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Calculate exploration bonus based on selection counts to encourage less selected actions\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1))\n    \n    # Adjust scores with exploration bonus\n    adjusted_scores = avg_scores + (epsilon * exploration_bonus)\n    \n    # Select action with the highest adjusted score\n    action_index = np.argmax(adjusted_scores)\n    \n    return action_index",
          "objective": -215.25628986556953,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores for each action and handle empty score sets\n    avg_scores = np.array([\n        np.mean(score_set.get(action_index, [0.0])) \n        for action_index in range(8)\n    ])\n\n    # Calculate exploration bonuses using a more adaptive approach\n    exploration_bonuses = np.array([\n        np.sqrt(2 * np.log(total_selection_count + 1) / (len(score_set.get(action_index, [])) + 1)) \n        if total_selection_count > 0 else np.inf \n        for action_index in range(8)\n    ])\n\n    # Compute combined scores: a balance of average score and exploration bonus\n    combined_scores = avg_scores + exploration_bonuses\n\n    # Penalize scores based on the remaining time slots\n    time_factor = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = combined_scores * time_factor\n\n    # Select the action index with the highest adjusted score\n    action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": -214.85050915162498,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores for each action\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    \n    # Calculate selection counts\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Epsilon parameters\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    epsilon_decay_rate = 0.99\n    epsilon = epsilon_max * (1 - (current_time_slot / total_time_slots)) ** epsilon_decay_rate\n    epsilon = max(epsilon, epsilon_min) \n    \n    # Exploration bonus with log scaling\n    explore_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1))\n    explore_bonus[selection_counts == 0] = np.inf  # Assign infinite bonus for unselected actions\n    \n    # Calculate combined scores using a weighted approach\n    combined_scores = (1 - epsilon) * avg_scores + epsilon * explore_bonus\n\n    # Select action based on combined scores\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -214.19891682683934,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores and selection counts\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Dynamic epsilon-greedy parameters\n    epsilon_max = 1.0\n    epsilon_min = 0.05\n    epsilon_decay_rate = 0.9\n    epsilon = epsilon_min + (epsilon_max - epsilon_min) * (1 - (current_time_slot / total_time_slots) ** epsilon_decay_rate)\n    \n    # Exploration bonus\n    explore_bonus = np.where(selection_counts == 0, np.inf, np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1)))\n    \n    # Combined scores: exploitation and exploration balance\n    combined_scores = avg_scores + epsilon * explore_bonus\n\n    # Select action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -214.1569403546174,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize parameters\n    exploration_weight = 1.0  # Weight to favor exploration\n    exploitation_weight = 1.0  # Weight to favor exploitation\n    averages = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        avg_score = np.mean(scores) if scores else 0\n        selection_count = len(scores)\n        \n        # Calculate exploration component\n        exploration_value = (np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1))\n                             if selection_count > 0 else np.inf)\n        exploration_value *= (1 - (current_time_slot / total_time_slots))\n        \n        # Enhanced combined score\n        combined_score = (exploitation_weight * avg_score) + (exploration_weight * exploration_value)\n        averages.append(combined_score)\n    \n    # Select action with the highest combined score\n    action_index = np.argmax(averages)\n    \n    return action_index",
          "objective": -214.13821907976256,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores and counts\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Handle zero counts for normalization\n    normalized_counts = np.where(selection_counts > 0, selection_counts, 1)\n    \n    # Dynamic epsilon calculation\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    epsilon_decay_rate = (epsilon_max - epsilon_min) / total_time_slots\n    epsilon = max(epsilon_min, epsilon_max - epsilon_decay_rate * current_time_slot)\n\n    # Standard deviation for score variability\n    score_stddev = np.array([np.std(scores) if scores else 0.0 for scores in score_set.values()])\n    \n    # Exploration bonus calculation\n    explore_bonus = np.sqrt(np.log(total_selection_count + 1) / normalized_counts) + score_stddev\n\n    # Combined score calculation\n    combined_scores = avg_scores + epsilon * explore_bonus\n\n    # Select action with highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -214.11057369427922,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores and selection counts\n    avg_scores = np.array([np.mean(scores) if scores else 0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Handle edge case when total_selection_count is 0\n    if total_selection_count == 0:\n        return np.random.randint(num_actions)\n    \n    # Dynamic epsilon-greedy strategy\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    epsilon_decay = (epsilon_max - epsilon_min) / total_time_slots\n    epsilon = max(epsilon_min, epsilon_max - epsilon_decay * current_time_slot)\n    \n    # Exploration term based on counts\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1))\n    \n    # Combined scores\n    combined_scores = avg_scores + epsilon * exploration_bonus\n    \n    # Select the action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -214.07380008758025,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n\n    # Initialize parameters\n    avg_scores = np.array([np.mean(scores) if scores else 0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n\n    # Calculate epsilon with a minimum threshold and decreasing strategy\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    epsilon = max(epsilon_min, epsilon_max * (1 - (current_time_slot / total_time_slots)))\n\n    # Handle zero selection counts to avoid division by zero\n    adjusted_selection_counts = selection_counts + 1  # Avoid zero by adding 1\n\n    # Exploration factor using the square root of the logarithm\n    exploration_factor = np.sqrt(np.log(total_selection_count + 1) / adjusted_selection_counts)\n\n    # Combine scores with consideration of exploration\n    combined_scores = avg_scores + epsilon * exploration_factor\n\n    # Select the action with the highest combined score\n    action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": -213.63106613858054,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize parameters\n    epsilon = 1.0 - (total_selection_count / (total_time_slots + 1))\n    epsilon = max(0.1, min(epsilon, 1.0))  # Dynamic exploration factor limits\n    averages = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        avg_score = np.mean(scores) if scores else 0\n        selection_count = len(scores)\n        \n        # Avoid division by zero for less frequently selected actions\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else np.inf\n        \n        # Combine average score and exploration value\n        combined_score = avg_score + epsilon * exploration_value\n        \n        averages.append(combined_score)\n    \n    # Select action with the highest combined score\n    action_index = np.argmax(averages)\n    \n    return action_index",
          "objective": -213.61632345873284,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores\n    avg_scores = np.array([np.mean(scores) if scores else 0 for scores in score_set.values()])\n    \n    # Calculate selection counts\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Handle edge case when total_selection_count is 0\n    if total_selection_count == 0:\n        return np.random.randint(num_actions)\n    \n    # Epsilon decay strategy\n    epsilon_max = 1.0\n    epsilon_min = 0.05\n    decay_rate = 0.01\n    epsilon = max(epsilon_min, epsilon_max * (1 - decay_rate * current_time_slot))\n    \n    # Exploration term with added stability\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n    \n    # Combined scores\n    combined_scores = avg_scores + epsilon * exploration_bonus\n    \n    # Select the action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -213.47274648836844,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores and selection counts\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n\n    # Calculate the standard deviation for each action\n    score_stddev = np.array([np.std(scores) if len(scores) > 1 else 0.0 for scores in score_set.values()])\n\n    # Epsilon decay parameters\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    epsilon_decay_rate = 0.98\n    epsilon = epsilon_min + (epsilon_max - epsilon_min) * np.exp(-epsilon_decay_rate * (current_time_slot / total_time_slots))\n    \n    # Exploration bonus using standard deviation for uncertainty\n    explore_bonus = np.where(selection_counts > 0, np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1)), np.inf)\n\n    # Calculate combined scores incorporating uncertainty\n    combined_scores = avg_scores + epsilon * explore_bonus - score_stddev\n    \n    # Select action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -213.14726934088392,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores for each action with safety for zero\n    avg_scores = {\n        action_index: (np.mean(scores) if scores else 0.0)\n        for action_index, scores in score_set.items()\n    }\n    \n    # Calculate exploration rate (epsilon) inversely proportional to current_time_slot\n    min_epsilon = 0.05  # Minimum exploration probability\n    max_epsilon = 0.8   # Maximum exploration probability\n    epsilon = min_epsilon + (max_epsilon - min_epsilon) * (1 - (current_time_slot / total_time_slots))\n    \n    # Initialize exploration vs. exploitation decision\n    if np.random.rand() < epsilon:\n        # Explore: Add a small bonus for less tried actions\n        exploration_scores = {\n            action_index: avg_scores[action_index] + (1 / (1 + len(scores))) for action_index, scores in score_set.items()\n        }\n        action_index = max(exploration_scores, key=exploration_scores.get)\n    else:\n        # Exploit: Select action with the highest average score\n        action_index = max(avg_scores, key=avg_scores.get)\n\n    return action_index",
          "objective": -212.93933396332244,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores of each action\n    avg_scores = {\n        action_index: (np.mean(scores) if scores else 0.0)\n        for action_index, scores in score_set.items()\n    }\n    \n    # Dynamic exploration rate with smoothing factor\n    min_epsilon = 0.05\n    max_epsilon = 0.8\n    epsilon = max_epsilon * (1 - (current_time_slot / total_time_slots)) + min_epsilon\n    \n    # Count selections\n    selection_counts = {action_index: len(scores) for action_index, scores in score_set.items()}\n    \n    # Ensure all actions are explored at least once if it's the first round\n    if total_selection_count == 0:\n        return np.random.randint(0, 8)\n    \n    # Action selection based on exploration versus exploitation\n    if np.random.rand() < epsilon:\n        # Exploration: Prefer actions with fewer selections\n        exploration_scores = {\n            action_index: avg_scores[action_index] + (1 / (1 + selection_counts[action_index])) for action_index in avg_scores\n        }\n        action_index = max(exploration_scores, key=exploration_scores.get)\n    else:\n        # Exploitation: Prefer the action with the highest average score\n        action_index = max(avg_scores, key=avg_scores.get)\n    \n    return action_index",
          "objective": -211.89907011743784,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores and selection counts\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Dynamic epsilon-greedy parameters\n    epsilon_max = 1.0\n    epsilon_min = 0.05\n    epsilon_decay_factor = 0.95\n    epsilon = epsilon_max * (epsilon_decay_factor ** current_time_slot)\n    epsilon = max(epsilon, epsilon_min)\n    \n    # Exploration bonus\n    explore_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n    \n    # Combined scores: exploitation and exploration balance\n    combined_scores = avg_scores + epsilon * explore_bonus\n    \n    # Select action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -211.21781289394582,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores and selection counts\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Set exploration parameters\n    epsilon_initial = 1.0\n    epsilon_min = 0.05\n    epsilon_decay = 0.95\n    epsilon = max(epsilon_min, epsilon_initial * (epsilon_decay ** current_time_slot))\n    \n    # Exploration bonus\n    unexplored = selection_counts == 0\n    explore_bonus = np.where(unexplored, np.inf, np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1)))\n    \n    # Combined scores\n    combined_scores = avg_scores + epsilon * explore_bonus\n    \n    # Select the action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -211.20709131353993,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores and selection counts\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Dynamic epsilon computation\n    min_exploration_rate = 0.1\n    exploration_rate = max(min_exploration_rate, 1 - (current_time_slot / total_time_slots))\n\n    # Handling selection counts to avoid division by zero\n    explore_bonus = np.where(selection_counts > 0,\n                              np.sqrt(np.log(total_selection_count) / selection_counts),\n                              np.inf)\n    \n    # Combine exploitation and exploration\n    combined_scores = avg_scores + exploration_rate * explore_bonus\n\n    # Select action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -210.88074297665952,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize parameters\n    exploration_factor = max(0, (total_time_slots - current_time_slot) / total_time_slots)\n    epsilon = 1.0 / (current_time_slot + 1)  # Decrease exploration with time\n    \n    # Calculate averages, selection counts, and UCB values\n    averages = []\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        avg_score = np.mean(scores) if scores else 0\n        selection_count = len(scores)\n        \n        # Calculate UCB\n        exploration_value = (exploration_factor * np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1))) if selection_count > 0 else np.inf\n        combined_score = avg_score + (epsilon * exploration_value)\n        \n        averages.append(combined_score)\n    \n    # Select action with the highest combined score\n    action_index = np.argmax(averages)\n    \n    return action_index",
          "objective": -210.7074015336223,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    avg_scores = np.array([np.mean(scores) if scores else 0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n\n    # Dynamic epsilon decay for exploration\n    epsilon_initial = 1.0\n    epsilon_min = 0.05\n    epsilon_decay = (epsilon_initial - epsilon_min) / total_time_slots\n    epsilon = max(epsilon_min, epsilon_initial - (epsilon_decay * current_time_slot))\n    \n    # Calculate exploration bonus\n    explore_bonus = np.where(selection_counts == 0, np.inf, np.sqrt(np.log(total_selection_count) / (selection_counts + 1)))\n    \n    # Combined scores with exploration\n    combined_scores = avg_scores + epsilon * explore_bonus\n\n    # Select the action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -209.8919835009899,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize parameters\n    action_count = 8\n    epsilon = 0.1  # Epsilon for epsilon-greedy exploration\n    averages = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n    \n    # Calculate averages and selection counts\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        averages[action_index] = np.mean(scores) if scores else 0\n        selection_counts[action_index] = len(scores)\n    \n    # Dynamic exploration factor based on selection counts\n    exploration_values = np.zeros(action_count)\n    for action_index in range(action_count):\n        if selection_counts[action_index] == 0:\n            exploration_values[action_index] = np.inf  # Prioritize unselected actions\n        else:\n            exploration_values[action_index] = np.sqrt(np.log(total_selection_count) / selection_counts[action_index])\n\n    # Adjust exploration based on current time slot\n    exploration_factors = exploration_values * (1 - (current_time_slot / total_time_slots))\n\n    # Combined score using epsilon-greedy approach\n    probabilities = np.random.rand(action_count)\n    if np.any(probabilities < epsilon):\n        action_index = np.random.choice(np.arange(action_count)[probabilities < epsilon])\n    else:\n        combined_scores = averages + exploration_factors\n        action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -209.84525270292662,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores and selection counts while handling empty scores\n    avg_scores = np.array([np.mean(scores) if scores else 0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n\n    # Prevent division by zero\n    selection_counts = np.where(selection_counts == 0, 1, selection_counts)\n    \n    # Dynamic epsilon-greedy strategy\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    epsilon = epsilon_min + (epsilon_max - epsilon_min) * (1 - (current_time_slot / total_time_slots))\n    \n    # Exploration term with log scaling\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / selection_counts)\n    \n    # Combined scores considering exploration\n    combined_scores = avg_scores + epsilon * exploration_bonus\n    \n    # Select the action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -209.83279353561676,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize parameters\n    exploration_weight = 1.5  # Adjusted weight for exploration\n    exploitation_weight = 1.0  # Weight for exploitation\n    averages = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        avg_score = np.mean(scores) if scores else 0\n        selection_count = len(scores)\n        \n        # Calculate exploration component with a dynamic damping factor\n        confidence_interval = (np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1))\n                               if selection_count > 0 else np.inf)\n        damping_factor = 1 - (current_time_slot / total_time_slots)\n        exploration_value = confidence_interval * damping_factor\n        \n        # Combined score\n        combined_score = (exploitation_weight * avg_score) + (exploration_weight * exploration_value)\n        averages.append(combined_score)\n    \n    # Select action with the highest combined score\n    action_index = np.argmax(averages)\n    \n    return action_index",
          "objective": -209.81603512033573,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores while handling empty score lists\n    avg_scores = np.array([np.mean(scores) if scores else 0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Handle the case when total_selection_count is 0\n    if total_selection_count == 0:\n        return np.random.randint(num_actions)\n\n    # Dynamic epsilon-greedy strategy\n    epsilon_start = 1.0\n    epsilon_end = 0.05\n    epsilon_decay = (epsilon_start - epsilon_end) * (current_time_slot / total_time_slots)\n    epsilon = max(epsilon_end, epsilon_start - epsilon_decay)\n\n    # Exploration term (UCB style)\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n    \n    # Combined scores\n    combined_scores = avg_scores + epsilon * exploration_bonus\n\n    # Select the action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -209.2005806186782,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize exploration rate parameters\n    epsilon_initial = 1.0\n    epsilon_final = 0.01\n    epsilon_decay = 0.99\n    \n    # Calculate epsilon based on current time slot\n    epsilon = max(epsilon_final, epsilon_initial * (epsilon_decay ** current_time_slot))\n    \n    # Calculate average scores for each action\n    avg_scores = {\n        action_index: (np.mean(scores) if scores else 0.0)\n        for action_index, scores in score_set.items()\n    }\n\n    # Generate a random number to decide whether to exploit or explore\n    if np.random.rand() < epsilon:\n        # Exploration: randomly select an action\n        action_index = np.random.choice(list(score_set.keys()))\n    else:\n        # Exploitation: select the action with the highest average score\n        action_index = max(avg_scores, key=avg_scores.get)\n    \n    return action_index",
          "objective": -209.15627345934206,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 1.0 / (current_time_slot + 1)  # Epsilon decreases with time\n    avg_scores = {action_index: (np.mean(scores) if scores else 0.0) for action_index, scores in score_set.items()}\n\n    # Initialized count for exploration\n    exploration_counts = {action_index: len(scores) for action_index, scores in score_set.items()}\n    \n    # UCB calculation\n    exploration_bonuses = {\n        action_index: np.sqrt(2 * np.log(total_selection_count) / (n_sq + 1)) if n_sq > 0 else np.inf\n        for action_index, n_sq in exploration_counts.items()\n    }\n\n    # Combined scores using epsilon-greedy approach\n    combined_scores = {}\n    for action_index in range(8):\n        combined_scores[action_index] = (\n            (1 - epsilon) * avg_scores.get(action_index, 0) + \n            epsilon * (exploration_bonuses.get(action_index, 0) + avg_scores.get(action_index, 0))\n        )\n\n    # Penalize scores for remaining time slots\n    time_factor = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = {action_index: score * time_factor for action_index, score in combined_scores.items()}\n\n    # Select the action index with the highest adjusted score\n    action_index = max(adjusted_scores, key=adjusted_scores.get)\n    \n    return action_index",
          "objective": -209.1531421552163,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores with error handling for empty score lists\n    avg_scores = np.array([np.mean(scores) if scores else 0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n\n    # Initialize exploration parameters\n    min_epsilon = 0.1\n    max_epsilon = 1.0\n    epsilon_decay = (max_epsilon - min_epsilon) / total_time_slots\n    current_epsilon = max(min_epsilon, max_epsilon - epsilon_decay * current_time_slot)\n    \n    # Randomly decide to explore or exploit\n    if np.random.rand() < current_epsilon:\n        action_index = np.random.randint(num_actions)\n    else:\n        # Avoid division by zero in selection counts\n        adjusted_counts = selection_counts + 1e-5\n        \n        # Calculate scores including a confidence term\n        confidence_bonus = np.sqrt(np.log(total_selection_count + 1) / adjusted_counts)\n        combined_scores = avg_scores + confidence_bonus\n        \n        action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -208.98325757590894,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in action_indices])\n    \n    # Handle selection counts to avoid division by zero\n    selection_counts = np.array([len(score_set[action]) for action in action_indices])\n    selection_counts = np.clip(selection_counts, 1, None)  # Ensure counts are at least 1\n\n    # Epsilon decay for exploration\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n    \n    # Randomly select an action for exploration with probability epsilon\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploitation based on average scores\n        adjusted_scores = scores / selection_counts\n        action_index = action_indices[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -208.6758838152624,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores for each action\n    avg_scores = np.array([np.mean(scores) if len(scores) > 0 else 0.0 for scores in score_set.values()])\n    \n    # Calculate selection counts\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Avoid division by zero for explore bonus\n    selection_counts = np.where(selection_counts == 0, 1, selection_counts)\n    \n    # Exploration bonus calculation\n    explore_bonus = np.sqrt(np.log(total_selection_count + 1) / selection_counts)\n\n    # Dynamic epsilon calculation based on time\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    epsilon_decay_rate = 0.95\n    remaining_time_fraction = (total_time_slots - current_time_slot) / total_time_slots\n    epsilon = epsilon_min + (epsilon_max - epsilon_min) * (remaining_time_fraction ** epsilon_decay_rate)\n\n    # Combined score calculation\n    combined_scores = (1 - epsilon) * avg_scores + epsilon * explore_bonus\n\n    # Select action based on combined scores\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -207.75674152472226,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    \n    # Calculate selection counts\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Adaptive epsilon calculation\n    epsilon_max = 1.0\n    epsilon_min = 0.05\n    epsilon_decay_rate = 0.1\n    epsilon = max(epsilon_min, epsilon_max * np.exp(-epsilon_decay_rate * current_time_slot / total_time_slots))\n    \n    # Exploration bonus calculation\n    explore_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1)) if total_selection_count > 0 else np.ones(num_actions)\n    \n    # Combined scores: exploitation and exploration balance\n    combined_scores = avg_scores + epsilon * explore_bonus\n\n    # Select action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -207.7165588849357,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Initialize averages and selection counts\n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    for action_index, scores in score_set.items():\n        if scores:\n            avg_scores[action_index] = np.mean(scores)\n            selection_counts[action_index] = len(scores)\n\n    # Dynamic epsilon-greedy parameters\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    epsilon_decay_rate = 0.9\n    epsilon = epsilon_min + (epsilon_max - epsilon_min) * (1 - (current_time_slot / total_time_slots))\n\n    # Confidence interval calculation\n    confidence_intervals = np.where(selection_counts > 0, \n                                      np.sqrt((2 * np.log(total_selection_count + 1)) / selection_counts), \n                                      np.inf)\n    \n    # Calculate combined expected values\n    combined_scores = avg_scores + epsilon * confidence_intervals\n\n    # Select action based on combined scores\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -207.59211352158135,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize parameters\n    exploration_weight = 1.5  # Weight to favor exploration\n    exploitation_weight = 1.0  # Weight to favor exploitation\n    averages = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        avg_score = np.mean(scores) if scores else 0.0\n        selection_count = len(scores)\n\n        # Calculate exploitation component\n        exploitation_value = avg_score\n        \n        # Calculate exploration component\n        exploration_value = 0\n        if selection_count > 0:\n            exploration_value = np.sqrt(np.log(total_selection_count) / selection_count)\n        else:\n            exploration_value = np.inf  # Favor actions that have never been selected\n        \n        # Temporal adjustment to exploration\n        time_decay = 1 - (current_time_slot / total_time_slots)\n        exploration_value *= time_decay\n        \n        # Combined score with a balance between exploitation and exploration\n        combined_score = (exploitation_weight * exploitation_value) + (exploration_weight * exploration_value)\n        averages.append(combined_score)\n    \n    # Select action with the highest combined score\n    action_index = np.argmax(averages)\n    \n    return action_index",
          "objective": -207.4794170321977,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores with handling empty score lists\n    avg_scores = {action_index: np.mean(scores) if scores else 0.0 for action_index, scores in score_set.items()}\n    \n    # Calculate action selection counts\n    selection_counts = {action_index: len(scores) for action_index, scores in score_set.items()}\n    \n    # Dynamic epsilon based on current time slot\n    min_epsilon = 0.05\n    max_epsilon = 0.8\n    epsilon_decay = (max_epsilon - min_epsilon) * (current_time_slot / total_time_slots)\n    epsilon = max(min_epsilon, max_epsilon - epsilon_decay)\n    \n    # UCB-based exploration scores\n    exploration_scores = {\n        action_index: avg_scores[action_index] + np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts[action_index] + 1)) \n        if selection_counts[action_index] > 0 else float('inf') \n        for action_index in score_set\n    }\n\n    # Calculate combined scores promoting above-average actions\n    above_average_actions = [index for index, score in avg_scores.items() if score >= np.mean(list(avg_scores.values()))]\n    \n    # Weighted selection\n    weighted_scores = {}\n    for action_index in score_set:\n        exploitation_score = avg_scores[action_index] * (1 - epsilon)\n        exploration_score = exploration_scores[action_index] * epsilon\n        weighted_scores[action_index] = exploitation_score + exploration_score\n\n    # Select the action with the highest combined score\n    action_index = max(above_average_actions, key=lambda idx: weighted_scores[idx]) if above_average_actions else max(weighted_scores, key=weighted_scores.get)\n    \n    return action_index",
          "objective": -207.00731223033392,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize average scores and selection counts\n    avg_scores = {action_index: (np.mean(scores) if scores else 0.0) for action_index, scores in score_set.items()}\n    selection_counts = {action_index: len(scores) for action_index, scores in score_set.items()}\n    \n    # Dynamic exploration rate\n    min_epsilon = 0.05\n    max_epsilon = 0.8\n    epsilon = min_epsilon + (max_epsilon - min_epsilon) * (1 - (current_time_slot / total_time_slots))\n    \n    # Ensure exploration if no action has been selected\n    if total_selection_count == 0:\n        return np.random.randint(0, 8)\n    \n    # Exploration and exploitation\n    if np.random.rand() < epsilon:\n        # Normalize selection counts for exploration bonus\n        exploration_scores = {\n            action_index: avg_scores[action_index] + (1 / (1 + selection_counts[action_index])) \n            for action_index in avg_scores\n        }\n        action_index = max(exploration_scores, key=exploration_scores.get)\n    else:\n        # Exploit based on average scores\n        action_index = max(avg_scores, key=avg_scores.get)\n\n    return action_index",
          "objective": -206.50364549827842,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores for each action\n    avg_scores = {\n        action_index: (np.mean(scores) if scores else 0.0)\n        for action_index, scores in score_set.items()\n    }\n\n    # Define epsilon and gradually decrease it over time slots\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    epsilon_decay = (epsilon_max - epsilon_min) / total_time_slots\n\n    # Calculate current epsilon value\n    epsilon = max(epsilon_min, epsilon_max - epsilon_decay * current_time_slot)\n\n    # Explore vs exploit decision\n    if np.random.rand() < epsilon:\n        # Exploration: randomly select an action\n        action_index = np.random.choice(list(range(8)))\n    else:\n        # Exploitation: select the best performing action based on average score\n        action_index = max(avg_scores, key=avg_scores.get)\n\n    return action_index",
          "objective": -206.18270789350294,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores, with a handle for cases where no scores exist\n    avg_scores = {\n        action_index: (np.mean(scores) if scores else 0.0)\n        for action_index, scores in score_set.items()\n    }\n    \n    # Calculate selection counts\n    selection_counts = {action_index: len(scores) for action_index, scores in score_set.items()}\n    \n    # Dynamic epsilon for exploration\n    min_epsilon = 0.05\n    max_epsilon = 0.8\n    epsilon_decay = max_epsilon * (1 - current_time_slot / total_time_slots)\n    epsilon = max(min_epsilon, epsilon_decay)\n    \n    if total_selection_count == 0:\n        # Random selection for the first action\n        action_index = np.random.randint(0, 8)\n    else:\n        if np.random.rand() < epsilon:\n            # Exploration: favor actions with lower selection counts\n            exploration_scores = {\n                action_index: avg_scores[action_index] + (1 / (1 + selection_counts[action_index])) \n                for action_index in avg_scores\n            }\n            action_index = max(exploration_scores, key=exploration_scores.get)\n        else:\n            # Exploitation: select the action with the highest average score\n            action_index = max(avg_scores, key=avg_scores.get)\n\n    return action_index",
          "objective": -206.01633326439654,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Initialize parameters\n    avg_scores = np.array([np.mean(scores) if scores else 0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Calculate epsilon with a minimum threshold\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    epsilon = max(epsilon_min, epsilon_max * (1 - (current_time_slot / total_time_slots)))\n\n    # Exploration factor using total selection count\n    exploration_factor = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1))\n    \n    # Combined scores considering both exploitation and exploration\n    combined_scores = avg_scores + (epsilon * exploration_factor)\n    \n    # Select action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -205.7624858497254,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores, managing potential empty score lists\n    avg_scores = {action_index: np.mean(scores) if scores else 0.0 for action_index, scores in score_set.items()}\n\n    # Dynamic exploration rate based on total selections\n    min_epsilon = 0.05\n    max_epsilon = 0.8\n    decay_factor = min(1, (total_selection_count / (total_time_slots * 10)))  # Normalize selection count\n    epsilon = max(min_epsilon, max_epsilon * (1 - decay_factor))\n\n    # Count selections for each action\n    selection_counts = {action_index: len(scores) for action_index, scores in score_set.items()}\n\n    # Ensure selection counts do not lead to division by zero\n    selection_counts = {action_index: count if count > 0 else 1 for action_index, count in selection_counts.items()}\n    \n    # UCB-based exploration\n    exploration_scores = {\n        action_index: avg_scores[action_index] + np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts[action_index])\n        for action_index in avg_scores\n    }\n\n    # Action selection based on epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        action_index = max(exploration_scores, key=exploration_scores.get)\n    else:\n        action_index = max(avg_scores, key=avg_scores.get)\n\n    return action_index",
          "objective": -205.50662630471788,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores for each action, while handling empty score lists\n    avg_scores = {\n        action_index: (np.mean(scores) if scores else 0.0)\n        for action_index, scores in score_set.items()\n    }\n    \n    # Calculate selection frequency for each action\n    selection_counts = {action_index: len(scores) for action_index, scores in score_set.items()}\n    \n    # Dynamic exploration rate - decays over time with a base level\n    min_epsilon = 0.05\n    max_epsilon = 0.8\n    epsilon = min_epsilon + (max_epsilon - min_epsilon) * (1 - (current_time_slot / total_time_slots))\n    \n    # If no action has been tried, select randomly\n    if total_selection_count == 0:\n        action_index = np.random.randint(0, 8)\n    else:\n        # Compute exploration bonus based on the number of selections per action\n        exploration_bonus = {\n            action_index: (1 / (1 + selection_counts[action_index])) for action_index in avg_scores\n        }\n        \n        # Combine average score and exploration bonus\n        combined_scores = {\n            action_index: avg_scores[action_index] + (exploration_bonus[action_index] * epsilon) \n            for action_index in avg_scores\n        }\n        \n        # Select action based on the highest combined score\n        action_index = max(combined_scores, key=combined_scores.get)\n    \n    return action_index",
          "objective": -204.86346217013246,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n\n    # Calculate average scores and selection counts\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n\n    # Dynamic epsilon calculation\n    epsilon_max = 1.0\n    epsilon_min = 0.05\n    epsilon_decay_rate = 0.9\n    epsilon = epsilon_min + (epsilon_max - epsilon_min) * np.exp(-epsilon_decay_rate * current_time_slot / total_time_slots)\n\n    # Safe exploration bonus calculation\n    explore_bonus = np.zeros(num_actions)\n    with np.errstate(divide='ignore', invalid='ignore'):\n        explore_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1))\n    \n    # Normalize explore bonus to prevent bias from higher scores\n    explore_bonus = np.where(selection_counts == 0, np.inf, explore_bonus)\n    \n    # Combined scores to prioritize exploration and exploitation\n    combined_scores = avg_scores + epsilon * explore_bonus\n\n    # Select action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -204.8179115899199,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores, treating empty score lists as 0.0\n    avg_scores = {action_index: np.mean(scores) if scores else 0.0 for action_index, scores in score_set.items()}\n    \n    # Dynamic exploration rate based on current time slot\n    min_epsilon = 0.05\n    max_epsilon = 0.8\n    decay_rate = current_time_slot / total_time_slots\n    epsilon = max(min_epsilon, max_epsilon * (1 - decay_rate))\n    \n    # Count selections for each action\n    selection_counts = {action_index: len(scores) for action_index, scores in score_set.items()}\n    \n    # Compute the UCB exploration term\n    exploration_scores = {\n        action_index: (avg_scores[action_index] + \n                       np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts[action_index] + 1))\n                       if selection_counts[action_index] > 0 else float('inf'))\n        for action_index in avg_scores\n    }\n    \n    # Choose action based on exploration or exploitation\n    if np.random.rand() < epsilon:\n        action_index = max(exploration_scores, key=exploration_scores.get)\n    else:\n        action_index = max(avg_scores, key=avg_scores.get)\n\n    return action_index",
          "objective": -204.75279645902805,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    min_epsilon = 0.05  # Minimum exploration rate\n    initial_epsilon = 0.5  # Starting exploration rate\n    decay_rate = 0.005  # Decay rate for epsilon\n\n    # Calculate averages and selection counts\n    averages = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        averages[action_index] = np.mean(scores) if scores else 0\n\n    # Calculate dynamic epsilon\n    epsilon = max(min_epsilon, initial_epsilon - decay_rate * current_time_slot)\n    \n    # Exploration term based on selection counts\n    exploration_values = np.zeros(action_count)\n    for action_index in range(action_count):\n        if selection_counts[action_index] == 0:\n            exploration_values[action_index] = np.inf  # Encourage exploration of unselected actions\n        else:\n            exploration_values[action_index] = np.sqrt(np.log(total_selection_count) / selection_counts[action_index])\n\n    # Calculate scores combining averages and exploration values\n    combined_scores = averages + exploration_values\n\n    # Select action based on epsilon-greedy strategy\n    probabilities = np.random.rand(action_count)\n    if np.any(probabilities < epsilon):\n        action_index = np.random.choice(np.arange(action_count)[probabilities < epsilon])\n    else:\n        action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": -204.61069923730298,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores and selection counts\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Set exploration parameters\n    epsilon_initial = 1.0\n    epsilon_min = 0.05\n    epsilon_decay = 0.999\n    epsilon = max(epsilon_min, epsilon_initial * (epsilon_decay ** current_time_slot))\n    \n    # Exploration factor: ensure all actions are selected at least once\n    explore_bonus = np.where(selection_counts == 0, np.inf, np.sqrt(np.log(total_selection_count) / (selection_counts + 1)))\n    \n    # Combined scores considering exploration\n    combined_scores = avg_scores + epsilon * explore_bonus\n    \n    # Select the action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -203.79170329913387,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores handling empty lists safely\n    avg_scores = np.array([np.mean(scores) if scores else 0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Dynamic epsilon-greedy strategy\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    epsilon_decay = 1 - (current_time_slot / total_time_slots)\n    epsilon = epsilon_min + (epsilon_max - epsilon_min) * epsilon_decay\n    \n    # Exploration bonus based on selection counts\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1))\n    \n    # Combine average scores and exploration incentives\n    combined_scores = avg_scores + epsilon * exploration_bonus\n    \n    # Ensure at least some exploration\n    exploration_threshold = epsilon * np.random.rand(num_actions)\n    combined_scores += exploration_threshold\n    \n    # Select the action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -203.75782121016366,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores, handling zero-length lists\n    avg_scores = np.array([np.mean(scores) if scores else 0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Dynamic epsilon for exploration\n    epsilon_initial = 1.0\n    epsilon_min = 0.1\n    epsilon_decay = 0.98\n    epsilon = max(epsilon_min, epsilon_initial * (epsilon_decay ** current_time_slot))\n\n    # Exploration term based on selection counts\n    # To encourage exploration, we add a bonus for less-explored actions\n    bonus_exploration = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1))\n    exploration_bonus = np.where(selection_counts == 0, np.inf, bonus_exploration)\n    \n    # Combined scores to favor high averages with exploration consideration\n    combined_scores = avg_scores + epsilon * exploration_bonus\n    \n    # Select the action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -203.7553316298006,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores for each action and handle possible empty lists\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    \n    # Calculate selection counts\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n\n    # Compute exploration parameters\n    max_exploration = 1.0\n    min_exploration = 0.1\n    exploration_decay = 0.95\n    exploration_factor = max_exploration - (max_exploration - min_exploration) * (current_time_slot / total_time_slots) ** exploration_decay\n    \n    # Avoid division by zero and apply exploration bonus\n    explore_bonus = np.where(selection_counts > 0, \n                              np.sqrt(np.log(total_selection_count) / selection_counts), \n                              np.inf)\n    \n    # Calculate combined scores\n    combined_scores = avg_scores + exploration_factor * explore_bonus\n\n    # Select action based on combined scores\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -203.06642222571986,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    \n    # Calculate selection counts\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Avoid division by zero\n    normalized_counts = np.where(selection_counts > 0, selection_counts, 1)\n    \n    # Dynamic epsilon calculation\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    epsilon_decay_rate = (epsilon_max - epsilon_min) / total_time_slots\n    epsilon = max(epsilon_min, epsilon_max - epsilon_decay_rate * current_time_slot)\n\n    # Exploration bonus based on normalized counts and the average score's variance\n    variances = np.array([np.var(scores) if len(scores) > 1 else 0.0 for scores in score_set.values()])\n    explore_bonus = np.sqrt(np.log(total_selection_count + 1) / normalized_counts) + variances\n\n    # Combined scores calculation\n    combined_scores = avg_scores + epsilon * explore_bonus\n\n    # Select the action with the highest combined score\n    action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": -202.61444030732935,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores and selection counts\n    avg_scores = np.array([np.mean(scores) if scores else 0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Set exploration parameters\n    epsilon_initial = 1.0\n    epsilon_min = 0.05\n    epsilon_decay = 0.999\n    epsilon = max(epsilon_min, epsilon_initial * (epsilon_decay ** current_time_slot))\n    \n    # Explore every action at least once\n    explore_bonus = (total_selection_count - selection_counts) < 1\n    \n    # Update bonus for exploration: add a large value if action was never selected\n    exploration_bonus = np.where(explore_bonus, np.inf, np.sqrt(np.log(total_selection_count) / (selection_counts + 1)))\n    \n    # Combined scores considering exploration\n    combined_scores = avg_scores + epsilon * exploration_bonus\n    \n    # Select the action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -202.20327543363305,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in action_indices])\n    \n    selection_counts = np.array([len(score_set[action]) for action in action_indices])\n    selection_counts = np.clip(selection_counts, 1, None)\n\n    # Upper confidence bound (UCB) calculation\n    confidence_bounds = np.sqrt(np.log(total_selection_count + 1) / selection_counts)\n    ucb_scores = scores + confidence_bounds\n\n    # Epsilon decay for exploration\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n\n    # Randomly select an action for exploration with probability epsilon\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = action_indices[np.argmax(ucb_scores)]\n\n    return action_index",
          "objective": -201.9366303219093,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores for each action while handling empty score lists\n    avg_scores = {\n        action_index: (np.mean(scores) if scores else 0.0)\n        for action_index, scores in score_set.items()\n    }\n    \n    # Dynamic exploration rate - decays with time but retains a minimum\n    min_epsilon = 0.05\n    max_epsilon = 0.8\n    epsilon = min_epsilon + (max_epsilon - min_epsilon) * (1 - (current_time_slot / total_time_slots))\n    \n    # Calculate selection frequency for each action\n    selection_counts = {action_index: len(scores) for action_index, scores in score_set.items()}\n    \n    # If no action has been tried, ensure exploration\n    if total_selection_count == 0:\n        action_index = np.random.randint(0, 8)\n    else:\n        # Select between exploration and exploitation\n        if np.random.rand() < epsilon:\n            # Explore: Encourage less tried actions\n            exploration_scores = {\n                action_index: avg_scores[action_index] + (1 / (1 + selection_counts[action_index])) for action_index in avg_scores\n            }\n            action_index = max(exploration_scores, key=exploration_scores.get)\n        else:\n            # Exploit: Select action with highest average score\n            action_index = max(avg_scores, key=avg_scores.get)\n    \n    return action_index",
          "objective": -201.90745547900787,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Compute average scores with handling for empty score lists\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    \n    # Count of selections for each action\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Dynamic epsilon for exploration-exploitation balance\n    epsilon_max = 1.0\n    epsilon_min = 0.05\n    epsilon_decay_rate = 0.9\n    epsilon = epsilon_min + (epsilon_max - epsilon_min) * (current_time_slot / total_time_slots) ** epsilon_decay_rate\n    \n    # Exploration bonus using uncertainty principle\n    explore_bonus = np.where(selection_counts > 0, np.sqrt(np.log(total_selection_count) / selection_counts), np.inf)\n    \n    # Combine scores for decision making\n    combined_scores = avg_scores + epsilon * explore_bonus\n    \n    # Select action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -201.51335438541167,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores for each action\n    avg_scores = np.array([np.mean(scores) if scores else 0 for scores in score_set.values()])\n    \n    # Calculate the number of selections for each action\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Epsilon value for exploration, with a decay that retains a minimum exploration\n    epsilon = max(0.1, 1.0 - (current_time_slot / total_time_slots))\n    \n    # Exploration adjustment based on selection counts to reduce bias towards frequently selected actions\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1))\n    \n    # Combine average scores with exploration bonus and epsilon for decision making\n    combined_scores = avg_scores + (epsilon * exploration_bonus)\n    \n    # Select action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -201.32211345252242,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in action_indices])\n    selection_counts = np.array([len(score_set[action]) for action in action_indices])\n\n    # Dynamic exploration factor based on time slot\n    decay_rate = 0.1\n    exploration_factor = np.exp(-decay_rate * (total_time_slots - current_time_slot))\n\n    # Prevent division by zero and apply logarithmic scaling\n    selection_counts = np.clip(selection_counts, 1, None)  # Ensure counts are at least 1\n    exploration_scores = np.sqrt(exploration_factor * np.log(total_selection_count + 1) / selection_counts)\n\n    # Combine scores with exploration weights\n    adjusted_scores = scores + exploration_scores\n\n    # Select action with the highest adjusted score\n    action_index = action_indices[np.argmax(adjusted_scores)]\n\n    return action_index",
          "objective": -201.03216483841126,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores and selection counts\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n\n    # Variance calculation for each action\n    variances = np.array([np.var(scores) if len(scores) > 1 else 0.0 for scores in score_set.values()])\n\n    # Dynamic epsilon calculation\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    epsilon_decay_rate = 0.95\n    epsilon = epsilon_min + (epsilon_max - epsilon_min) * ((1 - current_time_slot / total_time_slots) ** epsilon_decay_rate)\n    \n    # Calculate exploration bonuses\n    explore_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n\n    # Enhanced score calculation incorporating average scores, variance, and exploration\n    adjusted_scores = avg_scores + 0.5 * variances  # Weight variance to promote exploration of variable actions\n    exploration_adjusted_scores = (1 - epsilon) * adjusted_scores + epsilon * explore_bonus\n    combined_scores = np.nan_to_num(exploration_adjusted_scores)\n\n    # Select action based on combined scores\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -201.02105029581884,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate epsilon proportional to exploration\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n    \n    # Generate a random value for exploration vs exploitation decision\n    random_value = np.random.rand()\n    \n    # Calculate average scores and selection counts\n    averages = []\n    selection_counts = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        average_score = np.mean(scores) if scores else 0\n        count = len(scores)\n        \n        # Use a higher exploration probability for less frequently selected actions\n        # Explore less for actions that have been selected more frequently\n        exploration_factor = (total_selection_count - count) / (total_selection_count + 1)  # Avoid division by zero\n        adjusted_average = average_score * (1 + exploration_factor)\n        \n        averages.append(adjusted_average)\n        selection_counts.append(count)\n    \n    # Decide action based on exploration/exploitation strategy\n    if random_value < epsilon:  # Exploration\n        action_index = np.random.choice([i for i in range(8) if selection_counts[i] < total_selection_count / 8])\n        # If all actions are selected sufficiently, explore randomly\n        if selection_counts.count(0) == 0:\n            action_index = np.random.randint(0, 8)  \n    else:  # Exploitation\n        action_index = np.argmax(averages)\n        \n    return action_index",
          "objective": -200.72974555002224,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores and selection counts\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Epsilon calculation for exploration\n    epsilon_base = 1.0\n    epsilon_min = 0.05\n    epsilon_decay = 0.99\n    epsilon = max(epsilon_min, epsilon_base * (epsilon_decay ** current_time_slot))\n    \n    # Ensure every action is explored at least once\n    explore_bonus = np.log(total_selection_count + 1) / (selection_counts + 1)\n    \n    # Combined scores for selection\n    combined_scores = avg_scores + epsilon * explore_bonus\n    \n    # Select action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -200.66257192967316,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Dynamic epsilon-greedy strategy parameters\n    epsilon_initial = 1.0\n    epsilon_min = 0.05\n    epsilon_decay = 0.99\n    epsilon = max(epsilon_min, epsilon_initial * (epsilon_decay ** current_time_slot))\n    \n    # Calculate exploration bonus\n    explore_bonus = np.where(selection_counts == 0, np.inf, np.sqrt(np.log(total_selection_count) / (selection_counts + 1)))\n    \n    # Combined scores\n    combined_scores = avg_scores + epsilon * explore_bonus\n    \n    # Ensure at least minimal exploration of actions\n    min_explore_count = 0.1 * total_selection_count\n    explore_mask = selection_counts < min_explore_count\n    if np.any(explore_mask):\n        combined_scores[explore_mask] += 0.1  # give a slight boost to less-explored actions\n    \n    # Select the action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -200.6327546996947,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize parameters\n    action_indices = np.arange(len(score_set))  # Indices of actions\n    scores = np.array([np.mean(scores) if scores else 0 for scores in score_set.values()])\n    n_selections = np.array([len(scores) for scores in score_set.values()])\n    \n    # Avoid division by zero and calculate the exploration term\n    exploration_term = np.sqrt(np.log(total_selection_count + 1) / (n_selections + 1e-6))\n\n    # Epsilon decay calculation\n    epsilon = (1.0 - (current_time_slot / total_time_slots)) ** 2  # Squared for stronger decay\n\n    # Combined score for balance of exploration and exploitation\n    combined_scores = scores + (epsilon * exploration_term)\n\n    # Select action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -200.6149865044211,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n\n    # Calculate average scores and selection counts\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n\n    # Adaptive epsilon-greedy parameters\n    epsilon_max = 1.0\n    epsilon_min = 0.05\n    epsilon_decay_rate = 0.9\n    epsilon = epsilon_min + (epsilon_max - epsilon_min) * np.exp(-epsilon_decay_rate * current_time_slot / total_time_slots)\n    \n    # Exploration bonus\n    explore_bonus = np.where(selection_counts == 0, np.inf, np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1)))\n\n    # Combined scores: exploitation and exploration balance\n    combined_scores = avg_scores + epsilon * explore_bonus\n\n    # Select action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -200.52520894973748,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in action_indices])\n    selection_counts = np.array([len(score_set[action]) for action in action_indices])\n    \n    # Dynamic exploration decrease factor\n    exploration_factor = 1.0 * (1 - current_time_slot / total_time_slots)  # Decreasing exploration over time\n    exploration_factor = max(0.1, exploration_factor)  # Ensure minimum exploration\n\n    # Calculate upper confidence bounds\n    selection_counts = np.clip(selection_counts, 1, None)  # Prevent division by zero\n    ucb_scores = scores + np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts)\n    \n    # Choose action index based on exploration-exploitation trade-off\n    if np.random.rand() < exploration_factor:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        action_index = action_indices[np.argmax(ucb_scores)]  # Exploit\n    \n    return action_index",
          "objective": -200.48546861025022,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores, handling empty score lists\n    avg_scores = {action_index: np.mean(scores) if scores else 0.0 for action_index, scores in score_set.items()}\n    \n    # Count of selections for each action\n    selection_counts = {action_index: len(scores) for action_index, scores in score_set.items()}\n    \n    # Dynamic epsilon based on current time slot\n    min_epsilon = 0.1\n    max_epsilon = 0.9\n    epsilon_decay = (max_epsilon - min_epsilon) * (current_time_slot / total_time_slots)\n    epsilon = max(min_epsilon, max_epsilon - epsilon_decay)\n    \n    # Compute UCB-based exploration scores\n    exploration_scores = {\n        action_index: avg_scores[action_index] + np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts[action_index] + 1)) \n        if selection_counts[action_index] > 0 else float('inf') \n        for action_index in score_set\n    }\n    \n    # Weighted selection using epsilon-greedy approach\n    weighted_scores = {\n        action_index: (1 - epsilon) * avg_scores[action_index] + epsilon * exploration_scores[action_index]\n        for action_index in score_set\n    }\n    \n    # Select the action with the highest weighted score\n    action_index = max(weighted_scores, key=weighted_scores.get)\n    \n    return action_index",
          "objective": -200.42384025877186,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize parameters\n    epsilon = max(0.1, 1.0 - (current_time_slot / total_time_slots))\n    \n    averages = []\n    exploration_values = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        avg_score = np.mean(scores) if scores else 0\n        selection_count = len(scores)\n        \n        # UCB calculation\n        if selection_count > 0:\n            exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1))\n        else:\n            exploration_value = float('inf')  # Favor exploration for actions not selected yet\n        \n        # Combine exploitation and exploration\n        combined_score = avg_score + (epsilon * exploration_value)\n        averages.append(combined_score)\n    \n    # Select action with the highest combined score\n    action_index = np.argmax(averages)\n    \n    return action_index",
          "objective": -200.30151777995277,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores and selection counts\n    avg_scores = np.array([np.mean(scores) if scores else 0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Handle cases where selection counts may be zero\n    selection_counts = np.maximum(selection_counts, 1)  # Prevent division by zero\n    \n    # Dynamic epsilon-greedy strategy\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    epsilon = epsilon_min + (epsilon_max - epsilon_min) * (1 - (current_time_slot / total_time_slots))\n    \n    # Exploration term\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / selection_counts)\n    \n    # Combined scores\n    combined_scores = avg_scores + epsilon * exploration_bonus\n    \n    # Select the action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -200.04790381561153,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores for each action\n    avg_scores = {\n        action_index: (np.mean(scores) if scores else 0.0)\n        for action_index, scores in score_set.items()\n    }\n\n    # Dynamic epsilon based on remaining time slots\n    epsilon = max(0.1, (total_time_slots - current_time_slot) / total_time_slots)\n\n    # Randomly explore with probability epsilon\n    if np.random.rand() < epsilon:\n        return np.random.choice(list(score_set.keys()))\n\n    # Calculate exploration bonuses using modified UCB\n    exploration_bonuses = {}\n    for action_index in range(8):\n        n_sq = len(score_set.get(action_index, []))\n        if n_sq > 0:\n            exploration_bonuses[action_index] = np.sqrt(2 * np.log(total_selection_count + 1) / n_sq)\n        else:\n            exploration_bonuses[action_index] = np.inf  # Encourage exploration for untested actions\n\n    # Combine average scores and exploration bonuses\n    combined_scores = {\n        action_index: avg_scores[action_index] + exploration_bonuses[action_index]\n        for action_index in range(8)\n    }\n\n    # Select the action with the highest combined score\n    action_index = max(combined_scores, key=combined_scores.get)\n\n    return action_index",
          "objective": -199.70037403325435,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores and selection counts\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Set exploration parameters\n    epsilon_initial = 1.0\n    epsilon_min = 0.05\n    epsilon_decay = 0.99\n    epsilon = max(epsilon_min, epsilon_initial * (epsilon_decay ** current_time_slot))\n\n    # Calculate the exploration bonus\n    explore_bonus = np.where(selection_counts == 0, np.inf, np.sqrt(np.log(total_selection_count) / (selection_counts + 1)))\n\n    # Combined scores considering exploration\n    combined_scores = avg_scores + epsilon * explore_bonus\n    \n    # Select the action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -199.68082097544135,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores, handling division by zero\n    avg_scores = {\n        action_index: (np.mean(scores) if scores else 0.0)\n        for action_index, scores in score_set.items()\n    }\n    \n    # Calculate exploration bonuses using the Softmax approach for a smoother balance\n    exploration_bonuses = np.zeros(8)\n    for action_index in range(8):\n        n_sq = len(score_set.get(action_index, []))\n        if n_sq > 0:\n            exploration_bonuses[action_index] = np.sqrt(2 * np.log(total_selection_count) / n_sq)\n        else:\n            exploration_bonuses[action_index] = np.inf  # Encourage exploration for untested actions\n\n    # Combining scores with Softmax to normalize exploration bonuses\n    combined_scores = np.array([avg_scores.get(action_index, 0) + exploration_bonuses[action_index] for action_index in range(8)])\n    combined_scores -= combined_scores.max()  # For numerical stability\n    softmax_scores = np.exp(combined_scores) / np.sum(np.exp(combined_scores))\n\n    # Penalize scores based on the remaining time slots\n    time_factor = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = softmax_scores * time_factor\n\n    # Select the action index with the highest score\n    action_index = np.argmax(adjusted_scores)\n    \n    return action_index",
          "objective": -199.17923876526532,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n\n    # Calculate total scores and selection counts\n    total_scores = np.array([sum(scores) for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n\n    # Calculate average scores with handling of division by zero\n    avg_scores = np.where(selection_counts > 0, total_scores / selection_counts, 0.0)\n\n    # Dynamic epsilon calculation\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    epsilon_decay = (epsilon_max - epsilon_min) / total_time_slots\n    epsilon = max(epsilon_min, epsilon_max - epsilon_decay * current_time_slot)\n\n    # Exploration bonus based on selection counts\n    explore_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1))  # avoid division by zero\n\n    # Combined scores calculation\n    combined_scores = avg_scores + epsilon * explore_bonus\n\n    # Select the action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -198.6541857350373,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores and selection counts\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Define exploration parameters\n    epsilon_max = 0.5\n    epsilon_min = 0.05\n    epsilon_decay_rate = total_time_slots / (total_time_slots - current_time_slot + 1)  # Reduce exploration over time\n    epsilon = max(epsilon_min, epsilon_max * epsilon_decay_rate)\n    \n    # Calculate explore bonus for under-selected actions to encourage exploration\n    explore_bonus = np.where(selection_counts > 0, np.sqrt(np.log(total_selection_count) / selection_counts), np.inf)\n    \n    # Combine scores taking into account exploration\n    combined_scores = avg_scores + epsilon * explore_bonus\n    \n    # Select the action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -198.50285235027644,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores for each action, handling empty score cases\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    \n    # Calculate selection counts\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Define epsilon parameters for exploration\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    epsilon_decay_rate = 0.9\n    epsilon = max(epsilon_min, epsilon_max * (1 - current_time_slot / total_time_slots))\n    \n    # Exploration bonus to encourage trying less selected actions\n    explore_bonus = np.where(selection_counts == 0, np.inf, np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1)))\n    \n    # Combine average scores and exploration bonuses\n    combined_scores = avg_scores + epsilon * explore_bonus\n\n    # Select action based on combined scores\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -197.7527887938346,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize scores and counts\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Calculate a pseudo count for actions not yet explored\n    pseudo_counts = 1  # To avoid zero division\n    total_counts = counts + pseudo_counts\n    \n    # Calculate dynamic epsilon based on the current time slot\n    epsilon = max(0.1, 1 - current_time_slot / total_time_slots)\n\n    # Calculate the exploration bonus using UCB (Upper Confidence Bound) approach\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (total_counts + 1e-5))  # 1e-5 to prevent division by zero\n    scores_with_bonus = avg_scores + exploration_bonus\n\n    # Decide whether to explore or exploit\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(len(score_set))\n    else:\n        action_index = np.argmax(scores_with_bonus)\n\n    return action_index",
          "objective": -197.2950247092254,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize parameters\n    alpha = 0.5  # Weight for combining exploration and exploitation\n    beta = 0.5   # Weight for the loss of exploration value over time\n    averages = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        avg_score = np.mean(scores) if scores else 0\n        selection_count = len(scores)\n        \n        # Calculate exploration component\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else np.inf\n        exploration_score = exploration_value * (1 - (current_time_slot / total_time_slots))\n        \n        # Combine average score and exploration\n        combined_score = alpha * avg_score + beta * exploration_score\n        averages.append(combined_score)\n    \n    # Select action with the highest combined score\n    action_index = np.argmax(averages)\n    \n    return action_index",
          "objective": -197.07355620880145,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores, treating empty score lists as 0.0\n    avg_scores = {action_index: np.mean(scores) if scores else 0.0 for action_index, scores in score_set.items()}\n    \n    # Dynamic exploration rate based on current time slot\n    min_epsilon = 0.05\n    max_epsilon = 0.8\n    decay_rate = current_time_slot / total_time_slots\n    epsilon = max(min_epsilon, max_epsilon * (1 - decay_rate))\n    \n    # Count selections for each action\n    selection_counts = {action_index: len(scores) for action_index, scores in score_set.items()}\n    \n    # UCB exploration term\n    exploration_scores = {\n        action_index: (avg_scores[action_index] + \n                       np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts[action_index] + 1))\n                       if selection_counts[action_index] > 0 else float('inf'))\n        for action_index in avg_scores\n    }\n    \n    # Randomly decide between exploration and exploitation\n    if np.random.rand() < epsilon:\n        action_index = max(exploration_scores, key=exploration_scores.get)\n    else:\n        action_index = max(avg_scores, key=avg_scores.get)\n    \n    return action_index",
          "objective": -197.0337771044146,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize parameters\n    alpha = 0.7  # Weight for exploitation\n    beta = 0.3   # Weight for exploration\n    averages = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        avg_score = np.mean(scores) if scores else 0\n        selection_count = len(scores) \n        \n        # Calculate exploration component\n        if selection_count > 0:\n            exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1))\n        else:\n            exploration_value = np.inf\n            \n        # Adjust exploration decaying over time\n        exploration_score = exploration_value * (1 - current_time_slot / total_time_slots) \n        \n        # Combine average score and exploration\n        combined_score = alpha * avg_score + beta * exploration_score\n        averages.append(combined_score)\n    \n    # Select action with the highest combined score\n    action_index = np.argmax(averages)\n    \n    return action_index",
          "objective": -196.9962465875443,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Initialize parameters\n    avg_scores = np.array([np.mean(scores) if scores else 0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n\n    # Ensure no division by zero\n    selection_counts = np.maximum(selection_counts, 1)\n    \n    # Dynamic epsilon based on time slot\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    epsilon_decay = (1 - (current_time_slot / total_time_slots))\n    epsilon = max(epsilon_min, epsilon_max * epsilon_decay)\n\n    # Exploration factor using logarithmic scaling\n    exploration_factor = np.sqrt(np.log(total_selection_count + 1) / selection_counts)\n    \n    # Combined scores: favoring both historical performance and exploration\n    combined_scores = avg_scores + (epsilon * exploration_factor)\n    \n    # Select action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -196.12423382693137,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Compute average scores and selection counts\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Dynamic epsilon parameters\n    epsilon_max = 1.0\n    epsilon_min = 0.05\n    epsilon_decay_rate = 0.9\n    epsilon = epsilon_min + (epsilon_max - epsilon_min) * (1 - np.clip(current_time_slot / total_time_slots, 0, 1))\n    \n    # Calculate exploration bonus\n    explore_bonus = np.where(selection_counts > 0, np.sqrt(np.log(total_selection_count) / selection_counts), np.inf)\n    \n    # Combined scores\n    combined_scores = avg_scores + epsilon * explore_bonus\n    \n    # Select action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -196.046533906425,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores while handling empty score lists\n    avg_scores = {\n        action_index: np.mean(scores) if scores else 0.0 \n        for action_index, scores in score_set.items()\n    }\n    \n    # Dynamic epsilon adjustment based on current time slot and total time slots\n    min_epsilon = 0.1\n    max_epsilon = 0.7\n    epsilon = min_epsilon + (max_epsilon - min_epsilon) * (current_time_slot / total_time_slots)\n    \n    # Count selections for each action\n    selection_counts = {\n        action_index: len(scores) for action_index, scores in score_set.items()\n    }\n    \n    # Default selection in case all actions have been explored\n    if total_selection_count == 0:\n        return np.random.randint(0, 8)\n    \n    # Calculate exploration scores with stability\n    exploration_scores = {\n        action_index: avg_scores[action_index] + np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts[action_index] + 1))\n        if selection_counts[action_index] > 0 else float('inf') \n        for action_index in avg_scores\n    }\n    \n    # Epsilon-greedy selection process\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(\n            [k for k in exploration_scores], \n            p=np.array(list(exploration_scores.values())) / sum(exploration_scores.values())\n        )\n    else:\n        action_index = max(avg_scores, key=avg_scores.get)\n    \n    return action_index",
          "objective": -196.0425816889364,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = 0.1  # Exploration factor\n    action_indices = list(score_set.keys())\n    \n    # Calculate average scores for each action\n    avg_scores = []\n    for action in action_indices:\n        scores = score_set[action]\n        if len(scores) > 0:\n            avg_score = np.mean(scores)\n        else:\n            avg_score = 0  # If no scores, consider it as a score of zero\n        avg_scores.append(avg_score)\n\n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        action_index = action_indices[np.argmax(avg_scores)]  # Exploit\n\n    return action_index",
          "objective": -195.44155493851432,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Constants\n    epsilon = 0.1  # Exploration factor\n    beta = 0.5     # Factor to balance exploration and exploitation\n\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    # Calculate average scores and selection counts\n    avg_scores = []\n    exploration_scores = []\n    \n    for action in action_indices:\n        scores = score_set[action]\n        if len(scores) > 0:\n            avg_score = np.mean(scores)\n            selection_count = len(scores)\n        else:\n            avg_score = 0\n            selection_count = 0\n        \n        avg_scores.append(avg_score)\n        exploration_scores.append(1 / (selection_count + 1))  # +1 to avoid division by zero\n\n    # Calculate a combined score based on average scores and exploration\n    combined_scores = np.array(avg_scores) + beta * np.array(exploration_scores)\n\n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        action_index = action_indices[np.argmax(combined_scores)]  # Exploit\n\n    return action_index",
          "objective": -195.1997687547873,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores and selection counts\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Handle zero counts to prevent division by zero\n    scores_variance = np.array([np.var(scores) if len(scores) > 1 else 0.0 for scores in score_set.values()])\n\n    # Epsilon decay parameters\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    epsilon_decay_rate = 0.98\n    epsilon = epsilon_min + (epsilon_max - epsilon_min) * np.exp(-epsilon_decay_rate * (current_time_slot / total_time_slots))\n\n    # Compute exploration weight\n    explore_bonus = np.where(selection_counts > 0, np.sqrt(np.log(total_selection_count) / selection_counts), np.inf)\n    \n    # Calculate combined scores with variance term for exploration\n    combined_scores = avg_scores + epsilon * explore_bonus + scores_variance\n\n    # Select action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -194.98691345782296,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores and selection counts\n    avg_scores = np.array([np.mean(scores) if scores else 0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Dynamic epsilon for exploration\n    epsilon_initial = 1.0\n    epsilon_min = 0.05\n    epsilon_decay = (total_time_slots - current_time_slot) / total_time_slots\n    epsilon = max(epsilon_min, epsilon_initial * epsilon_decay)\n    \n    # Ensure every action is selected at least once for a valid exploration\n    unexplored = (total_selection_count - selection_counts) < 1\n    exploration_bonus = np.where(unexplored, np.inf, np.sqrt(np.log(total_selection_count) / (selection_counts + 1)))\n    \n    # Combined scores considering exploration and exploration probability\n    combined_scores = avg_scores + epsilon * exploration_bonus\n    \n    # Select the action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -194.52956825656477,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores and selection counts\n    avg_scores = np.array([np.mean(scores) if scores else 0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Handle edge case when total_selection_count is 0\n    if total_selection_count == 0:\n        return np.random.randint(num_actions)\n    \n    # Dynamic epsilon-greedy strategy\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    epsilon_decay = 0.9\n    epsilon = epsilon_min + (epsilon_max - epsilon_min) * (1 - current_time_slot / total_time_slots)\n    # Adaptive epsilon decay\n    epsilon = max(epsilon_min, epsilon * (epsilon_decay ** current_time_slot))\n    \n    # Exploration bonus term\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n    \n    # Combined scores (exploitation and exploration)\n    combined_scores = avg_scores + epsilon * exploration_bonus\n    \n    # Select the action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -194.4415452787604,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores and handle empty score lists\n    avg_scores = {\n        action_index: (np.mean(scores) if scores else 0.0)\n        for action_index, scores in score_set.items()\n    }\n    \n    # Initialize exploration factors\n    exploration_factors = {}\n    for action_index in range(8):\n        if action_index in score_set and score_set[action_index]:\n            exploration_factors[action_index] = np.sqrt(np.log(total_selection_count) / (len(score_set[action_index]) + 1))\n        else:\n            exploration_factors[action_index] = np.sqrt(np.log(total_selection_count + 1))  # Encourage exploration\n        \n    # Calculate effective scores\n    effective_scores = {\n        action_index: avg_scores[action_index] + exploration_factors.get(action_index, 0)\n        for action_index in range(8)\n    }\n    \n    # Adjust scores based on remaining time slots\n    time_adjusted_scores = {\n        action_index: effective_scores[action_index] * (total_time_slots - current_time_slot) / total_time_slots\n        for action_index in range(8)\n    }\n    \n    # Select the action index with the highest adjusted score\n    action_index = max(time_adjusted_scores, key=time_adjusted_scores.get)\n    \n    return action_index",
          "objective": -194.3641358132156,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n\n    # Calculate average scores and selection counts\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n\n    # Dynamic epsilon-greedy parameters\n    epsilon_max = 1.0\n    epsilon_min = 0.05\n    epsilon_decay_rate = 0.9\n    epsilon = epsilon_min + (epsilon_max - epsilon_min) * (1 - (current_time_slot / total_time_slots) ** epsilon_decay_rate)\n    \n    # Exploration bonus\n    explore_bonus = np.where(selection_counts == 0, np.inf, np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1)))\n\n    # Combined scores: exploitation and exploration balance\n    combined_scores = avg_scores + epsilon * explore_bonus\n\n    # Select action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -193.5141765592744,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores and variances\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    variances = np.array([np.var(scores) if len(scores) > 1 else 0.0 for scores in score_set.values()])\n    \n    # Dynamic epsilon-greedy parameters\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    epsilon_decay_rate = 0.9\n    epsilon = epsilon_min + (epsilon_max - epsilon_min) * (1 - current_time_slot / total_time_slots)\n    \n    # Exploration bonus based on selection counts and variances\n    exploration_bonus = np.where(selection_counts == 0, np.inf, \n                                   np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1))) + variances\n    \n    # Combined scores\n    combined_scores = avg_scores + epsilon * exploration_bonus\n    \n    # Select the action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -193.02842618730529,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = len(score_set)\n    avg_scores = np.zeros(action_count)\n    \n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        if scores:\n            avg_scores[action_index] = np.mean(scores)\n        \n    exploration_weights = np.zeros(action_count)\n    \n    for action_index in range(action_count):\n        selection_count = len(score_set.get(action_index, []))\n        if total_selection_count > 0:\n            exploration_weights[action_index] = np.sqrt(np.log(total_selection_count) / (selection_count + 1))\n        else:\n            exploration_weights[action_index] = 1.0\n\n    # Blend average scores with exploration weights\n    combined_scores = avg_scores + exploration_weights\n    \n    # Adjust for remaining time slots\n    time_adjusted_scores = combined_scores * (total_time_slots - current_time_slot) / total_time_slots\n    \n    action_index = np.argmax(time_adjusted_scores)\n    \n    return action_index",
          "objective": -192.94729180234702,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores for each action while handling empty score lists\n    avg_scores = {action_index: (np.mean(scores) if scores else 0.0) for action_index, scores in score_set.items()}\n\n    # Dynamic epsilon decay\n    min_epsilon = 0.05\n    max_epsilon = 0.8\n    epsilon = min_epsilon + (max_epsilon - min_epsilon) * (1 - (current_time_slot / total_time_slots))\n\n    # Count selections for each action\n    selection_counts = {action_index: len(scores) for action_index, scores in score_set.items()}\n\n    # Handle no selections case\n    if total_selection_count == 0:\n        return np.random.randint(0, 8)\n\n    # Calculate scores for exploration and exploitation\n    exploration_scores = {}\n    for action_index in avg_scores:\n        # Risk factor to increase exploration for under-selected actions\n        exploration_factor = 1 / (1 + selection_counts[action_index]) if selection_counts[action_index] > 0 else 1.0\n        exploration_scores[action_index] = avg_scores[action_index] + exploration_factor\n\n    # Select action based on exploration vs exploitation\n    if np.random.rand() < epsilon:\n        action_index = max(exploration_scores, key=exploration_scores.get)\n    else:\n        action_index = max(avg_scores, key=avg_scores.get)\n\n    return action_index",
          "objective": -192.94258617740957,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Initialize score arrays\n    avg_scores = np.zeros(num_actions)\n    selection_counts = np.zeros(num_actions)\n    \n    # Calculate average scores and selection counts\n    for action_index, scores in score_set.items():\n        if scores:\n            avg_scores[action_index] = np.mean(scores)\n            selection_counts[action_index] = len(scores)\n    \n    # Set exploration parameters\n    epsilon_initial = 1.0\n    epsilon_min = 0.01\n    epsilon_decay = 0.995\n    epsilon = max(epsilon_min, epsilon_initial * (epsilon_decay ** current_time_slot))\n    \n    # Compute the exploration bonus\n    unexplored = (total_selection_count - selection_counts) < 1\n    exploration_bonus = np.where(unexplored, np.inf, np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1)))\n    \n    # Combine average scores with exploration bonuses\n    dynamic_scores = avg_scores + epsilon * exploration_bonus\n    \n    # Time decay factor to diminish the impact of earlier scores\n    time_decay = 0.5 * (1 - current_time_slot / total_time_slots)\n    adjusted_scores = dynamic_scores * (1 - time_decay) + avg_scores * time_decay\n    \n    # Select the action with the highest adjusted score\n    action_index = np.argmax(adjusted_scores)\n    \n    return action_index",
          "objective": -192.73770118337526,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in action_indices])\n    selection_counts = np.array([len(score_set[action]) for action in action_indices])\n\n    # Dynamic exploration factor based on total selection count\n    epsilon_initial = 1.0\n    epsilon_final = 0.1\n    exploration_decay = (epsilon_initial - epsilon_final) / total_time_slots\n    epsilon = max(epsilon_final, epsilon_initial - exploration_decay * current_time_slot)\n\n    # Select action with epsilon-greedy approach\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        # Use Upper Confidence Bound (UCB) for exploitation\n        selection_counts = np.clip(selection_counts, 1, None)  # Ensure no zero division\n        ucb_scores = scores + np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts)\n        action_index = action_indices[np.argmax(ucb_scores)]\n\n    return action_index",
          "objective": -192.61840482686188,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = np.array(list(score_set.keys()))\n    \n    # Calculate average scores\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in action_indices])\n    \n    # Count the number of times each action has been selected\n    selection_counts = np.array([len(score_set[action]) for action in action_indices])\n    \n    # Set epsilon dynamically\n    epsilon = max(0.1, 0.9 * (1 - current_time_slot / total_time_slots))\n\n    # Generate random number to decide between exploration and exploitation\n    if np.random.rand() < epsilon:\n        # Exploration: Select action randomly\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploitation: Calculate adjusted scores\n        selection_counts = np.clip(selection_counts, 1, None)  # Ensure counts are at least 1\n        exploration_scores = np.sqrt(np.log(total_selection_count + 1) / selection_counts)\n        adjusted_scores = scores + exploration_scores\n        \n        # Select action with the highest adjusted score\n        action_index = action_indices[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -191.91426489048817,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n\n    # Initialize parameters\n    avg_scores = np.array([np.mean(scores) if scores else 0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n\n    # Ensure no division by zero\n    selection_counts = np.maximum(selection_counts, 1)\n\n    # Epsilon decay for exploration\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    epsilon_decay = (1 - (current_time_slot / total_time_slots))\n    epsilon = max(epsilon_min, epsilon_max * epsilon_decay)\n\n    # Exploration factor \n    exploration_factor = np.sqrt(np.log(total_selection_count + 1) / selection_counts)\n    \n    # Combine historical scores and exploration\n    combined_scores = avg_scores + (epsilon * exploration_factor)\n\n    # Select action with the highest combined score\n    action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": -191.72563734224246,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize average scores with zero for unselected actions\n    avg_scores = {action_index: (np.mean(scores) if scores else 0.0) for action_index, scores in score_set.items()}\n    \n    # Initialize counts for each action\n    selection_counts = {action_index: len(scores) for action_index, scores in score_set.items()}\n    \n    # Define a base exploration factor influenced by total selection count\n    exploration_factor = np.log(total_selection_count + 1) / (np.array(list(selection_counts.values())) + 1)\n    \n    # Average scores combined with exploration factor\n    combined_scores = {}\n    for action_index in range(8):\n        exploit_score = avg_scores.get(action_index, 0.0)\n        explore_score = exploration_factor[action_index] if action_index in selection_counts else np.inf\n        combined_scores[action_index] = exploit_score + explore_score\n    \n    # Penalize scores based on remaining time slots\n    time_factor = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = {action_index: score * time_factor for action_index, score in combined_scores.items()}\n\n    # Select the action index with the highest adjusted score\n    action_index = max(adjusted_scores, key=adjusted_scores.get)\n    \n    return action_index",
          "objective": -191.59492873234535,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in action_indices])\n    selection_counts = np.array([len(score_set[action]) for action in action_indices])\n    \n    # Dynamic exploration factor\n    epsilon_initial = 1.0\n    epsilon_final = 0.1\n    exploration_decay = (epsilon_initial - epsilon_final) / total_time_slots\n    epsilon = max(epsilon_final, epsilon_initial - exploration_decay * current_time_slot)\n\n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        # UCB calculation\n        ucb_scores = scores + np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n        action_index = action_indices[np.argmax(ucb_scores)]\n    \n    return action_index",
          "objective": -191.05662557582914,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_factor = 0.1\n    action_values = []\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Diminish exploration impact over time\n        time_factor = (total_time_slots - current_time_slot) / total_time_slots\n        \n        if selection_count > 0:\n            exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1))\n        else:\n            exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count + 1))\n        \n        action_value = time_factor * average_score + exploration_bonus\n        action_values.append(action_value)\n    \n    action_index = np.argmax(action_values)\n    \n    return action_index",
          "objective": -190.50942311059703,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores for each action\n    avg_scores = {\n        action_index: (np.mean(scores) if scores else 0.0)\n        for action_index, scores in score_set.items()\n    }\n    \n    # Dynamic exploration rate (epsilon)\n    min_epsilon = 0.1\n    epsilon = max(min_epsilon, 1.0 - (current_time_slot / total_time_slots))\n    \n    # Calculate exploration term based on frequency of selection\n    exploration_scores = {\n        action_index: 0.0 if total_selection_count == 0 else np.sqrt(np.log(total_selection_count) / (len(scores) + 1))\n        for action_index, scores in score_set.items()\n    }\n    \n    # Combine scores with exploration\n    combined_scores = {\n        action_index: (1 - epsilon) * avg_scores.get(action_index, 0) + epsilon * exploration_scores.get(action_index, 0)\n        for action_index in range(8)\n    }\n    \n    # Select action index with the highest score\n    action_index = max(combined_scores, key=combined_scores.get)\n    \n    return action_index",
          "objective": -190.24518283087937,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = len(score_set)\n    epsilon = max(0.01, (total_time_slots - current_time_slot) / total_time_slots)  # Dynamic epsilon\n\n    # Calculate average scores\n    avg_scores = np.array([\n        np.mean(score_set[i]) if i in score_set and score_set[i] else 0.0 \n        for i in range(action_count)\n    ])\n\n    # Calculate selection counts\n    selection_counts = np.array([\n        len(score_set[i]) if i in score_set else 0 \n        for i in range(action_count)\n    ])\n\n    # Calculate a score adjustment based on selection counts\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1))  # Avoid division by zero\n    exploration_bonus[selection_counts == 0] = np.inf  # Encourage full exploration of untested actions\n\n    # Combined score with exploration and epsilon-greedy approach\n    combined_scores = avg_scores + exploration_bonus\n    selection_probs = (1 - epsilon) * (combined_scores / np.sum(combined_scores)) + (epsilon / action_count)\n\n    # Select action based on calculated probabilities\n    action_index = np.random.choice(range(action_count), p=selection_probs)\n\n    return action_index",
          "objective": -188.06189906671477,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize parameters\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n    epsilon = 1.0 / (current_time_slot + 1)\n    \n    action_values = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        avg_score = np.mean(scores) if scores else 0\n        selection_count = len(scores)\n        \n        if selection_count > 0:\n            ucb_value = np.sqrt((np.log(total_selection_count + 1) / selection_count))\n        else:\n            ucb_value = np.inf  # Encourage exploration for untried actions\n        \n        combined_score = avg_score + exploration_factor * ucb_value + epsilon * np.random.rand()\n        \n        action_values.append(combined_score)\n\n    action_index = np.argmax(action_values)\n    \n    return action_index",
          "objective": -187.55725924297477,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    \n    # Calculate the average scores and handle cases where division by zero occurs\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in action_indices])\n    selection_counts = np.array([len(score_set[action]) for action in action_indices])\n    \n    # Dynamic epsilon adjustment for exploration\n    epsilon = max(0, 0.1 * (1 - (current_time_slot / total_time_slots)))\n\n    # Random exploration occurs with probability epsilon\n    if np.random.rand() < epsilon:\n        return np.random.choice(action_indices)\n    \n    # Calculate exploitation scores\n    adjusted_scores = scores.copy()\n    \n    # Calculate exploration bonuses (ensure no division by zero)\n    with np.errstate(divide='ignore', invalid='ignore'):\n        exploration_scores = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1))\n    \n    # Combine exploitation and exploration scores\n    adjusted_scores += exploration_scores\n    \n    # Select the action with the highest adjusted score\n    action_index = action_indices[np.argmax(adjusted_scores)]\n\n    return action_index",
          "objective": -187.20334291674834,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores and selection counts avoiding division by zero\n    avg_scores = np.array([np.mean(scores) if scores else 0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Dynamic epsilon-greedy strategy\n    epsilon_initial = 1.0\n    epsilon_min = 0.1\n    epsilon_decay = 0.995\n    epsilon = max(epsilon_min, epsilon_initial * (epsilon_decay ** current_time_slot))\n    \n    # Determine exploration needs\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1))\n    \n    # Combine scores with exploration consideration\n    scores_with_exploration = avg_scores + epsilon * exploration_bonus\n    \n    # Select the action with the highest score\n    action_index = np.argmax(scores_with_exploration)\n    \n    return action_index",
          "objective": -187.08949116893928,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    \n    # Calculate average scores and handle division by zero\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in action_indices])\n    selection_counts = np.array([len(score_set[action]) for action in action_indices])\n    \n    # Dynamic epsilon decay for exploration\n    epsilon = max(0.05, 1 - (current_time_slot / total_time_slots))  # Minimum exploration probability\n    \n    # Random exploration\n    if np.random.rand() < epsilon:\n        return np.random.choice(action_indices)\n    \n    # Calculate adjusted scores for exploitation with exploration bonus\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1))\n    adjusted_scores = scores + exploration_bonus\n    \n    # Select the action with the highest adjusted score\n    action_index = action_indices[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -186.70493249834868,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores\n    avg_scores = {\n        action_index: (np.mean(scores) if scores else 0.0)\n        for action_index, scores in score_set.items()\n    }\n    \n    # Dynamic epsilon based on the current time slot\n    min_epsilon = 0.05\n    max_epsilon = 0.8\n    epsilon_decay = (max_epsilon - min_epsilon) * (current_time_slot / total_time_slots)\n    epsilon = max(min_epsilon, max_epsilon - epsilon_decay)\n    \n    # Count the number of times actions have been selected\n    selection_counts = {action_index: len(scores) for action_index, scores in score_set.items()}\n    \n    # Select action\n    if total_selection_count == 0 or np.random.rand() < epsilon:\n        # Exploration: Prefer actions with lower selection counts\n        exploration_scores = {\n            action_index: avg_scores[action_index] + (1 / (1 + selection_counts[action_index] if selection_counts[action_index] > 0 else 1))\n            for action_index in avg_scores\n        }\n        action_index = max(exploration_scores, key=exploration_scores.get)\n    else:\n        # Exploitation: Prefer actions with higher average scores\n        action_index = max(avg_scores, key=avg_scores.get)\n    \n    return action_index",
          "objective": -186.49458725368456,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in action_indices])\n    selection_counts = np.array([len(score_set[action]) for action in action_indices])\n\n    # Dynamic exploration factor based on total selection count\n    exploration_rate = max(0.1, 1.0 - (total_selection_count / (total_time_slots * 10)))\n\n    # Calculating softmax probabilities for exploration\n    exp_scores = np.exp(scores / exploration_rate)\n    softmax_probs = exp_scores / np.sum(exp_scores)\n\n    # Randomly select action based on softmax probabilities\n    action_index = np.random.choice(action_indices, p=softmax_probs)\n\n    return action_index",
          "objective": -184.9469759510759,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores and selection counts, with safeguards for empty lists\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n\n    # Epsilon decay parameters\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    epsilon_decay_rate = 0.99\n    epsilon = max(epsilon_min, epsilon_max * (total_time_slots - current_time_slot) / total_time_slots)\n\n    # Handle infinite explore_bonus when selection_counts are zero\n    explore_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n\n    # Incorporate variance as a secondary factor for exploration\n    score_variance = np.array([np.var(scores) if len(scores) > 1 else 0.0 for scores in score_set.values()])\n    combined_scores = avg_scores + epsilon * (explore_bonus + score_variance)\n\n    # Select action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -184.58393547104387,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n    random_value = np.random.rand()\n    \n    averages = [np.mean(scores) if scores else 0 for scores in score_set.values()]\n    selection_counts = [len(scores) for scores in score_set.values()]\n    \n    # Calculate exploration bonus based on selection counts\n    exploration_bonus = [(total_selection_count - count + 1) / (total_selection_count + 1) for count in selection_counts]\n    adjusted_scores = np.array(averages) + np.array(exploration_bonus)\n    \n    if random_value < epsilon:  # Exploration\n        threshold = total_selection_count / len(score_set)\n        explorables = [i for i, count in enumerate(selection_counts) if count < threshold]\n        action_index = np.random.choice(explorables) if explorables else np.random.randint(0, 8)\n    else:  # Exploitation\n        action_index = np.argmax(adjusted_scores)\n    \n    return action_index",
          "objective": -184.23946938921563,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    avg_scores = []\n    n_actions = 8\n\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        if scores:\n            avg_score = np.mean(scores)\n        else:\n            avg_score = 0.0\n        \n        action_count = len(scores)\n\n        # Avoid division by zero and encourage exploration of less selected actions\n        exploration_factor = np.sqrt(np.log(total_selection_count + 1) / (action_count + 1)) if total_selection_count > 0 else 1.0\n        \n        # Incorporating a temporal decay factor to favor actions with better historical performance \n        decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        \n        effective_score = avg_score * decay_factor + exploration_factor\n        avg_scores.append(effective_score)\n\n    action_index = np.argmax(avg_scores)\n    \n    return action_index",
          "objective": -184.05265631190713,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in action_indices])\n    selection_counts = np.array([len(score_set[action]) for action in action_indices])\n\n    # Explore/exploit balance based on current progress\n    exploration_rate = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n    \n    # Dynamic adaptation based on time slot\n    time_decay = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = scores + exploration_rate * time_decay\n\n    # Select action with the highest adjusted score\n    action_index = action_indices[np.argmax(adjusted_scores)]\n\n    return action_index",
          "objective": -183.99496068589784,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores for each action\n    avg_scores = {\n        action_index: (np.mean(scores) if scores else 0.0)\n        for action_index, scores in score_set.items()\n    }\n\n    # Calculate the exploration rate using an epsilon-greedy strategy\n    epsilon = max(0.1, 1.0 - (current_time_slot / total_time_slots))\n    \n    # Select action based on exploration-exploitation trade-off\n    if np.random.rand() < epsilon:  # Explore with probability epsilon\n        action_index = np.random.choice(list(score_set.keys()))  # Randomly select from available actions\n    else:  # Exploit based on average scores\n        combined_scores = {\n            action_index: avg_scores[action_index] for action_index in score_set.keys()\n        }\n        action_index = max(combined_scores, key=combined_scores.get)\n\n    return action_index",
          "objective": -183.8640788747577,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Dynamic epsilon decay based on the current time slot\n    max_epsilon = 0.5\n    min_epsilon = 0.1\n    epsilon = max_epsilon - (max_epsilon - min_epsilon) * (current_time_slot / total_time_slots)\n\n    action_indices = list(score_set.keys())\n    n_actions = len(action_indices)\n    \n    avg_scores = []\n    exploration_scores = []\n    \n    for action in action_indices:\n        scores = score_set[action]\n        selection_count = len(scores)\n        \n        avg_score = np.mean(scores) if selection_count > 0 else 0\n        exploration_score = 1 / (selection_count + 1)  # Smoother exploration\n        \n        avg_scores.append(avg_score)\n        exploration_scores.append(exploration_score)\n\n    # Calculate the combined scores\n    combined_scores = np.array(avg_scores) + exploration_scores\n\n    # Epsilon-greedy selection mechanism\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        action_index = action_indices[np.argmax(combined_scores)]  # Exploit\n\n    return action_index",
          "objective": -182.9415078283823,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores while handling empty score lists\n    avg_scores = np.array([\n        np.mean(scores) if scores else 0.0\n        for scores in score_set.values()\n    ])\n    \n    # Dynamic exploration rate\n    min_epsilon = 0.05\n    max_epsilon = 0.8\n    epsilon = min_epsilon + (max_epsilon - min_epsilon) * (1 - (current_time_slot / total_time_slots))\n    \n    # Calculate selection counts\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Ensure exploration on first selection\n    if total_selection_count == 0:\n        return np.random.randint(0, 8)\n    \n    # Random number for epsilon-greedy decision\n    if np.random.rand() < epsilon:\n        # Encourage exploration\n        exploration_scores = avg_scores + (1 / (1 + selection_counts))\n        action_index = np.random.choice(np.flatnonzero(exploration_scores == exploration_scores.max()))\n    else:\n        # Exploit best known action\n        action_index = np.random.choice(np.flatnonzero(avg_scores == avg_scores.max()))\n    \n    return action_index",
          "objective": -182.7604108091603,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize parameters\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))  # Dynamic epsilon\n    averages = []\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        avg_score = np.mean(scores) if scores else 0\n        selection_count = len(scores)\n        \n        # Exploration probability based on the epsilon-greedy strategy\n        exploration_value = (1 - selection_count / (total_selection_count + 1)) if selection_count < total_selection_count else 0\n\n        # Combined score calculation\n        combined_score = avg_score + (epsilon * exploration_value)\n        averages.append(combined_score)\n\n    # Select action with the highest combined score\n    action_index = np.argmax(averages)\n\n    return action_index",
          "objective": -182.61789597038756,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores and selection counts\n    avg_scores = np.array([np.mean(scores) if scores else 0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Set exploration parameters\n    epsilon_initial = 1.0\n    epsilon_min = 0.05\n    epsilon_decay = 0.99\n    epsilon = max(epsilon_min, epsilon_initial * (epsilon_decay ** current_time_slot))\n    \n    # Ensure exploration for actions that have not been selected yet\n    explore_bonus = (total_selection_count - selection_counts) < 1\n    exploration_bonus = np.where(explore_bonus, np.inf, np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1)))\n\n    # Combine scores considering exploration\n    combined_scores = avg_scores + epsilon * exploration_bonus\n    \n    # Select the action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -182.20789169427806,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = len(score_set)\n    \n    # Calculate average scores and selection counts\n    avg_scores = np.zeros(action_count)\n    selection_counts = np.zeros(action_count)\n    \n    for action_index, scores in score_set.items():\n        avg_scores[action_index] = np.mean(scores) if scores else 0.0\n        selection_counts[action_index] = len(scores)\n    \n    # Calculate action probabilities using Softmax function\n    exp_scores = np.exp(avg_scores - np.max(avg_scores))  # Stability improvement\n    action_probs = exp_scores / np.sum(exp_scores)\n\n    # Dynamic exploration parameter based on total_selection_count\n    min_epsilon = 0.05\n    max_epsilon = 0.3\n    epsilon = min_epsilon + (max_epsilon - min_epsilon) * (1 - total_selection_count / (total_time_slots * action_count))\n\n    # Epsilon-greedy decision\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_count)\n    else:\n        action_index = np.random.choice(action_count, p=action_probs)\n\n    return action_index",
          "objective": -182.09462832479798,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = np.array(list(score_set.keys()))\n    \n    # Calculate average scores, handling any zero-length lists \n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in action_indices])\n    \n    # Count the number of selections for each action\n    selection_counts = np.array([len(score_set[action]) for action in action_indices])\n    \n    # Dynamic epsilon based on the current time slot\n    epsilon = max(0.1, 0.9 * (1 - current_time_slot / total_time_slots))\n\n    # Exploration-exploitation choice\n    if np.random.rand() < epsilon:\n        # Exploration: Randomly select an action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploitation: Calculate adjusted scores with uncertainty\n        selection_counts = np.clip(selection_counts, 1, None)  # Prevent division by zero\n        uncertainty_scores = np.sqrt(np.log(total_selection_count + 1) / selection_counts)\n        \n        # Adjusted scores: Weighted score + uncertainty component\n        adjusted_scores = scores + uncertainty_scores\n        \n        # Select the action with the highest adjusted score\n        action_index = action_indices[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -181.42614583957098,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in action_indices])\n    selection_counts = np.array([len(score_set[action]) for action in action_indices])\n\n    # Dynamic exploration factor\n    epsilon_initial = 1.0\n    epsilon_final = 0.1\n    exploration_decay = (epsilon_initial - epsilon_final) / total_time_slots\n    epsilon = max(epsilon_final, epsilon_initial - exploration_decay * current_time_slot)\n\n    # Softmax temperatures based on selection counts\n    temperature = np.maximum(selection_counts, 1)  # Avoid division by zero\n    softmax_scores = np.exp(scores / temperature) / np.sum(np.exp(scores / temperature))\n\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = np.random.choice(action_indices, p=softmax_scores)\n\n    return action_index",
          "objective": -181.25182523632992,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    avg_scores = []\n    n_actions = 8\n\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        num_selections = len(scores)\n        \n        avg_score = np.mean(scores) if num_selections > 0 else 0.0\n        \n        # Exploration term with a small constant to avoid division by zero\n        exploration_factor = np.sqrt(np.log(total_selection_count + 1) / (num_selections + 1))\n        \n        # Time decay factor\n        time_decay = (total_time_slots - current_time_slot) / total_time_slots\n        \n        effective_score = avg_score + exploration_factor * time_decay      \n        avg_scores.append(effective_score)\n\n    action_index = np.argmax(avg_scores)\n    \n    return action_index",
          "objective": -180.6576628830762,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores while handling empty score lists\n    avg_scores = np.array([\n        np.mean(scores) if scores else 0.0 \n        for scores in score_set.values()\n    ])\n    \n    # Count selection frequency for each action\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Dynamic exploration parameter\n    min_epsilon = 0.05\n    max_epsilon = 0.8\n    epsilon = max(min_epsilon, max_epsilon * (1 - current_time_slot / total_time_slots))\n\n    # Adjust for total selection count for exploration\n    exploration_bonus = np.where(selection_counts > 0, 1 / (1 + selection_counts), 1)\n\n    # Combine average scores with exploration bonuses\n    exploration_scores = avg_scores + exploration_bonus\n    \n    if total_selection_count == 0 or np.random.rand() < epsilon:\n        # Explore\n        action_index = np.random.choice(np.arange(len(exploration_scores)), p=exploration_scores / np.sum(exploration_scores))\n    else:\n        # Exploit\n        action_index = np.argmax(avg_scores)\n\n    return action_index",
          "objective": -180.55505434820614,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    avg_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        avg_score = np.mean(scores) if scores else 0.0\n        \n        # Calculate exploration factor using logarithmic exploration\n        if total_selection_count > 0:\n            exploration_factor = np.sqrt(np.log(total_selection_count + 1) / (len(scores) + 1))\n        else:\n            exploration_factor = 1.0  # Explore all actions if nothing has been selected\n        \n        # Combine average score with exploration factor\n        effective_score = avg_score + exploration_factor\n        \n        # Apply a decay based on the current time slot\n        decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        time_adjusted_score = effective_score * decay_factor\n        \n        avg_scores.append(time_adjusted_score)\n\n    # Select the action with the highest adjusted score\n    action_index = np.argmax(avg_scores)\n    \n    return action_index",
          "objective": -179.56942155273686,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores for each action\n    avg_scores = {action_index: np.mean(scores) if scores else 0.0 for action_index, scores in score_set.items()}\n    \n    # Calculate selection counts for exploration\n    selection_counts = {action_index: len(scores) for action_index, scores in score_set.items()}\n    \n    # Configure exploration bonus\n    exploration_bonuses = {\n        action_index: np.sqrt(2 * np.log(total_selection_count + 1) / (count + 1)) \n        if count > 0 else np.inf \n        for action_index, count in selection_counts.items()\n    }\n\n    # Weighting factor based on remaining time slots\n    time_weighting = (total_time_slots - current_time_slot) / total_time_slots\n    exploitation_scores = {action_index: avg_scores[action_index] * time_weighting for action_index in avg_scores}\n    \n    # Combine scores with exploration bonuses\n    combined_scores = {\n        action_index: exploitation_scores[action_index] + exploration_bonuses[action_index]\n        for action_index in avg_scores\n    }\n\n    # Select action index with the highest combined score\n    action_index = max(combined_scores, key=combined_scores.get)\n    \n    return action_index",
          "objective": -179.22495087552556,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores for each action\n    avg_scores = {\n        action_index: (np.mean(scores) if scores else 0.0)\n        for action_index, scores in score_set.items()\n    }\n\n    # Define parameters for epsilon-greedy\n    min_epsilon = 0.1  # Minimum exploration probability\n    max_epsilon = 1.0  # Maximum exploration probability\n    decay_factor = (max_epsilon - min_epsilon) / total_time_slots\n    \n    # Calculate epsilon based on current time slot\n    epsilon = max(min_epsilon, max_epsilon - decay_factor * current_time_slot)\n\n    # Determine if we should explore or exploit\n    if np.random.rand() < epsilon:\n        # Explore: Select a random action\n        action_index = np.random.choice(range(8))\n    else:\n        # Exploit: Select action with the highest average score\n        action_index = max(avg_scores, key=avg_scores.get)\n    \n    return action_index",
          "objective": -179.11240141732483,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores for each action while handling empty score lists\n    avg_scores = {\n        action_index: (np.mean(scores) if scores else 0.0)\n        for action_index, scores in score_set.items()\n    }\n    \n    # Dynamic exploration rate - decays with time but retains a minimum\n    min_epsilon = 0.05\n    max_epsilon = 0.8\n    epsilon = min_epsilon + (max_epsilon - min_epsilon) * (1 - (current_time_slot / total_time_slots))\n\n    # Calculate selection frequency for each action and handle division by zero\n    selection_counts = {action_index: len(scores) for action_index, scores in score_set.items()}\n    \n    # Initialize action_index\n    action_index = None\n\n    # If no action has been tried, ensure exploration\n    if total_selection_count == 0:\n        action_index = np.random.randint(0, 8)\n    else:\n        # Select between exploration and exploitation\n        if np.random.rand() < epsilon:\n            # Explore: Encourage under-explored actions\n            exploration_scores = {\n                action_index: avg_scores[action_index] + (1 / (1 + selection_counts[action_index]))\n                for action_index in avg_scores\n            }\n            action_index = max(exploration_scores, key=exploration_scores.get)\n        else:\n            # Exploit: Select action with highest average score\n            action_index = max(avg_scores, key=avg_scores.get)\n\n    return action_index",
          "objective": -178.91505973583614,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in action_indices])\n    selection_counts = np.array([len(score_set[action]) for action in action_indices])\n\n    # Exploration factor (higher for early slots, decreases over time)\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n\n    # Calculate score adjustments for exploration\n    exploration_scores = np.sqrt(exploration_factor * np.log(total_selection_count + 1) / (selection_counts + 1))\n\n    # Combine exploitation and exploration scores\n    adjusted_scores = scores + exploration_scores\n\n    # Select action with the highest adjusted score\n    action_index = action_indices[np.argmax(adjusted_scores)]\n\n    return action_index",
          "objective": -177.82173271822717,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores for each action with safe handling for empty lists\n    avg_scores = {action_index: (np.mean(scores) if scores else 0.0) for action_index, scores in score_set.items()}\n    \n    # Define the exploration factor using a decaying epsilon strategy\n    min_epsilon = 0.05\n    max_epsilon = 0.8\n    epsilon = max(min_epsilon, max_epsilon * (1 - (current_time_slot / total_time_slots)))\n\n    # Calculate selection counts for each action\n    selection_counts = {action_index: len(scores) for action_index, scores in score_set.items()}\n    total_actions = len(score_set)\n    \n    # Create a score adjustment based on selection counts to encourage exploration of less tried actions\n    exploration_scores = {\n        action_index: avg_scores[action_index] + (1 / (1 + selection_counts[action_index] if selection_counts[action_index] > 0 else total_selection_count))\n        for action_index in avg_scores\n    }\n    \n    # Randomly decide between exploration and exploitation\n    if total_selection_count == 0 or np.random.rand() < epsilon:\n        # Explore: Select action with highest adjusted score\n        action_index = max(exploration_scores, key=exploration_scores.get)\n    else:\n        # Exploit: Select action with highest average score\n        action_index = max(avg_scores, key=avg_scores.get)\n\n    return action_index",
          "objective": -177.46635595715483,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores, handling empty score lists\n    avg_scores = {action_index: np.mean(scores) if scores else 0.0 for action_index, scores in score_set.items()}\n    \n    # Calculate action selection counts\n    selection_counts = {action_index: len(scores) for action_index, scores in score_set.items()}\n    \n    # Compute the exploration factor\n    min_epsilon = 0.05\n    max_epsilon = 0.8\n    decay_rate = total_selection_count / (total_time_slots * 8)\n    epsilon = max(min_epsilon, max_epsilon * (1 - decay_rate))\n    \n    # UCB-based exploration scores\n    exploration_scores = {\n        action_index: avg_scores[action_index] + np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts[action_index] + 1)) \n        if selection_counts[action_index] > 0 else float('inf') \n        for action_index in avg_scores\n    }\n\n    # Weighted selection combining exploitation and exploration\n    total_score = sum(avg_scores.values())\n    weighted_avg_scores = {\n        action_index: (avg_scores[action_index] * (1 - epsilon) + exploration_scores[action_index] * epsilon) / (total_score if total_score > 0 else 1)\n        for action_index in avg_scores\n    }\n    \n    # Select action based on the calculated weighted scores\n    action_index = max(weighted_avg_scores, key=weighted_avg_scores.get)\n    \n    return action_index",
          "objective": -177.1931597414857,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Adaptive exploration parameter\n    epsilon_initial = 1.0\n    epsilon_min = 0.05\n    epsilon_decay_factor = 0.99\n    remaining_time_slots = total_time_slots - current_time_slot\n    epsilon = max(epsilon_min, epsilon_initial * (epsilon_decay_factor ** (total_time_slots - remaining_time_slots)))\n    \n    # Explore bonus to encourage selection of under-explored actions\n    explore_bonus = np.where(selection_counts == 0, np.inf, np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1)))\n\n    # Combined scores: prioritize exploitation with a damping factor\n    combined_scores = avg_scores + epsilon * explore_bonus\n    \n    # Select action with the highest score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -177.11760834170778,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores and counts for each action\n    avg_scores = {\n        action_index: (np.mean(scores) if scores else 0.0)\n        for action_index, scores in score_set.items()\n    }\n    \n    counts = {\n        action_index: len(scores)\n        for action_index, scores in score_set.items()\n    }\n    \n    # Calculate exploration factor\n    exploration_factor = 1.0 / (np.array(list(counts.values())) + 1)  # Add 1 to avoid division by zero\n    exploration_factor = exploration_factor / exploration_factor.sum()\n    \n    # Calculate dynamic epsilon based on the current time slot\n    epsilon = max(0.1, 1 - current_time_slot / total_time_slots)  \n    \n    # Decide whether to explore or exploit\n    if np.random.rand() < epsilon:\n        # Explore: Select an action probabilistically based on exploration factor\n        action_index = np.random.choice(list(avg_scores.keys()), p=exploration_factor)\n    else:\n        # Exploit: Select the action with the highest average score\n        action_index = max(avg_scores, key=avg_scores.get)\n    \n    return action_index",
          "objective": -176.45163258147093,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize average scores and selection counts\n    avg_scores = {action_index: np.mean(scores) if scores else 0.0 for action_index, scores in score_set.items()}\n    selection_counts = {action_index: len(scores) for action_index, scores in score_set.items()}\n\n    # Dynamic exploration rate\n    min_epsilon = 0.05\n    max_epsilon = 0.8\n    epsilon = min_epsilon + (max_epsilon - min_epsilon) * (1 - (current_time_slot / total_time_slots))\n\n    if total_selection_count == 0:\n        return np.random.randint(0, 8)\n\n    # Calculate effective scores for exploration\n    effective_scores = {\n        action_index: avg_scores[action_index] + (1 / (1 + selection_counts[action_index])) for action_index in avg_scores\n    }\n\n    # Epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        action_index = max(effective_scores, key=effective_scores.get)  # Explore\n    else:\n        action_index = max(avg_scores, key=avg_scores.get)  # Exploit\n        \n    return action_index",
          "objective": -176.2846428294192,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize parameters\n    n_actions = 8\n    exploration_weight = (total_time_slots - current_time_slot) / total_time_slots\n    epsilon_decay = 1.0 / (current_time_slot + 1)  # Decrease exploration over time\n\n    # Initialize lists to hold averages and selection counts\n    averages = np.zeros(n_actions)\n    selection_counts = np.zeros(n_actions)\n\n    # Calculate averages and selection counts\n    for action_index in range(n_actions):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        averages[action_index] = np.mean(scores) if scores else 0\n\n    # Calculate UCB values and combined score\n    combined_scores = np.zeros(n_actions)\n    for action_index in range(n_actions):\n        if selection_counts[action_index] > 0:\n            exploration_value = exploration_weight * np.sqrt(np.log(total_selection_count) / selection_counts[action_index])\n        else:\n            exploration_value = np.inf  # Inf for untried actions\n        \n        combined_scores[action_index] = averages[action_index] + (epsilon_decay * exploration_value)\n    \n    # Select action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -176.00965602753791,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize parameters\n    epsilon = 1.0 / (current_time_slot + 1)  # Decreasing exploration rate\n\n    # Initialize lists to store average scores and exploration values\n    action_scores = []\n    \n    # Calculate scores for each action\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        avg_score = np.mean(scores) if scores else 0\n        selection_count = len(scores)  # How many times action was selected\n        \n        # UCB calculation\n        if selection_count > 0:\n            exploration_value = np.sqrt(np.log(total_selection_count) / selection_count)\n        else:\n            exploration_value = float('inf')  # Prioritize unselected actions\n\n        # Combine exploitation and exploration\n        combined_score = avg_score + (epsilon * exploration_value)\n        action_scores.append(combined_score)\n\n    # Apply softmax to get a probability distribution from action scores\n    exp_scores = np.exp(action_scores - np.max(action_scores))\n    probabilities = exp_scores / np.sum(exp_scores)\n    \n    # Sample an action based on the probability distribution\n    action_index = np.random.choice(np.arange(8), p=probabilities)\n    \n    return action_index",
          "objective": -175.04343129118598,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores and handle empty score lists\n    avg_scores = {action_index: np.mean(scores) if scores else 0.0 for action_index, scores in score_set.items()}\n    \n    # Action selection counts\n    selection_counts = {action_index: len(scores) for action_index, scores in score_set.items()}\n    \n    # Dynamic epsilon based on total selections and current time slot\n    min_epsilon = 0.05\n    max_epsilon = 0.8\n    epsilon_decay = (max_epsilon - min_epsilon) * (current_time_slot / total_time_slots)\n    epsilon = max(min_epsilon, max_epsilon - epsilon_decay)\n    \n    # UCB-based exploration bonuses\n    exploration_scores = {\n        action_index: np.sqrt(2 * np.log(total_selection_count + 1) / (selection_counts[action_index] + 1)) if selection_counts[action_index] > 0 else float('inf') \n        for action_index in avg_scores\n    }\n    \n    # Combined scores using the dynamic epsilon\n    combined_scores = {\n        action_index: (1 - epsilon) * avg_scores[action_index] + epsilon * exploration_scores[action_index]\n        for action_index in avg_scores\n    }\n    \n    # Select action based on the highest combined score\n    action_index = max(combined_scores, key=combined_scores.get)\n    \n    return action_index",
          "objective": -174.18106959575775,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n\n    # Calculate average scores and selection counts\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n\n    # Avoid division by zero for actions never selected\n    selection_counts = np.where(selection_counts == 0, 1, selection_counts)\n\n    # Dynamic epsilon-greedy parameters\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    epsilon_decay_rate = 0.95\n    epsilon = epsilon_min + (epsilon_max - epsilon_min) * (1 - (current_time_slot / total_time_slots) ** 0.5)\n\n    # Exploration bonus, utilizing softmax policy instead\n    explore_bonus = np.sqrt(np.log(total_selection_count + 1) / selection_counts)\n    \n    # Combined scores: weighted exploitation and exploration\n    combined_scores = avg_scores + epsilon * explore_bonus\n\n    # Select action with the highest combined score\n    action_index = np.argmax(combined_scores)\n\n    return action_index",
          "objective": -174.15985306656376,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in action_indices])\n    selection_counts = np.array([len(score_set[action]) for action in action_indices])\n    \n    # Calculate exploration factor\n    epsilon_initial = 1.0\n    epsilon_final = 0.1\n    exploration_decay = (epsilon_initial - epsilon_final) / total_time_slots\n    epsilon = max(epsilon_final, epsilon_initial - exploration_decay * current_time_slot)\n\n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        # Bayesian Upper Confidence Bound\n        exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n        ucb_scores = scores + exploration_bonus\n        action_index = action_indices[np.argmax(ucb_scores)]\n\n    return action_index",
          "objective": -173.7206290286734,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    avg_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        if scores:\n            avg_score = np.mean(scores)\n        else:\n            avg_score = 0.0\n        \n        # Factor in the exploration term based on selection count\n        if total_selection_count > 0:\n            exploration_factor = np.sqrt(np.log(total_selection_count) / (len(scores) + 1))  # Add 1 to prevent division by zero\n        else:\n            exploration_factor = 1.0  # If nothing has been selected, we explore all\n        \n        effective_score = avg_score + exploration_factor\n        \n        avg_scores.append(effective_score)\n\n    # Normalizing scores over time slots to promote diversity\n    time_adjusted_scores = [\n        avg_scores[action_index] * (total_time_slots - current_time_slot) / total_time_slots \n        for action_index in range(8)\n    ]\n    \n    action_index = np.argmax(time_adjusted_scores)\n    \n    return action_index",
          "objective": -173.66514097370163,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores for each action\n    avg_scores = {\n        action_index: (np.mean(scores) if scores else 0.0)\n        for action_index, scores in score_set.items()\n    }\n\n    # Dynamic epsilon-greedy for exploration\n    epsilon = max(0.1, (total_time_slots - current_time_slot) / total_time_slots)\n\n    # Select random action for exploration\n    if np.random.rand() < epsilon:\n        return np.random.choice(list(score_set.keys()))\n\n    # Calculate exploration bonuses using UCB\n    exploration_bonuses = {}\n    for action_index in range(8):\n        if action_index in score_set and score_set[action_index]:\n            n_sq = len(score_set[action_index])\n            exploration_bonuses[action_index] = np.sqrt(2 * np.log(total_selection_count + 1) / (n_sq + 1))\n        else:\n            exploration_bonuses[action_index] = np.inf  # Encourage exploration for untested actions\n\n    # Compute combined scores\n    combined_scores = {\n        action_index: avg_scores[action_index] + exploration_bonuses.get(action_index, 0)\n        for action_index in range(8)\n    }\n\n    # Select the action index with the highest score\n    action_index = max(combined_scores, key=combined_scores.get)\n\n    return action_index",
          "objective": -173.27786284186334,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores and selection counts\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Adaptive epsilon calculation\n    epsilon_max = 1.0\n    epsilon_min = 0.05\n    epsilon_decay_duration = total_time_slots / 2  # decay over half of the time slots\n    epsilon = epsilon_min + (epsilon_max - epsilon_min) * (1 - np.clip(current_time_slot / epsilon_decay_duration, 0, 1))\n    \n    # Exploration bonus\n    explore_bonus = np.where(selection_counts == 0, np.inf, np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1)))\n\n    # Combined scores and adjustment for exploration\n    combined_scores = avg_scores + epsilon * explore_bonus\n    \n    # Select action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -173.13488426830304,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Adaptive epsilon based on the current time slot\n    epsilon = max(0.1 * (1 - current_time_slot / total_time_slots), 0.01)\n    action_indices = list(score_set.keys())\n    \n    # Calculate average scores for each action\n    avg_scores = np.zeros(len(action_indices))\n    for i, action in enumerate(action_indices):\n        if score_set[action]:  # Check if the list is not empty\n            avg_scores[i] = np.mean(score_set[action])\n        else:\n            avg_scores[i] = 0  # If no scores, zero average score\n\n    # Epsilon-greedy selection with an adaptive exploration strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)  # Explore\n    else:\n        action_index = action_indices[np.argmax(avg_scores)]  # Exploit\n\n    return action_index",
          "objective": -171.40724224031456,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize parameters\n    num_actions = 8\n    exploration_weight = 0.5  # Weight for exploration vs exploitation\n    exploit_weight = 1 - exploration_weight\n    \n    # Initialize lists to hold average scores and selection counts\n    avg_scores = []\n    selection_counts = []\n    \n    for action_index in range(num_actions):\n        scores = score_set.get(action_index, [])\n        avg_score = np.mean(scores) if scores else 0\n        selection_count = len(scores)\n        \n        # Store average score and selection count\n        avg_scores.append(avg_score)\n        selection_counts.append(selection_count)\n    \n    # Calculate exploration values using softmax for exploration\n    exploration_values = [\n        np.sqrt((np.log(total_selection_count + 1) / (count + 1))) if count > 0 else np.inf \n        for count in selection_counts\n    ]\n\n    # Normalize exploration values with a softmax approach\n    exploration_probs = np.exp(exploration_weight * np.array(exploration_values))\n    exploration_probs /= np.sum(exploration_probs)\n\n    # Combine scores with exploitation-oriented choice\n    combined_scores = np.array(avg_scores) * exploit_weight + exploration_probs * exploration_weight\n    \n    # Select action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -171.39888873284377,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores and selection counts\n    avg_scores = np.array([np.mean(scores) if scores else 0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Handle edge case when total_selection_count is 0\n    if total_selection_count == 0:\n        return np.random.randint(num_actions)\n    \n    # Dynamic epsilon-greedy strategy\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    epsilon_decay = 0.1  # Decay rate to control how quickly epsilon decreases\n    epsilon = max(epsilon_min, epsilon_max * (1 - current_time_slot / total_time_slots))\n    \n    # Exploration term\n    exploration_bonus = np.sqrt(np.log(total_selection_count) / (selection_counts + 1e-5))\n    \n    # Combined scores, encouraging selection of actions with higher average scores\n    combined_scores = avg_scores + epsilon * exploration_bonus\n    \n    # Select the action using a balanced approach\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -169.32680588175702,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Dynamic epsilon based on current time slot\n    epsilon = max(1.0 - (current_time_slot / total_time_slots), 0.1)\n    \n    action_values = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n        \n        # Calculate the average score with proper handling of empty score lists\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate a confidence interval term for exploration\n        if selection_count > 0:\n            exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1))\n        else:\n            exploration_bonus = np.sqrt(np.log(total_selection_count + 1))\n        \n        # Calculate the action value\n        action_value = average_score + exploration_bonus\n        \n        action_values.append(action_value)\n    \n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        # Exploration: select a random action\n        action_index = np.random.randint(0, 8)\n    else:\n        # Exploitation: select the action with the highest value\n        action_index = np.argmax(action_values)\n    \n    return action_index",
          "objective": -168.14572907442457,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))  # Decay epsilon over time\n    random_value = np.random.rand()\n    \n    if random_value < epsilon:  # Exploration\n        action_index = np.random.randint(0, 8)  # Select a random action\n    else:  # Exploitation\n        averages = []\n        for action_index in range(8):\n            scores = score_set[action_index]\n            if len(scores) > 0:\n                average_score = np.mean(scores)\n            else:\n                average_score = 0  # If no scores, assume average score is 0\n            averages.append(average_score)\n        \n        action_index = np.argmax(averages)  # Select action with the highest average score\n        \n    return action_index",
          "objective": -168.02147629112355,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_factor = 0.1  # Adjust this value as needed for exploration vs exploitation\n    action_values = []\n\n    for action_index in range(8):\n        scores = score_set[action_index]\n        selection_count = len(scores)\n        \n        # Compute average score and handle division by zero\n        if selection_count > 0:\n            average_score = np.mean(scores)\n        else:\n            average_score = 0\n        \n        # Calculate exploration bonus based on exploration factor\n        exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1))\n        \n        # Combine average score and exploration bonus\n        action_value = average_score + exploration_bonus\n        action_values.append(action_value)\n    \n    # Select the action with the highest value\n    action_index = np.argmax(action_values)\n    \n    return action_index",
          "objective": -167.33186484100403,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0.0 for action in action_indices])\n    selection_counts = np.array([len(score_set[action]) for action in action_indices])\n\n    # Dynamic exploration factor based on time slot with a more aggressive decay\n    decay_rate = 0.2\n    exploration_factor = np.exp(-decay_rate * (total_time_slots - current_time_slot))\n\n    # Adding a small epsilon to avoid division by zero\n    selection_counts = np.clip(selection_counts, 1, None)  # Ensure counts are at least 1\n    exploration_scores = (exploration_factor * np.log(total_selection_count + 1)) / selection_counts\n\n    # Combine scores with exploration weights, prioritizing higher scores\n    adjusted_scores = scores + exploration_scores\n\n    # Select action with the highest adjusted score while using a stochastic approach\n    probabilities = adjusted_scores / np.sum(adjusted_scores)\n    action_index = np.random.choice(action_indices, p=probabilities)\n\n    return action_index",
          "objective": -166.55251856560955,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores, protecting against empty score lists\n    avg_scores = {\n        action_index: (np.mean(scores) if scores else 0.0)\n        for action_index, scores in score_set.items()\n    }\n    \n    num_actions = len(score_set)\n    \n    # Dynamic exploration rate\n    min_epsilon = 0.05\n    max_epsilon = 0.8\n    epsilon = min_epsilon + (max_epsilon - min_epsilon) * (1 - (current_time_slot / total_time_slots)) \n    \n    # Selection counts\n    selection_counts = {action_index: len(scores) for action_index, scores in score_set.items()}\n    \n    # Select action\n    if total_selection_count == 0 or np.random.rand() < epsilon:\n        # Exploration: use inverse of selection counts for less-explored actions\n        selection_priority = {\n            action_index: (1 / (1 + selection_counts[action_index])) * (avg_scores[action_index] + 1e-10)\n            for action_index in range(num_actions)\n        }\n        action_index = max(selection_priority, key=selection_priority.get)\n    else:\n        # Exploitation: pick action with the highest average score\n        action_index = max(avg_scores, key=avg_scores.get)\n    \n    return action_index",
          "objective": -165.83673852187576,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in action_indices])\n    selection_counts = np.array([len(score_set[action]) for action in action_indices])\n\n    # Dynamic epsilon decay for exploration\n    epsilon_initial = 1.0\n    epsilon_final = 0.1\n    exploration_decay = (epsilon_initial - epsilon_final) / total_time_slots\n    epsilon = max(epsilon_final, epsilon_initial - exploration_decay * current_time_slot)\n\n    # Select action using epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        # Use Enhanced UCB\n        selection_counts = np.clip(selection_counts, 1, None)  # Prevent division by zero\n        mean_scores = np.nan_to_num(scores)\n        confidence_bounds = np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts)\n        ucb_scores = mean_scores + confidence_bounds\n        action_index = action_indices[np.argmax(ucb_scores)]\n\n    return action_index",
          "objective": -165.00804314024785,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters\n    n_actions = 8\n    exploration_factor = max(0.1, 1 - (current_time_slot / total_time_slots))\n    \n    # Calculate average scores and visit counts\n    averages = np.zeros(n_actions)\n    visit_counts = np.zeros(n_actions)\n\n    for action_index in range(n_actions):\n        scores = score_set[action_index]\n        visit_counts[action_index] = len(scores)\n        \n        if visit_counts[action_index] > 0:\n            averages[action_index] = np.mean(scores)\n    \n    # UCB calculation for balancing exploration and exploitation\n    if total_selection_count > 0:\n        confidence_bounds = np.sqrt(np.log(total_selection_count) / (visit_counts + 1e-5))  # Avoid division by zero\n        values = averages + confidence_bounds * exploration_factor\n    else:\n        values = np.random.rand(n_actions)  # Random if no selections made yet\n    \n    # Select action based on calculated values\n    action_index = np.argmax(values)\n    \n    return action_index",
          "objective": -164.9853487084359,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = np.array(list(score_set.keys()))\n    \n    # Calculate average scores; handle division by zero\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in action_indices])\n    \n    # Selection counts\n    selection_counts = np.array([len(score_set[action]) for action in action_indices])\n    \n    # Dynamic epsilon for exploration vs exploitation\n    epsilon = max(0.1, 0.9 * (1 - current_time_slot / total_time_slots))\n    \n    # Weighted exploration factor\n    exploration_factor = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n\n    if np.random.rand() < epsilon:  # Exploration\n        action_index = np.random.choice(action_indices)\n    else:  # Exploitation\n        adjusted_scores = scores + exploration_factor\n        action_index = action_indices[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -164.9060103532791,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in action_indices])\n    selection_counts = np.array([len(score_set[action]) for action in action_indices])\n    \n    # Avoid division by zero by using max of selection counts and allowing exploration\n    adjusted_scores = np.divide(scores, selection_counts, out=np.zeros_like(scores, dtype=float), where=selection_counts > 0)\n    \n    # Epsilon decay strategy for exploration\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n    \n    # Randomly select an action for exploration or exploit the best one\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        # Use a softmax approach to balance exploration and exploitation\n        probabilities = np.exp(adjusted_scores - np.max(adjusted_scores))  # Subtract max for numerical stability\n        probabilities /= np.sum(probabilities)\n        action_index = np.random.choice(action_indices, p=probabilities)\n    \n    return action_index",
          "objective": -164.70916220786,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize parameters\n    exploration_factor = np.sqrt(np.log(total_selection_count + 1) / (np.array([len(scores) for scores in score_set.values()]) + 1))\n    avg_scores = np.array([np.mean(scores) if scores else 0 for scores in score_set.values()])\n    \n    # Calculate epsilon decay\n    epsilon = 1.0 - (current_time_slot / total_time_slots)\n\n    # Combine exploitation and exploration with adjusted exploration factor\n    combined_scores = avg_scores + (epsilon * exploration_factor)\n    \n    # Select action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -161.34380861255391,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize parameters\n    epsilon = 1.0 / (current_time_slot + 1)\n    \n    averages = []\n    exploration_values = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        avg_score = np.mean(scores) if scores else 0\n        selection_count = len(scores)  # Number of times the action has been selected\n        \n        # UCB calculation\n        if selection_count > 0:\n            exploration_value = np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1))\n        else:\n            exploration_value = float('inf')  # Encourage exploration for unselected actions\n        \n        # Combine exploitation and exploration\n        combined_score = avg_score + (epsilon * exploration_value)\n        averages.append(combined_score)\n    \n    # Select action with the highest combined score\n    action_index = np.argmax(averages)\n    \n    return action_index",
          "objective": -161.04216968183013,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize parameters\n    exploration_weight = 1.5  # Increased weight to favor exploration\n    exploitation_weight = 1.0  # Keep exploitation weight stable\n    averages = []\n\n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        avg_score = np.mean(scores) if scores else 0\n        selection_count = len(scores)\n\n        # Handle selection count to avoid division by zero\n        selection_count = selection_count if selection_count > 0 else 1\n\n        # Calculate exploration component\n        exploration_value = np.sqrt(np.log(total_selection_count + 1) / selection_count)\n        exploration_value *= (1 - (current_time_slot / total_time_slots))\n\n        # Combine scores with exploration and exploitation\n        combined_score = (exploitation_weight * avg_score) + (exploration_weight * exploration_value)\n        averages.append(combined_score)\n\n    # Select action with the highest combined score\n    action_index = np.argmax(averages)\n    \n    return action_index",
          "objective": -160.88429364762422,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores for each action\n    avg_scores = {action_index: (np.mean(scores) if scores else 0.0) for action_index, scores in score_set.items()}\n    \n    # Epsilon decay for exploration\n    epsilon_max = 1.0\n    epsilon_min = 0.05\n    epsilon_decay = (epsilon_max - epsilon_min) * (current_time_slot / total_time_slots)\n    epsilon = max(epsilon_min, epsilon_max - epsilon_decay)\n\n    # Exploration-exploitation balance\n    combined_scores = {}\n    for action_index in range(8):\n        if action_index in score_set and score_set[action_index]:\n            n_sq = len(score_set[action_index])\n            exploration_bonus = np.sqrt((2 * np.log(total_selection_count + 1)) / (n_sq + 1))\n            combined_scores[action_index] = avg_scores[action_index] + exploration_bonus\n        else:\n            combined_scores[action_index] = np.inf  # Favor unexplored actions\n\n    # Apply epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice([i for i in range(8) if i in score_set])\n    else:\n        action_index = max(combined_scores, key=combined_scores.get)\n    \n    return action_index",
          "objective": -160.85215894595828,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores and counts for each action\n    avg_scores = np.array([\n        np.mean(scores) if scores else 0.0 for scores in score_set.values()\n    ])\n    \n    action_counts = np.array([\n        len(scores) for scores in score_set.values()\n    ])\n\n    # Use Upper Confidence Bound (UCB) for exploration-exploitation balance\n    total_count = np.sum(action_counts)\n    if total_count == 0:\n        return np.random.randint(num_actions)\n\n    ucb_values = (avg_scores + \n                  np.sqrt(2 * np.log(total_selection_count + 1) / (action_counts + 1e-5)))\n    \n    # Select action based on derived UCB values\n    action_index = np.argmax(ucb_values)\n    \n    # Introduce exploration factor based on current_time_slot\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n    if np.random.rand() < exploration_factor:\n        unexplored_actions = np.where(action_counts == 0)[0]\n        if unexplored_actions.size > 0:\n            return np.random.choice(unexplored_actions)\n    \n    return action_index",
          "objective": -159.27460181385177,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores and total scores for each action\n    avg_scores = {action_index: (np.mean(scores) if scores else 0.0) for action_index, scores in score_set.items()}\n    total_scores = {action_index: sum(scores) for action_index, scores in score_set.items()}\n    \n    # Dynamic exploration probability with minimum thresholds\n    min_epsilon = 0.05\n    max_epsilon = 0.5\n    epsilon = min_epsilon + (max_epsilon - min_epsilon) * (1 - (current_time_slot / total_time_slots))\n    \n    # Calculate selection frequency\n    selection_counts = {action_index: len(scores) for action_index, scores in score_set.items()}\n    \n    # If no actions have been selected, randomly select one\n    if total_selection_count == 0:\n        return np.random.randint(0, 8)\n\n    # Calculate exploration and exploitation scores\n    exploration_scores = {\n        action_index: (avg_scores[action_index] + (1 / (1 + selection_counts[action_index]))) \n        for action_index in avg_scores\n    }\n\n    # Selection strategy based on exploration vs exploitation\n    if np.random.rand() < epsilon:\n        action_index = max(exploration_scores, key=exploration_scores.get)\n    else:\n        action_index = max(avg_scores, key=avg_scores.get)\n\n    return action_index",
          "objective": -155.10099070585937,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_factor = max(0, (total_time_slots - current_time_slot) / total_time_slots)\n    \n    averages = []\n    for action_index in range(8):\n        if score_set[action_index]:\n            avg_score = np.mean(score_set[action_index])\n        else:\n            avg_score = 0\n        \n        selection_count = len(score_set[action_index])\n        exploration_value = exploration_factor / (selection_count + 1)  # Adding 1 to avoid division by zero\n        combined_score = avg_score + exploration_value\n        \n        averages.append(combined_score)\n    \n    action_index = np.argmax(averages)\n    \n    return action_index",
          "objective": -152.01966506687796,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores for each action, handling empty score lists\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    \n    # Normalize average scores to relative performance\n    normalized_avg_scores = avg_scores / (total_selection_count + 1)  # Plus one to avoid division by zero\n    \n    # Count how many times each action has been selected\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Dynamic epsilon-greedy parameters\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    epsilon_decay = 0.9\n    epsilon = epsilon_max * (epsilon_min / epsilon_max) ** (current_time_slot / total_time_slots)\n    \n    # Exploration bonus\n    explore_bonus = np.where(selection_counts == 0, np.inf, \n                             np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1)))\n    \n    # Calculate combined scores\n    combined_scores = normalized_avg_scores + epsilon * explore_bonus\n\n    # Select action based on combined scores\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -150.54087121892098,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores with safeguard against empty selection histories\n    avg_scores = np.array([np.mean(scores) if scores else 0 for scores in score_set.values()])\n    \n    # Capture selection counts\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Incorporate a decay mechanism for exploration probability\n    epsilon_min = 0.05\n    epsilon_initial = 1.0\n    epsilon_decay = 0.999\n    epsilon = max(epsilon_min, epsilon_initial * (epsilon_decay ** current_time_slot))\n    \n    # Explore unselected actions\n    explore_bonus = (total_selection_count - selection_counts) < 1\n    \n    # Calculation of exploration bonus\n    exploration_bonus = np.where(explore_bonus, np.inf, np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1)))\n    \n    # Combine scores to weigh exploration against exploitation\n    combined_scores = avg_scores + epsilon * exploration_bonus\n    \n    # Select action index with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -148.070260611281,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    avg_scores = []\n    exploration_factor = 0.1\n    \n    for action in action_indices:\n        if len(score_set[action]) > 0:\n            average_score = np.mean(score_set[action])\n        else:\n            average_score = 0\n        \n        exploration_bonus = exploration_factor / (1 + len(score_set[action])) if total_selection_count > 0 else 1\n        avg_scores.append(average_score + exploration_bonus)\n    \n    weights = np.array(avg_scores)\n    weights /= np.sum(weights) if np.sum(weights) > 0 else 1  # Normalizing weights\n\n    action_index = np.random.choice(action_indices, p=weights)\n    \n    return action_index",
          "objective": -147.77932505268112,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores and selection counts safely\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Set exploration parameters\n    epsilon_initial = 1.0\n    epsilon_min = 0.05\n    epsilon_decay = 0.999\n    epsilon = max(epsilon_min, epsilon_initial * (epsilon_decay ** current_time_slot))\n    \n    # Exploration factor: ensure all actions are selected at least once\n    explore_bonus = np.where(selection_counts == 0, np.inf, np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1)))\n    \n    # Combined scores considering exploration\n    combined_scores = avg_scores + epsilon * explore_bonus\n    \n    # Select the action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -146.8506328517997,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Parameters for epsilon-greedy strategy\n    min_epsilon = 0.1\n    epsilon = max(min_epsilon, (total_time_slots - current_time_slot) / total_time_slots)\n\n    # Calculate average scores and selection counts for each action\n    avg_scores = {}\n    selection_counts = {}\n    \n    for action_index, scores in score_set.items():\n        if scores:\n            avg_scores[action_index] = np.mean(scores)\n            selection_counts[action_index] = len(scores)\n        else:\n            avg_scores[action_index] = 0.0\n            selection_counts[action_index] = 0\n\n    # Calculate Upper Confidence Bound (UCB) for each action\n    ucb_scores = {}\n    for action_index in range(8):\n        count = selection_counts.get(action_index, 0)\n        avg_score = avg_scores.get(action_index, 0.0)\n        \n        if count == 0:\n            ucb_scores[action_index] = float('inf')  # Prioritize unselected actions\n        else:\n            ucb_scores[action_index] = avg_score + np.sqrt((2 * np.log(total_selection_count)) / count)\n\n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        # Explore: select an action randomly from those available\n        action_index = np.random.choice([i for i in range(8) if i in score_set])\n    else:\n        # Exploit: select the action with the highest UCB\n        action_index = max(ucb_scores, key=ucb_scores.get)\n\n    return action_index",
          "objective": -145.56900262408814,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores for each action\n    avg_scores = {\n        action_index: (np.mean(scores) if scores else 0.0)\n        for action_index, scores in score_set.items()\n    }\n    \n    # Calculate exploration rate (epsilon) inversely proportional to current time_slot\n    min_epsilon = 0.05\n    max_epsilon = 0.8\n    epsilon = min_epsilon + (max_epsilon - min_epsilon) * (1 - (current_time_slot / total_time_slots))\n\n    # Calculate action selection probabilities\n    prob_scores = {\n        action_index: avg_scores[action_index] * (total_selection_count / (len(scores) + 1)) if scores else 0\n        for action_index, scores in score_set.items()\n    }\n    \n    # Normalize probabilities\n    total_prob = sum(prob_scores.values())\n    if total_prob > 0:\n        normalized_probs = {action_index: score / total_prob for action_index, score in prob_scores.items()}\n    else:\n        normalized_probs = {action_index: 1 / len(score_set) for action_index in score_set.keys()}\n\n    # Decision based on epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        # Exploration: Select action randomly based on normalized probabilities\n        action_index = np.random.choice(list(normalized_probs.keys()), p=list(normalized_probs.values()))\n    else:\n        # Exploitation: Select action with highest average score\n        action_index = max(avg_scores, key=avg_scores.get)\n\n    return action_index",
          "objective": -142.50799611280834,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    \n    # Calculate scores and selection counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in action_indices])\n    selection_counts = np.array([len(score_set[action]) for action in action_indices])\n    \n    # Avoid zero division\n    selection_counts = np.clip(selection_counts, 1, None)\n\n    # Dynamic exploration factor\n    epsilon_initial = 1.0\n    epsilon_final = 0.1\n    exploration_decay = (epsilon_initial - epsilon_final) / total_time_slots\n    epsilon = max(epsilon_final, epsilon_initial - exploration_decay * current_time_slot)\n\n    # Epsilon-greedy action selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        # Use Upper Confidence Bound with Added Exploration Factor\n        ucb_scores = scores + np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts)\n        exploration_bonus = (total_time_slots - current_time_slot) / total_time_slots  # Increase preference for under-picked actions\n        action_index = action_indices[np.argmax(ucb_scores * exploration_bonus)]\n\n    return action_index",
          "objective": -142.11994351785043,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores for each action, default to 0 if no scores exist\n    avg_scores = {action_index: np.mean(scores) if scores else 0.0 for action_index, scores in score_set.items()}\n    \n    # Dynamic epsilon based on current time slot and total time slots\n    min_epsilon = 0.05\n    max_epsilon = 0.8\n    epsilon_decay = (current_time_slot / total_time_slots)\n    epsilon = max(min_epsilon, max_epsilon * (1 - epsilon_decay))\n    \n    # Count the number of selections for each action\n    selection_counts = {action_index: len(scores) for action_index, scores in score_set.items()}\n    \n    # Handle exploration strategy (UCB)\n    exploration_scores = {\n        action_index: avg_scores[action_index] + np.sqrt(2 * np.log(total_selection_count + 1) / (count + 1))\n        if count > 0 else float('inf') \n        for action_index, count in selection_counts.items()\n    }\n\n    # Random selection based on epsilon\n    if np.random.rand() < epsilon:\n        action_index = max(exploration_scores, key=exploration_scores.get)\n    else:\n        action_index = max(avg_scores, key=avg_scores.get)\n    \n    return action_index",
          "objective": -140.95293128178974,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize average scores and selection counts\n    avg_scores = np.zeros(8)\n    selection_counts = np.zeros(8)\n    \n    for action_index, scores in score_set.items():\n        selection_counts[action_index] = len(scores)\n        avg_scores[action_index] = np.mean(scores) if scores else 0.0\n\n    # Dynamic exploration rate\n    exploration_rate = max(0.1, 1 - (total_selection_count / (total_time_slots * 5)))\n    \n    # Exploration decision\n    if np.random.rand() < exploration_rate:\n        action_index = np.random.choice([action for action in range(8) if action in score_set])\n    else:\n        # Calculate exploration bonuses (UCB)\n        exploration_bonuses = np.zeros(8)\n        for action_index in range(8):\n            if selection_counts[action_index] > 0:\n                exploration_bonuses[action_index] = np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts[action_index])\n            else:\n                exploration_bonuses[action_index] = np.inf  # Infinite exploration bonus for unselected actions\n        \n        # Combine average scores and exploration bonuses\n        combined_scores = avg_scores + exploration_bonuses\n\n        # Penalize scores based on the remaining time slots\n        time_factor = (total_time_slots - current_time_slot) / total_time_slots\n        adjusted_scores = combined_scores * time_factor\n\n        # Select the action with the highest adjusted score\n        action_index = np.argmax(adjusted_scores)\n\n    return action_index",
          "objective": -138.89467610038014,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in action_indices])\n    selection_counts = np.array([len(score_set[action]) for action in action_indices])\n    \n    # Adaptive exploration factor\n    epsilon_initial = 1.0\n    epsilon_final = 0.1\n    epsilon = epsilon_initial - (epsilon_initial - epsilon_final) * (current_time_slot / total_time_slots)\n    \n    # Calculate UCB scores\n    ucb_scores = np.zeros(len(action_indices))\n    for i, action in enumerate(action_indices):\n        if selection_counts[i] > 0:\n            ucb_scores[i] = scores[i] + np.sqrt(2 * np.log(total_selection_count + 1) / selection_counts[i])\n        else:\n            ucb_scores[i] = np.inf  # Give unselected actions a high score for exploration\n\n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(action_indices)\n    else:\n        action_index = action_indices[np.argmax(ucb_scores)]\n    \n    return action_index",
          "objective": -137.06319070373675,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    average_scores = []\n    \n    for action_index in action_indices:\n        scores = score_set[action_index]\n        if len(scores) > 0:\n            avg_score = np.mean(scores)\n        else:\n            avg_score = 0\n        average_scores.append(avg_score)\n\n    exploration_rate = max(0.1, 1 - (total_selection_count / (total_time_slots * 2)))\n    exploration_values = [np.sqrt(np.log(total_selection_count + 1) / (len(score_set[action_index]) + 1)) for action_index in action_indices]\n\n    combined_scores = [(1 - exploration_rate) * average_scores[action_index] + exploration_rate * exploration_values[action_index] for action_index in range(len(action_indices))]\n    \n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -133.3264300826284,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores for each action with error handling\n    avg_scores = {\n        action_index: np.mean(scores) if scores else 0.0\n        for action_index, scores in score_set.items()\n    }\n    \n    # Calculate selection counts\n    selection_counts = {\n        action_index: len(scores)\n        for action_index, scores in score_set.items()\n    }\n    \n    # Exploration term using Thompson Sampling approach\n    exploration_bonuses = {\n        action_index: np.sqrt(2 * np.log(total_selection_count + 1) / (count + 1))\n        if count > 0 else np.inf\n        for action_index, count in selection_counts.items()\n    }\n    \n    # Combine average scores with exploration bonuses\n    combined_scores = {\n        action_index: avg_scores[action_index] + exploration_bonuses[action_index]\n        for action_index in score_set.keys()\n    }\n\n    # Dynamic adjustment for remaining time slots\n    time_factor = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = {\n        action_index: combined_scores[action_index] * (1 + time_factor)\n        for action_index in score_set.keys()\n    }\n\n    # Select the action index with the highest adjusted score\n    action_index = max(adjusted_scores, key=adjusted_scores.get)\n    \n    return action_index",
          "objective": -130.41865549907746,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    \n    # Calculate average scores and counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in action_indices])\n    selection_counts = np.array([len(score_set[action]) for action in action_indices])\n    \n    # Handling division by zero\n    selection_counts = np.clip(selection_counts, 1, None)  # Ensure counts are at least 1\n    \n    # Epsilon decay for exploration\n    epsilon_max = 0.6\n    epsilon_min = 0.05\n    epsilon_decay = (epsilon_max - epsilon_min) * (current_time_slot / total_time_slots) + epsilon_min\n    epsilon = np.random.rand()\n    \n    # Selection based on epsilon-greedy strategy\n    if epsilon < epsilon_decay:\n        # Explore: Randomly select an action\n        action_index = np.random.choice(action_indices)\n    else:\n        # Exploit: Select action with the highest average score\n        adjusted_scores = scores + (np.log(total_selection_count + 1) / selection_counts)\n        action_index = action_indices[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -127.61242805179126,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores and ensure safe handling of division by zero\n    avg_scores = np.array([np.mean(scores) if scores else 0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()]) + 1  # +1 to avoid division by zero\n    \n    # Calculate exploration factor (UCB-style)\n    exploration_factor = np.sqrt(np.log(total_selection_count + 1) / selection_counts)\n    \n    # Epsilon decay based on current time slot\n    epsilon = 1.0 - (current_time_slot / total_time_slots)\n    \n    # Combine exploitation and exploration\n    combined_scores = avg_scores + (epsilon * exploration_factor)\n    \n    # Select action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -124.59396406559395,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores and handle empty score lists\n    avg_scores = {\n        action_index: (np.mean(scores) if scores else 0.0)\n        for action_index, scores in score_set.items()\n    }\n    \n    # Handle exploration using UCB (Upper Confidence Bound) strategy\n    exploration_factors = {\n        action_index: np.sqrt(np.log(total_selection_count + 1) / (len(scores) + 1)) if scores else np.sqrt(np.log(total_selection_count + 1))\n        for action_index, scores in score_set.items()\n    }\n\n    # Calculate effective scores with decay factor for the remaining time slots\n    decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n    effective_scores = {\n        action_index: (avg_scores[action_index] + exploration_factors[action_index]) * decay_factor\n        for action_index in range(8)\n    }\n    \n    # Select the action index with the highest effective score\n    action_index = max(effective_scores, key=effective_scores.get)\n    \n    return action_index",
          "objective": -117.46574727334553,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores for each action\n    avg_scores = {action_index: np.mean(scores) if scores else 0.0 for action_index, scores in score_set.items()}\n    \n    # Calculate selection counts for exploration\n    selection_counts = {action_index: len(scores) for action_index, scores in score_set.items()}\n    \n    # Exploration factor using UCB\n    exploration_bonuses = {\n        action_index: np.sqrt(2 * np.log(total_selection_count + 1) / (count + 1)) \n        if count > 0 else np.inf \n        for action_index, count in selection_counts.items()\n    }\n\n    # Combine average scores and exploration bonuses\n    combined_scores = {\n        action_index: avg_scores[action_index] + exploration_bonuses[action_index]\n        for action_index in range(8)\n    }\n\n    # Dynamic adjustment based on remaining time slots\n    time_factor = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = {\n        action_index: combined_scores[action_index] * time_factor\n        for action_index in range(8)\n    }\n\n    # Select the action index with the highest adjusted score\n    action_index = max(adjusted_scores, key=adjusted_scores.get)\n    \n    return action_index",
          "objective": -111.4760315127881,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    avg_scores = []\n    min_exploration_factor = 0.1\n    max_exploration_factor = 0.5\n    \n    # Calculate average scores and explore-exploit balance\n    for action in action_indices:\n        if score_set[action]:\n            average_score = np.mean(score_set[action])\n        else:\n            average_score = 0\n\n        # Calculate exploration factor based on selection history\n        if total_selection_count > 0:\n            exploration_bonus = min_exploration_factor / (1 + len(score_set[action]))\n        else:\n            exploration_bonus = max_exploration_factor\n\n        # Incorporate time decay into exploration bonus\n        time_decay_factor = (total_time_slots - current_time_slot) / total_time_slots\n        avg_scores.append(average_score + exploration_bonus * time_decay_factor)\n\n    # Normalize weights and select action based on calculated scores\n    weights = np.array(avg_scores)\n    weights /= np.sum(weights) if np.sum(weights) > 0 else 1  # Avoid division by zero\n\n    action_index = np.random.choice(action_indices, p=weights)\n    \n    return action_index",
          "objective": -96.86235415973437,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    \n    # Initialize scores and selection counts\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in action_indices])\n    selection_counts = np.array([len(score_set[action]) for action in action_indices])\n    \n    # Dynamic epsilon calculation\n    epsilon = 1 - (current_time_slot / total_time_slots)\n    epsilon = max(epsilon, 0.1)  # Ensure a minimum exploration probability\n    \n    # Explore with probability epsilon\n    if np.random.rand() < epsilon:\n        return np.random.choice(action_indices)\n\n    # Adjusted scores for exploitation and exploration\n    exploration_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1))\n    adjusted_scores = scores + exploration_bonus\n\n    # Select action with the highest adjusted score\n    action_index = action_indices[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -95.58985409083292,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores for each action\n    avg_scores = {}\n    for action_index, scores in score_set.items():\n        avg_scores[action_index] = np.mean(scores) if scores else 0.0\n\n    # Calculate the selection counts\n    selection_counts = {action_index: len(scores) for action_index, scores in score_set.items()}\n\n    # Dynamic exploration rate\n    min_epsilon = 0.05\n    max_epsilon = 0.8\n    epsilon = min_epsilon + (max_epsilon - min_epsilon) * (current_time_slot / total_time_slots)\n\n    # UCB based selection if total selections are sufficient\n    if total_selection_count > 0:\n        confidence_bounds = {\n            action_index: avg_scores[action_index] + np.sqrt((2 * np.log(total_selection_count)) / (selection_counts[action_index] + 1e-5))\n            for action_index in avg_scores\n        }\n        # Determine action using epsilon-greedy strategy with exploration and exploitation\n        if np.random.rand() < epsilon:\n            # Explore: Select based on selection counts inverse (to encourage less tried)\n            exploration_scores = {\n                action_index: avg_scores[action_index] + (1 / (selection_counts[action_index] + 1)) for action_index in avg_scores\n            }\n            action_index = max(exploration_scores, key=exploration_scores.get)\n        else:\n            # Exploit: Select action with the highest upper confidence bound\n            action_index = max(confidence_bounds, key=confidence_bounds.get)\n    else:\n        # If no selections, select randomly\n        action_index = np.random.randint(0, 8)\n\n    return action_index",
          "objective": -88.90676710709707,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    \n    # Calculate average scores and handle division by zero\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in action_indices])\n    selection_counts = np.array([len(score_set[action]) for action in action_indices])\n    \n    # Dynamic epsilon calculation that decreases over time\n    epsilon = 1 - (current_time_slot / total_time_slots)\n    \n    # Randomly explore with probability epsilon\n    if np.random.rand() < epsilon:\n        return np.random.choice(action_indices)\n    \n    # Calculate adjusted scores for exploitation\n    exploitation_scores = scores\n    \n    # Calculate exploration bonuses\n    exploration_scores = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1))\n    \n    # Combine exploitation and exploration scores\n    adjusted_scores = exploitation_scores + exploration_scores\n\n    # Select the action with the highest adjusted score\n    action_index = action_indices[np.argmax(adjusted_scores)]\n\n    return action_index",
          "objective": -70.65995452209631,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores for each action\n    avg_scores = {action_index: np.mean(scores) if scores else 0.0 for action_index, scores in score_set.items()}\n    \n    # Calculate dynamic exploration parameter (epsilon)\n    epsilon = max(0.1, 1 - total_selection_count / (total_time_slots * 2))  # Decrease exploration over time\n    exploration = np.random.rand() < epsilon\n\n    # Calculate exploration bonuses (UCB) if not exploring\n    exploration_bonuses = {\n        action_index: np.sqrt(2 * np.log(total_selection_count + 1) / (len(score_set[action_index]) + 1))\n        if action_index in score_set and score_set[action_index] else np.inf\n        for action_index in range(8)\n    }\n\n    # Combined scores\n    combined_scores = {\n        action_index: avg_scores[action_index] + (exploration_bonuses[action_index] if not exploration else 0)\n        for action_index in range(8)\n    }\n\n    # Penalize scores based on remaining time slots\n    time_factor = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = {action_index: combined_scores[action_index] * time_factor for action_index in range(8)}\n\n    # Select action\n    if exploration:\n        action_index = np.random.choice([action for action in range(8) if action in score_set])\n    else:\n        action_index = max(adjusted_scores, key=adjusted_scores.get)\n    \n    return action_index",
          "objective": -67.37707718967283,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    \n    # Calculate average scores and handle division by zero\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in action_indices])\n    selection_counts = np.array([len(score_set[action]) for action in action_indices])\n    \n    # Dynamic epsilon calculation that changes over time\n    epsilon = np.sqrt(1 - (current_time_slot / total_time_slots))\n    \n    # Random exploration with probability epsilon\n    if np.random.rand() < epsilon:\n        return np.random.choice(action_indices)\n    \n    # Calculate exploitation scores adjusted by selection counts for better balance\n    adjusted_scores = scores + np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1e-5))\n    \n    # Select the action with the highest adjusted score\n    action_index = action_indices[np.argmax(adjusted_scores)]\n    \n    return action_index",
          "objective": -66.08704748352073,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    exploration_factor = 0.1  # Adjust this value for exploration vs exploitation\n    action_values = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_count = len(scores)\n\n        # Compute average score; handle division by zero\n        average_score = np.mean(scores) if selection_count > 0 else 0\n        \n        # Calculate exploration bonus\n        exploration_bonus = exploration_factor * np.sqrt(np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else exploration_factor * np.sqrt(np.log(total_selection_count + 1))\n        \n        # Incorporate a decay based on time slot to focus on recent actions over time\n        time_weighting = (total_time_slots - current_time_slot) / total_time_slots\n        \n        # Combine average score, exploration bonus, and time weighting\n        action_value = (average_score + exploration_bonus) * time_weighting\n        action_values.append(action_value)\n    \n    # Select the action with the highest value\n    action_index = np.argmax(action_values)\n    \n    return action_index",
          "objective": -65.80400863634372,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate average scores for each action\n    avg_scores = {\n        action_index: (np.mean(scores) if scores else 0.0)\n        for action_index, scores in score_set.items()\n    }\n\n    # Epsilon decay strategy based on time-left for exploration\n    exploration_factor = max(0.05, 1 - (current_time_slot / total_time_slots))\n    epsilon = exploration_factor * (0.1 + (current_time_slot / total_time_slots * 0.9))\n\n    # Exploration step\n    if np.random.rand() < epsilon:\n        return np.random.choice(list(score_set.keys()))\n    \n    # Compute UCB-based exploration bonuses\n    exploration_bonuses = {}\n    for action_index in range(8):\n        n_sq = len(score_set.get(action_index, []))\n        if n_sq > 0:\n            bonus = np.sqrt(2 * np.log(total_selection_count) / n_sq)\n        else:\n            bonus = np.inf  # Encourage selection of untried actions\n        exploration_bonuses[action_index] = bonus\n\n    # Combine average scores and exploration bonuses\n    combined_scores = {\n        action_index: avg_scores[action_index] + exploration_bonuses[action_index]\n        for action_index in range(8)\n    }\n\n    # Select action with highest combined score\n    action_index = max(combined_scores, key=combined_scores.get)\n\n    return action_index",
          "objective": -31.698323045177744,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    avg_scores = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        avg_score = np.mean(scores) if scores else 0.0\n        avg_scores.append(avg_score)\n\n    # Dynamic epsilon based on current time slot\n    epsilon = (total_time_slots - current_time_slot) / total_time_slots\n\n    if np.random.rand() < epsilon:  # Exploration\n        action_index = np.random.randint(0, 8)  # Random action\n    else:  # Exploitation\n        action_index = np.argmax(avg_scores)\n    \n    return action_index",
          "objective": -15.406768390220861,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    averages = []\n    exploration = 0.0\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        if scores:\n            avg_score = np.mean(scores)\n        else:\n            avg_score = 0.0\n            \n        # Urgency factor for exploration based on total selections and current time slot\n        if total_selection_count > 0:\n            exploration_factor = (1 - (len(scores) / total_selection_count)) * (1 - (current_time_slot / total_time_slots))\n        else:\n            exploration_factor = 1.0\n        \n        # Calculate a combined score considering both exploitation and exploration\n        combined_score = avg_score + exploration_factor * (1 / (1 + len(scores)))\n        averages.append(combined_score)\n    \n    action_index = np.argmax(averages)\n    return action_index",
          "objective": 44.23221775546449,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    epsilon = max(0.1, 1 - current_time_slot / total_time_slots)  # Exploration probability\n    \n    # Calculate average scores and selection counts\n    average_scores = np.zeros(8)\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        if scores:\n            average_scores[action_index] = np.mean(scores)\n    \n    # Determine whether to exploit or explore\n    if np.random.rand() < epsilon:\n        # Exploration: select a random action\n        action_index = np.random.randint(0, 8)\n    else:\n        # Exploitation: select the action with the highest average score\n        action_index = np.argmax(average_scores)\n    \n    return action_index",
          "objective": 131.06809816305633,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in action_indices])\n    selection_counts = np.array([len(score_set[action]) for action in action_indices])\n\n    # Hypothetical bonus for exploration with a nonlinear decay\n    exploration_bonus = np.log1p(total_selection_count) / (selection_counts + 1)\n\n    # Exponential decay based on current time slot relative to total time slots\n    exploration_decay = np.exp(-current_time_slot / total_time_slots)\n\n    # Combine scores with exploration weights\n    adjusted_scores = scores + (exploration_decay * exploration_bonus)\n\n    # Select action with the highest adjusted score\n    action_index = action_indices[np.argmax(adjusted_scores)]\n\n    return action_index",
          "objective": 195.06689026579988,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize parameters\n    exploration_factor = max(0, (total_time_slots - current_time_slot) / total_time_slots)\n    total_actions = 8\n    \n    # Calculate averages, selection counts, and selection probabilities\n    combined_scores = []\n    for action_index in range(total_actions):\n        scores = score_set.get(action_index, [])\n        avg_score = np.mean(scores) if scores else 0\n        selection_count = len(scores)\n        \n        # Compute exploration value\n        exploration_value = np.sqrt((2 * np.log(total_selection_count + 1)) / (selection_count + 1)) if selection_count > 0 else np.inf\n        combined_score = avg_score + (exploration_factor * exploration_value)\n        \n        combined_scores.append(combined_score)\n    \n    # Select action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": 199.70335324325947,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_indices = list(score_set.keys())\n    num_actions = len(action_indices)\n    \n    # Calculate average scores, handling empty score lists\n    scores = np.array([np.mean(score_set[action]) if score_set[action] else 0 for action in action_indices])\n    \n    # Calculate selection counts and handle zero counts\n    selection_counts = np.array([len(score_set[action]) for action in action_indices])\n    selection_counts[selection_counts == 0] = 1  # Avoid division by zero\n    \n    # Dynamic exploration factor that decreases over time\n    exploration_factor = (total_time_slots - current_time_slot) / total_time_slots\n    \n    # Calculate exploration scores\n    exploration_scores = np.sqrt(np.log(total_selection_count + 1) / selection_counts) * exploration_factor\n\n    # Combine scores based on exploitation and exploration\n    adjusted_scores = scores + exploration_scores\n\n    # Select the action with the highest adjusted score\n    action_index = action_indices[np.argmax(adjusted_scores)]\n\n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    action_count = 8\n    avg_scores = np.zeros(action_count)\n    exploration_coefficients = np.zeros(action_count)\n\n    for action_index in range(action_count):\n        scores = score_set.get(action_index, [])\n        \n        if scores:\n            avg_scores[action_index] = np.mean(scores)\n            n_sq = len(scores) + 1  # Avoid division by zero\n            exploration_coefficients[action_index] = np.sqrt(2 * np.log(total_selection_count + 1) / n_sq)\n        else:\n            exploration_coefficients[action_index] = np.inf  # Encourage exploration for untested actions\n\n    time_factor = (total_time_slots - current_time_slot) / total_time_slots\n    adjusted_scores = avg_scores + exploration_coefficients\n    adjusted_scores *= time_factor\n\n    action_index = np.argmax(adjusted_scores)\n    \n    return action_index",
          "objective": -126.96465256802975,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Calculate exploration probability based on time slot\n    epsilon = max(0.1, 1 - (current_time_slot / total_time_slots))\n    \n    # Random value for exploration vs exploitation decision\n    random_value = np.random.rand()\n    \n    # Calculate average scores and selection counts\n    averages = np.zeros(8)\n    selection_counts = np.zeros(8)\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        selection_counts[action_index] = len(scores)\n        if scores:\n            averages[action_index] = np.mean(scores)\n\n    # Calculate selection probabilities\n    adjusted_scores = averages + (epsilon * (1 - selection_counts / (total_selection_count + 1)))\n    \n    # Select action based on exploration/exploitation\n    if random_value < epsilon:  # Exploration\n        action_index = np.random.choice(np.flatnonzero(selection_counts < total_selection_count / 8)) if np.any(selection_counts < total_selection_count / 8) else np.random.randint(0, 8)\n    else:  # Exploitation\n        action_index = np.argmax(adjusted_scores)\n        \n    return action_index",
          "objective": 217.43883058793642,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    # Initialize parameters\n    exploration_weight = 1.5  # Weight for exploration\n    exploitation_weight = 1.0  # Weight for exploitation\n    decay_factor = 0.9  # Factor to decay exploration value over time\n    averages = []\n    \n    for action_index in range(8):\n        scores = score_set.get(action_index, [])\n        avg_score = np.mean(scores) if scores else 0\n        selection_count = len(scores)\n        \n        # Calculate exploration component\n        exploration_value = (np.log(total_selection_count + 1) / (selection_count + 1)) if selection_count > 0 else np.inf\n        exploration_score = exploration_weight * (exploration_value * (1 - (current_time_slot / total_time_slots))**decay_factor)\n\n        # Combine average score and exploration\n        combined_score = exploitation_weight * avg_score + exploration_score\n        averages.append(combined_score)\n    \n    # Select action with the highest combined score\n    action_index = np.argmax(averages)\n    \n    return action_index",
          "objective": 263.3157106220706,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores and selection counts\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Set exploration parameters\n    epsilon_initial = 1.0\n    epsilon_min = 0.1\n    epsilon_decay = 0.97\n    epsilon = max(epsilon_min, epsilon_initial * (epsilon_decay ** current_time_slot))\n    \n    # Exploration bonus factor based on selection counts\n    explore_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1))\n    \n    # Variance of scores for each action\n    score_variance = np.array([np.var(scores) if len(scores) > 1 else 0.0 for scores in score_set.values()])\n    \n    # Combined scores: encourage exploration and reward variance\n    combined_scores = avg_scores + epsilon * explore_bonus + score_variance\n    \n    # Select action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": [],
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores and selection counts\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Epsilon-greedy exploration rate\n    epsilon_max = 1.0\n    epsilon_min = 0.05\n    epsilon_decay = 0.1\n    epsilon = epsilon_min + (epsilon_max - epsilon_min) * np.exp(-epsilon_decay * current_time_slot / total_time_slots)\n    \n    # Variance-based exploration bonus\n    score_variances = np.array([np.var(scores) if len(scores) > 1 else 0.0 for scores in score_set.values()])\n    explore_bonus = np.sqrt(np.log(total_selection_count + 1) / (selection_counts + 1)) + score_variances\n    \n    # Combined scores: exploitation and exploration balance\n    combined_scores = avg_scores + epsilon * explore_bonus\n\n    # Select action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -311.9467886719003,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n\n    # Calculate average scores and selection counts\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Handle zero selection counts\n    normalized_counts = np.where(selection_counts > 0, selection_counts, 1)\n\n    # Dynamic epsilon decay\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    epsilon_decay = (epsilon_max - epsilon_min) / total_time_slots\n    epsilon = max(epsilon_min, epsilon_max - epsilon_decay * current_time_slot)\n\n    # Upper Confidence Bound (UCB) for exploration\n    confidence_intervals = np.sqrt(2 * np.log(total_selection_count + 1) / normalized_counts)\n    \n    # Improved combined score calculation\n    combined_scores = avg_scores + confidence_intervals\n    \n    # Softmax selection for smoother decision making\n    combined_scores_exp = np.exp(combined_scores - np.max(combined_scores))  # For numerical stability\n    probabilities = combined_scores_exp / np.sum(combined_scores_exp)\n    \n    # Epsilon-greedy selection\n    if np.random.rand() < epsilon:\n        action_index = np.random.choice(num_actions)  # Explore\n    else:\n        action_index = np.random.choice(num_actions, p=probabilities)  # Exploit based on computed probabilities\n    \n    return action_index",
          "objective": -222.34546193801114,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores and selection counts\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n\n    # Epsilon decay parameters\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    epsilon_decay_rate = 0.98\n    epsilon = epsilon_min + (epsilon_max - epsilon_min) * np.exp(-epsilon_decay_rate * (current_time_slot / total_time_slots))\n\n    # Handling selection counts to avoid division by zero\n    explore_bonus = np.where(selection_counts > 0, np.sqrt(np.log(total_selection_count) / selection_counts), np.inf)\n\n    # Combined scores: average scores plus exploration bonus scaled by epsilon\n    combined_scores = avg_scores + epsilon * explore_bonus\n    \n    # Select action with the highest combined score\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -221.21679169805566,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores and selection counts\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Handle zero counts for UCB calculation\n    normalized_counts = np.where(selection_counts > 0, selection_counts, 1)\n\n    # Dynamic epsilon calculation\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    epsilon_decay_rate = (epsilon_max - epsilon_min) / total_time_slots\n    epsilon = max(epsilon_min, epsilon_max - epsilon_decay_rate * current_time_slot)\n\n    # Exploration bonus using Upper Confidence Bound (UCB)\n    uncertainty = np.sqrt(2 * np.log(total_selection_count + 1) / normalized_counts)\n    combined_scores = avg_scores + uncertainty\n    \n    # Apply epsilon greedy strategy\n    if np.random.rand() < epsilon:\n        action_index = np.random.randint(0, num_actions)  # Explore\n    else:\n        action_index = np.argmax(combined_scores)  # Exploit\n\n    return action_index",
          "objective": -220.88164854177512,
          "other_inf": null
     },
     {
          "algorithm": null,
          "code": "import numpy as np\n\ndef selection(score_set, total_selection_count, current_time_slot, total_time_slots):\n    num_actions = len(score_set)\n    \n    # Calculate average scores and handle cases with no selections\n    avg_scores = np.array([np.mean(scores) if scores else 0.0 for scores in score_set.values()])\n    selection_counts = np.array([len(scores) for scores in score_set.values()])\n    \n    # Define epsilon parameters\n    epsilon_max = 1.0\n    epsilon_min = 0.1\n    epsilon_decay_rate = 0.99\n    epsilon = epsilon_min + (epsilon_max - epsilon_min) * (1 - current_time_slot / total_time_slots)\n    \n    # Calculate exploration bonuses (using variance)\n    exploration_bonus = np.where(selection_counts > 0, \n                                  np.sqrt(np.log(total_selection_count) / selection_counts), \n                                  np.inf)\n    variance = np.array([np.var(scores) if scores else 1.0 for scores in score_set.values()])\n    \n    # Combine scores with exploration bonus and variance\n    combined_scores = avg_scores + epsilon * exploration_bonus + variance * 0.1  # Adjust variance contribution\n\n    # Select action based on combined scores\n    action_index = np.argmax(combined_scores)\n    \n    return action_index",
          "objective": -209.61791777342043,
          "other_inf": null
     }
]